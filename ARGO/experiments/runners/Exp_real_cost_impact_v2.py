#!/usr/bin/env python
"""
å®éªŒ1: æ£€ç´¢æˆæœ¬å½±å“ (çœŸå®LLMç‰ˆæœ¬ - ä¿®æ­£ç‰ˆ)
============================================
ä½¿ç”¨çœŸå®çš„Qwenæ¨¡å‹å’ŒåµŒå…¥æ¨¡å‹ï¼Œå¤šGPUå¹¶è¡Œ

ä¿®æ­£å†…å®¹:
1. âœ“ æ·»åŠ Randomç­–ç•¥
2. âœ“ åŸºçº¿ç­–ç•¥ä½¿ç”¨åŠ¨æ€Î¸*ï¼ˆè€Œéç¡¬ç¼–ç 0.9ï¼‰
3. âœ“ æ”¯æŒå°è§„æ¨¡æµ‹è¯•æ¨¡å¼å’Œå¤§è§„æ¨¡å®éªŒæ¨¡å¼åˆ‡æ¢
4. âœ“ å›¾è¡¨å‘½åä¸å®éªŒè®¾è®¡æ–‡æ¡£ä¸€è‡´

ç¡¬ä»¶è¦æ±‚:
- å¤šå¼ GPU (æ”¯æŒ RTX 3060 x8)
- CUDAç¯å¢ƒ

æ¨¡å‹:
- LLM: Qwen2.5-7B-Instruct æˆ– Qwen2.5-14B-Instruct
- Embedding: all-MiniLM-L6-v2
- æ£€ç´¢åº“: Chroma (ORANè§„èŒƒæ–‡æ¡£)
"""

import os
import sys
import torch
import numpy as np
import yaml
import json
import random
import matplotlib.pyplot as plt
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import argparse

# æ·»åŠ srcè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))
from progress import ProgressTracker
from complexity import QuestionComplexityClassifier
from oran_benchmark_loader import ORANBenchmark

# å°è¯•å¯¼å…¥chromadb (å¯èƒ½å¤±è´¥)
try:
    import chromadb
    CHROMADB_AVAILABLE = True
except ImportError as e:
    print(f"âš  ChromaDBä¸å¯ç”¨: {e}")
    print(f"  å°†ä½¿ç”¨æ¨¡æ‹Ÿæ£€ç´¢æ¨¡å¼")
    CHROMADB_AVAILABLE = False
    chromadb = None

# æ·»åŠ è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from oran_benchmark_loader import ORANBenchmark

sys.path.insert(0, '../ARGO_MDP/src')
from mdp_solver import MDPSolver


class RealCostImpactExperiment:
    """å®éªŒ1: æ£€ç´¢æˆæœ¬å½±å“ - çœŸå®LLMç‰ˆæœ¬ (ä¿®æ­£ç‰ˆ)"""
    
    def __init__(
        self,
        config_path: str = "configs/multi_gpu_data_calibrated.yaml",
        policy_config_path: Optional[str] = None,
        llm_model_path: str = "/data/user/huangxiaolin/ARGO/RAG_Models/models/Qwen2.5-14B-Instruct",
        embedding_model_path: str = "/data/user/huangxiaolin/ARGO/models/all-MiniLM-L6-v2",
        chroma_db_path: str = "/data/user/huangxiaolin/ARGO2/ARGO/Environments/chroma_store",
        test_mode: str = "small",  # "small" (å¿«é€Ÿæµ‹è¯•), "full" (å®Œæ•´å®éªŒ), æˆ– "custom" (è‡ªå®šä¹‰)
        n_test_questions: Optional[int] = None,  # è‡ªå®šä¹‰é—®é¢˜æ•°é‡ï¼ˆä»…ç”¨äºcustomæ¨¡å¼ï¼‰
        difficulty: str = "hard",
        seed: int = 42,
        gpu_ids: List[int] = None
    ):
        """
        Args:
            config_path: MDPé…ç½®æ–‡ä»¶è·¯å¾„
            policy_config_path: è‡ªé€‚åº”ç­–ç•¥é…ç½®æ–‡ä»¶è·¯å¾„
            llm_model_path: Qwenæ¨¡å‹æœ¬åœ°è·¯å¾„
            embedding_model_path: åµŒå…¥æ¨¡å‹æœ¬åœ°è·¯å¾„
            chroma_db_path: Chromaæ•°æ®åº“è·¯å¾„
            test_mode: "small" (10é¢˜, 5ä¸ªc_rç‚¹), "full" (å…¨éƒ¨~12Ké¢˜, 10ä¸ªc_rç‚¹), æˆ– "custom" (è‡ªå®šä¹‰)
            n_test_questions: è‡ªå®šä¹‰é—®é¢˜æ•°é‡ (ä»…å½“test_mode="custom"æ—¶ä½¿ç”¨)
            difficulty: é—®é¢˜éš¾åº¦ ("easy", "medium", "hard")
            seed: éšæœºç§å­
            gpu_ids: ä½¿ç”¨çš„GPU IDåˆ—è¡¨ï¼Œå¦‚ [0,1,2,3]
        """
        self.policy_config_path = policy_config_path
        # æ ¹æ®æµ‹è¯•æ¨¡å¼è®¾ç½®å‚æ•°
        if test_mode == "small":
            self.n_test_questions = 10
            self.n_cost_steps = 5
            self.mode_desc = "å°è§„æ¨¡æµ‹è¯•æ¨¡å¼ (å¿«é€ŸéªŒè¯)"
        elif test_mode == "full":
            self.n_test_questions = None  # ä½¿ç”¨å…¨éƒ¨æ•°æ®é›†
            self.n_cost_steps = 10
            self.mode_desc = "å®Œæ•´å®éªŒæ¨¡å¼ (å…¨éƒ¨æ•°æ®)"
        elif test_mode == "custom":
            if n_test_questions is None:
                raise ValueError("customæ¨¡å¼å¿…é¡»æŒ‡å®š n_test_questions å‚æ•°")
            self.n_test_questions = n_test_questions
            self.n_cost_steps = 10
            self.mode_desc = f"è‡ªå®šä¹‰æ¨¡å¼ ({n_test_questions}é¢˜)"
        else:
            raise ValueError(f"test_modeå¿…é¡»æ˜¯'small', 'full', æˆ– 'custom'ï¼Œå½“å‰å€¼: {test_mode}")
        
        self.test_mode = test_mode
        
        print(f"\n{'='*80}")
        print(f"å®éªŒ1: æ£€ç´¢æˆæœ¬å½±å“ (çœŸå®LLMç‰ˆæœ¬ - ä¿®æ­£ç‰ˆ)")
        print(f"{'='*80}")
        print(f"è¿è¡Œæ¨¡å¼: {self.mode_desc}")
        print(f"LLMæ¨¡å‹: {llm_model_path}")
        print(f"åµŒå…¥æ¨¡å‹: {embedding_model_path}")
        print(f"é—®é¢˜éš¾åº¦: {difficulty.upper()}")
        print(f"é—®é¢˜æ•°é‡: {self.n_test_questions if self.n_test_questions else 'å…¨éƒ¨ (~12K)'}")
        print(f"c_ré‡‡æ ·ç‚¹: {self.n_cost_steps}ä¸ª")
        print(f"{'='*80}\n")
        
        self.config_path = config_path
        self.llm_model_path = llm_model_path
        self.embedding_model_path = embedding_model_path
        self.chroma_db_path = chroma_db_path
        self.difficulty = difficulty
        self.seed = seed
        
        # GPUé…ç½®
        if not torch.cuda.is_available():
            raise RuntimeError("éœ€è¦GPU!")
        
        self.n_gpus = torch.cuda.device_count()
        self.gpu_ids = gpu_ids if gpu_ids else list(range(min(4, self.n_gpus)))
        
        print(f"GPUé…ç½®:")
        print(f"  å¯ç”¨GPU: {self.n_gpus}å¼ ")
        print(f"  ä½¿ç”¨GPU: {self.gpu_ids}")
        for i in self.gpu_ids:
            name = torch.cuda.get_device_name(i)
            mem = torch.cuda.get_device_properties(i).total_memory / 1e9
            print(f"    GPU {i}: {name} ({mem:.1f}GB)")
        print()
        
        # åŠ è½½é…ç½®
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
            
        # åŠ è½½ç­–ç•¥é…ç½®
        self.policy_config = None
        if self.policy_config_path:
            print(f"åŠ è½½è‡ªé€‚åº”ç­–ç•¥é…ç½®: {self.policy_config_path}")
            with open(self.policy_config_path, 'r') as f:
                self.policy_config = yaml.safe_load(f)
        
        # è®¾ç½®éšæœºç§å­
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        
        # åŠ è½½æ•°æ®é›†
        print("åŠ è½½ORAN-Bench-13Kæ•°æ®é›†...")
        self.benchmark = ORANBenchmark()
        
        if self.n_test_questions:
            self.test_questions = self.benchmark.sample_questions(
                n=self.n_test_questions,
                difficulty=difficulty,
                seed=seed
            )
        else:
            # ä½¿ç”¨å…¨éƒ¨æµ‹è¯•é›†ï¼ˆä¼ å…¥è¶…å¤§æ•°å­—ï¼Œsample_questionsä¼šè‡ªåŠ¨é™åˆ¶ä¸ºå®é™…æ•°é‡ï¼‰
            # ä»statsä¸­è·å–è¯¥éš¾åº¦çš„æ€»é¢˜æ•°
            total_count = self.benchmark.stats[difficulty]
            self.test_questions = self.benchmark.sample_questions(
                n=total_count,
                difficulty=difficulty,
                seed=seed
            )
        
        print(f"âœ“ åŠ è½½äº† {len(self.test_questions)} é“ {difficulty.upper()} é—®é¢˜\n")
        
        # åŠ è½½åµŒå…¥æ¨¡å‹
        print(f"åŠ è½½åµŒå…¥æ¨¡å‹: {embedding_model_path}")
        self.embedding_model = SentenceTransformer(embedding_model_path)
        self.embedding_model = self.embedding_model.to(f'cuda:{self.gpu_ids[0]}')
        print(f"âœ“ åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ (GPU {self.gpu_ids[0]})\n")
        
        # åˆå§‹åŒ–è‡ªé€‚åº”ç»„ä»¶
        if self.policy_config:
            print("åˆå§‹åŒ–è‡ªé€‚åº”ç»„ä»¶ (ComplexityClassifier)...")
            # ProgressTracker å°†åœ¨æ¯ä¸ªé—®é¢˜ä¸­å®ä¾‹åŒ–
            self.classifier = QuestionComplexityClassifier()
            print("âœ“ è‡ªé€‚åº”ç»„ä»¶å·²å°±ç»ª")
        else:
            self.classifier = None

        # ğŸ†• é¢„è®¡ç®—æ‰€æœ‰é—®é¢˜çš„embeddings (ä¼˜åŒ–æ£€ç´¢é€Ÿåº¦)
        print(f"{'='*80}")
        print(f"é¢„è®¡ç®—é—®é¢˜embeddings (ä¼˜åŒ–æ£€ç´¢æ€§èƒ½)...")
        print(f"{'='*80}")
        self.query_embeddings = {}
        
        import time
        start_time = time.time()
        
        for idx, q in enumerate(self.test_questions):
            question_text = q['question']
            
            # é¿å…é‡å¤è®¡ç®—ï¼ˆè™½ç„¶ç†è®ºä¸Šä¸ä¼šæœ‰é‡å¤ï¼‰
            if question_text not in self.query_embeddings:
                # ç›´æ¥è¿”å›numpyæ•°ç»„ï¼Œé¿å…GPUè½¬æ¢
                embedding = self.embedding_model.encode(
                    question_text, 
                    convert_to_tensor=False,
                    show_progress_bar=False
                )
                self.query_embeddings[question_text] = embedding.tolist()
            
            # æ¯100ä¸ªæ‰“å°ä¸€æ¬¡è¿›åº¦
            if (idx + 1) % 100 == 0:
                elapsed = time.time() - start_time
                avg_time = elapsed / (idx + 1)
                remaining = avg_time * (len(self.test_questions) - idx - 1)
                print(f"  è¿›åº¦: {idx+1}/{len(self.test_questions)} "
                      f"({(idx+1)/len(self.test_questions)*100:.1f}%) - "
                      f"é¢„è®¡å‰©ä½™: {remaining:.0f}ç§’")
        
        elapsed = time.time() - start_time
        print(f"\nâœ“ é¢„è®¡ç®—å®Œæˆ!")
        print(f"  - é—®é¢˜æ•°: {len(self.query_embeddings)}")
        print(f"  - è€—æ—¶: {elapsed:.1f}ç§’ ({elapsed/60:.1f}åˆ†é’Ÿ)")
        
        if len(self.query_embeddings) > 0:
            print(f"  - å¹³å‡: {elapsed/len(self.query_embeddings)*1000:.1f}ms/é—®é¢˜")
            print(f"  - å†…å­˜å ç”¨: ~{len(self.query_embeddings) * 384 * 4 / 1024 / 1024:.2f} MB")
        else:
            print(f"  âš ï¸  è­¦å‘Š: æ²¡æœ‰é¢„è®¡ç®—ä»»ä½•embeddings")
        
        print(f"{'='*80}\n")
        
        # åŠ è½½Chromaæ£€ç´¢åº“
        print(f"è¿æ¥Chromaæ•°æ®åº“: {chroma_db_path}")
        if CHROMADB_AVAILABLE:
            try:
                self.chroma_client = chromadb.PersistentClient(path=chroma_db_path)
                self.collection = self.chroma_client.get_collection("oran_specs")
                print(f"âœ“ Chromaé›†åˆåŠ è½½æˆåŠŸ (æ–‡æ¡£æ•°: {self.collection.count()})\n")
            except Exception as e:
                print(f"âš  Chromaé›†åˆåŠ è½½å¤±è´¥: {e}")
                print(f"  å°†ä½¿ç”¨æ¨¡æ‹Ÿæ£€ç´¢æ¨¡å¼\n")
                self.collection = None
        else:
            print(f"âš  ChromaDBä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ£€ç´¢æ¨¡å¼\n")
            self.collection = None
        
        # åŠ è½½LLMæ¨¡å‹
        print(f"åŠ è½½LLMæ¨¡å‹: {llm_model_path}")
        self._load_llm()
        
        print(f"\n{'='*80}")
        print(f"åˆå§‹åŒ–å®Œæˆ!")
        print(f"{'='*80}\n")
    
    def _load_llm(self):
        """åŠ è½½LLM (å¤šGPUå¹¶è¡Œ)"""
        print(f"  ä½¿ç”¨ {len(self.gpu_ids)} å¼ GPUåŠ è½½æ¨¡å‹...")
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.llm_model_path,
            trust_remote_code=True,
            padding_side='left'
        )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # ä½¿ç”¨Accelerateè‡ªåŠ¨åˆ†é…åˆ°å¤šä¸ªGPU
        max_memory = {i: "10GB" for i in self.gpu_ids}
        max_memory["cpu"] = "30GB"
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.llm_model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            max_memory=max_memory,
            trust_remote_code=True,
            offload_folder="offload"
        )
        
        self.model.eval()
        
        print(f"âœ“ LLMæ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"  Device map: {self.model.hf_device_map}")
    
    def create_mdp_config(self, c_r: float) -> Dict:
        """åˆ›å»ºMDPé…ç½®"""
        mdp_config = self.config['mdp'].copy()
        mdp_config['c_r'] = c_r
        
        # æ·»åŠ  U_grid_size (å…¼å®¹æ€§)
        if 'U_grid_size' not in mdp_config and 'grid_size' in mdp_config:
            mdp_config['U_grid_size'] = mdp_config['grid_size']
        
        # åŠ è½½è‡ªé€‚åº”ç­–ç•¥é…ç½®
        policy_config = None
        if self.policy_config_path and os.path.exists(self.policy_config_path):
            with open(self.policy_config_path, 'r') as f:
                policy_config = yaml.safe_load(f)
                # Extract the 'policy' section
                policy_config = policy_config.get('policy', policy_config)
        
        return {
            'mdp': mdp_config,
            'policy': policy_config,
            'quality': self.config.get('quality', {'mode': 'linear', 'k': 5.0}),
            'solver': {
                'max_iterations': 1000,
                'convergence_threshold': 1e-6,
                'verbose': False
            }
        }
    
    def solve_mdp(self, c_r: float) -> tuple:
        """æ±‚è§£MDPè·å–é˜ˆå€¼"""
        print(f"  æ±‚è§£MDP (c_r={c_r:.3f})...", end=" ")
        
        config = self.create_mdp_config(c_r)
        solver = MDPSolver(config)
        solver.solve()
        
        theta_cont = solver.theta_cont
        theta_star = solver.theta_star
        
        print(f"Î¸_cont={theta_cont:.3f}, Î¸*={theta_star:.3f}")
        return theta_cont, theta_star
    
    def retrieve_documents(self, question: str, top_k: int = 3) -> List[str]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£ (ä½¿ç”¨é¢„è®¡ç®—çš„embeddings)"""
        if self.collection is None:
            # æ¨¡æ‹Ÿæ£€ç´¢(å¦‚æœChromaä¸å¯ç”¨)
            return [f"æ¨¡æ‹Ÿæ–‡æ¡£ {i+1}: O-RAN specification content related to the query." for i in range(top_k)]
        
        # âœ… ä½¿ç”¨é¢„è®¡ç®—çš„embedding (é¿å…é‡å¤ç¼–ç )
        query_embedding = self.query_embeddings.get(question)
        
        # å¦‚æœç¼“å­˜ä¸­æ²¡æœ‰ï¼ˆç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼‰ï¼Œæ‰ä¸´æ—¶è®¡ç®—
        if query_embedding is None:
            print(f"âš ï¸  ç¼“å­˜æœªå‘½ä¸­ï¼Œä¸´æ—¶è®¡ç®—embedding: {question[:60]}...")
            query_embedding = self.embedding_model.encode(
                question, 
                convert_to_tensor=False,
                show_progress_bar=False
            ).tolist()
        
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k,
            include=["documents"]
        )
        
        documents = results.get("documents", [[]])[0]
        return documents
    
    def generate_answer(self, question: Dict, context: str = "") -> tuple:
        """ä½¿ç”¨LLMç”Ÿæˆç­”æ¡ˆ
        
        Returns:
            (answer_index, confidence, response_text)
        """
        prompt = self._create_prompt(question, context)
        
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=2048
        )
        
        # ç§»åŠ¨åˆ°ç¬¬ä¸€ä¸ªGPU (Accelerateä¼šè‡ªåŠ¨å¤„ç†åç»­çš„åˆ†å¸ƒ)
        inputs = {k: v.to(f'cuda:{self.gpu_ids[0]}') for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=10,
                temperature=0.1,
                do_sample=False,
                pad_token_id=self.tokenizer.pad_token_id
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        answer = self._extract_answer(response)
        
        # ç®€å•çš„ç½®ä¿¡åº¦ä¼°è®¡
        confidence = 0.7 if context else 0.5
        
        return answer, confidence, response
    
    def _create_prompt(self, question: Dict, context: Optional[str] = None) -> str:
        """åˆ›å»ºæç¤ºè¯"""
        context_part = f"\n\nContext:\n{context}\n" if context else ""
        
        # å¤„ç†å¼‚å¸¸æƒ…å†µï¼šç¡®ä¿æœ‰4ä¸ªé€‰é¡¹
        options = question.get('options', [])
        if len(options) < 4:
            print(f"âš ï¸  é—®é¢˜é€‰é¡¹æ•°å¼‚å¸¸: {len(options)}ä¸ª - {question['question'][:60]}...")
            # è·³è¿‡æ­¤é—®é¢˜æˆ–å¡«å……é»˜è®¤é€‰é¡¹
            options = options + ['N/A'] * (4 - len(options))
        elif len(options) > 4:
            print(f"âš ï¸  é—®é¢˜é€‰é¡¹æ•°å¼‚å¸¸: {len(options)}ä¸ª - {question['question'][:60]}...")
            options = options[:4]  # åªå–å‰4ä¸ª
        
        prompt = f"""You are an O-RAN standards expert. Answer the following question.{context_part}
Question: {question['question']}

Options:
1. {options[0]}
2. {options[1]}
3. {options[2]}
4. {options[3]}

Answer with only the number (1, 2, 3, or 4):"""
        
        return prompt
    
    def _extract_answer(self, response: str) -> int:
        """ä»å“åº”ä¸­æå–ç­”æ¡ˆ"""
        import re
        
        response = response.lower()
        matches = re.findall(r'\b([1-4])\b', response)
        
        if matches:
            return int(matches[-1])
        
        return 1  # é»˜è®¤
    
    def simulate_argo_policy(self, question: Dict, theta_cont: float, theta_star: float, c_r: float) -> Dict:
        """æ‰§è¡ŒARGOç­–ç•¥"""
        # å¦‚æœå¯ç”¨äº†è‡ªé€‚åº”ç­–ç•¥é…ç½®ï¼Œä½¿ç”¨æ–°é€»è¾‘
        if self.policy_config:
            return self._simulate_adaptive_policy(question, c_r)

        U = 0.0
        C = 0.0
        retrieval_count = 0
        reason_count = 0
        
        delta_r = self.config['mdp']['delta_r']
        delta_p = self.config['mdp']['delta_p']
        c_p = self.config['mdp']['c_p']
        p_s = self.config['mdp']['p_s']
        
        max_steps = 20
        final_answer = None
        
        for step in range(max_steps):
            if U >= theta_star:
                break
            
            if U < theta_cont:
                # Retrieve
                retrieval_count += 1
                C += c_r
                
                # çœŸå®æ£€ç´¢
                docs = self.retrieve_documents(question['question'], top_k=3)
                context = " ".join(docs)
                
                # ç”¨æ£€ç´¢æˆåŠŸç‡æ¨¡æ‹Ÿ
                if random.random() < p_s:
                    U += delta_r
                    final_answer, _, _ = self.generate_answer(question, context)
                else:
                    final_answer, _, _ = self.generate_answer(question, context)
            else:
                # Reason
                reason_count += 1
                C += c_p
                U += delta_p
                
                # æ— æ£€ç´¢æ¨ç†
                final_answer, _, _ = self.generate_answer(question, "")
        
        # æœ€ç»ˆè´¨é‡
        quality = min(U / 1.0, 1.0)
        
        correct = (final_answer == question['correct_answer']) if final_answer else False
        
        return {
            'quality': quality,
            'cost': C,
            'retrieval_count': retrieval_count,
            'reason_count': reason_count,
            'steps': step + 1,
            'correct': correct
        }
    
    def _simulate_adaptive_policy(self, question: Dict, c_r: float) -> Dict:
        """ä½¿ç”¨è‡ªé€‚åº”ç­–ç•¥é…ç½®æ‰§è¡Œ (ProgressTracker + ComplexityClassifier)"""
        q_text = question['question']
        
        # 1. åˆ†ç±»å¤æ‚åº¦
        complexity = self.classifier.classify(q_text)
        policy_params = self.policy_config['policy'][complexity]
        
        theta_star = policy_params['theta_star']
        theta_cont = policy_params['theta_cont']
        max_steps = policy_params['max_steps']
        
        # 2. åˆå§‹åŒ–çŠ¶æ€
        tracker = ProgressTracker(q_text)
        U = 0.0
        C = 0.0
        retrieval_count = 0
        reason_count = 0
        accumulated_context = ""
        current_answer = ""
        
        c_p = self.config['mdp']['c_p']
        
        # 3. é€æ­¥æ‰§è¡Œ
        final_step = 0
        for step in range(max_steps):
            final_step = step + 1
            
            # æ£€æŸ¥ç»ˆæ­¢
            if U >= theta_star:
                break
                
            # å†³ç­–
            if U < theta_cont:
                # Action: Retrieve
                retrieval_count += 1
                C += c_r
                
                # æ£€ç´¢
                new_docs = self.retrieve_documents(q_text, top_k=3)
                new_context = " ".join(new_docs)
                # ç®€å•æ‹¼æ¥ï¼Œå®é™…åº”ç”¨å¯èƒ½éœ€è¦å»é‡æˆ–æ‘˜è¦
                accumulated_context = (accumulated_context + " " + new_context).strip()
                
                # ç”Ÿæˆç­”æ¡ˆ
                ans_idx, _, ans_text = self.generate_answer(question, accumulated_context)
                current_answer = ans_text
                
                # æ›´æ–°è¿›åº¦
                step_data = {
                    'intermediate_answer': current_answer,
                    'retrieved_docs': new_docs,
                    'confidence': 0.6
                }
                U = tracker.update('retrieve', step_data)
                
            else:
                # Action: Reason
                reason_count += 1
                C += c_p
                
                # æ¨ç† (ä½¿ç”¨å·²æœ‰ä¸Šä¸‹æ–‡)
                ans_idx, _, ans_text = self.generate_answer(question, accumulated_context)
                current_answer = ans_text
                
                # æ›´æ–°è¿›åº¦
                step_data = {
                    'intermediate_answer': current_answer,
                    'confidence': 0.7
                }
                U = tracker.update('reason', step_data)
        
        # æœ€ç»ˆç»“æœ
        final_ans_idx = self._extract_answer(current_answer)
        correct = (final_ans_idx == question['correct_answer'])
        
        return {
            'quality': min(U, 1.0),
            'cost': C,
            'retrieval_count': retrieval_count,
            'reason_count': reason_count,
            'steps': final_step,
            'correct': correct,
            'complexity': complexity
        }
    
    def simulate_always_retrieve_policy(self, question: Dict, c_r: float, theta_star: float) -> Dict:
        """Always-RetrieveåŸºçº¿ (ä¿®æ­£: ä½¿ç”¨åŠ¨æ€Î¸*)"""
        U = 0.0
        C = 0.0
        retrieval_count = 0
        
        delta_r = self.config['mdp']['delta_r']
        p_s = self.config['mdp']['p_s']
        
        max_steps = 20
        final_answer = None
        
        for step in range(max_steps):
            if U >= theta_star:  # â† ä½¿ç”¨ä¼ å…¥çš„theta_star
                break
            
            retrieval_count += 1
            C += c_r
            
            docs = self.retrieve_documents(question['question'], top_k=3)
            context = " ".join(docs)
            
            if random.random() < p_s:
                U += delta_r
            
            final_answer, _, _ = self.generate_answer(question, context)
        
        quality = min(U / 1.0, 1.0)
        correct = (final_answer == question['correct_answer']) if final_answer else False
        
        return {
            'quality': quality,
            'cost': C,
            'retrieval_count': retrieval_count,
            'reason_count': 0,
            'steps': step + 1,
            'correct': correct
        }
    
    def simulate_always_reason_policy(self, question: Dict, theta_star: float) -> Dict:
        """Always-ReasonåŸºçº¿ (ä¿®æ­£: ä½¿ç”¨åŠ¨æ€Î¸*)"""
        U = 0.0
        C = 0.0
        reason_count = 0
        
        delta_p = self.config['mdp']['delta_p']
        c_p = self.config['mdp']['c_p']
        
        max_steps = 20
        final_answer = None
        
        for step in range(max_steps):
            if U >= theta_star:  # â† ä½¿ç”¨ä¼ å…¥çš„theta_star
                break
            
            reason_count += 1
            C += c_p
            U += delta_p
            
            final_answer, _, _ = self.generate_answer(question, "")
        
        quality = min(U / 1.0, 1.0)
        correct = (final_answer == question['correct_answer']) if final_answer else False
        
        return {
            'quality': quality,
            'cost': C,
            'retrieval_count': 0,
            'reason_count': reason_count,
            'steps': step + 1,
            'correct': correct
        }
    
    def simulate_random_policy(self, question: Dict, c_r: float, theta_star: float) -> Dict:
        """RandomåŸºçº¿: éšæœºé€‰æ‹©Retrieveæˆ–Reason (æ–°å¢)"""
        U = 0.0
        C = 0.0
        retrieval_count = 0
        reason_count = 0
        
        delta_r = self.config['mdp']['delta_r']
        delta_p = self.config['mdp']['delta_p']
        c_p = self.config['mdp']['c_p']
        p_s = self.config['mdp']['p_s']
        
        max_steps = 20
        final_answer = None
        
        for step in range(max_steps):
            if U >= theta_star:  # â† ä½¿ç”¨ä¼ å…¥çš„theta_star
                break
            
            # éšæœºé€‰æ‹©åŠ¨ä½œ (50% Retrieve, 50% Reason)
            if random.random() < 0.5:
                # Retrieve
                retrieval_count += 1
                C += c_r
                docs = self.retrieve_documents(question['question'], top_k=3)
                context = " ".join(docs)
                if random.random() < p_s:
                    U += delta_r
                final_answer, _, _ = self.generate_answer(question, context)
            else:
                # Reason
                reason_count += 1
                C += c_p
                U += delta_p
                final_answer, _, _ = self.generate_answer(question, "")
        
        quality = min(U / 1.0, 1.0)
        correct = (final_answer == question['correct_answer']) if final_answer else False
        
        return {
            'quality': quality,
            'cost': C,
            'retrieval_count': retrieval_count,
            'reason_count': reason_count,
            'steps': step + 1,
            'correct': correct
        }
    
    def evaluate_all_policies(self, c_r: float, theta_cont: float, theta_star: float) -> Dict:
        """è¯„ä¼°æ‰€æœ‰ç­–ç•¥ (ä¿®æ­£: æ·»åŠ Randomï¼Œä¼ å…¥Î¸*)"""
        results = {
            'ARGO': [],
            'Always-Retrieve': [],
            'Always-Reason': [],
            'Random': []  # â† æ–°å¢Randomç­–ç•¥
        }
        
        print(f"\n  è¯„ä¼° {len(self.test_questions)} é“é—®é¢˜...")
        
        for i, question in enumerate(self.test_questions, 1):
            if i % 10 == 0:
                print(f"    è¿›åº¦: {i}/{len(self.test_questions)}")
            
            # ARGO
            result = self.simulate_argo_policy(question, theta_cont, theta_star, c_r)
            results['ARGO'].append(result)
            
            # Always-Retrieve (ä¼ å…¥theta_star)
            result = self.simulate_always_retrieve_policy(question, c_r, theta_star)
            results['Always-Retrieve'].append(result)
            
            # Always-Reason (ä¼ å…¥theta_star)
            result = self.simulate_always_reason_policy(question, theta_star)
            results['Always-Reason'].append(result)
            
            # Random (ä¼ å…¥theta_star)
            result = self.simulate_random_policy(question, c_r, theta_star)
            results['Random'].append(result)
        
        return results
    
    def run_experiment(
        self,
        c_r_min_multiplier: float = 1.0,
        c_r_max_multiplier: float = 10.0
    ):
        """è¿è¡Œå®éªŒ"""
        c_p = self.config['mdp']['c_p']
        c_r_values = np.linspace(
            c_r_min_multiplier * c_p,
            c_r_max_multiplier * c_p,
            self.n_cost_steps  # â† ä½¿ç”¨æ ¹æ®test_modeè®¾å®šçš„æ­¥æ•°
        )
        
        print(f"\n{'='*80}")
        print(f"å¼€å§‹å®éªŒ - æ£€ç´¢æˆæœ¬å½±å“")
        print(f"{'='*80}")
        print(f"è¿è¡Œæ¨¡å¼: {self.mode_desc}")
        print(f"c_rèŒƒå›´: {c_r_values[0]:.3f} ~ {c_r_values[-1]:.3f} (æ‰«æ {self.n_cost_steps} ä¸ªç‚¹)")
        print(f"c_på›ºå®š: {c_p:.3f}")
        print(f"é—®é¢˜æ•°é‡: {len(self.test_questions)}")
        print(f"ç­–ç•¥æ•°é‡: 4 (ARGO, Always-Retrieve, Always-Reason, Random)")
        print(f"æ€»è¯„ä¼°æ¬¡æ•°: {self.n_cost_steps} Ã— 4ç­–ç•¥ Ã— {len(self.test_questions)}é¢˜ = {self.n_cost_steps * 4 * len(self.test_questions)}")
        print(f"{'='*80}\n")
        
        all_results = []
        self.raw_results = []  # ğŸ†• åˆå§‹åŒ–è¯¦ç»†ç»“æœåˆ—è¡¨
        
        for i, c_r in enumerate(c_r_values, 1):
            print(f"\n[{i}/{self.n_cost_steps}] c_r = {c_r:.4f} ({c_r/c_p:.1f}x c_p)")
            print(f"{'-'*80}")
            
            # æ±‚è§£MDP
            theta_cont, theta_star = self.solve_mdp(c_r)
            
            # è¯„ä¼°æ‰€æœ‰ç­–ç•¥
            results = self.evaluate_all_policies(c_r, theta_cont, theta_star)
            
            # ğŸ†• ä¿å­˜è¯¦ç»†ç»“æœ
            self.raw_results.append({
                'c_r': c_r,
                'details': results
            })
            
            # èšåˆç»“æœ
            aggregated = {
                'c_r': c_r,
                'theta_cont': theta_cont,
                'theta_star': theta_star
            }
            
            for policy_name, policy_results in results.items():
                avg_quality = np.mean([r['quality'] for r in policy_results])
                avg_cost = np.mean([r['cost'] for r in policy_results])
                avg_retrievals = np.mean([r['retrieval_count'] for r in policy_results])
                avg_reasons = np.mean([r['reason_count'] for r in policy_results])
                accuracy = np.mean([r['correct'] for r in policy_results])
                
                aggregated[f'{policy_name}_quality'] = avg_quality
                aggregated[f'{policy_name}_cost'] = avg_cost
                aggregated[f'{policy_name}_retrievals'] = avg_retrievals
                aggregated[f'{policy_name}_reasons'] = avg_reasons
                aggregated[f'{policy_name}_accuracy'] = accuracy
                
                print(f"  {policy_name:20s}: Accuracy={accuracy:.1%}, Quality={avg_quality:.3f}, "
                      f"Cost={avg_cost:.3f}, Retrievals={avg_retrievals:.1f}")
            
            all_results.append(aggregated)
        
        self.results = all_results
        
        print(f"\n{'='*80}")
        print(f"å®éªŒå®Œæˆ!")
        print(f"{'='*80}\n")
        
        return all_results
    
    def save_results(self, output_dir: str = "draw_figs/data"):
        """ä¿å­˜ç»“æœ"""
        os.makedirs(output_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # æ ¹æ®test_modeé€‰æ‹©æ–‡ä»¶ååç¼€
        if self.test_mode == "small":
            mode_suffix = "small"
        elif self.test_mode == "full":
            mode_suffix = "full"
        else:  # custom
            mode_suffix = "custom"
        
        filename = f"exp1_real_cost_impact_{mode_suffix}_{timestamp}.json"
        filepath = os.path.join(output_dir, filename)
        
        # ä¿å­˜å®Œæ•´ç»“æœ + å…ƒæ•°æ®
        output_data = {
            'metadata': {
                'test_mode': self.test_mode,
                'n_questions': len(self.test_questions),
                'difficulty': self.difficulty,
                'n_cost_steps': self.n_cost_steps,
                'seed': self.seed,  # â† æ·»åŠ seedåˆ°å…ƒæ•°æ®
                'timestamp': timestamp
            },
            'results': self.results,
            'raw_results': self.raw_results
        }
        
        with open(filepath, 'w') as f:
            json.dump(output_data, f, indent=2)
        
        print(f"âœ“ ç»“æœå·²ä¿å­˜: {filepath}")
        return filepath
    
    def plot_results(self, output_dir: str = "figs"):
        """ç»˜åˆ¶ç»“æœ (ä¿®æ­£: æŒ‰å®éªŒè®¾è®¡æ–‡æ¡£è¦æ±‚ç»˜åˆ¶2å¼ å›¾)"""
        os.makedirs(output_dir, exist_ok=True)
        
        c_r_values = [r['c_r'] for r in self.results]
        mode_suffix = "small" if self.test_mode == "small" else "full"
        
        # ====================================================================
        # å›¾1.A: Cost vs. Accuracy (æŒ‰å®éªŒè®¾è®¡æ–‡æ¡£è¦æ±‚)
        # ====================================================================
        plt.figure(figsize=(10, 6))
        
        policies = ['ARGO', 'Always-Retrieve', 'Always-Reason', 'Random']
        colors = ['#2E86AB', '#A23B72', '#F18F01', '#6A994E']
        markers = ['o', 's', '^', 'D']
        
        for policy, color, marker in zip(policies, colors, markers):
            accuracy = [r[f'{policy}_accuracy'] for r in self.results]
            plt.plot(c_r_values, accuracy, marker=marker, label=policy, 
                    linewidth=2.5, markersize=8, color=color, alpha=0.8)
        
        plt.xlabel('Retrieval Cost ($c_r$)', fontsize=13, fontweight='bold')
        plt.ylabel('Average Accuracy', fontsize=13, fontweight='bold')
        plt.title('Graph 1.A: Cost vs. Accuracy', fontsize=15, fontweight='bold', pad=15)
        plt.legend(fontsize=11, loc='best', framealpha=0.9)
        plt.grid(True, alpha=0.3, linestyle='--')
        plt.tight_layout()
        
        fig1_path = os.path.join(output_dir, f'exp1_graph1A_cost_vs_accuracy_{mode_suffix}.png')
        plt.savefig(fig1_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ“ å›¾è¡¨å·²ä¿å­˜: {fig1_path}")
        
        # ====================================================================
        # å›¾1.B: Cost vs. Retrieval Calls (æŒ‰å®éªŒè®¾è®¡æ–‡æ¡£è¦æ±‚)
        # ====================================================================
        plt.figure(figsize=(10, 6))
        
        # åªç»˜åˆ¶æœ‰æ£€ç´¢è¡Œä¸ºçš„ç­–ç•¥ (Always-Reasonä¸æ£€ç´¢ï¼Œæ‰€ä»¥ä¸ç”»)
        retrieval_policies = ['ARGO', 'Always-Retrieve', 'Random']
        retrieval_colors = ['#2E86AB', '#A23B72', '#6A994E']
        retrieval_markers = ['o', 's', 'D']
        
        for policy, color, marker in zip(retrieval_policies, retrieval_colors, retrieval_markers):
            retrievals = [r[f'{policy}_retrievals'] for r in self.results]
            plt.plot(c_r_values, retrievals, marker=marker, label=policy, 
                    linewidth=2.5, markersize=8, color=color, alpha=0.8)
        
        plt.xlabel('Retrieval Cost ($c_r$)', fontsize=13, fontweight='bold')
        plt.ylabel('Average Retrieval Calls ($E[R_T]$)', fontsize=13, fontweight='bold')
        plt.title('Graph 1.B: Cost vs. Retrieval Calls', fontsize=15, fontweight='bold', pad=15)
        plt.legend(fontsize=11, loc='best', framealpha=0.9)
        plt.grid(True, alpha=0.3, linestyle='--')
        plt.tight_layout()
        
        fig2_path = os.path.join(output_dir, f'exp1_graph1B_cost_vs_retrievals_{mode_suffix}.png')
        plt.savefig(fig2_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ“ å›¾è¡¨å·²ä¿å­˜: {fig2_path}")
        
        # ====================================================================
        # é¢å¤–å›¾: Cost vs. Total Cost (è¡¥å……åˆ†æ)
        # ====================================================================
        plt.figure(figsize=(10, 6))
        
        for policy, color, marker in zip(policies, colors, markers):
            total_cost = [r[f'{policy}_cost'] for r in self.results]
            plt.plot(c_r_values, total_cost, marker=marker, label=policy, 
                    linewidth=2.5, markersize=8, color=color, alpha=0.8)
        
        plt.xlabel('Retrieval Cost ($c_r$)', fontsize=13, fontweight='bold')
        plt.ylabel('Average Total Cost', fontsize=13, fontweight='bold')
        plt.title('Supplementary: Cost vs. Total Cost', fontsize=15, fontweight='bold', pad=15)
        plt.legend(fontsize=11, loc='best', framealpha=0.9)
        plt.grid(True, alpha=0.3, linestyle='--')
        plt.tight_layout()
        
        fig3_path = os.path.join(output_dir, f'exp1_supplementary_cost_vs_total_{mode_suffix}.png')
        plt.savefig(fig3_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ“ å›¾è¡¨å·²ä¿å­˜: {fig3_path}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='å®éªŒ1: æ£€ç´¢æˆæœ¬å½±å“ (çœŸå®LLMç‰ˆæœ¬)')
    parser.add_argument('--mode', type=str, default='small', choices=['small', 'full', 'custom'],
                       help='æµ‹è¯•æ¨¡å¼: small (10é¢˜, å¿«é€ŸéªŒè¯), full (å…¨éƒ¨~12Ké¢˜), custom (è‡ªå®šä¹‰)')
    parser.add_argument('--n-questions', type=int, default=None,
                       help='è‡ªå®šä¹‰é—®é¢˜æ•°é‡ (ä»…ç”¨äº --mode custom)')
    parser.add_argument('--difficulty', type=str, default='hard', choices=['easy', 'medium', 'hard'],
                       help='é—®é¢˜éš¾åº¦')
    parser.add_argument('--gpus', type=str, default='0,1,2,3',
                       help='ä½¿ç”¨çš„GPU IDåˆ—è¡¨ï¼Œé€—å·åˆ†éš”ï¼Œä¾‹å¦‚: 0,1,2,3')
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­')
    parser.add_argument('--model-path', type=str, default=None,
                       help='LLMæ¨¡å‹è·¯å¾„ (å¯é€‰ï¼Œç”¨äºè¦†ç›–é»˜è®¤çš„14Bæ¨¡å‹)')
    parser.add_argument('--config-path', type=str, default='configs/multi_gpu_data_calibrated.yaml',
                       help='MDPé…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤ä½¿ç”¨data_calibratedç‰ˆæœ¬ï¼Œc_p=0.02)')
    parser.add_argument('--policy-config-path', type=str, default=None,
                       help='è‡ªé€‚åº”ç­–ç•¥é…ç½®æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # éªŒè¯å‚æ•°
    if args.mode == 'custom' and args.n_questions is None:
        parser.error("--mode custom å¿…é¡»æŒ‡å®š --n-questions")
    
    # è§£æGPUåˆ—è¡¨
    gpu_ids = [int(x.strip()) for x in args.gpus.split(',')]
    
    # ç¡®å®šæ¨¡å‹è·¯å¾„
    if args.model_path:
        llm_model_path = args.model_path
    else:
        llm_model_path = "/data/user/huangxiaolin/ARGO/RAG_Models/models/Qwen2.5-14B-Instruct"
    
    print(f"\nå¯åŠ¨å‚æ•°:")
    print(f"  æ¨¡å¼: {args.mode}")
    if args.mode == 'custom':
        print(f"  é—®é¢˜æ•°: {args.n_questions}")
    print(f"  éš¾åº¦: {args.difficulty}")
    print(f"  GPU: {gpu_ids}")
    print(f"  ç§å­: {args.seed}")
    print(f"  æ¨¡å‹: {Path(llm_model_path).name}\n")
    
    # é…ç½®
    experiment = RealCostImpactExperiment(
        config_path=args.config_path,
        policy_config_path=args.policy_config_path,
        llm_model_path=llm_model_path,
        embedding_model_path="/data/user/huangxiaolin/ARGO/models/all-MiniLM-L6-v2",
        test_mode=args.mode,
        n_test_questions=args.n_questions,  # â† ä¼ å…¥è‡ªå®šä¹‰é—®é¢˜æ•°
        difficulty=args.difficulty,
        seed=args.seed,
        gpu_ids=gpu_ids
    )
    
    # è¿è¡Œå®éªŒ
    results = experiment.run_experiment(
        c_r_min_multiplier=1.0,
        c_r_max_multiplier=10.0
    )
    
    # ä¿å­˜ç»“æœ
    experiment.save_results()
    
    # ç»˜å›¾
    experiment.plot_results()
    
    print("\nå®éªŒ1å®Œæˆ!")
    print(f"\nä½¿ç”¨æç¤º:")
    if args.mode == "small":
        print(f"  å½“å‰æ˜¯å°è§„æ¨¡æµ‹è¯•æ¨¡å¼ï¼Œå¦‚æœè¿è¡ŒæˆåŠŸï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿è¡Œå®Œæ•´å®éªŒ:")
        print(f"  python Exp_real_cost_impact_v2.py --mode custom --n-questions 100 --difficulty {args.difficulty} --gpus {args.gpus}")
    elif args.mode == "custom":
        print(f"  âœ“ Customæ¨¡å¼è¿è¡ŒæˆåŠŸ")
        print(f"  å¦‚éœ€ç»Ÿè®¡åˆ†æï¼Œè¯·è¿è¡Œå¤šä¸ªç§å­åä½¿ç”¨:")
        print(f"  python Exp1_aggregate_and_analyze.py")


if __name__ == "__main__":
    main()
