`torch_dtype` is deprecated! Use `dtype` instead!
The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.

启动参数:
  模式: small
  难度: hard
  GPU: [2]
  种子: 42


================================================================================
实验1: 检索成本影响 - 3B快速验证版本
================================================================================
运行模式: 小规模测试模式 (快速验证代码)
LLM模型: /data/user/huangxiaolin/ARGO/RAG_Models/models/Qwen2.5-1.5B-Instruct
嵌入模型: /data/user/huangxiaolin/ARGO/models/all-MiniLM-L6-v2
问题难度: HARD
问题数量: 10
c_r采样点: 5个
================================================================================

GPU配置:
  可用GPU: 8张
  使用GPU: [2] (3B模型单卡即可)
    GPU 2: NVIDIA GeForce RTX 3060 (12.6GB)

加载ORAN-Bench-13K数据集...
Loaded ORAN-Bench-13K (清洗后):
  Easy: 1139 questions
  Medium: 9570 questions
  Hard: 3224 questions (清洗后)
  Total: 13933 questions
✓ 加载了 10 道 HARD 问题

加载嵌入模型: /data/user/huangxiaolin/ARGO/models/all-MiniLM-L6-v2
✓ 嵌入模型加载成功 (GPU 2)

================================================================================
预计算问题embeddings (优化检索性能)...
================================================================================

✓ 预计算完成!
  - 问题数: 10
  - 耗时: 0.4秒 (0.0分钟)
  - 平均: 38.4ms/问题
  - 内存占用: ~0.01 MB
================================================================================

连接Chroma数据库: /data/user/huangxiaolin/ARGO2/ARGO/Environments/chroma_store
✓ Chroma集合加载成功 (文档数: 436279)

加载LLM模型: /data/user/huangxiaolin/ARGO/RAG_Models/models/Qwen2.5-1.5B-Instruct
  使用 1 张GPU加载模型...
✓ LLM模型加载成功
  Device map: {'': 2}

================================================================================
初始化完成!
================================================================================


================================================================================
开始实验 - 检索成本影响
================================================================================
运行模式: 小规模测试模式 (快速验证代码)
c_r范围: 0.020 ~ 0.200 (扫描 5 个点)
c_p固定: 0.020
问题数量: 10
策略数量: 4 (ARGO, Always-Retrieve, Always-Reason, Random)
总评估次数: 5 × 4策略 × 10题 = 200
================================================================================


[1/5] c_r = 0.0200 (1.0x c_p)
--------------------------------------------------------------------------------
  求解MDP (c_r=0.020)... θ_cont=0.920, θ*=1.000

  评估 10 道问题...
Traceback (most recent call last):
  File "/data/user/huangxiaolin/ARGO2/ARGO/Exp_1.5B_pilot.py", line 1046, in <module>
    main()
  File "/data/user/huangxiaolin/ARGO2/ARGO/Exp_1.5B_pilot.py", line 1027, in main
    results = experiment.run_experiment(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/user/huangxiaolin/ARGO2/ARGO/Exp_1.5B_pilot.py", line 846, in run_experiment
    results = self.evaluate_all_policies(c_r, theta_cont, theta_star)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/user/huangxiaolin/ARGO2/ARGO/Exp_1.5B_pilot.py", line 795, in evaluate_all_policies
    result = self.simulate_argo_policy(question, theta_cont, theta_star, c_r)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/user/huangxiaolin/ARGO2/ARGO/Exp_1.5B_pilot.py", line 530, in simulate_argo_policy
    sub_query = self.decompose_query(
                ^^^^^^^^^^^^^^^^^^^^^
  File "/data/user/huangxiaolin/ARGO2/ARGO/Exp_1.5B_pilot.py", line 420, in decompose_query
    history_summary = "\n".join([f"Q: {q}\nA: {r[:100]}..." for q, r in history[-2:]])  # 最近2步
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/user/huangxiaolin/ARGO2/ARGO/Exp_1.5B_pilot.py", line 420, in <listcomp>
    history_summary = "\n".join([f"Q: {q}\nA: {r[:100]}..." for q, r in history[-2:]])  # 最近2步
                                               ~^^^^^^
TypeError: 'int' object is not subscriptable
