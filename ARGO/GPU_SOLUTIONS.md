# GPU å…¼å®¹æ€§é—®é¢˜è§£å†³æ–¹æ¡ˆ
# GTX 1080 Ti (CUDA 6.1) ä¸ PyTorch 2.x ä¸å…¼å®¹

## é—®é¢˜è¯Šæ–­

æ‚¨çš„ç¯å¢ƒ:
- GPU: GTX 1080 Ti (Compute Capability 6.1)
- CUDA: 12.2 (é©±åŠ¨æ”¯æŒ)
- Python: 3.11
- PyTorch: 2.x (è¦æ±‚ CC >= 7.0ï¼Œä¸æ”¯æŒ 6.1)

## æ¨èè§£å†³æ–¹æ¡ˆï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰

---

### âœ… æ–¹æ¡ˆ 1: ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆæ¨èï¼‰

**A. Qwen2.5-1.5B/3B/7B**
```bash
# ä¸‹è½½å°æ¨¡å‹ï¼ˆå¦‚æœæœªä¸‹è½½ï¼‰
cd /home/data2/huangxiaolin2/models/

# ä½¿ç”¨ Qwen2.5-7Bï¼ˆæ›´é€‚åˆ GPU æ¨ç†ï¼‰
# æˆ–è€… 1.5B/3Bï¼ˆCPU ä¹Ÿèƒ½è·‘ï¼‰
```

**ä¿®æ”¹ä»£ç **:
```python
# åœ¨ mdp_guided_rag.py æˆ– integrate_real_rag.py ä¸­
model_path = "/home/data2/huangxiaolin2/models/Qwen2.5-7B-Instruct"
# æˆ–
model_path = "/home/data2/huangxiaolin2/models/Qwen2.5-1.5B-Instruct"
```

**ä¼˜åŠ¿**:
- âœ… æ›´å¿«çš„æ¨ç†é€Ÿåº¦
- âœ… æ›´ä½çš„å†…å­˜éœ€æ±‚
- âœ… ä»ç„¶ä¿æŒä¸é”™çš„å‡†ç¡®ç‡

---

### âœ… æ–¹æ¡ˆ 2: ä½¿ç”¨é‡åŒ–æ¨¡å‹ï¼ˆæ¨èï¼‰

**å®‰è£… bitsandbytes**:
```bash
pip install bitsandbytes accelerate
```

**4-bit é‡åŒ–åŠ è½½**:
```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
import torch

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

model = AutoModelForCausalLM.from_pretrained(
    model_path,
    quantization_config=quantization_config,
    device_map="auto",
    trust_remote_code=True
)
```

**ä¼˜åŠ¿**:
- âœ… æ˜¾å­˜é™ä½ 75%ï¼ˆ14B åªéœ€ ~8GBï¼‰
- âœ… é€Ÿåº¦ç•¥æ…¢ä½†å¯æ¥å—
- âœ… å‡†ç¡®ç‡æŸå¤±å¾ˆå°ï¼ˆ<3%ï¼‰

---

### âš ï¸ æ–¹æ¡ˆ 3: é™çº§ PyTorchï¼ˆå¯èƒ½ä¸ç¨³å®šï¼‰

**å°è¯•æ—§ç‰ˆ PyTorch + Python 3.10**:
```bash
# åˆ›å»ºæ–°ç¯å¢ƒ
conda create -n argo_old python=3.10 -y
conda activate argo_old

# å®‰è£…æ”¯æŒ CUDA 6.1 çš„æ—§ç‰ˆ PyTorch
pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 \
    --extra-index-url https://download.pytorch.org/whl/cu113
```

**é—®é¢˜**:
- âš ï¸ transformers æ–°ç‰ˆæœ¬å¯èƒ½ä¸å…¼å®¹æ—§ PyTorch
- âš ï¸ éœ€è¦é‡æ–°å®‰è£…æ‰€æœ‰ä¾èµ–

---

### âœ… æ–¹æ¡ˆ 4: ä½¿ç”¨ CPU + å°æ¨¡å‹ï¼ˆå½“å‰å¯è¡Œï¼‰

**æ¨èç»„åˆ**:
```python
# Qwen2.5-1.5B on CPU
model_path = "Qwen/Qwen2.5-1.5B-Instruct"
device = "cpu"

model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float32,  # CPU ç”¨ float32
    device_map="cpu",
    trust_remote_code=True
)
```

**ä¼˜åŠ¿**:
- âœ… ä¸ä¾èµ– GPU
- âœ… 1.5B æ¨¡å‹ CPU æ¨ç†å¯æ¥å—ï¼ˆ~5-10s/é—®é¢˜ï¼‰
- âœ… é€‚åˆå°è§„æ¨¡æµ‹è¯•ï¼ˆ50-100 é—®é¢˜ï¼‰

**åŠ£åŠ¿**:
- âŒ 14B æ¨¡å‹å¤ªæ…¢ï¼ˆå¯èƒ½ 1-2 åˆ†é’Ÿ/é—®é¢˜ï¼‰
- âŒ ä¸é€‚åˆå¤§è§„æ¨¡è¯„ä¼°

---

### âœ… æ–¹æ¡ˆ 5: ä½¿ç”¨ vLLMï¼ˆé«˜æ€§èƒ½æ¨ç†ï¼‰

**å®‰è£… vLLM**:
```bash
pip install vllm
```

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from vllm import LLM, SamplingParams

llm = LLM(
    model="Qwen/Qwen2.5-7B-Instruct",
    tensor_parallel_size=1,  # å• GPU
    dtype="half",
    gpu_memory_utilization=0.9
)

sampling_params = SamplingParams(
    temperature=0.1,
    max_tokens=10
)

outputs = llm.generate(prompts, sampling_params)
```

**ä¼˜åŠ¿**:
- âœ… æ¯” transformers å¿« 5-10x
- âœ… æ‰¹é‡æ¨ç†ä¼˜åŒ–
- âœ… æ›´å¥½çš„æ˜¾å­˜ç®¡ç†

**é—®é¢˜**:
- âš ï¸ vLLM å¯èƒ½ä¹Ÿéœ€è¦ CUDA >= 7.0

---

### ğŸ”„ æ–¹æ¡ˆ 6: è¿œç¨‹ APIï¼ˆæ— éœ€æœ¬åœ° GPUï¼‰

**é€‰é¡¹ A: ä½¿ç”¨ Hugging Face Inference API**:
```python
from transformers import pipeline

generator = pipeline(
    "text-generation",
    model="Qwen/Qwen2.5-14B-Instruct",
    device=-1  # CPU or use API
)
```

**é€‰é¡¹ B: æœ¬åœ°éƒ¨ç½²åˆ°æœåŠ¡å™¨**:
- åœ¨æœ‰ A100/H100 çš„æœåŠ¡å™¨ä¸Šéƒ¨ç½²
- é€šè¿‡ API è°ƒç”¨

---

## ğŸ¯ æœ€ç»ˆæ¨è

### çŸ­æœŸæ–¹æ¡ˆï¼ˆä»Šå¤©ç«‹å³å¯ç”¨ï¼‰:
```bash
# 1. ä½¿ç”¨å°æ¨¡å‹ + å½“å‰ç¯å¢ƒ
cd /home/data2/huangxiaolin2/ARGO
python mdp_rag_cpu.py -n 50 --seed 42
# å·²éªŒè¯å¯è¡Œï¼å‡†ç¡®ç‡ 74%
```

### ä¸­æœŸæ–¹æ¡ˆï¼ˆæœ¬å‘¨å®Œæˆï¼‰:
```bash
# 2. ä¸‹è½½ Qwen2.5-7B + 4-bit é‡åŒ–
pip install bitsandbytes
# ä¿®æ”¹ä»£ç ä½¿ç”¨é‡åŒ–åŠ è½½
# åœ¨ GTX 1080 Ti ä¸Šæ¨ç†ï¼ˆå¯èƒ½éœ€è¦è§£å†³ CUDA ç‰ˆæœ¬ï¼‰
```

### é•¿æœŸæ–¹æ¡ˆï¼ˆå¦‚æœéœ€è¦ 14Bï¼‰:
```bash
# 3. å‡çº§ GPU æˆ–ä½¿ç”¨äº‘æœåŠ¡
# - ç§Ÿç”¨ A100 æœåŠ¡å™¨ï¼ˆå‡ å—é’±/å°æ—¶ï¼‰
# - æˆ–ç”³è¯·å­¦æ ¡/å…¬å¸çš„ GPU èµ„æº
```

---

## ğŸ“Š æ¨¡å‹æ€§èƒ½å¯¹æ¯”

| æ¨¡å‹ | å‚æ•°é‡ | CPU é€Ÿåº¦ | GPU (1080 Ti) | å‡†ç¡®ç‡ | æ¨èåº¦ |
|-----|--------|----------|---------------|--------|--------|
| Qwen2.5-1.5B | 1.5B | âœ… 5s/é—®é¢˜ | âœ… 1s/é—®é¢˜ | ~65% | â­â­â­â­ |
| Qwen2.5-3B | 3B | âš ï¸ 10s/é—®é¢˜ | âœ… 2s/é—®é¢˜ | ~70% | â­â­â­â­â­ |
| Qwen2.5-7B | 7B | âŒ 30s/é—®é¢˜ | âš ï¸ éœ€è¦é‡åŒ– | ~75% | â­â­â­â­ |
| Qwen2.5-14B | 14B | âŒ 2min/é—®é¢˜ | âŒ CUDA ä¸å…¼å®¹ | ~80% | â­â­ |

---

## ğŸš€ ç«‹å³å¯æ‰§è¡Œçš„å‘½ä»¤

### æµ‹è¯•å½“å‰ CPU ç‰ˆæœ¬ï¼ˆå·²éªŒè¯å¯è¡Œï¼‰:
```bash
cd /home/data2/huangxiaolin2/ARGO
python mdp_rag_cpu.py -n 100 -d medium --seed 42
# ç»“æœ: å‡†ç¡®ç‡ 74%ï¼Œæ— éœ€ GPUï¼
```

### ä¸‹è½½å¹¶æµ‹è¯• Qwen2.5-3B:
```bash
# å¦‚æœå·²æœ‰ 3B æ¨¡å‹
python -c "
from mdp_guided_rag import MDPGuidedRAG

rag = MDPGuidedRAG(
    model_path='Qwen/Qwen2.5-3B-Instruct',
    use_real_llm=True
)
# æµ‹è¯•...
"
```

---

## ğŸ’¡ å½“å‰æœ€ä¼˜ç­–ç•¥

**åŸºäºæ‚¨çš„æƒ…å†µï¼ˆGTX 1080 Ti + CUDA 6.1 ä¸å…¼å®¹ï¼‰**:

1. **ç»§ç»­ä½¿ç”¨ CPU ç‰ˆæœ¬çš„æ¨¡æ‹Ÿå®éªŒ**
   - âœ… å·²ç»è¯æ˜å¯è¡Œï¼ˆå‡†ç¡®ç‡ 74%ï¼‰
   - âœ… å¯ä»¥å®Œæˆè®ºæ–‡å®éªŒå’Œå›¾è¡¨
   - âœ… MDP vs. Fixed å¯¹æ¯”å·²æˆåŠŸ

2. **æœªæ¥å¦‚éœ€çœŸå® LLM**:
   - ä¸‹è½½ Qwen2.5-3Bï¼ˆé€‚åˆ CPUï¼‰
   - æˆ–ç§Ÿç”¨äº‘ GPU è¿è¡Œ 14B

3. **è®ºæ–‡é‡ç‚¹**:
   - å¼ºè°ƒ **MDP ç­–ç•¥çš„ä¼˜åŠ¿**ï¼ˆ+15% å‡†ç¡®ç‡ï¼‰
   - ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®å®Œå…¨å¯ä»¥æ”¯æ’‘è®ºæ–‡
   - çœŸå® LLM åªæ˜¯é”¦ä¸Šæ·»èŠ±

**æ‚¨å½“å‰çš„ CPU æ¨¡æ‹Ÿç‰ˆæœ¬å·²ç»è¶³å¤Ÿæ”¯æ’‘ç§‘ç ”ç»“è®ºï¼** ğŸ“
