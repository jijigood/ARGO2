# ğŸš€ ARGOé€‰æ‹©é¢˜å¿«é€Ÿå‚è€ƒ

## ğŸ“‹ ä¸€è¡Œä»£ç å›ç­”é€‰æ‹©é¢˜

```python
from src.argo_system import ARGOSystem

argo = ARGOSystem(model_name="Qwen/Qwen2.5-1.5B-Instruct", retriever_mode="chroma", chroma_dir="chroma_db")
answer, choice, _, _ = argo.answer_question(question, options=["é€‰é¡¹1", "é€‰é¡¹2", "é€‰é¡¹3", "é€‰é¡¹4"])
```

---

## ğŸ¯ æ ¸å¿ƒAPI

### åˆå§‹åŒ–

```python
argo = ARGOSystem(
    model_name="Qwen/Qwen2.5-1.5B-Instruct",  # LLMæ¨¡å‹
    retriever_mode="chroma",                   # æ£€ç´¢æ¨¡å¼: "chroma" æˆ– "mock"
    chroma_dir="chroma_db",                    # Chromaæ•°æ®åº“è·¯å¾„
    use_mdp=True,                              # æ˜¯å¦ä½¿ç”¨MDPç­–ç•¥
    max_steps=5,                               # æœ€å¤§æ¨ç†æ­¥æ•°
    verbose=True                               # æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¿‡ç¨‹
)
```

### å›ç­”é—®é¢˜

```python
answer, choice, history, metadata = argo.answer_question(
    question="é—®é¢˜æ–‡æœ¬",                        # å¿…éœ€
    options=["é€‰é¡¹1", "é€‰é¡¹2", "é€‰é¡¹3", "é€‰é¡¹4"], # å¯é€‰ï¼Œæä¾›åˆ™ä¸ºé€‰æ‹©é¢˜
    return_history=True                        # æ˜¯å¦è¿”å›æ¨ç†å†å²
)
```

### è¿”å›å€¼

| å˜é‡ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `answer` | `str` | è¯¦ç»†è§£é‡Š |
| `choice` | `str` or `None` | é€‰é¡¹ç¼–å· ("1"/"2"/"3"/"4") |
| `history` | `List[Dict]` or `None` | æ¨ç†å†å² |
| `metadata` | `Dict` | å…ƒæ•°æ®ï¼ˆæ­¥æ•°ã€è€—æ—¶ç­‰ï¼‰ |

---

## ğŸ“Š æ•°æ®é›†æ ¼å¼

### fin_H_clean.json

```json
[
  "é—®é¢˜æ–‡æœ¬",
  ["1. é€‰é¡¹1", "2. é€‰é¡¹2", "3. é€‰é¡¹3", "4. é€‰é¡¹4"],
  "2"  // æ­£ç¡®ç­”æ¡ˆ
]
```

### ä½¿ç”¨ç¤ºä¾‹

```python
import json

with open('data/benchmark/ORAN-Bench-13K/Benchmark/fin_H_clean.json', 'r') as f:
    dataset = json.load(f)

for item in dataset:
    question = item[0]
    options = [opt.split('. ', 1)[1] for opt in item[1]]  # å»æ‰ "1. " å‰ç¼€
    correct = item[2]
    
    _, choice, _, _ = argo.answer_question(question, options=options)
    print(f"é¢„æµ‹={choice}, æ­£ç¡®={correct}, {'âœ…' if choice==correct else 'âŒ'}")
```

---

## ğŸ” è¾“å‡ºæ ¼å¼

### LLMç”Ÿæˆ

```xml
<answer long>è¯¦ç»†æ¨ç†è¿‡ç¨‹...</answer long>
<answer short>Option X is correct because...</answer short>
<choice>X</choice>
```

### æå–é€»è¾‘

1. **ä¸»æå–**: `<choice>(\d)</choice>`
2. **å›é€€æå–**: `Option (\d)` æˆ– `é€‰é¡¹(\d)`
3. **é»˜è®¤**: è¿”å› `None`

---

## âš™ï¸ æ€§èƒ½ä¼˜åŒ–

### åŠ å¿«é€Ÿåº¦

```python
argo = ARGOSystem(
    model_name="Qwen/Qwen2.5-1.5B-Instruct",
    use_mdp=False,       # ç¦ç”¨MDPï¼Œä½¿ç”¨å›ºå®šç­–ç•¥
    max_steps=3,         # å‡å°‘æœ€å¤§æ­¥æ•°
    verbose=False        # å…³é—­è¯¦ç»†è¾“å‡º
)
```

### æå‡å‡†ç¡®ç‡

```python
argo = ARGOSystem(
    model_name="Qwen/Qwen2.5-7B-Instruct",  # ä½¿ç”¨æ›´å¤§æ¨¡å‹
    use_mdp=True,                            # å¯ç”¨MDPæ™ºèƒ½å†³ç­–
    max_steps=5,                             # å…è®¸æ›´å¤šæ¨ç†æ­¥æ•°
)

# è°ƒæ•´ç”Ÿæˆå‚æ•°
from src.synthesizer import AnswerSynthesizer
synthesizer = AnswerSynthesizer(
    model=model,
    tokenizer=tokenizer,
    temperature=0.3,    # é™ä½æ¸©åº¦æé«˜ç¡®å®šæ€§
    max_answer_length=256
)
```

---

## ğŸ§ª å¿«é€Ÿæµ‹è¯•

### æµ‹è¯•è„šæœ¬

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
python test_multiple_choice.py

# è¿è¡Œç¤ºä¾‹
python example_mcq.py 1  # å•é¢˜ç¤ºä¾‹
python example_mcq.py 2  # æ‰¹é‡ç¤ºä¾‹
```

### å•å…ƒæµ‹è¯•

```python
def test_choice_extraction():
    from src.synthesizer import AnswerSynthesizer
    
    # Mockå¯¹è±¡
    class MockModel: pass
    class MockTokenizer: pass
    
    synth = AnswerSynthesizer(MockModel(), MockTokenizer())
    
    # æµ‹è¯•æå–
    _, choice = synth._postprocess_answer(
        '<choice>3</choice>',
        has_options=True
    )
    assert choice == '3', "æå–å¤±è´¥"
```

---

## ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡

### è®¡ç®—å‡†ç¡®ç‡

```python
from sklearn.metrics import accuracy_score, classification_report

predictions = [...]  # é¢„æµ‹ç»“æœ
ground_truth = [...]  # æ­£ç¡®ç­”æ¡ˆ

# å‡†ç¡®ç‡
acc = accuracy_score(ground_truth, predictions)
print(f"å‡†ç¡®ç‡: {acc*100:.2f}%")

# è¯¦ç»†æŠ¥å‘Š
print(classification_report(
    ground_truth, 
    predictions,
    target_names=["Option 1", "Option 2", "Option 3", "Option 4"]
))
```

### å…ƒæ•°æ®ç»Ÿè®¡

```python
total_steps = []
total_times = []

for item in dataset:
    _, _, _, metadata = argo.answer_question(...)
    total_steps.append(metadata['total_steps'])
    total_times.append(metadata['elapsed_time'])

print(f"å¹³å‡æ­¥æ•°: {np.mean(total_steps):.1f}")
print(f"å¹³å‡è€—æ—¶: {np.mean(total_times):.2f}s")
```

---

## âš ï¸ å¸¸è§é—®é¢˜

### Q1: choiceè¿”å›Noneæ€ä¹ˆåŠï¼Ÿ

**A**: LLMå¯èƒ½æ²¡æœ‰ç”Ÿæˆæ­£ç¡®æ ¼å¼ã€‚æ£€æŸ¥ï¼š
1. æ¨¡å‹æ˜¯å¦å¤ªå°ï¼ˆå»ºè®®7B+ï¼‰
2. Promptæ˜¯å¦æ­£ç¡®ä¼ é€’options
3. ä½¿ç”¨å›é€€é»˜è®¤å€¼

```python
if choice is None:
    choice = "1"  # é»˜è®¤é€‰é¡¹1
```

### Q2: å‡†ç¡®ç‡å¾ˆä½æ€ä¹ˆåŠï¼Ÿ

**A**: å°è¯•ä»¥ä¸‹ä¼˜åŒ–ï¼š
1. ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹
2. æ”¹å–„æ£€ç´¢è´¨é‡ï¼ˆæ›´å¤šæ–‡æ¡£ã€æ›´å¥½çš„embeddingï¼‰
3. å¢åŠ æ¨ç†æ­¥æ•°
4. è°ƒæ•´MDPå‚æ•°

### Q3: è¿è¡Œå¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ

**A**: æ€§èƒ½ä¼˜åŒ–ï¼š
1. ä½¿ç”¨GPU (`device="cuda"`)
2. å‡å°‘max_steps
3. ä½¿ç”¨mock retrieveræµ‹è¯•
4. å…³é—­verboseæ¨¡å¼

### Q4: å¦‚ä½•è°ƒè¯•æ¨ç†è¿‡ç¨‹ï¼Ÿ

**A**: æŸ¥çœ‹æ¨ç†å†å²ï¼š

```python
answer, choice, history, _ = argo.answer_question(..., return_history=True)

for i, step in enumerate(history):
    print(f"Step {i+1}: {step['action']}")
    if step['action'] == 'retrieve':
        print(f"  Query: {step['subquery']}")
        print(f"  Success: {step['retrieval_success']}")
    print(f"  Answer: {step['intermediate_answer'][:100]}...")
```

---

## ğŸ”— ç›¸å…³æ–‡ä»¶

| æ–‡ä»¶ | ç”¨é€” |
|------|------|
| `MULTIPLE_CHOICE_SUPPORT.md` | ğŸ“š å®Œæ•´ä½¿ç”¨æ–‡æ¡£ |
| `MCQ_UPDATE_SUMMARY.md` | ğŸ“ æ›´æ–°æ€»ç»“ |
| `CHANGELOG.md` | ğŸ“‹ ç‰ˆæœ¬å†å² |
| `test_multiple_choice.py` | ğŸ§ª æµ‹è¯•è„šæœ¬ |
| `example_mcq.py` | ğŸ’¡ ç¤ºä¾‹ä»£ç  |

---

## ğŸ“ æœ€ä½³å®è·µ

### âœ… æ¨è

- ä½¿ç”¨ `fin_H_clean.json` (æ¸…æ´—åçš„æ•°æ®é›†)
- æ¸…ç†é€‰é¡¹å‰ç¼€ (`split('. ', 1)[1]`)
- è®°å½•æ¨ç†å†å²ç”¨äºåˆ†æ
- æ‰¹é‡å¤„ç†æ—¶å…³é—­verbose

### âŒ é¿å…

- ç›´æ¥ä½¿ç”¨ `fin_H.json` (å«19ä¸ªå¼‚å¸¸)
- å¿˜è®°æ¸…ç†é€‰é¡¹æ ¼å¼
- åœ¨å°æ¨¡å‹ä¸ŠæœŸæœ›é«˜å‡†ç¡®ç‡
- åŒæ—¶è¿è¡Œå¤šä¸ªARGOå®ä¾‹ï¼ˆå†…å­˜ä¸è¶³ï¼‰

---

## ğŸ’¡ ç¤ºä¾‹ä»£ç ç‰‡æ®µ

### å®Œæ•´è¯„ä¼°æµç¨‹

```python
import json
from src.argo_system import ARGOSystem
from sklearn.metrics import accuracy_score

# 1. åŠ è½½æ•°æ®
with open('data/benchmark/ORAN-Bench-13K/Benchmark/fin_H_clean.json', 'r') as f:
    dataset = json.load(f)

# 2. åˆå§‹åŒ–ç³»ç»Ÿ
argo = ARGOSystem(
    model_name="Qwen/Qwen2.5-1.5B-Instruct",
    retriever_mode="chroma",
    chroma_dir="chroma_db",
    verbose=False
)

# 3. æ‰¹é‡æ¨ç†
predictions, ground_truth = [], []

for item in dataset[:100]:  # å‰100é¢˜
    q, opts, ans = item[0], item[1], item[2]
    clean_opts = [o.split('. ', 1)[1] for o in opts]
    
    _, choice, _, _ = argo.answer_question(q, options=clean_opts)
    predictions.append(choice if choice else "1")
    ground_truth.append(ans)

# 4. è®¡ç®—å‡†ç¡®ç‡
acc = accuracy_score(ground_truth, predictions)
print(f"Accuracy: {acc*100:.2f}%")
```

---

**ç‰ˆæœ¬**: V2.1  
**æ›´æ–°**: 2024-11-03  
**å¿«é€Ÿå¸®åŠ©**: æŸ¥çœ‹ `MULTIPLE_CHOICE_SUPPORT.md`
