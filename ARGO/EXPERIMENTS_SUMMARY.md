# ARGO实验总结 - 真实LLM版本

## 实验环境
- **LLM模型**: Qwen2.5-3B-Instruct (快速推理)
- **嵌入模型**: all-MiniLM-L6-v2
- **检索库**: ChromaDB (436,279 ORAN规范文档)
- **硬件**: 8x NVIDIA RTX 3060 (12.6GB each)
- **测试集**: 30道 Hard 难度问题 (ORAN-Bench-13K)

---

## 实验1: 检索成本影响 ✅ 已完成

### 实验设计
- **固定参数**: p_s=0.8, c_p=0.02, δ_r=0.25, δ_p=0.08
- **扫描变量**: c_r (0.02 → 0.20, 7个点)
- **对比策略**: ARGO vs Always-Retrieve vs Always-Reason

### 核心发现
1. **ARGO的成本自适应**:
   - 低成本 (c_r=0.02): 5.1次检索
   - 高成本 (c_r≥0.08): 0.0次检索
   - **减少幅度**: 100% (完全停止检索)

2. **MDP阈值演化**:
   - c_r=0.02: θ_cont=0.920 → 鼓励检索
   - c_r≥0.08: θ_cont=0.000 → 避免检索

3. **基线策略**:
   - Always-Retrieve: 保持~4.8次 (无法适应)
   - Always-Reason: 始终0次 (固定)

### 图表 (3张)
- `exp1_real_cost_vs_quality.png` - 质量对比
- `exp1_real_cost_vs_retrievals.png` - **检索次数对比 ⭐核心**
- `exp1_real_cost_vs_accuracy.png` - 准确率对比

### 验证结论
✅ ARGO能够根据检索成本动态调整策略
✅ 当成本过高时,ARGO选择纯推理模式
✅ 基线策略无法感知成本变化

---

## 实验2: 检索成功率影响 ✅ 已完成

### 实验设计
- **固定参数**: c_r=0.05, c_p=0.02, δ_r=0.25, δ_p=0.08
- **扫描变量**: p_s (0.50 → 0.95, 7个点)
- **对比策略**: ARGO vs Always-Retrieve vs Always-Reason

### 核心发现
1. **ARGO的成功率自适应**:
   - 低成功率 (p_s=0.50): 0.0次检索 (不靠谱→避免)
   - 高成功率 (p_s=0.95): 1.1次检索 (值得信赖→允许)
   - **趋势**: 成功率↑ → 检索次数↑

2. **MDP阈值演化**:
   - p_s=0.50: θ_cont=0.000 → 完全避免检索
   - p_s=0.65: θ_cont=0.060 → 开始允许检索
   - p_s=0.95: θ_cont=0.140 → 适度鼓励检索

3. **Always-Retrieve的被动适应**:
   - p_s=0.50: 7.8次 (需要多次才能成功)
   - p_s=0.95: 4.2次 (更快达到质量阈值)
   - **注意**: 这不是主动决策,而是被动结果

### 图表 (5张)
- `exp2_real_ps_vs_quality.png` - 质量对比
- `exp2_real_ps_vs_retrievals.png` - **检索次数对比 ⭐核心**
- `exp2_real_ps_vs_accuracy.png` - 准确率对比
- `exp2_real_ps_vs_cost.png` - 成本对比
- `exp2_real_threshold_evolution.png` - **MDP阈值演化 ⭐重要**

### 验证结论
✅ ARGO能够感知检索成功率并调整策略
✅ 低成功率时避免无效检索,节省成本
✅ 高成功率时允许检索,提升质量

---

## 实验3: Pareto边界 - 成本质量权衡 🚧 正在运行

### 实验设计
- **固定参数**: 所有环境参数 (δ_r, δ_p, p_s, c_r, c_p)
- **扫描变量**: μ (成本权重, 0.0 → 10.0, 10个点)
- **对比策略**: ARGO (10个点) vs Always-Retrieve vs Always-Reason (单点)

### 预期结果
1. **ARGO的Pareto边界**:
   - μ=0.0 (质量优先): 高成本, 高质量 (接近Always-Retrieve)
   - μ=10.0 (成本优先): 低成本, 低质量 (接近Always-Reason)
   - 中间形成光滑曲线

2. **基线策略位置**:
   - Always-Retrieve: (高成本, 高质量) 单点
   - Always-Reason: (低成本, 低质量) 单点
   - **关键**: 都落在ARGO曲线下方或上面 (次优)

3. **核心验证**:
   - ARGO曲线是Pareto最优边界
   - 任何成本下,ARGO都能提供最优质量
   - μ提供"调节旋钮",生成整个最优策略族

### 图表 (预期)
- `exp3_real_pareto_frontier.png` - **Pareto边界图 ⭐最重要**
  - X轴: 平均成本 E[C_T]
  - Y轴: 平均质量 E[Q(O)]
  - ARGO: 曲线 (Pareto边界)
  - 基线: 点 (次优)

### 运行状态
- **开始时间**: 2025-10-29 05:30
- **预计完成**: ~50-60分钟
- **当前进度**: 正在加载模型...

---

## 实验对比总结

| 实验 | 独立变量 | 固定变量 | 核心验证 | 状态 |
|------|---------|---------|---------|------|
| **实验1** | c_r (成本) | p_s, c_p, δ | 成本自适应性 | ✅ 完成 |
| **实验2** | p_s (成功率) | c_r, c_p, δ | 成功率感知 | ✅ 完成 |
| **实验3** | μ (权重) | 所有环境参数 | Pareto最优性 | 🚧 运行中 |

---

## 关键洞察

### 1. ARGO的三重自适应能力
- **成本自适应**: 高成本→减少检索 (实验1)
- **成功率自适应**: 低成功率→避免检索 (实验2)
- **偏好自适应**: μ调节→生成策略族 (实验3)

### 2. 基线策略的局限性
- **Always-Retrieve**: 无法感知成本和成功率
- **Always-Reason**: 质量受限于LLM能力
- **共同问题**: 固定策略,无法适应环境变化

### 3. MDP的核心作用
- 通过阈值 (θ_cont, θ*) 实现动态决策
- 阈值随环境参数自动调整
- 提供理论最优保证 (Bellman方程)

---

## 论文贡献点

### 核心图表优先级
1. **实验3 Pareto边界** ⭐⭐⭐⭐⭐
   - 证明ARGO是最优策略族
   - 最具说服力的视觉证据

2. **实验1 检索次数vs成本** ⭐⭐⭐⭐
   - 展示成本自适应性
   - ARGO vs 基线的直接对比

3. **实验2 阈值演化** ⭐⭐⭐
   - 展示MDP的内部机制
   - 解释为什么ARGO能自适应

### 论文写作建议
1. **引言**: 以实验3的Pareto边界图开篇
2. **方法**: 用MDP阈值图解释决策逻辑
3. **实验**: 实验1→实验2→实验3递进式展开
4. **讨论**: 强调μ的调节能力和实用价值

---

## 下一步工作

### 实验3完成后
- [ ] 验证Pareto边界的凸性
- [ ] 检查基线点是否确实次优
- [ ] 分析μ的敏感度

### 额外实验建议
- [ ] 不同难度问题的对比 (Easy vs Medium vs Hard)
- [ ] 更大规模测试 (100+ questions)
- [ ] 其他基线策略 (Random, Hybrid)

### 代码优化
- [ ] 添加并行评估 (加速实验)
- [ ] 实现增量保存 (防止中断丢失)
- [ ] 优化内存使用 (支持更大模型)

---

**生成时间**: 2025-10-29  
**作者**: ARGO实验团队  
**版本**: Real LLM v1.0
