# ORAN-Bench-13K RAG è¯„ä¼°ç³»ç»Ÿ - æ–‡ä»¶ç´¢å¼•

## ğŸ“ æ ¸å¿ƒæ–‡ä»¶

### 1. åŸºå‡†æ•°æ®åŠ è½½å™¨
**æ–‡ä»¶**: `oran_benchmark_loader.py`  
**çŠ¶æ€**: âœ… å®Œæˆå¹¶æµ‹è¯•  
**è¡Œæ•°**: ~200 è¡Œ  
**åŠŸèƒ½**: åŠ è½½å’Œç®¡ç† ORAN-Bench-13K æ•°æ®é›†  

**æ ¸å¿ƒç±»/å‡½æ•°**:
- `ORANBenchmark()` - ä¸»ç±»
  - `_load_questions(filename)` - åŠ è½½ JSONL æ–‡ä»¶
  - `sample_questions(n, difficulty, seed)` - é‡‡æ ·é—®é¢˜
  - `format_question_for_llm(question)` - æ ¼å¼åŒ–ä¸ºæç¤º
  - `check_answer(question, predicted)` - éªŒè¯ç­”æ¡ˆ

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from oran_benchmark_loader import ORANBenchmark
benchmark = ORANBenchmark()
questions = benchmark.sample_questions(n=50, difficulty='medium', seed=42)
```

---

### 2. RAG è¯„ä¼°æ¡†æ¶
**æ–‡ä»¶**: `Exp_RAG_benchmark.py`  
**çŠ¶æ€**: âœ… å®Œæˆï¼ˆæ¨¡æ‹Ÿæ¨¡å¼ï¼‰  
**è¡Œæ•°**: ~340 è¡Œ  
**åŠŸèƒ½**: è¯„ä¼°ä¸åŒæ£€ç´¢ç­–ç•¥åœ¨åŸºå‡†ä¸Šçš„è¡¨ç°  

**æ ¸å¿ƒå‡½æ•°**:
- `extract_answer_number(llm_output)` - æå–ç­”æ¡ˆæ•°å­—
- `evaluate_rag_on_benchmark(benchmark, questions, config)` - è¯„ä¼° RAG ç³»ç»Ÿ
- `run_benchmark_experiment(n_questions, difficulty, seed)` - è¿è¡Œå®Œæ•´å®éªŒ
- `analyze_by_difficulty(results, questions)` - éš¾åº¦çº§åˆ«åˆ†æ
- `save_results(results, questions, filename)` - ä¿å­˜ç»“æœ

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from Exp_RAG_benchmark import run_benchmark_experiment, save_results
results, questions = run_benchmark_experiment(n_questions=100, seed=42)
save_results(results, questions, 'my_results.json')
```

**æ”¯æŒçš„ç­–ç•¥**:
1. `optimal` - MDP æœ€ä¼˜ç­–ç•¥
2. `fixed_k3` - å›ºå®šæ£€ç´¢ 3 ä¸ªæ–‡æ¡£
3. `fixed_k5` - å›ºå®šæ£€ç´¢ 5 ä¸ªæ–‡æ¡£
4. `fixed_k7` - å›ºå®šæ£€ç´¢ 7 ä¸ªæ–‡æ¡£
5. `adaptive` - è‡ªé€‚åº”ç­–ç•¥

---

### 3. ç»“æœå¯è§†åŒ–
**æ–‡ä»¶**: `plot_benchmark_results.py`  
**çŠ¶æ€**: âœ… å®Œæˆ  
**è¡Œæ•°**: ~250 è¡Œ  
**åŠŸèƒ½**: ç”Ÿæˆè¯„ä¼°ç»“æœçš„å¯è§†åŒ–å›¾è¡¨  

**ç”Ÿæˆçš„å›¾è¡¨**:
1. `benchmark_strategy_comparison.png` - ç­–ç•¥å¯¹æ¯”ï¼ˆæŸ±çŠ¶å›¾ï¼‰
2. `benchmark_difficulty_breakdown.png` - éš¾åº¦çº§åˆ«åˆ†è§£ï¼ˆåˆ†ç»„æŸ±çŠ¶å›¾ï¼‰
3. `benchmark_confusion_fixed_k5.png` - æ··æ·†çŸ©é˜µï¼ˆçƒ­åŠ›å›¾ï¼‰
4. `benchmark_retrieval_impact.png` - æ£€ç´¢æ·±åº¦å½±å“ï¼ˆæŠ˜çº¿å›¾ï¼‰
5. `benchmark_summary.txt` - æ–‡æœ¬æ‘˜è¦

**ä½¿ç”¨ç¤ºä¾‹**:
```bash
python plot_benchmark_results.py
```

---

### 4. çœŸå® RAG é›†æˆæ¨¡æ¿
**æ–‡ä»¶**: `integrate_real_rag.py`  
**çŠ¶æ€**: ğŸ“ æ¨¡æ¿ï¼ˆå¾…æµ‹è¯•ï¼‰  
**è¡Œæ•°**: ~300 è¡Œ  
**åŠŸèƒ½**: é›†æˆ Qwen2.5-14B-Instruct å’Œæ£€ç´¢å™¨çš„ç¤ºä¾‹ä»£ç   

**æ ¸å¿ƒå‡½æ•°**:
- `load_qwen_model(model_path)` - åŠ è½½ LLM æ¨¡å‹
- `load_retriever()` - åŠ è½½å‘é‡æ£€ç´¢å™¨
- `rag_inference(model, tokenizer, retriever, question, top_k)` - RAG æ¨ç†
- `evaluate_with_real_rag(model, tokenizer, retriever, questions)` - æ‰¹é‡è¯„ä¼°

**ä½¿ç”¨ç¤ºä¾‹**:
```python
from integrate_real_rag import load_qwen_model, load_retriever, rag_inference

# åŠ è½½æ¨¡å‹
model, tokenizer = load_qwen_model()
retriever = load_retriever()

# æ¨ç†
llm_output = rag_inference(model, tokenizer, retriever, question, top_k=5)
```

---

### 5. å¿«é€Ÿå¯åŠ¨è„šæœ¬
**æ–‡ä»¶**: `run_benchmark_eval.sh`  
**çŠ¶æ€**: âœ… å®Œæˆ  
**ç±»å‹**: Bash è„šæœ¬  
**åŠŸèƒ½**: ä¸€é”®è¿è¡Œæ‰€æœ‰è¯„ä¼°æ­¥éª¤  

**æ‰§è¡Œæ­¥éª¤**:
1. æ£€æŸ¥åŸºå‡†æ•°æ®
2. æµ‹è¯•åŠ è½½å™¨
3. è¿è¡Œè¯„ä¼°å®éªŒ
4. ç”Ÿæˆå¯è§†åŒ–
5. æ˜¾ç¤ºç»“æœæ‘˜è¦

**ä½¿ç”¨æ–¹æ³•**:
```bash
cd /home/data2/huangxiaolin2/ARGO
./run_benchmark_eval.sh
```

---

## ğŸ“š æ–‡æ¡£æ–‡ä»¶

### 1. å®Œæ•´ä½¿ç”¨æŒ‡å—
**æ–‡ä»¶**: `ORAN_BENCHMARK_README.md`  
**è¡Œæ•°**: ~400 è¡Œ  
**å†…å®¹**:
- ç³»ç»Ÿæ¦‚è¿°
- æ ¸å¿ƒç»„ä»¶è¯¦è§£
- å®éªŒç»“æœåˆ†æ
- ä½¿ç”¨æŒ‡å—ï¼ˆAPI + å‘½ä»¤è¡Œï¼‰
- é›†æˆçœŸå® RAG çš„æ­¥éª¤
- æ–‡ä»¶ç»“æ„è¯´æ˜

---

### 2. é¡¹ç›®æ€»ç»“
**æ–‡ä»¶**: `ORAN_BENCHMARK_SUMMARY.md`  
**è¡Œæ•°**: ~300 è¡Œ  
**å†…å®¹**:
- é¡¹ç›®ç›®æ ‡
- å·²å®Œæˆå·¥ä½œæ¸…å•
- å®éªŒç»“æœå’Œå…³é”®å‘ç°
- æŠ€æœ¯å®ç°ç»†èŠ‚
- ä¸‹ä¸€æ­¥è®¡åˆ’
- ä¸ ARGO_MDP çš„å…³ç³»

---

### 3. æ–‡ä»¶ç´¢å¼•ï¼ˆæœ¬æ–‡ä»¶ï¼‰
**æ–‡ä»¶**: `ORAN_BENCHMARK_INDEX.md`  
**åŠŸèƒ½**: å¿«é€Ÿå¯¼èˆªæ‰€æœ‰é¡¹ç›®æ–‡ä»¶  

---

## ğŸ“Š æ•°æ®æ–‡ä»¶

### 1. ORAN-Bench-13K æ•°æ®é›†
**ä½ç½®**: `ORAN-Bench-13K/Benchmark/`  
**æ ¼å¼**: JSONLï¼ˆæ¯è¡Œä¸€ä¸ª JSON æ•°ç»„ï¼‰  

**æ–‡ä»¶åˆ—è¡¨**:
- `fin_E.json` - 1,139 Easy é—®é¢˜
- `fin_M.json` - 9,570 Medium é—®é¢˜
- `fin_H.json` - 3,243 Hard é—®é¢˜

**æ•°æ®æ ¼å¼**:
```json
[
  "é—®é¢˜æ–‡æœ¬",
  ["1. é€‰é¡¹1", "2. é€‰é¡¹2", "3. é€‰é¡¹3", "4. é€‰é¡¹4"],
  "æ­£ç¡®ç­”æ¡ˆç´¢å¼• (1-4)"
]
```

---

### 2. è¯„ä¼°ç»“æœ
**ä½ç½®**: `draw_figs/data/`  
**æ ¼å¼**: JSON  

**æ–‡ä»¶ç¤ºä¾‹**:
- `oran_benchmark_mixed.json` - æ··åˆéš¾åº¦è¯„ä¼°ç»“æœ
- `oran_benchmark_easy.json` - Easy éš¾åº¦è¯„ä¼°ç»“æœ
- `oran_benchmark_medium.json` - Medium éš¾åº¦è¯„ä¼°ç»“æœ
- `oran_benchmark_hard.json` - Hard éš¾åº¦è¯„ä¼°ç»“æœ

**ç»“æœç»“æ„**:
```json
{
  "timestamp": "2025-10-28T11:20:35.843380",
  "benchmark": "ORAN-Bench-13K",
  "num_questions": 100,
  "results": {
    "optimal": {
      "correct": 74,
      "total": 100,
      "accuracy": 0.74,
      "details": [...]
    },
    ...
  }
}
```

---

## ğŸ–¼ï¸ å¯è§†åŒ–è¾“å‡º

**ä½ç½®**: `draw_figs/`  

| æ–‡ä»¶å | ç±»å‹ | æè¿° | å¤§å° |
|-------|------|------|------|
| `benchmark_strategy_comparison.png` | å›¾è¡¨ | ç­–ç•¥å‡†ç¡®ç‡å¯¹æ¯”æŸ±çŠ¶å›¾ | 154 KB |
| `benchmark_difficulty_breakdown.png` | å›¾è¡¨ | éš¾åº¦çº§åˆ«åˆ†ç»„æŸ±çŠ¶å›¾ | 165 KB |
| `benchmark_confusion_fixed_k5.png` | å›¾è¡¨ | ç­”æ¡ˆæ··æ·†çŸ©é˜µçƒ­åŠ›å›¾ | 129 KB |
| `benchmark_retrieval_impact.png` | å›¾è¡¨ | æ£€ç´¢æ·±åº¦å½±å“æŠ˜çº¿å›¾ | 158 KB |
| `benchmark_summary.txt` | æ–‡æœ¬ | ç­–ç•¥æ’åæ‘˜è¦ | 712 B |

---

## ğŸ”§ ä¾èµ–é…ç½®

### Python ç¯å¢ƒ
**è·¯å¾„**: `/root/miniconda/envs/ARGO/bin/python`  
**ç‰ˆæœ¬**: Python 3.11  

### å¿…éœ€åŒ…ï¼ˆå½“å‰ï¼‰
```
numpy
matplotlib
json (å†…ç½®)
pathlib (å†…ç½®)
datetime (å†…ç½®)
```

### å¯é€‰åŒ…ï¼ˆçœŸå® RAGï¼‰
```
torch
transformers
langchain-community
sentence-transformers
chromadb
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ¡ˆ 1: ä¸€é”®è¿è¡Œï¼ˆæ¨èï¼‰
```bash
cd /home/data2/huangxiaolin2/ARGO
./run_benchmark_eval.sh
```

### æ–¹æ¡ˆ 2: åˆ†æ­¥æ‰§è¡Œ
```bash
# 1. æµ‹è¯•åŠ è½½å™¨
/root/miniconda/envs/ARGO/bin/python oran_benchmark_loader.py

# 2. è¿è¡Œè¯„ä¼°
/root/miniconda/envs/ARGO/bin/python Exp_RAG_benchmark.py

# 3. ç”Ÿæˆå¯è§†åŒ–
/root/miniconda/envs/ARGO/bin/python plot_benchmark_results.py
```

### æ–¹æ¡ˆ 3: Python API
```python
from oran_benchmark_loader import ORANBenchmark
from Exp_RAG_benchmark import run_benchmark_experiment, save_results

# åŠ è½½åŸºå‡†
benchmark = ORANBenchmark()

# è¿è¡Œè¯„ä¼°
results, questions = run_benchmark_experiment(n_questions=100, seed=42)

# ä¿å­˜ç»“æœ
save_results(results, questions, 'my_results.json')
```

---

## ğŸ“ˆ å®éªŒç»“æœé€Ÿè§ˆ

### æœ€ä½³é…ç½®ï¼ˆæ¨¡æ‹Ÿï¼‰
- **ç­–ç•¥**: Fixed K=5
- **å‡†ç¡®ç‡**: 85.0% (85/100)
- **éš¾åº¦åˆ†å¸ƒ**: Easy: 85.7%, Medium: 88.7%, Hard: 72.7%

### ç­–ç•¥æ’å
1. Fixed K=5 - 85.0%
2. Fixed K=7 - 81.0%
3. Optimal - 74.0%
4. Fixed K=3 - 73.0%
5. Adaptive - 68.0%

---

## ğŸ”„ å·¥ä½œæµç¨‹

```
1. æ•°æ®åŠ è½½
   oran_benchmark_loader.py
   â†“
2. RAG è¯„ä¼°
   Exp_RAG_benchmark.py
   â†“
3. ç»“æœä¿å­˜
   draw_figs/data/*.json
   â†“
4. å¯è§†åŒ–ç”Ÿæˆ
   plot_benchmark_results.py
   â†“
5. è¾“å‡ºå›¾è¡¨
   draw_figs/*.png
```

---

## ğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### ç«‹å³å¯åš
- [x] æµ‹è¯•åŸºå‡†åŠ è½½å™¨
- [x] è¿è¡Œæ¨¡æ‹Ÿè¯„ä¼°
- [x] ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
- [ ] é›†æˆçœŸå® LLM (integrate_real_rag.py)

### è¿›é˜¶ä»»åŠ¡
- [ ] å¤š GPU å¹¶è¡Œæ¨ç†
- [ ] é”™è¯¯æ¡ˆä¾‹åˆ†æ
- [ ] æ£€ç´¢è´¨é‡è¯„ä¼°
- [ ] é¢†åŸŸé€‚åº”ç ”ç©¶

---

## ğŸ“ å¸®åŠ©ä¸æ”¯æŒ

| éœ€æ±‚ | å‚è€ƒæ–‡ä»¶ |
|-----|---------|
| å¿«é€Ÿä¸Šæ‰‹ | `run_benchmark_eval.sh` |
| è¯¦ç»†æ–‡æ¡£ | `ORAN_BENCHMARK_README.md` |
| é¡¹ç›®æ€»ç»“ | `ORAN_BENCHMARK_SUMMARY.md` |
| API ä½¿ç”¨ | å„æ–‡ä»¶é¡¶éƒ¨çš„ docstring |
| çœŸå® RAG é›†æˆ | `integrate_real_rag.py` |

---

**æœ€åæ›´æ–°**: 2025-10-28  
**é¡¹ç›®ç‰ˆæœ¬**: 1.0  
**çŠ¶æ€**: âœ… æ ¸å¿ƒåŠŸèƒ½å®Œæˆï¼Œå¯æŠ•å…¥ä½¿ç”¨
