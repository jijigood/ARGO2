"""
æµ‹è¯•é€‰æ‹©é¢˜åŠŸèƒ½
================

éªŒè¯ARGOç³»ç»Ÿå¯¹O-RAN Benchmarké€‰æ‹©é¢˜çš„æ”¯æŒã€‚
"""

import json
import sys
import os

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from src.argo_system import ARGO_System


def test_single_mcq():
    """æµ‹è¯•å•ä¸ªé€‰æ‹©é¢˜"""
    print("="*80)
    print("æµ‹è¯•1: å•ä¸ªé€‰æ‹©é¢˜")
    print("="*80)
    
    # åˆå§‹åŒ–ç³»ç»Ÿï¼ˆä½¿ç”¨å°æ¨¡å‹å¿«é€Ÿæµ‹è¯•ï¼‰
    argo = ARGO_System(
        model_name="Qwen/Qwen2.5-1.5B-Instruct",
        retriever_mode="mock",  # ä½¿ç”¨mockæ¨¡å¼å¿«é€Ÿæµ‹è¯•
        use_mdp=False,
        max_steps=2,
        verbose=True
    )
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    question = "What is the role of the SM Fanout module in an O-DU when an E2 message is received?"
    options = [
        "It interacts with the E2 handler module to send the message to the appropriate internal module.",
        "It consults the SM Catalog module to identify the relevant SM specific modules and APIs.",
        "It maps E2 messages to their corresponding receiver modules and message contents.",
        "It sends the E2 message through the E2 Sender module."
    ]
    correct_answer = "2"
    
    print(f"\né—®é¢˜: {question}")
    print(f"\né€‰é¡¹:")
    for i, opt in enumerate(options, 1):
        print(f"  {i}. {opt}")
    print(f"\næ­£ç¡®ç­”æ¡ˆ: {correct_answer}")
    print("\n" + "="*80)
    
    # æ¨ç†
    answer, choice, history, metadata = argo.answer_question(
        question=question,
        options=options,
        return_history=True
    )
    
    # ç»“æœ
    print("\n" + "="*80)
    print("ç»“æœ:")
    print("="*80)
    print(f"è¯¦ç»†ç­”æ¡ˆ: {answer[:300]}...")
    print(f"\né€‰æ‹©çš„é€‰é¡¹: {choice}")
    print(f"æ­£ç¡®ç­”æ¡ˆ: {correct_answer}")
    print(f"åˆ¤å®š: {'âœ… æ­£ç¡®' if choice == correct_answer else 'âŒ é”™è¯¯'}")
    print(f"\næ¨ç†æ­¥æ•°: {metadata['total_steps']}")
    print(f"è€—æ—¶: {metadata['elapsed_time']:.2f}ç§’")
    
    return choice == correct_answer


def test_batch_mcq():
    """æµ‹è¯•æ‰¹é‡é€‰æ‹©é¢˜ï¼ˆä»æ•°æ®é›†åŠ è½½ï¼‰"""
    print("\n\n" + "="*80)
    print("æµ‹è¯•2: æ‰¹é‡é€‰æ‹©é¢˜ï¼ˆä»æ•°æ®é›†ï¼‰")
    print("="*80)
    
    # åŠ è½½æ•°æ®é›†
    dataset_path = "ORAN-Bench-13K/Benchmark/fin_H_clean.json"
    if not os.path.exists(dataset_path):
        print(f"âš ï¸  æ•°æ®é›†æœªæ‰¾åˆ°: {dataset_path}")
        print("è·³è¿‡æ‰¹é‡æµ‹è¯•")
        return True
    
    with open(dataset_path, 'r', encoding='utf-8') as f:
        dataset = json.load(f)
    
    print(f"åŠ è½½æ•°æ®é›†: {len(dataset)} é¢˜")
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    argo = ARGO_System(
        model_name="Qwen/Qwen2.5-1.5B-Instruct",
        retriever_mode="mock",
        use_mdp=False,
        max_steps=2,
        verbose=False  # å…³é—­è¯¦ç»†è¾“å‡º
    )
    
    # æµ‹è¯•å‰5é¢˜
    num_samples = min(5, len(dataset))
    results = []
    
    print(f"\nå¤„ç†å‰ {num_samples} é¢˜...")
    
    for i, item in enumerate(dataset[:num_samples]):
        question_text = item[0]
        raw_options = item[1]
        correct_answer = item[2]
        
        # æ¸…ç†é€‰é¡¹ï¼ˆç§»é™¤ "1. ", "2. " ç­‰å‰ç¼€ï¼‰
        options = [opt.split('. ', 1)[1] if '. ' in opt else opt 
                  for opt in raw_options]
        
        print(f"\né¢˜ç›® {i+1}/{num_samples}: {question_text[:60]}...")
        
        # æ¨ç†
        try:
            answer, choice, _, metadata = argo.answer_question(
                question=question_text,
                options=options,
                return_history=False
            )
            
            is_correct = (choice == correct_answer)
            result = {
                'question': question_text,
                'predicted': choice,
                'correct': correct_answer,
                'is_correct': is_correct,
                'steps': metadata['total_steps']
            }
            results.append(result)
            
            status = "âœ…" if is_correct else "âŒ"
            print(f"  é¢„æµ‹: {choice}, æ­£ç¡®: {correct_answer} {status}")
            
        except Exception as e:
            print(f"  âŒ é”™è¯¯: {e}")
            results.append({
                'question': question_text,
                'predicted': None,
                'correct': correct_answer,
                'is_correct': False,
                'steps': 0
            })
    
    # ç»Ÿè®¡
    print("\n" + "="*80)
    print("æ‰¹é‡æµ‹è¯•ç»“æœ:")
    print("="*80)
    
    total = len(results)
    correct_count = sum(1 for r in results if r['is_correct'])
    accuracy = correct_count / total if total > 0 else 0
    
    print(f"æ€»é¢˜æ•°: {total}")
    print(f"æ­£ç¡®æ•°: {correct_count}")
    print(f"å‡†ç¡®ç‡: {accuracy*100:.2f}%")
    
    # è¯¦ç»†ç»“æœ
    print("\nè¯¦ç»†ç»“æœ:")
    for i, r in enumerate(results, 1):
        status = "âœ…" if r['is_correct'] else "âŒ"
        print(f"{i}. {status} é¢„æµ‹={r['predicted']}, æ­£ç¡®={r['correct']}")
    
    return accuracy > 0  # åªè¦æœ‰é¢˜ç›®æ­£ç¡®å°±ç®—é€šè¿‡


def test_format_extraction():
    """æµ‹è¯•æ ¼å¼æå–"""
    print("\n\n" + "="*80)
    print("æµ‹è¯•3: æ ¼å¼æå–")
    print("="*80)
    
    from src.synthesizer import AnswerSynthesizer
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„synthesizerå®ä¾‹ï¼ˆä¸éœ€è¦çœŸå®modelï¼‰
    class MockModel:
        pass
    
    class MockTokenizer:
        pass
    
    synthesizer = AnswerSynthesizer(
        model=MockModel(),
        tokenizer=MockTokenizer()
    )
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {
            'name': 'å®Œæ•´æ ¼å¼',
            'raw': '<answer long>Detailed explanation...</answer long><answer short>Option 2 is correct</answer short><choice>2</choice>',
            'expected_choice': '2'
        },
        {
            'name': 'ä»…æœ‰choiceæ ‡ç­¾',
            'raw': 'Some reasoning text. <choice>3</choice>',
            'expected_choice': '3'
        },
        {
            'name': 'å›é€€æå– - Option',
            'raw': 'Based on the analysis, Option 4 is the correct answer.',
            'expected_choice': '4'
        },
        {
            'name': 'å›é€€æå– - ä¸­æ–‡',
            'raw': 'æ ¹æ®åˆ†æï¼Œé€‰é¡¹1æ˜¯æ­£ç¡®ç­”æ¡ˆã€‚',
            'expected_choice': '1'
        }
    ]
    
    all_passed = True
    for test in test_cases:
        answer, choice = synthesizer._postprocess_answer(
            test['raw'], 
            has_options=True
        )
        
        passed = (choice == test['expected_choice'])
        status = "âœ…" if passed else "âŒ"
        
        print(f"\n{status} {test['name']}")
        print(f"   è¾“å…¥: {test['raw'][:60]}...")
        print(f"   æå–: {choice}")
        print(f"   æœŸæœ›: {test['expected_choice']}")
        
        if not passed:
            all_passed = False
    
    return all_passed


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n")
    print("â•”" + "="*78 + "â•—")
    print("â•‘" + " "*20 + "ARGO é€‰æ‹©é¢˜åŠŸèƒ½æµ‹è¯•" + " "*20 + "â•‘")
    print("â•š" + "="*78 + "â•")
    
    tests = [
        ("å•ä¸ªé€‰æ‹©é¢˜", test_single_mcq),
        ("æ‰¹é‡é€‰æ‹©é¢˜", test_batch_mcq),
        ("æ ¼å¼æå–", test_format_extraction)
    ]
    
    results = []
    for name, test_func in tests:
        try:
            passed = test_func()
            results.append((name, passed))
        except Exception as e:
            print(f"\nâŒ æµ‹è¯•å¤±è´¥: {name}")
            print(f"é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            results.append((name, False))
    
    # æ€»ç»“
    print("\n\n" + "="*80)
    print("æµ‹è¯•æ€»ç»“")
    print("="*80)
    
    for name, passed in results:
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"{status}: {name}")
    
    total = len(results)
    passed_count = sum(1 for _, p in results if p)
    
    print(f"\næ€»è®¡: {passed_count}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed_count == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        return 0
    else:
        print(f"\nâš ï¸  {total - passed_count} ä¸ªæµ‹è¯•å¤±è´¥")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
