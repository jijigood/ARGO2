"""
Retriever - Phase 3.2 (Enhanced with ARGO Prompts V2.0)
========================================================

åŸºäºChromaå‘é‡æ•°æ®åº“çš„æ£€ç´¢å™¨ã€‚æ¥å—å­æŸ¥è¯¢ï¼Œè¿”å›ç›¸å…³æ–‡æ¡£ã€‚
æ¨¡æ‹Ÿæ£€ç´¢æˆåŠŸç‡p_sæœºåˆ¶ï¼Œå¹¶æ”¯æŒåŸºäºæ£€ç´¢æ–‡æ¡£çš„ç­”æ¡ˆç”Ÿæˆã€‚

Key Features:
- å‘é‡æ£€ç´¢ï¼šä½¿ç”¨embeddingæ¨¡å‹è¿›è¡Œè¯­ä¹‰æ£€ç´¢
- Top-kè¿”å›ï¼šè¿”å›æœ€ç›¸å…³çš„kä¸ªæ–‡æ¡£
- å¤±è´¥æ¨¡æ‹Ÿï¼šåŸºäºç›¸ä¼¼åº¦é˜ˆå€¼æ¨¡æ‹Ÿp_s
- æ‰¹é‡æ£€ç´¢ï¼šæ”¯æŒæ‰¹é‡æŸ¥è¯¢åŠ é€Ÿ
- ç­”æ¡ˆç”Ÿæˆï¼šåŸºäºæ£€ç´¢æ–‡æ¡£ç”Ÿæˆä¸­é—´ç­”æ¡ˆï¼ˆæ–°å¢ï¼‰

Input:
- q_t: å­æŸ¥è¯¢ï¼ˆå­—ç¬¦ä¸²ï¼‰
- k: è¿”å›æ–‡æ¡£æ•°é‡

Output:
- r_t: æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨ï¼ˆæˆ–ç©ºåˆ—è¡¨âˆ…ï¼‰
- success: æ˜¯å¦æˆåŠŸæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£
- answer: åŸºäºæ£€ç´¢æ–‡æ¡£çš„ç­”æ¡ˆï¼ˆå¯é€‰ï¼‰

Example:
    retriever = Retriever(
        chroma_dir="Environments/chroma_store",
        collection_name="oran_specs"
    )
    docs, success = retriever.retrieve("What is O-RAN latency?", k=3)
    
    # å¯é€‰ï¼šç”ŸæˆåŸºäºæ£€ç´¢çš„ç­”æ¡ˆ
    answer = retriever.generate_answer_from_docs(
        question="What is O-RAN latency?",
        docs=docs,
        model=model,
        tokenizer=tokenizer
    )
"""

# Chromaä½œä¸ºå¯é€‰ä¾èµ–
try:
    import chromadb
    from chromadb.config import Settings
    CHROMA_AVAILABLE = True
except ImportError:
    CHROMA_AVAILABLE = False
    chromadb = None

import torch
import numpy as np
import os
from typing import List, Dict, Tuple, Optional
import logging
from pathlib import Path
from .prompts import ARGOPrompts

logger = logging.getLogger(__name__)


class Retriever:
    """
    åŸºäºChromaçš„å‘é‡æ£€ç´¢å™¨
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. å‘é‡æ£€ç´¢ï¼šå°†æŸ¥è¯¢ç¼–ç ä¸ºembeddingï¼Œæ£€ç´¢æœ€ç›¸ä¼¼æ–‡æ¡£
    2. æˆåŠŸç‡æ¨¡æ‹Ÿï¼šåŸºäºç›¸ä¼¼åº¦é˜ˆå€¼æ¨¡æ‹Ÿp_s
    3. æ‰¹é‡å¤„ç†ï¼šæ”¯æŒæ‰¹é‡æŸ¥è¯¢æå‡æ•ˆç‡
    """
    
    def __init__(
        self,
        chroma_dir: str = "Environments/chroma_store",
        collection_name: str = "oran_specs",
        embedding_model_name: str = "all-MiniLM-L6-v2",
        similarity_threshold: float = 0.3,  # ä½äºæ­¤é˜ˆå€¼è§†ä¸ºæ£€ç´¢å¤±è´¥
        p_s_mode: str = "threshold",  # "threshold" or "random"
        p_s_value: float = 0.8,  # ä»…åœ¨randomæ¨¡å¼ä¸‹ä½¿ç”¨
        use_reranker: bool = True,  # æ˜¯å¦ä½¿ç”¨reranker
        reranker_model_path: Optional[str] = None,  # Rerankeræ¨¡å‹è·¯å¾„
        reranker_device: Optional[str] = None  # Rerankerè®¾å¤‡ï¼š'cuda', 'cpu', æˆ–Noneï¼ˆè‡ªåŠ¨é€‰æ‹©ï¼‰
    ):
        """
        Args:
            chroma_dir: Chromaæ•°æ®åº“å­˜å‚¨ç›®å½•
            collection_name: é›†åˆåç§°
            embedding_model_name: Embeddingæ¨¡å‹åç§°
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
            p_s_mode: æˆåŠŸç‡æ¨¡å¼
                - "threshold": åŸºäºç›¸ä¼¼åº¦é˜ˆå€¼
                - "random": å›ºå®šæ¦‚ç‡p_sï¼ˆç”¨äºå®éªŒï¼‰
            p_s_value: randomæ¨¡å¼ä¸‹çš„æˆåŠŸæ¦‚ç‡
            use_reranker: æ˜¯å¦ä½¿ç”¨rerankeré‡æ–°æ’åºæ£€ç´¢ç»“æœ
            reranker_model_path: Rerankeræ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„ï¼‰
            reranker_device: Rerankerè®¾å¤‡
                - 'cuda': å¼ºåˆ¶ä½¿ç”¨GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
                - 'cpu': å¼ºåˆ¶ä½¿ç”¨CPUï¼ˆè¾ƒæ…¢ä½†å†…å­˜å ç”¨å°ï¼‰
                - None: è‡ªåŠ¨é€‰æ‹©ï¼ˆä¼˜å…ˆGPUï¼Œå¤±è´¥æ—¶å›é€€åˆ°CPUï¼‰
        """
        self.chroma_dir = Path(chroma_dir)
        self.collection_name = collection_name
        self.similarity_threshold = similarity_threshold
        self.p_s_mode = p_s_mode
        self.p_s_value = p_s_value
        self.use_reranker = use_reranker
        # é»˜è®¤ä½¿ç”¨æ›´å°çš„bge-reranker-baseæ¨¡å‹ï¼ˆ278Må‚æ•°ï¼Œ1.1GBï¼‰è€Œä¸æ˜¯v2-m3ï¼ˆ568Må‚æ•°ï¼Œ2.3GBï¼‰
        # å¦‚æœæŒ‡å®šäº†è·¯å¾„ï¼Œä½¿ç”¨æŒ‡å®šè·¯å¾„ï¼›å¦åˆ™ä½¿ç”¨baseæ¨¡å‹
        self.reranker_model_name = "BAAI/bge-reranker-base"  # æ›´å°çš„æ¨¡å‹ï¼Œé€‚åˆGPUå†…å­˜å—é™çš„æƒ…å†µ
        self.reranker_model_path = reranker_model_path
        self.reranker_device_preference = reranker_device  # è®¾å¤‡åå¥½ï¼š'cuda', 'cpu', æˆ–Noneï¼ˆè‡ªåŠ¨ï¼‰
        
        # åˆå§‹åŒ–Rerankerï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.reranker_model = None
        self.reranker_tokenizer = None
        # ä½¿ç”¨WARNINGçº§åˆ«ç¡®ä¿è¾“å‡ºï¼ˆå³ä½¿æ²¡æœ‰é…ç½®logging.basicConfigï¼‰
        logger.warning(f"ğŸ”µ About to initialize reranker. use_reranker={self.use_reranker}")
        if self.use_reranker:
            logger.warning("âœ… use_reranker is True, calling _init_reranker()")
            self._init_reranker()
        else:
            logger.warning("âš ï¸ use_reranker is False, skipping reranker initialization")
        
        # æ£€æŸ¥Chromaæ˜¯å¦å¯ç”¨
        if not CHROMA_AVAILABLE:
            raise ImportError(
                "chromadb is not installed. Please install it with: pip install chromadb"
            )
        
        # åˆå§‹åŒ–Chromaå®¢æˆ·ç«¯
        logger.info(f"Initializing Chroma client from {self.chroma_dir}")
        
        try:
            self.client = chromadb.PersistentClient(path=str(self.chroma_dir))
            self.collection = self.client.get_collection(name=collection_name)
            
            # è·å–é›†åˆç»Ÿè®¡
            count = self.collection.count()
            logger.info(f"Loaded collection '{collection_name}' with {count} documents")
            
        except Exception as e:
            logger.error(f"Failed to load Chroma collection: {e}")
            raise RuntimeError(
                f"Cannot load collection '{collection_name}' from {chroma_dir}. "
                "Please run run_chroma_pipeline.py first."
            ) from e
        
        # ä½¿ç”¨WARNINGçº§åˆ«ç¡®ä¿æ—¥å¿—è¾“å‡ºï¼ˆå³ä½¿æ²¡æœ‰é…ç½®logging.basicConfigï¼‰
        reranker_status = "ENABLED" if (self.use_reranker and self.reranker_model) else "DISABLED"
        reranker_details = ""
        if self.use_reranker:
            if self.reranker_model:
                reranker_details = f" (model exists: True, device: {getattr(self, 'reranker_device', 'unknown')})"
            else:
                reranker_details = f" (model exists: False - initialization may have failed)"
        
        logger.warning(
            f"Retriever initialized: mode={p_s_mode}, "
            f"threshold={similarity_threshold:.3f}, p_s={p_s_value:.2f}, "
            f"reranker={reranker_status}{reranker_details}"
        )
    
    def _init_reranker(self):
        """åˆå§‹åŒ–Rerankeræ¨¡å‹"""
        # ä½¿ç”¨WARNINGçº§åˆ«ç¡®ä¿è¾“å‡ºï¼ˆå³ä½¿æ²¡æœ‰é…ç½®logging.basicConfigï¼‰
        logger.warning("="*80)
        logger.warning("RERANKER INITIALIZATION START")
        logger.warning(f"use_reranker: {self.use_reranker}")
        logger.warning(f"reranker_model_path: {self.reranker_model_path}")
        logger.warning(f"reranker_device_preference: {self.reranker_device_preference}")
        
        try:
            from transformers import AutoModelForSequenceClassification, AutoTokenizer
            logger.warning("âœ… transformers imported successfully")
            
            logger.warning(f"Loading reranker model...")
            
            # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°è·¯å¾„ï¼ˆå¦‚æœæä¾›ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨HuggingFace Hub
            if self.reranker_model_path and os.path.exists(self.reranker_model_path):
                logger.warning(f"âœ… reranker_model_path exists: {self.reranker_model_path}")
                
                # æ™ºèƒ½å¤„ç†HuggingFaceç¼“å­˜è·¯å¾„
                # å¦‚æœè·¯å¾„æ˜¯ç¼“å­˜æ ¹ç›®å½•ï¼ˆåŒ…å«snapshotså­ç›®å½•ï¼‰ï¼Œè‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„snapshot
                actual_model_path = self.reranker_model_path
                snapshots_dir = os.path.join(self.reranker_model_path, "snapshots")
                
                logger.warning(f"Checking for snapshots directory: {snapshots_dir}")
                if os.path.exists(snapshots_dir) and os.path.isdir(snapshots_dir):
                    # è¿™æ˜¯HuggingFaceç¼“å­˜æ ¹ç›®å½•ï¼Œéœ€è¦æ‰¾åˆ°snapshotsä¸‹çš„å®é™…æ¨¡å‹è·¯å¾„
                    logger.warning(f"âœ… Detected HuggingFace cache directory, looking for snapshots...")
                    snapshots = [d for d in os.listdir(snapshots_dir) 
                               if os.path.isdir(os.path.join(snapshots_dir, d))]
                    logger.warning(f"Found {len(snapshots)} snapshots: {snapshots}")
                    
                    if snapshots:
                        # ä½¿ç”¨æœ€æ–°çš„snapshotï¼ˆæŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼‰
                        snapshots.sort(key=lambda x: os.path.getmtime(os.path.join(snapshots_dir, x)), reverse=True)
                        actual_model_path = os.path.join(snapshots_dir, snapshots[0])
                        logger.warning(f"âœ… Found snapshot: {snapshots[0]}")
                        logger.warning(f"âœ… Using model path: {actual_model_path}")
                    else:
                        logger.warning(f"âš ï¸ No snapshots found in {snapshots_dir}, using original path")
                else:
                    logger.warning(f"Not a HuggingFace cache directory, using path directly: {actual_model_path}")
                
                try:
                    logger.warning(f"ğŸ”µ Trying to load from local path: {actual_model_path}")
                    logger.warning(f"   Checking if path exists: {os.path.exists(actual_model_path)}")
                    logger.warning(f"   Checking for config.json: {os.path.exists(os.path.join(actual_model_path, 'config.json'))}")
                    
                    self.reranker_tokenizer = AutoTokenizer.from_pretrained(
                        actual_model_path,
                        trust_remote_code=True
                    )
                    logger.warning("âœ… Tokenizer loaded successfully")
                    
                    self.reranker_model = AutoModelForSequenceClassification.from_pretrained(
                        actual_model_path,
                        trust_remote_code=True
                    )
                    logger.warning("âœ… Model loaded successfully from local path")
                except Exception as e1:
                    logger.warning(f"âŒ Failed to load from local path: {e1}")
                    import traceback
                    logger.warning(traceback.format_exc())
                    # å¦‚æœæœ¬åœ°è·¯å¾„å¤±è´¥ï¼Œå°è¯•Hub
                    logger.warning(f"ğŸ”„ Falling back to HuggingFace Hub: {self.reranker_model_name}")
                    try:
                        self.reranker_tokenizer = AutoTokenizer.from_pretrained(
                            self.reranker_model_name,
                            trust_remote_code=True
                        )
                        logger.warning("âœ… Tokenizer loaded from Hub")
                        
                        self.reranker_model = AutoModelForSequenceClassification.from_pretrained(
                            self.reranker_model_name,
                            trust_remote_code=True
                        )
                        logger.warning("âœ… Model loaded from HuggingFace Hub")
                    except Exception as e2:
                        logger.warning(f"âŒ Hub loading also failed: {e2}")
                        logger.warning(traceback.format_exc())
                        raise Exception(f"Both local path and Hub failed. Local error: {e1}, Hub error: {e2}")
            else:
                # æ²¡æœ‰æœ¬åœ°è·¯å¾„ï¼Œç›´æ¥ä»HubåŠ è½½
                logger.warning(f"Trying to load from HuggingFace Hub: {self.reranker_model_name}")
                logger.warning(f"  Model size: ~278M parameters, ~1.1GB (smaller than v2-m3)")
                try:
                    self.reranker_tokenizer = AutoTokenizer.from_pretrained(
                        self.reranker_model_name,
                        trust_remote_code=True
                    )
                    self.reranker_model = AutoModelForSequenceClassification.from_pretrained(
                        self.reranker_model_name,
                        trust_remote_code=True
                    )
                    logger.warning("âœ… Loaded from HuggingFace Hub")
                except Exception as e:
                    raise Exception(f"Failed to load from Hub: {e}")
            
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            self.reranker_model.eval()
            
            # ç§»åŠ¨åˆ°åˆé€‚çš„è®¾å¤‡
            if self.reranker_device_preference == 'cpu':
                # å¼ºåˆ¶ä½¿ç”¨CPU
                self.reranker_device = 'cpu'
                logger.warning("Using CPU for reranker (as specified)")
            elif self.reranker_device_preference == 'cuda' and torch.cuda.is_available():
                # å¼ºåˆ¶ä½¿ç”¨GPU
                try:
                    self.reranker_model = self.reranker_model.cuda()
                    self.reranker_device = 'cuda'
                    logger.warning("Using GPU for reranker (as specified)")
                except RuntimeError as e:
                    logger.warning(f"Failed to move reranker to GPU: {e}")
                    logger.warning("Falling back to CPU")
                    self.reranker_device = 'cpu'
            elif torch.cuda.is_available():
                # è‡ªåŠ¨é€‰æ‹©ï¼šå°è¯•GPUï¼Œå¤±è´¥æ—¶å›é€€åˆ°CPU
                try:
                    self.reranker_model = self.reranker_model.cuda()
                    self.reranker_device = 'cuda'
                    logger.warning("Using GPU for reranker (auto-selected)")
                except RuntimeError as e:
                    logger.warning(f"Failed to move reranker to GPU (out of memory?): {e}")
                    logger.warning("Falling back to CPU reranker")
                    self.reranker_device = 'cpu'
            else:
                # æ²¡æœ‰GPUå¯ç”¨
                self.reranker_device = 'cpu'
                logger.warning("Using CPU for reranker (no GPU available)")
            
            logger.warning(f"âœ… Reranker model loaded successfully on {self.reranker_device}")
            logger.warning("="*80)
            logger.warning("âœ… RERANKER INITIALIZATION SUCCESS")
            logger.warning(f"  Model: {self.reranker_model_name}")
            logger.warning(f"  Device: {self.reranker_device}")
            logger.warning(f"  Model exists: {self.reranker_model is not None}")
            logger.warning(f"  Tokenizer exists: {self.reranker_tokenizer is not None}")
            logger.warning("="*80)
            
        except Exception as e:
            # ä½¿ç”¨WARNINGçº§åˆ«ç¡®ä¿è¾“å‡ºï¼ˆå³ä½¿æ²¡æœ‰é…ç½®logging.basicConfigï¼‰
            logger.warning("="*80)
            logger.warning("âŒ RERANKER INITIALIZATION FAILED")
            logger.warning(f"Error type: {type(e).__name__}")
            logger.warning(f"Error message: {str(e)}")
            logger.warning("="*80)
            # è¾“å‡ºå®Œæ•´å †æ ˆåˆ°WARNINGçº§åˆ«
            import traceback
            logger.warning(traceback.format_exc())
            logger.warning("Continuing without reranker - using original retrieval scores")
            logger.warning("="*80)
            self.use_reranker = False
            self.reranker_model = None
            self.reranker_tokenizer = None
    
    def _rerank_documents(
        self,
        query: str,
        documents: List[str],
        original_scores: Optional[List[float]] = None
    ) -> Tuple[List[str], List[float]]:
        """
        ä½¿ç”¨Rerankerå¯¹æ£€ç´¢ç»“æœé‡æ–°æ’åº
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            documents: æ–‡æ¡£åˆ—è¡¨
            original_scores: åŸå§‹ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            (reranked_docs, reranked_scores): é‡æ–°æ’åºåçš„æ–‡æ¡£å’Œåˆ†æ•°
        """
        logger.warning(f"ğŸ”µ _rerank_documents called: use_reranker={self.use_reranker}, model exists={self.reranker_model is not None}, num_docs={len(documents)}")
        
        if not self.use_reranker or self.reranker_model is None:
            # å¦‚æœæ²¡æœ‰rerankerï¼Œè¿”å›åŸå§‹ç»“æœ
            logger.warning(f"âš ï¸ Reranker not available: use_reranker={self.use_reranker}, model is None={self.reranker_model is None}")
            scores = original_scores if original_scores else [1.0] * len(documents)
            return documents, scores
        
        if not documents:
            logger.warning("âš ï¸ No documents to rerank")
            return [], []
        
        try:
            logger.warning(f"ğŸ”„ Starting reranking for {len(documents)} documents...")
            # æ„å»ºquery-documentå¯¹
            pairs = [[query, doc] for doc in documents]
            
            # Tokenize
            with torch.no_grad():
                inputs = self.reranker_tokenizer(
                    pairs,
                    padding=True,
                    truncation=True,
                    return_tensors='pt',
                    max_length=512
                )
                
                # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
                if self.reranker_device == 'cuda':
                    inputs = {k: v.cuda() for k, v in inputs.items()}
                
                # è®¡ç®—rerankingåˆ†æ•°
                outputs = self.reranker_model(**inputs, return_dict=True)
                rerank_scores = outputs.logits.view(-1).float().cpu().numpy().tolist()
            
            # æŒ‰åˆ†æ•°é™åºæ’åº
            indexed_docs = list(zip(documents, rerank_scores))
            indexed_docs.sort(key=lambda x: x[1], reverse=True)
            
            reranked_docs = [doc for doc, _ in indexed_docs]
            reranked_scores = [score for _, score in indexed_docs]
            
            logger.warning(
                f"âœ… Reranked {len(documents)} documents. "
                f"Score range: [{min(reranked_scores):.3f}, {max(reranked_scores):.3f}]"
            )
            
            return reranked_docs, reranked_scores
            
        except Exception as e:
            logger.warning(f"âŒ Reranking error: {e}")
            import traceback
            logger.warning(traceback.format_exc())
            # å¦‚æœrerankingå¤±è´¥ï¼Œè¿”å›åŸå§‹ç»“æœ
            scores = original_scores if original_scores else [1.0] * len(documents)
            logger.warning(f"âš ï¸ Returning original results due to reranking error")
            return documents, scores
    
    def _retrieve_internal(
        self,
        query: str,
        k: int = 3,
        return_scores: bool = False,
        where_filter: Optional[Dict] = None,
    ) -> Tuple[List[str], bool, Optional[List[float]]]:
        try:
            results = self.collection.query(
                query_texts=[query],
                n_results=k,
                where=where_filter or None,
                include=['documents', 'distances', 'metadatas']
            )

            documents = results['documents'][0] if results['documents'] else []
            distances = results['distances'][0] if results['distances'] else []
            metadatas = results['metadatas'][0] if results['metadatas'] else []

            similarities = [1.0 / (1.0 + d) for d in distances]
            
            # æ³¨æ„ï¼šåœ¨åº”ç”¨rerankingä¹‹å‰å…ˆæ£€æŸ¥åŸºæœ¬æˆåŠŸæ¡ä»¶
            # å¦‚æœåˆå§‹æ£€ç´¢å¤±è´¥ï¼Œrerankingä¹Ÿä¸ä¼šå¸®åŠ©
            success = self._check_success(similarities)

            if not success:
                logger.info(
                    "Retrieval failed for query: '%s...' (max similarity: %.3f)",
                    query[:50],
                    max(similarities) if similarities else 0.0,
                )
                return ([], False, [] if return_scores else None)

            formatted_docs = []
            doc_metadata = []
            for doc, meta in zip(documents, metadatas):
                meta = meta or {}
                source = meta.get('source') or meta.get('doc_id') or 'unknown'
                formatted_doc = f"[Source: {source}] {doc}"
                formatted_docs.append(formatted_doc)
                doc_metadata.append(meta)
            
            # åº”ç”¨Rerankingï¼ˆå¦‚æœå¯ç”¨ï¼‰
            logger.warning(f"ğŸ”µ Reranker check: use_reranker={self.use_reranker}, model exists={self.reranker_model is not None}")
            if self.use_reranker and self.reranker_model is not None:
                logger.warning(f"âœ… Applying reranker for query: '{query[:50]}...'")
                # æå–åŸå§‹æ–‡æ¡£æ–‡æœ¬ï¼ˆä¸å«Sourceæ ‡è®°ï¼‰ç”¨äºreranking
                original_docs = documents
                try:
                    reranked_docs, rerank_scores = self._rerank_documents(
                        query, original_docs, similarities
                    )
                    logger.warning(f"âœ… Reranking completed: {len(reranked_docs)} documents reranked")
                except Exception as rerank_error:
                    logger.warning(f"âŒ Reranking failed: {rerank_error}")
                    import traceback
                    logger.warning(traceback.format_exc())
                    # å¦‚æœrerankingå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ç»“æœ
                    reranked_docs = original_docs
                    rerank_scores = similarities
                
                # é‡æ–°æ ¼å¼åŒ–æ–‡æ¡£ï¼ˆä¿æŒSourceä¿¡æ¯ï¼‰
                reranked_formatted = []
                doc_map = {doc: (formatted, meta) for doc, formatted, meta in 
                          zip(original_docs, formatted_docs, doc_metadata)}
                
                for doc in reranked_docs:
                    if doc in doc_map:
                        reranked_formatted.append(doc_map[doc][0])
                
                formatted_docs = reranked_formatted
                similarities = rerank_scores
                
                # ä½¿ç”¨WARNINGçº§åˆ«ç¡®ä¿æ—¥å¿—è¾“å‡º
                logger.warning(
                    "âœ… Retrieved and reranked %s documents for query: '%s...'",
                    len(formatted_docs),
                    query[:50],
                )
            else:
                # ä½¿ç”¨WARNINGçº§åˆ«è¾“å‡ºï¼Œæ–¹ä¾¿è°ƒè¯•
                if not self.use_reranker:
                    logger.warning(f"âš ï¸ Reranker disabled (use_reranker=False) for query: '{query[:50]}...'")
                elif self.reranker_model is None:
                    logger.warning(f"âš ï¸ Reranker model is None (use_reranker={self.use_reranker}) for query: '{query[:50]}...' - reranking skipped")
                else:
                    logger.warning(f"âš ï¸ Reranker check failed (use_reranker={self.use_reranker}, model is None={self.reranker_model is None}) for query: '{query[:50]}...'")
                    
                logger.warning(
                    "Retrieved %s documents (NO reranking) for query: '%s...'",
                    len(formatted_docs),
                    query[:50],
                )

            return formatted_docs, True, similarities if return_scores else None

        except Exception as e:
            logger.error(f"Retrieval error: {e}")
            return ([], False, [] if return_scores else None)

    def retrieve(
        self,
        query: str,
        k: int = 3,
        return_scores: bool = False
    ) -> Tuple[List[str], bool, Optional[List[float]]]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼ˆå•æŸ¥è¯¢ï¼‰
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            k: è¿”å›æ–‡æ¡£æ•°é‡
            return_scores: æ˜¯å¦è¿”å›ç›¸ä¼¼åº¦åˆ†æ•°
        
        Returns:
            (docs, success, scores):
                - docs: æ–‡æ¡£åˆ—è¡¨ï¼ˆå¦‚æœå¤±è´¥åˆ™ä¸ºç©ºåˆ—è¡¨ï¼‰
                - success: æ˜¯å¦æˆåŠŸæ£€ç´¢
                - scores: ç›¸ä¼¼åº¦åˆ†æ•°åˆ—è¡¨ï¼ˆä»…å½“return_scores=Trueï¼‰
        """
        return self._retrieve_internal(query, k, return_scores)

    def retrieve_with_filter(
        self,
        query: str,
        k: int = 3,
        section_filter: Optional[str] = None,
        work_group_filter: Optional[str] = None,
        return_scores: bool = False,
    ) -> Tuple[List[str], bool, Optional[List[float]]]:
        """æ£€ç´¢æ—¶åº”ç”¨section/work groupå…ƒæ•°æ®è¿‡æ»¤"""

        where_filter: Dict[str, str] = {}
        if section_filter:
            where_filter["section_id"] = section_filter
        if work_group_filter:
            where_filter["work_group"] = work_group_filter

        if where_filter:
            logger.debug(
                "Applying metadata filter: %s",
                {k: v for k, v in where_filter.items()},
            )

        return self._retrieve_internal(query, k, return_scores, where_filter or None)
    
    def _check_success(self, similarities: List[float]) -> bool:
        """
        åˆ¤æ–­æ£€ç´¢æ˜¯å¦æˆåŠŸ
        
        ä¸¤ç§æ¨¡å¼ï¼š
        1. threshold: åŸºäºæœ€å¤§ç›¸ä¼¼åº¦æ˜¯å¦è¶…è¿‡é˜ˆå€¼
        2. random: å›ºå®šæ¦‚ç‡p_sï¼ˆç”¨äºå®éªŒå¯¹æ¯”ï¼‰
        
        Args:
            similarities: ç›¸ä¼¼åº¦åˆ—è¡¨
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if not similarities:
            return False
        
        if self.p_s_mode == "threshold":
            # åŸºäºé˜ˆå€¼
            max_sim = max(similarities)
            return max_sim >= self.similarity_threshold
        
        elif self.p_s_mode == "random":
            # å›ºå®šæ¦‚ç‡ï¼ˆç”¨äºå®éªŒï¼‰
            import random
            return random.random() < self.p_s_value
        
        else:
            raise ValueError(f"Unknown p_s_mode: {self.p_s_mode}")
    
    def batch_retrieve(
        self,
        queries: List[str],
        k: int = 3,
        return_scores: bool = False
    ) -> List[Tuple[List[str], bool, Optional[List[float]]]]:
        """
        æ‰¹é‡æ£€ç´¢ï¼ˆåŠ é€Ÿï¼‰
        
        Args:
            queries: æŸ¥è¯¢åˆ—è¡¨
            k: æ¯ä¸ªæŸ¥è¯¢è¿”å›çš„æ–‡æ¡£æ•°
            return_scores: æ˜¯å¦è¿”å›åˆ†æ•°
        
        Returns:
            ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º (docs, success, scores)
        """
        try:
            # æ‰¹é‡æŸ¥è¯¢Chroma
            results = self.collection.query(
                query_texts=queries,
                n_results=k,
                include=['documents', 'distances', 'metadatas']
            )
            
            # è§£æç»“æœ
            batch_results = []
            
            for i in range(len(queries)):
                documents = results['documents'][i] if results['documents'] else []
                distances = results['distances'][i] if results['distances'] else []
                metadatas = results['metadatas'][i] if results['metadatas'] else []
                
                # è½¬æ¢ä¸ºç›¸ä¼¼åº¦
                similarities = [1.0 / (1.0 + d) for d in distances]
                
                # åˆ¤æ–­æˆåŠŸ
                success = self._check_success(similarities)
                
                if not success:
                    if return_scores:
                        batch_results.append(([], False, []))
                    else:
                        batch_results.append(([], False, None))
                    continue
                
                # æ ¼å¼åŒ–æ–‡æ¡£
                formatted_docs = []
                doc_metadata = []
                for doc, meta in zip(documents, metadatas):
                    meta = meta or {}
                    source = meta.get('source', 'unknown')
                    formatted_doc = f"[Source: {source}] {doc}"
                    formatted_docs.append(formatted_doc)
                    doc_metadata.append(meta)
                
                # åº”ç”¨Rerankingï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if self.use_reranker and self.reranker_model is not None:
                    # æå–åŸå§‹æ–‡æ¡£æ–‡æœ¬ç”¨äºreranking
                    original_docs = documents
                    reranked_docs, rerank_scores = self._rerank_documents(
                        queries[i], original_docs, similarities
                    )
                    
                    # é‡æ–°æ ¼å¼åŒ–æ–‡æ¡£ï¼ˆä¿æŒSourceä¿¡æ¯ï¼‰
                    reranked_formatted = []
                    doc_map = {doc: (formatted, meta) for doc, formatted, meta in 
                              zip(original_docs, formatted_docs, doc_metadata)}
                    
                    for doc in reranked_docs:
                        if doc in doc_map:
                            reranked_formatted.append(doc_map[doc][0])
                    
                    formatted_docs = reranked_formatted
                    similarities = rerank_scores
                
                if return_scores:
                    batch_results.append((formatted_docs, True, similarities))
                else:
                    batch_results.append((formatted_docs, True, None))
            
            logger.info(f"Batch retrieved for {len(queries)} queries")
            
            return batch_results
            
        except Exception as e:
            logger.error(f"Batch retrieval error: {e}")
            # è¿”å›æ‰€æœ‰å¤±è´¥
            if return_scores:
                return [([], False, [])] * len(queries)
            else:
                return [([], False, None)] * len(queries)
    
    def get_statistics(self) -> Dict[str, any]:
        """
        è·å–æ£€ç´¢å™¨ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡å­—å…¸
        """
        stats = {
            'collection_name': self.collection_name,
            'total_documents': self.collection.count(),
            'similarity_threshold': self.similarity_threshold,
            'p_s_mode': self.p_s_mode,
            'p_s_value': self.p_s_value
        }
        
        return stats
    
    def search_by_metadata(
        self,
        metadata_filter: Dict,
        k: int = 10
    ) -> List[Dict]:
        """
        åŸºäºå…ƒæ•°æ®è¿‡æ»¤æœç´¢ï¼ˆé«˜çº§åŠŸèƒ½ï¼‰
        
        Args:
            metadata_filter: å…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶ï¼Œä¾‹å¦‚ {"source": "O-RAN.WG4"}
            k: è¿”å›æ•°é‡
        
        Returns:
            åŒ¹é…çš„æ–‡æ¡£åˆ—è¡¨
        """
        try:
            results = self.collection.get(
                where=metadata_filter,
                limit=k,
                include=['documents', 'metadatas']
            )
            
            docs = []
            for doc, meta in zip(results['documents'], results['metadatas']):
                docs.append({
                    'document': doc,
                    'metadata': meta
                })
            
            logger.info(f"Found {len(docs)} documents matching metadata filter")
            
            return docs
            
        except Exception as e:
            logger.error(f"Metadata search error: {e}")
            return []
    
    def generate_answer_from_docs(
        self,
        question: str,
        docs: List[str],
        model,
        tokenizer,
        max_length: int = 256,
        temperature: float = 0.3,
        top_p: float = 0.95,
        original_question: Optional[str] = None,
        options: Optional[List[str]] = None
    ) -> str:
        """
        åŸºäºæ£€ç´¢æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆï¼ˆä½¿ç”¨ARGO V2.0 Promptsï¼‰
        
        è¿™ä¸ªæ–¹æ³•åœ¨æ£€ç´¢æˆåŠŸåè¢«è°ƒç”¨ï¼Œç”¨äºç”ŸæˆåŸºäºæ£€ç´¢å†…å®¹çš„ä¸­é—´ç­”æ¡ˆã€‚
        
        Args:
            question: å­æŸ¥è¯¢é—®é¢˜
            docs: æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
            model: LLMæ¨¡å‹
            tokenizer: å¯¹åº”çš„tokenizer
            max_length: æœ€å¤§ç­”æ¡ˆé•¿åº¦
            temperature: ç”Ÿæˆæ¸©åº¦
            top_p: nucleus samplingå‚æ•°
            original_question: åŸå§‹é—®é¢˜ï¼ˆæ•°æ®é›†å…¨éƒ¨æ˜¯é€‰æ‹©é¢˜ï¼Œæ€»æ˜¯ä¼ é€’ï¼‰
            options: é€‰æ‹©é¢˜é€‰é¡¹åˆ—è¡¨ï¼ˆæ•°æ®é›†å…¨éƒ¨æ˜¯é€‰æ‹©é¢˜ï¼Œæ€»æ˜¯ä¼ é€’ï¼‰
        
        Returns:
            ç”Ÿæˆçš„ç­”æ¡ˆå­—ç¬¦ä¸²
        """
        # æ„å»ºæç¤ºè¯ï¼ˆç›´æ¥ä¼ é€’åŸå§‹é¢˜ç›®å’Œé€‰é¡¹ï¼Œpromptå†…éƒ¨ä¼šåˆ¤æ–­æ˜¯å¦ä½¿ç”¨ï¼‰
        prompt = ARGOPrompts.build_retrieval_answer_prompt(
            question=question,
            retrieved_docs=docs,
            original_question=original_question,
            options=options
        )
        
        # Tokenize
        device = next(model.parameters()).device
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=2048
        ).to(device)
        
        # ç”Ÿæˆç­”æ¡ˆ
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_length,
                temperature=temperature,
                top_p=top_p,
                do_sample=True if temperature > 0 else False,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        
        # è§£ç 
        generated_ids = outputs[0][inputs['input_ids'].shape[1]:]
        answer = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
        
        # æ£€æŸ¥æ˜¯å¦è¿”å›äº†"æœªæ‰¾åˆ°ä¿¡æ¯"ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
        no_info_patterns = [
            "[No information found in O-RAN specs]",
            "No information found in O-RAN specs",
            "no information found in O-RAN specs",
            "```no information found in O-RAN specs```",
            "[No information found",
            "No information found"
        ]
        
        answer_lower = answer.lower()
        if any(pattern.lower() in answer_lower for pattern in no_info_patterns):
            # æ£€æŸ¥æ˜¯å¦åªæœ‰"No information found"è€Œæ²¡æœ‰å…¶ä»–æœ‰ç”¨ä¿¡æ¯
            # å¦‚æœç­”æ¡ˆä¸»è¦æ˜¯"No information found"ï¼Œè®¤ä¸ºæ£€ç´¢æ— æ•ˆ
            if len(answer.strip()) < 200:  # å¦‚æœç­”æ¡ˆå¾ˆçŸ­ï¼Œå¯èƒ½æ˜¯çº¯"No information found"
                logger.warning(f"LLM indicated no information found for: {question[:50]}...")
                return ""
            # å¦‚æœç­”æ¡ˆè¾ƒé•¿ï¼Œå¯èƒ½åŒ…å«ä¸€äº›è§£é‡Šï¼Œä¿ç•™ä½†æ ‡è®°ä¸ºä½è´¨é‡
            logger.warning(f"LLM indicated no information found (but answer has some content): {question[:50]}...")
            # ä»ç„¶è¿”å›ï¼Œä½†ä¼šè¢«æ ‡è®°ä¸ºä½è´¨é‡
        
        logger.info(f"Generated answer from {len(docs)} docs: {answer[:100]}...")
        
        return answer


class MockRetriever(Retriever):
    """
    æ¨¡æ‹Ÿæ£€ç´¢å™¨ï¼ˆç”¨äºæµ‹è¯•ï¼Œä¸ä¾èµ–Chromaï¼‰
    
    æ¨¡æ‹Ÿæ£€ç´¢è¡Œä¸ºï¼Œè¿”å›å›ºå®šå†…å®¹ï¼Œç”¨äºï¼š
    1. å•å…ƒæµ‹è¯•
    2. æ— æ•°æ®åº“ç¯å¢ƒçš„å¼€å‘
    3. å¿«é€ŸåŸå‹éªŒè¯
    """
    
    def __init__(
        self,
        p_s_value: float = 0.8,
        mock_docs: Optional[List[str]] = None
    ):
        """
        Args:
            p_s_value: æˆåŠŸæ¦‚ç‡
            mock_docs: æ¨¡æ‹Ÿè¿”å›çš„æ–‡æ¡£åˆ—è¡¨
        """
        self.p_s_value = p_s_value
        self.mock_docs = mock_docs or [
            "O-RAN specifies latency requirements for different network segments.",
            "The fronthaul latency budget is typically 100-200 microseconds.",
            "Control plane latency should not exceed 10ms for RRC procedures."
        ]
        
        logger.info(f"MockRetriever initialized with p_s={p_s_value}")
    
    def retrieve(
        self,
        query: str,
        k: int = 3,
        return_scores: bool = False
    ) -> Tuple[List[str], bool, Optional[List[float]]]:
        """æ¨¡æ‹Ÿæ£€ç´¢"""
        import random
        
        # éšæœºæˆåŠŸ/å¤±è´¥
        success = random.random() < self.p_s_value
        
        if not success:
            if return_scores:
                return [], False, []
            else:
                return [], False, None
        
        # è¿”å›æ¨¡æ‹Ÿæ–‡æ¡£
        docs = self.mock_docs[:k]
        scores = [0.9 - i*0.1 for i in range(len(docs))] if return_scores else None
        
        if return_scores:
            return docs, True, scores
        else:
            return docs, True, None
    
    def batch_retrieve(
        self,
        queries: List[str],
        k: int = 3,
        return_scores: bool = False
    ) -> List[Tuple[List[str], bool, Optional[List[float]]]]:
        """æ‰¹é‡æ¨¡æ‹Ÿæ£€ç´¢"""
        return [self.retrieve(q, k, return_scores) for q in queries]
    
    def get_statistics(self) -> Dict[str, any]:
        """è¿”å›æ¨¡æ‹Ÿç»Ÿè®¡"""
        return {
            'type': 'MockRetriever',
            'p_s_value': self.p_s_value,
            'num_mock_docs': len(self.mock_docs)
        }
