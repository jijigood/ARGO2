# å®éªŒè„šæœ¬è¯¦ç»†åˆ†æ

**åˆ†ææ—¶é—´**: 2025-10-29  
**åˆ†æå¯¹è±¡**: `Exp_retrieval_cost_impact.py` å’Œ `Exp_retrieval_success_impact.py`

---

## ğŸ“Š å®éªŒæ¦‚è§ˆå¯¹æ¯”

| ç»´åº¦ | å®éªŒ1 (æˆæœ¬å½±å“) | å®éªŒ2 (æˆåŠŸç‡å½±å“) |
|-----|----------------|------------------|
| **è„šæœ¬æ–‡ä»¶** | `Exp_retrieval_cost_impact.py` | `Exp_retrieval_success_impact.py` |
| **ä»£ç è¡Œæ•°** | 632è¡Œ | 648è¡Œ |
| **æ–‡ä»¶å¤§å°** | 22KB | 21KB |
| **æ ¸å¿ƒç±»** | `CostImpactExperiment` | `RetrievalSuccessExperiment` |

---

## ğŸ”¬ å®éªŒ1: æ£€ç´¢æˆæœ¬å½±å“ - è¯¦ç»†åˆ†æ

### åŸºæœ¬é…ç½®

```python
# æ–‡ä»¶: Exp_retrieval_cost_impact.py
class CostImpactExperiment:
    def __init__(
        self,
        config_path: str = "configs/multi_gpu.yaml",
        n_test_questions: int = 100,        # æµ‹è¯•é—®é¢˜æ•°é‡
        difficulty: str = "medium",         # é—®é¢˜éš¾åº¦
        seed: int = 42                      # éšæœºç§å­
    )
```

#### æ•°æ®é›†é…ç½®
- **æ•°æ®æº**: ORAN-Bench-13K
- **æ•°æ®è·¯å¾„**: `ORAN-Bench-13K/Benchmark/`
- **é—®é¢˜æ•°é‡**: 100é“ (ä»9570é“Mediumé—®é¢˜ä¸­æŠ½æ ·)
- **éš¾åº¦çº§åˆ«**: Medium (ä¸­ç­‰éš¾åº¦)
- **éšæœºç§å­**: 42 (ä¿è¯å¯é‡ç°)
- **æ•°æ®æ ¼å¼**: JSONL (æ¯è¡Œä¸€ä¸ªJSONæ•°ç»„)

#### æµ‹è¯•é—®é¢˜åˆ†å¸ƒ
```
æ€»é—®é¢˜æ± :
  - Easy: 1,139é¢˜
  - Medium: 9,570é¢˜  â† ä»è¿™é‡ŒæŠ½æ ·
  - Hard: 3,243é¢˜
  - Total: 13,952é¢˜

å®é™…ä½¿ç”¨: 100é¢˜ (Medium, seed=42)
```

### æ¨¡å‹é…ç½®

**æ³¨æ„**: è¿™ä¸ªå®éªŒä½¿ç”¨**ä»¿çœŸæ¨¡å‹**,ä¸æ˜¯çœŸå®LLM!

#### ä»¿çœŸæ¨¡å‹ç»„ä»¶

1. **è´¨é‡å‡½æ•° (Quality Function)**
```python
def simulate_quality_function(self, U: float) -> float:
    """
    æ¨¡æ‹Ÿè´¨é‡å‡½æ•° Ïƒ(U)
    
    æ¨¡å¼: Linear (çº¿æ€§)
    å…¬å¼: Ïƒ(U) = U / U_max
    """
    mode = "linear"  # ä»configè¯»å–
    U_max = 1.0
    return U / U_max  # ç®€å•çº¿æ€§æ˜ å°„
```

2. **æ£€ç´¢æ¨¡æ‹Ÿ**
```python
def simulate_argo_policy(self, question, theta_cont, theta_star):
    """
    æ¨¡æ‹ŸARGOç­–ç•¥æ‰§è¡Œ
    
    å‚æ•°:
      - delta_r = 0.25  (æ£€ç´¢æˆåŠŸæ—¶Uå¢é‡)
      - p_s = 0.8       (æ£€ç´¢æˆåŠŸæ¦‚ç‡)
      - max_steps = 20  (æœ€å¤§æ­¥æ•°)
    """
    if U < theta_cont:
        # Retrieve action
        if random() < 0.8:  # p_s = 0.8
            U += 0.25       # delta_r
    else:
        # Reason action
        U += 0.08           # delta_p
```

3. **åŸºçº¿ç­–ç•¥æ¨¡æ‹Ÿ**
```python
# Always-Retrieve: å›ºå®šæ£€ç´¢
def simulate_always_retrieve_policy(self, question):
    while U < 0.9:  # å›ºå®štheta_star
        retrieval_count += 1
        if random() < p_s:
            U += delta_r

# Always-Reason: å›ºå®šæ¨ç†
def simulate_always_reason_policy(self, question):
    while U < 0.9:
        reason_count += 1
        U += delta_p

# Random: éšæœº50-50
def simulate_random_policy(self, question):
    while U < 0.9:
        if random() < 0.5:
            retrieval_count += 1
            if random() < p_s:
                U += delta_r
        else:
            reason_count += 1
            U += delta_p
```

### MDPæ±‚è§£å™¨é…ç½®

```python
# ä» configs/multi_gpu.yaml åŠ è½½
config = {
    'mdp': {
        'U_max': 1.0,              # ä¿¡æ¯è¿›åº¦ä¸Šé™
        'delta_r': 0.25,           # æ£€ç´¢å¢é‡ (å›ºå®š)
        'delta_p': 0.08,           # æ¨ç†å¢é‡ (å›ºå®š)
        'p_s': 0.8,                # æ£€ç´¢æˆåŠŸç‡ (å›ºå®š)
        'c_r': [0.02 ~ 0.20],      # æ£€ç´¢æˆæœ¬ (å˜é‡!æ‰«æ10ä¸ªå€¼)
        'c_p': 0.02,               # æ¨ç†æˆæœ¬ (å›ºå®š)
        'mu': 0.6,                 # è´¨é‡æƒé‡
        'gamma': 0.98,             # æŠ˜æ‰£å› å­
        'U_grid_size': 101         # çŠ¶æ€ç©ºé—´ç¦»æ•£åŒ–ç²’åº¦
    },
    'quality': {
        'mode': 'linear',          # è´¨é‡å‡½æ•°ç±»å‹
        'k': 5.0                   # å‚æ•°k (linearæ¨¡å¼ä¸‹æœªä½¿ç”¨)
    },
    'solver': {
        'max_iterations': 1000,    # Value Iterationæœ€å¤§è¿­ä»£æ¬¡æ•°
        'convergence_threshold': 1e-6,  # æ”¶æ•›é˜ˆå€¼
        'verbose': False           # ä¸æ‰“å°è¯¦ç»†æ—¥å¿—
    }
}
```

### å®éªŒå‚æ•°æ‰«æ

```python
def run_experiment(
    self,
    c_r_min_multiplier: float = 1.0,   # c_ræœ€å°å€¼ = 1.0 * c_p
    c_r_max_multiplier: float = 10.0,  # c_ræœ€å¤§å€¼ = 10.0 * c_p
    n_steps: int = 10                  # æ‰«ææ­¥æ•°
):
    """
    æ‰«æc_rä»c_påˆ°10*c_p, 10ä¸ªå‡åŒ€åˆ†å¸ƒçš„ç‚¹
    """
    c_r_values = np.linspace(
        1.0 * 0.02,   # 0.020
        10.0 * 0.02,  # 0.200
        10
    )
    # ç»“æœ: [0.02, 0.04, 0.06, 0.08, 0.10, 0.12, 0.14, 0.16, 0.18, 0.20]
```

**å®é™…æµ‹è¯•çš„c_rå€¼:**
```
c_r = 0.020 (1.0x c_p)
c_r = 0.040 (2.0x c_p)
c_r = 0.060 (3.0x c_p)
c_r = 0.080 (4.0x c_p)
c_r = 0.100 (5.0x c_p)
c_r = 0.120 (6.0x c_p)
c_r = 0.140 (7.0x c_p)
c_r = 0.160 (8.0x c_p)
c_r = 0.180 (9.0x c_p)
c_r = 0.200 (10.0x c_p)
```

### è¯„ä¼°ç­–ç•¥

å¯¹æ¯ä¸ªc_rå€¼,è¯„ä¼°4ç§ç­–ç•¥:

```python
policies = {
    'ARGO': lambda q: self.simulate_argo_policy(q, theta_cont, theta_star),
    'Always-Retrieve': self.simulate_always_retrieve_policy,
    'Always-Reason': self.simulate_always_reason_policy,
    'Random': self.simulate_random_policy
}

# æ¯ä¸ªç­–ç•¥åœ¨100é“é—®é¢˜ä¸Šè¿è¡Œ
for question in self.test_questions:  # 100é¢˜
    result = policy_fn(question)
    # è®°å½•: quality, retrieval_count, reason_count, steps
```

### è®¡ç®—å¤æ‚åº¦

**æ€»è¿è¡Œæ¬¡æ•°**:
```
10 (c_rå€¼) Ã— 4 (ç­–ç•¥) Ã— 100 (é—®é¢˜) = 4,000 æ¬¡ç­–ç•¥æ‰§è¡Œ
10 (c_rå€¼) Ã— 1 (MDPæ±‚è§£) = 10 æ¬¡ Value Iteration
```

**å•æ¬¡Value Iterationå¤æ‚åº¦**:
```
çŠ¶æ€æ•°: 101 (U_grid_size)
åŠ¨ä½œæ•°: 3 (Retrieve, Reason, Terminate)
è¿­ä»£æ¬¡æ•°: ~100-200æ¬¡ (é€šå¸¸å¿«é€Ÿæ”¶æ•›)

å¤æ‚åº¦: O(101 Ã— 3 Ã— 200) â‰ˆ 60,600 æ¬¡çŠ¶æ€æ›´æ–°
```

**å®é™…è¿è¡Œæ—¶é—´**: ~2åˆ†é’Ÿ (æ— GPU,çº¯CPUä»¿çœŸ)

---

## ğŸ”¬ å®éªŒ2: æ£€ç´¢æˆåŠŸç‡å½±å“ - è¯¦ç»†åˆ†æ

### åŸºæœ¬é…ç½®

```python
# æ–‡ä»¶: Exp_retrieval_success_impact.py
class RetrievalSuccessExperiment:
    def __init__(
        self,
        config_path: str = "configs/multi_gpu.yaml",
        n_test_questions: int = 100,        # æµ‹è¯•é—®é¢˜æ•°é‡
        difficulty: str = "medium",         # é—®é¢˜éš¾åº¦
        seed: int = 42                      # éšæœºç§å­
    )
```

#### æ•°æ®é›†é…ç½®
- **æ•°æ®æº**: ORAN-Bench-13K (ä¸å®éªŒ1ç›¸åŒ)
- **é—®é¢˜æ•°é‡**: 100é“ (ç›¸åŒæŠ½æ ·,seed=42)
- **éš¾åº¦çº§åˆ«**: Medium
- **ä½¿ç”¨ç›¸åŒçš„100é“é¢˜**: ä¿è¯å®éªŒå¯æ¯”æ€§

### æ¨¡å‹é…ç½®

åŒæ ·ä½¿ç”¨**ä»¿çœŸæ¨¡å‹**,ä½†æœ‰å…³é”®å·®å¼‚:

#### å…³é”®å‚æ•°å˜åŒ–

```python
# å®éªŒ1å›ºå®šp_s, å˜åŒ–c_r
p_s = 0.8         (å›ºå®š)
c_r = [0.02~0.20] (å˜é‡)

# å®éªŒ2å›ºå®šc_r, å˜åŒ–p_s  
p_s = [0.3~1.0]   (å˜é‡)
c_r = 0.05        (å›ºå®š)
```

#### ä»¿çœŸæ¨¡å‹è°ƒæ•´

```python
def simulate_argo_policy(self, question, theta_cont, theta_star, p_s):
    """
    ä¸å®éªŒ1çš„åŒºåˆ«: p_sæ˜¯å‚æ•°!
    """
    max_steps = 30  # å¢åŠ åˆ°30 (å› ä¸ºä½p_så¯èƒ½éœ€è¦æ›´å¤šæ­¥)
    
    while U < theta_star and step < max_steps:
        if U < theta_cont:
            retrieval_count += 1
            if random() < p_s:  # ä½¿ç”¨å˜åŒ–çš„p_s!
                U += delta_r
        else:
            reason_count += 1
            U += delta_p
```

**ä¸ºä»€ä¹ˆmax_steps=30?**
- ä½p_sæ—¶(å¦‚0.3),æ£€ç´¢æˆåŠŸç‡ä½
- Always-Retrieveå¯èƒ½éœ€è¦å¾ˆå¤šæ¬¡é‡è¯•
- é¿å…æ— é™å¾ªç¯

### MDPæ±‚è§£å™¨é…ç½®

```python
config = {
    'mdp': {
        'U_max': 1.0,
        'delta_r': 0.25,           # å›ºå®š
        'delta_p': 0.08,           # å›ºå®š
        'p_s': [0.3 ~ 1.0],        # å˜é‡!æ‰«æ8ä¸ªå€¼
        'c_r': 0.05,               # å›ºå®š
        'c_p': 0.02,               # å›ºå®š
        'mu': 0.6,
        'gamma': 0.98,
        'U_grid_size': 101
    },
    'quality': {
        'mode': 'linear',
        'k': 5.0
    },
    'solver': {
        'max_iterations': 1000,
        'convergence_threshold': 1e-6,
        'verbose': False
    }
}
```

### å®éªŒå‚æ•°æ‰«æ

```python
def run_experiment(
    self,
    p_s_min: float = 0.3,    # æœ€å°æˆåŠŸç‡30%
    p_s_max: float = 1.0,    # æœ€å¤§æˆåŠŸç‡100%
    n_steps: int = 8         # æ‰«æ8ä¸ªç‚¹
):
    """
    æ‰«æp_sä»0.3åˆ°1.0, 8ä¸ªå‡åŒ€åˆ†å¸ƒçš„ç‚¹
    """
    p_s_values = np.linspace(0.3, 1.0, 8)
    # ç»“æœ: [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
```

**å®é™…æµ‹è¯•çš„p_så€¼:**
```
p_s = 0.30 (30%æˆåŠŸç‡)
p_s = 0.40 (40%æˆåŠŸç‡)
p_s = 0.50 (50%æˆåŠŸç‡)
p_s = 0.60 (60%æˆåŠŸç‡)
p_s = 0.70 (70%æˆåŠŸç‡)
p_s = 0.80 (80%æˆåŠŸç‡)
p_s = 0.90 (90%æˆåŠŸç‡)
p_s = 1.00 (100%æˆåŠŸç‡)
```

### è®¡ç®—å¤æ‚åº¦

**æ€»è¿è¡Œæ¬¡æ•°**:
```
8 (p_så€¼) Ã— 4 (ç­–ç•¥) Ã— 100 (é—®é¢˜) = 3,200 æ¬¡ç­–ç•¥æ‰§è¡Œ
8 (p_så€¼) Ã— 1 (MDPæ±‚è§£) = 8 æ¬¡ Value Iteration
```

**å®é™…è¿è¡Œæ—¶é—´**: ~2åˆ†é’Ÿ

---

## ğŸ“Š ä¸¤ä¸ªå®éªŒçš„å¯¹æ¯”æ€»ç»“

### æ•°æ®å±‚é¢

| ç»´åº¦ | å®éªŒ1 | å®éªŒ2 | è¯´æ˜ |
|-----|-------|-------|------|
| æ•°æ®é›† | ORAN-Bench-13K | ORAN-Bench-13K | ç›¸åŒ |
| é—®é¢˜æ•°é‡ | 100é¢˜ | 100é¢˜ | ç›¸åŒ |
| éš¾åº¦ | Medium | Medium | ç›¸åŒ |
| éšæœºç§å­ | 42 | 42 | **ç›¸åŒ100é¢˜!** |
| é—®é¢˜æ ¼å¼ | JSONL | JSONL | ç›¸åŒ |

### å‚æ•°å±‚é¢

| å‚æ•° | å®éªŒ1 | å®éªŒ2 |
|-----|-------|-------|
| **è‡ªå˜é‡** | c_r (æ£€ç´¢æˆæœ¬) | p_s (æ£€ç´¢æˆåŠŸç‡) |
| æ‰«æèŒƒå›´ | 0.02 ~ 0.20 | 0.3 ~ 1.0 |
| æ‰«æç‚¹æ•° | 10ä¸ª | 8ä¸ª |
| delta_r | 0.25 (å›ºå®š) | 0.25 (å›ºå®š) |
| delta_p | 0.08 (å›ºå®š) | 0.08 (å›ºå®š) |
| p_s | 0.8 (å›ºå®š) | å˜é‡ |
| c_r | å˜é‡ | 0.05 (å›ºå®š) |
| c_p | 0.02 (å›ºå®š) | 0.02 (å›ºå®š) |
| gamma | 0.98 (å›ºå®š) | 0.98 (å›ºå®š) |

### æ¨¡å‹å±‚é¢

| ç»„ä»¶ | å®éªŒ1 | å®éªŒ2 | è¯´æ˜ |
|-----|-------|-------|------|
| **LLMæ¨¡å‹** | âŒ æ—  | âŒ æ—  | ä½¿ç”¨ä»¿çœŸ |
| **åµŒå…¥æ¨¡å‹** | âŒ æ—  | âŒ æ—  | ä½¿ç”¨ä»¿çœŸ |
| **æ£€ç´¢å™¨** | âŒ æ—  | âŒ æ—  | ä½¿ç”¨ä»¿çœŸ |
| è´¨é‡å‡½æ•° | Linear | Linear | ç›¸åŒ |
| MDPæ±‚è§£å™¨ | Value Iteration | Value Iteration | ç›¸åŒ |
| çŠ¶æ€ç©ºé—´ | 101ç»´ | 101ç»´ | ç›¸åŒ |
| åŠ¨ä½œç©ºé—´ | 3ä¸ª | 3ä¸ª | ç›¸åŒ |

**é‡è¦**: è¿™ä¸¤ä¸ªå®éªŒéƒ½æ˜¯**çº¯ä»¿çœŸå®éªŒ**,ä¸éœ€è¦åŠ è½½ä»»ä½•LLMæˆ–åµŒå…¥æ¨¡å‹!

### è®¡ç®—èµ„æº

| èµ„æº | å®éªŒ1 | å®éªŒ2 |
|-----|-------|-------|
| **GPUéœ€æ±‚** | âŒ ä¸éœ€è¦ | âŒ ä¸éœ€è¦ |
| **CPU** | âœ… å•æ ¸è¶³å¤Ÿ | âœ… å•æ ¸è¶³å¤Ÿ |
| **å†…å­˜** | ~500MB | ~500MB |
| **è¿è¡Œæ—¶é—´** | ~2åˆ†é’Ÿ | ~2åˆ†é’Ÿ |
| **ç£ç›˜** | ~3KB (JSON) | ~3KB (JSON) |

### è¾“å‡ºå±‚é¢

| è¾“å‡º | å®éªŒ1 | å®éªŒ2 |
|-----|-------|-------|
| å›¾è¡¨æ•°é‡ | 3å¼  | 3å¼  |
| æ•°æ®æ–‡ä»¶ | 1ä¸ªJSON | 1ä¸ªJSON |
| æŠ¥å‘Šæ–‡æ¡£ | 1ä¸ªMD | 1ä¸ªMD |
| æ ¸å¿ƒå›¾ | cost_vs_retrievals | ps_vs_retrievals |

---

## ğŸ¯ å…³é”®è®¾è®¡å†³ç­–

### ä¸ºä»€ä¹ˆä½¿ç”¨ä»¿çœŸè€ŒéçœŸå®LLM?

#### ä¼˜ç‚¹:
1. **é€Ÿåº¦å¿«**: 2åˆ†é’Ÿ vs æ•°å°æ—¶(çœŸå®LLM)
2. **å¯æ§**: å‚æ•°ç¡®å®š,ç»“æœå¯é‡ç°
3. **æˆæœ¬ä½**: æ— éœ€GPU,æ— APIè´¹ç”¨
4. **ä¸“æ³¨MDP**: éªŒè¯MDPæ±‚è§£å™¨,è€ŒéLLMæ€§èƒ½

#### ç¼ºç‚¹:
1. **çœŸå®æ€§**: æ— æ³•åæ˜ çœŸå®RAGæ€§èƒ½
2. **è´¨é‡ç®€åŒ–**: Linearå‡½æ•°è¿‡äºç®€å•
3. **é€‚ç”¨æ€§**: éœ€è¦åç»­çœŸå®LLMéªŒè¯

### ä¸ºä»€ä¹ˆé€‰æ‹©100é¢˜?

1. **å¹³è¡¡**: è¶³å¤Ÿç»Ÿè®¡æ„ä¹‰,ä¸ä¼šå¤ªæ…¢
2. **å¯é‡ç°**: seed=42å›ºå®šæŠ½æ ·
3. **å¯æ‰©å±•**: å¯è½»æ¾æ”¹ä¸º1000é¢˜

### ä¸ºä»€ä¹ˆå›ºå®šå…¶ä»–å‚æ•°?

**å•å˜é‡æ§åˆ¶æ³•**:
- å®éªŒ1: åªå˜c_r,å›ºå®šp_s
- å®éªŒ2: åªå˜p_s,å›ºå®šc_r
- ç›®çš„: æ¸…æ™°å±•ç¤ºå•ä¸€å‚æ•°çš„å½±å“

---

## ğŸ’¡ å¦‚ä½•æ”¹ä¸ºçœŸå®LLMå®éªŒ?

å¦‚æœè¦ä½¿ç”¨çœŸå®æ¨¡å‹,éœ€è¦ä¿®æ”¹:

### 1. åŠ è½½LLMå’ŒåµŒå…¥æ¨¡å‹

```python
# åœ¨__init__ä¸­æ·»åŠ 
from transformers import AutoModelForCausalLM, AutoTokenizer

self.model_name = "Qwen/Qwen2.5-7B-Instruct"
self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
self.model = AutoModelForCausalLM.from_pretrained(
    self.model_name,
    device_map="auto",
    torch_dtype=torch.float16
)
```

### 2. é›†æˆçœŸå®æ£€ç´¢

```python
from chromadb import Client
from sentence_transformers import SentenceTransformer

self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
self.chroma_client = Client()
self.collection = self.chroma_client.get_collection("oran_specs")
```

### 3. æ›¿æ¢ä»¿çœŸå‡½æ•°

```python
def real_argo_policy(self, question, theta_cont, theta_star):
    """ä½¿ç”¨çœŸå®RAGç³»ç»Ÿ"""
    U = 0.0
    
    while U < theta_star:
        if U < theta_cont:
            # çœŸå®æ£€ç´¢
            docs = self.collection.query(
                query_texts=[question],
                n_results=5
            )
            context = docs['documents']
            
            # çœŸå®LLMæ¨ç†
            prompt = f"Context: {context}\n\nQuestion: {question}"
            inputs = self.tokenizer(prompt, return_tensors="pt")
            outputs = self.model.generate(**inputs, max_new_tokens=100)
            answer = self.tokenizer.decode(outputs[0])
            
            # è¯„ä¼°è´¨é‡(éœ€è¦ground truthæˆ–è¯„åˆ¤æ¨¡å‹)
            quality = evaluate_answer(answer, question['correct_answer'])
            U += quality
        else:
            # çœŸå®æ¨ç†
            # ...
```

### 4. è°ƒæ•´å‚æ•°

```python
# çœŸå®å®éªŒéœ€è¦æ›´å°‘é—®é¢˜(å› ä¸ºå¾ˆæ…¢)
n_test_questions = 20  # è€Œé100
n_steps = 5            # è€Œé10

# éœ€è¦GPU
device = "cuda:0"
```

### 5. é¢„è®¡èµ„æº

**çœŸå®LLMå®éªŒ**:
- GPU: è‡³å°‘1å¼ A100 (40GB)
- æ—¶é—´: æ¯é¢˜~30ç§’,æ€»è®¡~10åˆ†é’Ÿ(20é¢˜)
- å†…å­˜: ~20GB GPU VRAM
- æˆæœ¬: å¦‚æœç”¨API,~$1-5

---

## ğŸ“ˆ å®éªŒæ•°æ®æµ

### å®éªŒ1æ•°æ®æµ

```
è¾“å…¥:
  â”œâ”€ ORAN-Bench-13K/Benchmark/fin_M.json (9570é¢˜)
  â”œâ”€ configs/multi_gpu.yaml (MDPå‚æ•°)
  â””â”€ seed=42
         â†“
  [æŠ½æ ·100é¢˜] (ORANBenchmark.sample_questions)
         â†“
  [å¾ªç¯10æ¬¡,c_rä»0.02åˆ°0.20]
    â”œâ”€ [MDPæ±‚è§£] (MDPSolver.solve)
    â”‚    â””â”€ è¾“å‡º: Î¸_cont, Î¸*
    â”‚
    â”œâ”€ [è¯„ä¼°ARGOç­–ç•¥] (100é¢˜ Ã— simulate_argo_policy)
    â”œâ”€ [è¯„ä¼°Always-Retrieve] (100é¢˜)
    â”œâ”€ [è¯„ä¼°Always-Reason] (100é¢˜)
    â””â”€ [è¯„ä¼°Random] (100é¢˜)
         â†“
  [èšåˆç»“æœ]
    â”œâ”€ å¹³å‡è´¨é‡: 1.000
    â”œâ”€ å¹³å‡æ£€ç´¢æ¬¡æ•°: 5.1 â†’ 0.0
    â””â”€ å¹³å‡æ¨ç†æ¬¡æ•°: ...
         â†“
  [ä¿å­˜]
    â”œâ”€ JSON: draw_figs/data/exp1_*.json (3.2KB)
    â””â”€ PNG: figs/exp1_*.png (3å¼ ,482KB)
```

### å®éªŒ2æ•°æ®æµ

```
è¾“å…¥:
  â”œâ”€ ç›¸åŒçš„100é¢˜ (seed=42)
  â”œâ”€ configs/multi_gpu.yaml
  â””â”€ p_sèŒƒå›´: 0.3~1.0
         â†“
  [å¾ªç¯8æ¬¡,p_sä»0.3åˆ°1.0]
    â”œâ”€ [MDPæ±‚è§£] (p_så˜åŒ–)
    â”‚    â””â”€ è¾“å‡º: Î¸_cont, Î¸*
    â”‚
    â”œâ”€ [è¯„ä¼°ARGO] (p_sä½œä¸ºå‚æ•°)
    â”œâ”€ [è¯„ä¼°Always-Retrieve] (p_så½±å“ç»“æœ)
    â”œâ”€ [è¯„ä¼°Always-Reason] (p_sæ— å½±å“)
    â””â”€ [è¯„ä¼°Random] (p_så½±å“ç»“æœ)
         â†“
  [èšåˆç»“æœ]
    â”œâ”€ p_s=0.3: ARGO 0æ¬¡æ£€ç´¢, Always-R 12.7æ¬¡
    â”œâ”€ p_s=1.0: ARGO 1æ¬¡æ£€ç´¢, Always-R 4.0æ¬¡
    â””â”€ ...
         â†“
  [ä¿å­˜]
    â”œâ”€ JSON: draw_figs/data/exp2_*.json (3.4KB)
    â””â”€ PNG: figs/exp2_*.png (3å¼ ,707KB)
```

---

## ğŸ” ä»£ç è´¨é‡åˆ†æ

### ä»£ç ç»“æ„

```python
# ä¸¤ä¸ªè„šæœ¬éƒ½é‡‡ç”¨ç›¸åŒçš„ç±»ç»“æ„
class Experiment:
    __init__()              # åˆå§‹åŒ–,åŠ è½½æ•°æ®
    create_mdp_config()     # åˆ›å»ºMDPé…ç½®
    solve_mdp()             # æ±‚è§£MDP
    simulate_quality_function()  # è´¨é‡å‡½æ•°
    simulate_argo_policy()       # ARGOç­–ç•¥ä»¿çœŸ
    simulate_always_retrieve()   # Always-Retrieveä»¿çœŸ
    simulate_always_reason()     # Always-Reasonä»¿çœŸ
    simulate_random()            # Randomä»¿çœŸ
    evaluate_all_policies()      # è¯„ä¼°æ‰€æœ‰ç­–ç•¥
    run_experiment()             # ä¸»å®éªŒå¾ªç¯
    save_results()               # ä¿å­˜JSON
    plot_results()               # ç»˜å›¾
```

### ä»£ç å¤ç”¨

**å…±äº«é€»è¾‘** (~70%ä»£ç ç›¸åŒ):
- æ•°æ®åŠ è½½
- MDPé…ç½®ç”Ÿæˆ
- ä»¿çœŸå‡½æ•°ç»“æ„
- ç»“æœä¿å­˜
- ç»˜å›¾é€»è¾‘

**å·®å¼‚ç‚¹** (~30%):
- å‚æ•°æ‰«æ (c_r vs p_s)
- ä»¿çœŸå‡½æ•°å‚æ•°ä¼ é€’
- å›¾è¡¨æ ‡é¢˜å’Œæ ‡ç­¾

### æ”¹è¿›å»ºè®®

1. **æå–åŸºç±»**: åˆ›å»º`BaseExperiment`,å‡å°‘ä»£ç é‡å¤
2. **é…ç½®é©±åŠ¨**: ç”¨YAMLé…ç½®å®éªŒå‚æ•°
3. **å¹¶è¡ŒåŒ–**: ä½¿ç”¨å¤šè¿›ç¨‹åŠ é€Ÿç­–ç•¥è¯„ä¼°
4. **æ—¥å¿—**: æ·»åŠ loggingè€Œéprint

---

## ğŸ“ æ€»ç»“

### å®éªŒ1æ ¸å¿ƒè¦ç´ 
- **ç›®æ ‡**: éªŒè¯æˆæœ¬è‡ªé€‚åº”æ€§
- **æ•°æ®**: 100é¢˜Mediuméš¾åº¦
- **æ¨¡å‹**: ä»¿çœŸ(æ— LLM)
- **å‚æ•°**: æ‰«æc_r (10ä¸ªå€¼)
- **è¾“å‡º**: 3å¼ å›¾,1ä¸ªJSON
- **æ—¶é—´**: 2åˆ†é’Ÿ
- **ç»“è®º**: ARGOæ£€ç´¢æ¬¡æ•°ä»5.1é™è‡³0

### å®éªŒ2æ ¸å¿ƒè¦ç´ 
- **ç›®æ ‡**: éªŒè¯ä¸ç¡®å®šæ€§ç®¡ç†
- **æ•°æ®**: ç›¸åŒ100é¢˜
- **æ¨¡å‹**: ä»¿çœŸ(æ— LLM)
- **å‚æ•°**: æ‰«æp_s (8ä¸ªå€¼)
- **è¾“å‡º**: 3å¼ å›¾,1ä¸ªJSON
- **æ—¶é—´**: 2åˆ†é’Ÿ
- **ç»“è®º**: ä½p_sæ—¶ARGOé¿å…æ£€ç´¢

### å…³é”®ç‰¹ç‚¹
âœ… å¿«é€Ÿ: 2åˆ†é’Ÿå®Œæˆ  
âœ… å¯æ§: ä»¿çœŸä¿è¯å¯é‡ç°  
âœ… è½»é‡: æ— éœ€GPU  
âœ… ä¸“æ³¨: éªŒè¯MDPç†è®º  
âš ï¸ é™åˆ¶: éœ€è¦çœŸå®LLMéªŒè¯  

---

**æ–‡æ¡£ç”Ÿæˆæ—¶é—´**: 2025-10-29 01:10  
**åˆ†æè€…**: GitHub Copilot
