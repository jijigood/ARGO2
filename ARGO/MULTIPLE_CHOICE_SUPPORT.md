# é€‰æ‹©é¢˜æ”¯æŒè¯´æ˜

**æ›´æ–°æ—¶é—´**: 2024å¹´11æœˆ3æ—¥  
**ç‰ˆæœ¬**: ARGO Prompts V2.1

---

## âœ¨ æ–°å¢åŠŸèƒ½

ARGOç³»ç»Ÿç°å·²æ”¯æŒ**å¤šé€‰ä¸€é€‰æ‹©é¢˜**ï¼ˆMultiple Choice Questionsï¼‰æ ¼å¼ï¼Œä¸“ä¸ºO-RAN Benchmarkæ•°æ®é›†ï¼ˆfin_H_clean.jsonï¼‰ä¼˜åŒ–ã€‚

---

## ğŸ“‹ æ•°æ®é›†æ ¼å¼

O-RAN Benchmarkæ•°æ®é›† (`fin_H_clean.json`) æ ¼å¼ï¼š

```json
[
  "What is a key function of the O-RAN Fronthaul CUS Plane specification?",
  [
    "1. Support for slice differentiation to meet specific SLAs.",
    "2. Optimizing power consumption for the gNB DU system.",
    "3. Managing network security protocols.",
    "4. Determining the optimal frequency band for transmission."
  ],
  "1"
]
```

- **é—®é¢˜**: å­—ç¬¦ä¸²
- **é€‰é¡¹**: 4ä¸ªé€‰é¡¹çš„åˆ—è¡¨ï¼ˆæ ‡å·1-4ï¼‰
- **æ­£ç¡®ç­”æ¡ˆ**: "1"ã€"2"ã€"3" æˆ– "4"

---

## ğŸ”§ ä½¿ç”¨æ–¹æ³•

### æ–¹æ³•1: åŸºç¡€ç”¨æ³•

```python
from src.argo_system import ARGOSystem

# åˆå§‹åŒ–ç³»ç»Ÿ
argo = ARGOSystem(
    model_name="Qwen/Qwen2.5-1.5B-Instruct",
    retriever_mode="chroma",
    chroma_dir="chroma_db",
    use_mdp=True,
    verbose=True
)

# å‡†å¤‡é—®é¢˜å’Œé€‰é¡¹
question = "What is a key function of the O-RAN Fronthaul CUS Plane?"
options = [
    "Support for slice differentiation to meet specific SLAs.",
    "Optimizing power consumption for the gNB DU system.",
    "Managing network security protocols.",
    "Determining the optimal frequency band for transmission."
]

# å›ç­”é€‰æ‹©é¢˜
answer, choice, history, metadata = argo.answer_question(
    question=question,
    options=options,
    return_history=True
)

print(f"è¯¦ç»†ç­”æ¡ˆ: {answer}")
print(f"é€‰æ‹©çš„é€‰é¡¹: {choice}")  # è¾“å‡º: "1", "2", "3", æˆ– "4"
```

### æ–¹æ³•2: ä½¿ç”¨Benchmark Loader

```python
import json
from src.argo_system import ARGOSystem

# åŠ è½½æ•°æ®é›†
with open('ORAN-Bench-13K/Benchmark/fin_H_clean.json', 'r') as f:
    dataset = json.load(f)

# åˆå§‹åŒ–ç³»ç»Ÿ
argo = ARGOSystem(
    model_name="Qwen/Qwen2.5-1.5B-Instruct",
    retriever_mode="chroma",
    chroma_dir="chroma_db"
)

# å¤„ç†æ¯ä¸ªé—®é¢˜
results = []
for item in dataset[:10]:  # å¤„ç†å‰10é¢˜
    question_text = item[0]
    options = item[1]  # ["1. ...", "2. ...", "3. ...", "4. ..."]
    correct_answer = item[2]  # "1", "2", "3", æˆ– "4"
    
    # æ¸…ç†é€‰é¡¹ï¼ˆç§»é™¤ "1. ", "2. " ç­‰å‰ç¼€ï¼‰
    clean_options = [opt.split('. ', 1)[1] for opt in options]
    
    # ARGOæ¨ç†
    answer, predicted_choice, _, metadata = argo.answer_question(
        question=question_text,
        options=clean_options
    )
    
    # è¯„ä¼°
    is_correct = (predicted_choice == correct_answer)
    results.append({
        'question': question_text,
        'predicted': predicted_choice,
        'correct': correct_answer,
        'is_correct': is_correct,
        'steps': metadata['total_steps']
    })

# è®¡ç®—å‡†ç¡®ç‡
accuracy = sum(r['is_correct'] for r in results) / len(results)
print(f"å‡†ç¡®ç‡: {accuracy*100:.2f}%")
```

### æ–¹æ³•3: æ‰¹é‡å¤„ç†

```python
from src.argo_system import ARGOSystem
import json

# åŠ è½½æ•°æ®
with open('ORAN-Bench-13K/Benchmark/fin_H_clean.json', 'r') as f:
    dataset = json.load(f)

# åˆå§‹åŒ–ç³»ç»Ÿ
argo = ARGOSystem(
    model_name="Qwen/Qwen2.5-1.5B-Instruct",
    retriever_mode="chroma",
    chroma_dir="chroma_db",
    verbose=False  # å…³é—­è¯¦ç»†è¾“å‡ºåŠ å¿«é€Ÿåº¦
)

# æ‰¹é‡æ¨ç†
predictions = []
ground_truth = []

for item in dataset:
    question_text = item[0]
    options = [opt.split('. ', 1)[1] for opt in item[1]]
    correct_answer = item[2]
    
    # æ¨ç†ï¼ˆç®€åŒ–ç‰ˆï¼Œä¸è¿”å›å†å²ï¼‰
    _, choice, _, _ = argo.answer_question(
        question=question_text,
        options=options,
        return_history=False
    )
    
    predictions.append(choice if choice else "1")  # é»˜è®¤é€‰é¡¹1
    ground_truth.append(correct_answer)

# è¯„ä¼°
from sklearn.metrics import accuracy_score, classification_report

accuracy = accuracy_score(ground_truth, predictions)
print(f"\næ•´ä½“å‡†ç¡®ç‡: {accuracy*100:.2f}%")
print("\nè¯¦ç»†æŠ¥å‘Š:")
print(classification_report(ground_truth, predictions, 
                          target_names=["Option 1", "Option 2", "Option 3", "Option 4"]))
```

---

## ğŸ” è¾“å‡ºæ ¼å¼

### LLMç”Ÿæˆæ ¼å¼

LLMä¼šç”Ÿæˆä»¥ä¸‹æ ¼å¼çš„è¾“å‡ºï¼š

```xml
<answer long>
Based on the retrieved information from O-RAN specifications, the Control-User-Synchronization (CUS) Plane specification for fronthaul interface provides support for slice differentiation to meet specific Service Level Agreements (SLAs). This is mentioned in [O-RAN.WG4] specification which describes how different network slices can be configured with distinct QoS parameters...
</answer long>

<answer short>
Option 1 is correct because O-RAN fronthaul CUS Plane specification includes slice differentiation capabilities for meeting specific SLAs.
</answer short>

<choice>1</choice>
```

### è§£æåè¿”å›

```python
answer, choice, history, metadata = argo.answer_question(...)

# answer (str): è¯¦ç»†è§£é‡Š
"Based on the retrieved information from O-RAN specifications..."

# choice (str): "1", "2", "3", æˆ– "4"
"1"

# history (List[Dict]): æ¨ç†å†å²
[
    {
        'action': 'retrieve',
        'subquery': 'What is the CUS Plane in O-RAN fronthaul?',
        'retrieval_success': True,
        'retrieved_docs': [...],
        'intermediate_answer': '...',
        'confidence': 0.85,
        'progress': 0.35
    },
    ...
]

# metadata (Dict): å…ƒæ•°æ®
{
    'total_steps': 3,
    'final_uncertainty': 0.15,
    'retrieve_count': 2,
    'reason_count': 1,
    'successful_retrievals': 2,
    'elapsed_time': 5.23,
    'sources': ['O-RAN.WG4', 'O-RAN Security']
}
```

---

## ğŸ¯ Promptå·¥ç¨‹

### Synthesis Instruction

æ–°çš„synthesis instructionä¸“é—¨é’ˆå¯¹é€‰æ‹©é¢˜ä¼˜åŒ–ï¼š

```python
SYNTHESIS_INSTRUCTION = """You are an expert at synthesizing comprehensive answers from multi-step reasoning for O-RAN multiple-choice questions.

Task: Generate a complete, accurate answer to the original question based on the reasoning history, and select the correct option.

Guidelines:
1. Integrate ALL retrieved information
2. Use insights from intermediate reasoning steps
3. Analyze each option carefully based on gathered evidence
4. Provide a coherent, well-structured reasoning process
5. Cite sources when possible (e.g., O-RAN.WG4)
6. If information is insufficient, state what's missing
7. Clearly indicate the correct option number (1, 2, 3, or 4)

Format for Multiple Choice Questions:
<answer long>Detailed reasoning and explanation for why the correct option is chosen...</answer long>
<answer short>Option X is correct because [brief justification]</answer short>
<choice>X</choice>

where X is the option number (1, 2, 3, or 4).
"""
```

### é€‰é¡¹æ˜¾ç¤º

åœ¨synthesis promptä¸­ï¼Œé€‰é¡¹ä¼šè‡ªåŠ¨æ ¼å¼åŒ–ï¼š

```
Original Question: What is a key function of the O-RAN Fronthaul CUS Plane?

Options:
1. Support for slice differentiation to meet specific SLAs.
2. Optimizing power consumption for the gNB DU system.
3. Managing network security protocols.
4. Determining the optimal frequency band for transmission.

Retrieved Information:
[1] [O-RAN.WG4] The fronthaul CUS-Plane specification defines...
...

Analyze each option based on the evidence above and select the correct answer:
```

---

## ğŸ”¬ é²æ£’æ€§å¤„ç†

### å›é€€æœºåˆ¶

å¦‚æœLLMæ²¡æœ‰ç”Ÿæˆ `<choice>X</choice>` æ ‡ç­¾ï¼Œç³»ç»Ÿä¼šå°è¯•ä»æ–‡æœ¬ä¸­æå–ï¼š

```python
# æå–é€»è¾‘ï¼ˆåœ¨ synthesizer._postprocess_answer ä¸­ï¼‰
choice_match = re.search(r'<choice>(\d)</choice>', answer)
if choice_match:
    choice = choice_match.group(1)
else:
    # å›é€€ï¼šæŸ¥æ‰¾ "Option 3" æˆ– "é€‰é¡¹3"
    fallback_match = re.search(r'[Oo]ption\s*(\d)|é€‰é¡¹\s*(\d)', answer)
    if fallback_match:
        choice = fallback_match.group(1) or fallback_match.group(2)
```

### é»˜è®¤å€¼

å¦‚æœå®Œå…¨æ— æ³•æå–é€‰é¡¹ï¼Œ`choice` ä¼šè¿”å› `None`ï¼š

```python
answer, choice, _, _ = argo.answer_question(question, options)
if choice is None:
    print("è­¦å‘Š: æ— æ³•ä»LLMè¾“å‡ºä¸­æå–é€‰é¡¹")
    choice = "1"  # ä½¿ç”¨é»˜è®¤å€¼
```

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. è°ƒæ•´ç”Ÿæˆå‚æ•°

```python
from src.synthesizer import AnswerSynthesizer

synthesizer = AnswerSynthesizer(
    model=model,
    tokenizer=tokenizer,
    max_answer_length=256,  # é€‰æ‹©é¢˜ä¸éœ€è¦å¤ªé•¿çš„ç­”æ¡ˆ
    temperature=0.3,        # è¾ƒä½æ¸©åº¦æé«˜ç¡®å®šæ€§
    top_p=0.95
)
```

### 2. ä½¿ç”¨å›ºå®šç­–ç•¥åŠ é€Ÿ

```python
# å¯¹äºé€‰æ‹©é¢˜ï¼Œå¯ä»¥ä½¿ç”¨å›ºå®šç­–ç•¥å‡å°‘æ¨ç†æ­¥æ•°
argo = ARGOSystem(
    model_name="Qwen/Qwen2.5-1.5B-Instruct",
    retriever_mode="chroma",
    use_mdp=False,  # ç¦ç”¨MDPï¼Œä½¿ç”¨å›ºå®šç­–ç•¥
    max_steps=3     # é™åˆ¶æœ€å¤§æ­¥æ•°
)
```

### 3. æ‰¹é‡å¤„ç†

```python
# ä½¿ç”¨ batch_synthesize åŠ é€Ÿï¼ˆéœ€è¦æ‰‹åŠ¨æ„å»ºhistoryï¼‰
questions = [...]
histories = [...]
options_list = [...]

results = synthesizer.batch_synthesize(
    questions=questions,
    histories=histories,
    options_list=options_list
)
```

---

## ğŸ§ª æµ‹è¯•ç¤ºä¾‹

å®Œæ•´æµ‹è¯•è„šæœ¬ï¼š`test_multiple_choice.py`

```python
"""æµ‹è¯•é€‰æ‹©é¢˜åŠŸèƒ½"""
import json
from src.argo_system import ARGOSystem

def test_single_question():
    """æµ‹è¯•å•ä¸ªé€‰æ‹©é¢˜"""
    argo = ARGOSystem(
        model_name="Qwen/Qwen2.5-1.5B-Instruct",
        retriever_mode="chroma",
        chroma_dir="chroma_db",
        verbose=True
    )
    
    question = "What is the role of the SM Fanout module in an O-DU when an E2 message is received?"
    options = [
        "It interacts with the E2 handler module to send the message to the appropriate internal module.",
        "It consults the SM Catalog module to identify the relevant SM specific modules and APIs.",
        "It maps E2 messages to their corresponding receiver modules and message contents.",
        "It sends the E2 message through the E2 Sender module."
    ]
    correct = "2"
    
    answer, choice, history, metadata = argo.answer_question(
        question=question,
        options=options
    )
    
    print(f"\né—®é¢˜: {question}")
    print(f"\nç­”æ¡ˆ: {answer}")
    print(f"\né€‰æ‹©: {choice}")
    print(f"æ­£ç¡®ç­”æ¡ˆ: {correct}")
    print(f"ç»“æœ: {'âœ… æ­£ç¡®' if choice == correct else 'âŒ é”™è¯¯'}")
    print(f"\næ¨ç†æ­¥æ•°: {metadata['total_steps']}")
    print(f"è€—æ—¶: {metadata['elapsed_time']:.2f}ç§’")

if __name__ == "__main__":
    test_single_question()
```

---

## ğŸ“ æ³¨æ„äº‹é¡¹

### âœ… æ¨èåšæ³•

1. **æ¸…ç†é€‰é¡¹æ ¼å¼**: ç§»é™¤ "1. ", "2. " ç­‰å‰ç¼€
2. **ä½¿ç”¨fin_H_clean.json**: å·²ç§»é™¤å¼‚å¸¸æ•°æ®çš„æ¸…æ´—ç‰ˆæœ¬
3. **è®¾ç½®åˆç†çš„max_steps**: é€‰æ‹©é¢˜é€šå¸¸2-4æ­¥å³å¯
4. **è®°å½•æ¨ç†å†å²**: ä¾¿äºåˆ†æé”™è¯¯åŸå› 

### âš ï¸ æ³¨æ„äº‹é¡¹

1. **é€‰é¡¹é¡ºåº**: ç¡®ä¿é€‰é¡¹åˆ—è¡¨é¡ºåºä¸æ•°æ®é›†ä¸€è‡´
2. **ç­”æ¡ˆæ ¼å¼**: æ­£ç¡®ç­”æ¡ˆå¿…é¡»æ˜¯ "1", "2", "3", "4"ï¼ˆå­—ç¬¦ä¸²ï¼‰
3. **LLMèƒ½åŠ›**: å°æ¨¡å‹å¯èƒ½éš¾ä»¥å‡†ç¡®ç†è§£å¤æ‚çš„O-RANæŠ€æœ¯é—®é¢˜
4. **æ£€ç´¢è´¨é‡**: ç­”æ¡ˆå‡†ç¡®æ€§é«˜åº¦ä¾èµ–äºæ£€ç´¢åˆ°çš„æ–‡æ¡£è´¨é‡

### ğŸ› å·²çŸ¥é™åˆ¶

1. **ä»…æ”¯æŒå•é€‰é¢˜**: ä¸æ”¯æŒå¤šé€‰é¢˜æˆ–åˆ¤æ–­é¢˜
2. **å›ºå®š4ä¸ªé€‰é¡¹**: ä¸æ”¯æŒ2-3ä¸ªé€‰é¡¹çš„é¢˜ç›®
3. **è¯­è¨€é™åˆ¶**: ä¸»è¦é’ˆå¯¹è‹±æ–‡O-RANæœ¯è¯­ä¼˜åŒ–

---

## ğŸ”„ å‘åå…¼å®¹

### æ™®é€šé—®ç­”ä»ç„¶æ”¯æŒ

```python
# ä¸æä¾› options å‚æ•°ï¼Œæ­£å¸¸å·¥ä½œ
answer, choice, history, metadata = argo.answer_question(
    question="Explain the O-RAN E2 interface"
)

# choice å°†ä¸º None
assert choice is None
```

### è¿”å›å€¼å˜åŒ–

| ç‰ˆæœ¬ | è¿”å›æ ¼å¼ |
|------|----------|
| V2.0 | `(answer, history, metadata)` |
| V2.1 | `(answer, choice, history, metadata)` |

**è¿ç§»å»ºè®®**: åœ¨ç°æœ‰ä»£ç ä¸­æ·»åŠ  `choice` æ¥æ”¶å˜é‡å³å¯ã€‚

---

## ğŸ“š ç›¸å…³æ–‡ä»¶

| æ–‡ä»¶ | è¯´æ˜ |
|------|------|
| `src/prompts.py` | æ›´æ–°SYNTHESIS_INSTRUCTIONå’Œbuild_synthesis_prompt |
| `src/synthesizer.py` | æ·»åŠ optionså‚æ•°å’Œchoiceæå–é€»è¾‘ |
| `src/argo_system.py` | æ›´æ–°answer_questionæ–¹æ³•ç­¾å |
| `ORAN-Bench-13K/Benchmark/fin_H_clean.json` | æ¸…æ´—åçš„3224é¢˜O-RANé€‰æ‹©é¢˜æ•°æ®é›† |
| `DATA_CLEANING_SUMMARY.md` | æ•°æ®æ¸…æ´—è¯¦ç»†æŠ¥å‘Š |
| `ORAN_TERMINOLOGY_CHECK.md` | O-RANæœ¯è¯­ä½¿ç”¨æ£€æŸ¥ |

---

**ç‰ˆæƒ**: ARGO Team  
**è®¸å¯**: MIT License  
**æ›´æ–°æ—¥æœŸ**: 2024å¹´11æœˆ3æ—¥

---

## ğŸ‰ å¿«é€Ÿå¼€å§‹

```bash
# 1. å‡†å¤‡ç¯å¢ƒ
cd /data/user/huangxiaolin/ARGO2/ARGO

# 2. è¿è¡Œæµ‹è¯•
python test_multiple_choice.py

# 3. åœ¨æ•°æ®é›†ä¸Šè¯„ä¼°
python run_benchmark_evaluation.py --dataset fin_H_clean.json --max_samples 100
```

ç¥æ‚¨ä½¿ç”¨æ„‰å¿«ï¼ğŸš€
