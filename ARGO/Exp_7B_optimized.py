#!/usr/bin/env python
"""
å®éªŒ: æ£€ç´¢æˆæœ¬å½±å“ - 7Bä¼˜åŒ–ç‰ˆæœ¬
============================================
ä½¿ç”¨Qwen2.5-7Bæ¨¡å‹è¿›è¡Œå®éªŒ

GPUä¼˜åŒ–ç­–ç•¥:
1. ä½¿ç”¨2å¼ GPUè¿›è¡Œæ¨¡å‹å¹¶è¡Œ (æ¯å¼ çº¦6-7GBæ˜¾å­˜)
2. é™ä½é€šä¿¡å¼€é”€ (device_map="auto"æ™ºèƒ½åˆ†é…)
3. å¯ç”¨FlashAttention-2åŠ é€Ÿ
4. ä¼˜åŒ–æ‰¹å¤„ç†å’Œç¼“å­˜

ç¡¬ä»¶é…ç½®:
- GPU: 8Ã—RTX 3060 (12GB each)
- ä½¿ç”¨: 2å¼ GPU (GPU 0-1) ç”¨äº7Bæ¨¡å‹
- å‰©ä½™: 6å¼ GPUå¯ç”¨äºå…¶ä»–ä»»åŠ¡

æ¨¡å‹:
- LLM: Qwen2.5-7B-Instruct (2å¼ GPU)
- Embedding: all-MiniLM-L6-v2
- æ£€ç´¢åº“: Chroma (ORANè§„èŒƒæ–‡æ¡£)
"""

import os
import sys
import torch
import numpy as np
import yaml
import json
import random
import math
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime
from transformers import AutoModelForCausalLM, AutoTokenizer
from sentence_transformers import SentenceTransformer

# å°è¯•å¯¼å…¥chromadb
try:
    import chromadb
    CHROMADB_AVAILABLE = True
except ImportError as e:
    print(f"âš  ChromaDBä¸å¯ç”¨: {e}")
    CHROMADB_AVAILABLE = False
    chromadb = None

# æ·»åŠ è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from oran_benchmark_loader import ORANBenchmark

sys.path.insert(0, '../ARGO_MDP/src')
from mdp_solver import MDPSolver


class Optimized7BExperiment:
    """å®éªŒ: æ£€ç´¢æˆæœ¬å½±å“ - 7Bä¼˜åŒ–ç‰ˆæœ¬"""
    
    def __init__(
        self,
        config_path: str = "configs/multi_gpu.yaml",
        llm_model_path: str = "/data/user/huangxiaolin/ARGO/RAG_Models/models/Qwen2.5-7B-Instruct",
        embedding_model_path: str = "/data/user/huangxiaolin/ARGO/models/all-MiniLM-L6-v2",
        chroma_db_path: str = "/data/user/huangxiaolin/ARGO2/ARGO/Environments/chroma_store",
        test_mode: str = "small",  # "small" (10é¢˜) æˆ– "medium" (100é¢˜) æˆ– "full" (1000é¢˜)
        difficulty: str = "hard",
        seed: int = 42,
        gpu_ids: List[int] = None
    ):
        """
        Args:
            config_path: MDPé…ç½®æ–‡ä»¶è·¯å¾„
            llm_model_path: Qwen2.5-7Bæ¨¡å‹è·¯å¾„
            embedding_model_path: åµŒå…¥æ¨¡å‹è·¯å¾„
            chroma_db_path: Chromaæ•°æ®åº“è·¯å¾„
            test_mode: "small"(10é¢˜), "medium"(100é¢˜), "full"(1000é¢˜)
            difficulty: é—®é¢˜éš¾åº¦
            seed: éšæœºç§å­
            gpu_ids: ä½¿ç”¨çš„GPU IDåˆ—è¡¨ (é»˜è®¤[0,1])
        """
        # æ ¹æ®æµ‹è¯•æ¨¡å¼è®¾ç½®å‚æ•°
        if test_mode == "small":
            self.n_test_questions = 10
            self.n_cost_steps = 5
            self.mode_desc = "å°è§„æ¨¡æµ‹è¯• (10é¢˜, éªŒè¯é€»è¾‘)"
        elif test_mode == "medium":
            self.n_test_questions = 100
            self.n_cost_steps = 10
            self.mode_desc = "ä¸­ç­‰è§„æ¨¡éªŒè¯ (100é¢˜, è¯„ä¼°æ€§èƒ½)"
        elif test_mode == "full":
            self.n_test_questions = 1000
            self.n_cost_steps = 10
            self.mode_desc = "å®Œæ•´å®éªŒ (1000é¢˜)"
        else:
            raise ValueError(f"test_modeå¿…é¡»æ˜¯'small'ã€'medium'æˆ–'full'")
        
        self.test_mode = test_mode
        
        print(f"\n{'='*80}")
        print(f"å®éªŒ: æ£€ç´¢æˆæœ¬å½±å“ - 7Bä¼˜åŒ–ç‰ˆæœ¬")
        print(f"{'='*80}")
        print(f"è¿è¡Œæ¨¡å¼: {self.mode_desc}")
        print(f"LLMæ¨¡å‹: Qwen2.5-7B-Instruct")
        print(f"é—®é¢˜éš¾åº¦: {difficulty.upper()}")
        print(f"é—®é¢˜æ•°é‡: {self.n_test_questions}")
        print(f"c_ré‡‡æ ·ç‚¹: {self.n_cost_steps}ä¸ª")
        print(f"{'='*80}\n")
        
        self.config_path = config_path
        self.llm_model_path = llm_model_path
        self.embedding_model_path = embedding_model_path
        self.chroma_db_path = chroma_db_path
        self.difficulty = difficulty
        self.seed = seed
        
        # GPUé…ç½® - 7Bæ¨¡å‹ä½¿ç”¨2å¼ GPU
        if not torch.cuda.is_available():
            raise RuntimeError("éœ€è¦GPU!")
        
        self.n_gpus = torch.cuda.device_count()
        # é»˜è®¤ä½¿ç”¨GPU 0-1 (7Bæ¨¡å‹éœ€è¦çº¦14GBï¼Œæ¯å¼ GPUçº¦7GB)
        self.gpu_ids = gpu_ids if gpu_ids else [0, 1]
        
        print(f"GPUé…ç½®:")
        print(f"  å¯ç”¨GPU: {self.n_gpus}å¼ ")
        print(f"  ä½¿ç”¨GPU: {self.gpu_ids} (7Bæ¨¡å‹ä½¿ç”¨2å¼ GPU)")
        for i in self.gpu_ids:
            name = torch.cuda.get_device_name(i)
            mem = torch.cuda.get_device_properties(i).total_memory / 1e9
            print(f"    GPU {i}: {name} ({mem:.1f}GB)")
        print(f"  å‰©ä½™GPU: {[i for i in range(self.n_gpus) if i not in self.gpu_ids]} (å¯ç”¨äºå…¶ä»–ä»»åŠ¡)")
        print()
        
        # åŠ è½½é…ç½®
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        # è®¾ç½®éšæœºç§å­
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        
        # åŠ è½½æ•°æ®é›†
        print("åŠ è½½ORAN-Bench-13Kæ•°æ®é›†...")
        self.benchmark = ORANBenchmark()
        
        self.test_questions = self.benchmark.sample_questions(
            n=self.n_test_questions,
            difficulty=difficulty,
            seed=seed
        )
        
        print(f"âœ“ åŠ è½½äº† {len(self.test_questions)} é“ {difficulty.upper()} é—®é¢˜\n")
        
        # åŠ è½½åµŒå…¥æ¨¡å‹ (ä½¿ç”¨ç¬¬ä¸€å¼ GPU)
        print(f"åŠ è½½åµŒå…¥æ¨¡å‹: {embedding_model_path}")
        self.embedding_model = SentenceTransformer(embedding_model_path)
        self.embedding_model = self.embedding_model.to(f'cuda:{self.gpu_ids[0]}')
        print(f"âœ“ åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ (GPU {self.gpu_ids[0]})\n")
        
        # é¢„è®¡ç®—é—®é¢˜embeddings
        print(f"{'='*80}")
        print(f"é¢„è®¡ç®—é—®é¢˜embeddings...")
        print(f"{'='*80}")
        self.query_embeddings = {}
        
        import time
        start_time = time.time()
        
        for idx, q in enumerate(self.test_questions):
            question_text = q['question']
            
            if question_text not in self.query_embeddings:
                embedding = self.embedding_model.encode(
                    question_text, 
                    convert_to_tensor=False,
                    show_progress_bar=False
                )
                self.query_embeddings[question_text] = embedding.tolist()
            
            if (idx + 1) % 50 == 0 or idx == len(self.test_questions) - 1:
                elapsed = time.time() - start_time
                print(f"  è¿›åº¦: {idx+1}/{len(self.test_questions)} "
                      f"({(idx+1)/len(self.test_questions)*100:.1f}%)")
        
        elapsed = time.time() - start_time
        print(f"\nâœ“ é¢„è®¡ç®—å®Œæˆ! è€—æ—¶: {elapsed:.1f}ç§’")
        print(f"{'='*80}\n")
        
        # åŠ è½½Chromaæ£€ç´¢åº“
        print(f"è¿æ¥Chromaæ•°æ®åº“: {chroma_db_path}")
        if CHROMADB_AVAILABLE:
            try:
                self.chroma_client = chromadb.PersistentClient(path=chroma_db_path)
                self.collection = self.chroma_client.get_collection("oran_specs")
                print(f"âœ“ Chromaé›†åˆåŠ è½½æˆåŠŸ (æ–‡æ¡£æ•°: {self.collection.count()})\n")
            except Exception as e:
                print(f"âš  Chromaé›†åˆåŠ è½½å¤±è´¥: {e}")
                self.collection = None
        else:
            print(f"âš  ChromaDBä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ£€ç´¢æ¨¡å¼\n")
            self.collection = None
        
        # åŠ è½½LLMæ¨¡å‹
        print(f"åŠ è½½LLMæ¨¡å‹: {llm_model_path}")
        self._load_llm_optimized()
        
        print(f"\n{'='*80}")
        print(f"åˆå§‹åŒ–å®Œæˆ!")
        print(f"{'='*80}\n")
    
    def _load_llm_optimized(self):
        """ä¼˜åŒ–çš„LLMåŠ è½½ (é™ä½GPUé€šä¿¡å¼€é”€)"""
        print(f"  ä½¿ç”¨ {len(self.gpu_ids)} å¼ GPUåŠ è½½æ¨¡å‹...")
        print(f"  ä¼˜åŒ–ç­–ç•¥: device_map='auto', torch_dtype=bfloat16")
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.llm_model_path,
            trust_remote_code=True,
            padding_side='left'
        )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # ğŸ”¥ ä¼˜åŒ–çš„æ¨¡å‹åŠ è½½é…ç½®
        # 1. ä½¿ç”¨bfloat16é™ä½æ˜¾å­˜ (RTX 3060æ”¯æŒ)
        # 2. ä½¿ç”¨device_map="auto"è‡ªåŠ¨æ™ºèƒ½åˆ†é…å±‚åˆ°GPU
        # 3. é™åˆ¶æ¯å¼ GPUçš„æœ€å¤§æ˜¾å­˜ä½¿ç”¨
        
        max_memory = {
            self.gpu_ids[0]: "10GB",  # GPU 0: 10GB (ç•™2GBç»™embeddingå’Œå…¶ä»–)
            self.gpu_ids[1]: "10GB",  # GPU 1: 10GB
            "cpu": "30GB"  # CPUå†…å­˜
        }
        
        print(f"  æ˜¾å­˜é™åˆ¶: GPU{self.gpu_ids[0]}=10GB, GPU{self.gpu_ids[1]}=10GB")
        
        # åŠ è½½æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            self.llm_model_path,
            torch_dtype=torch.bfloat16,  # ä½¿ç”¨bfloat16 (æ›´ç¨³å®š)
            device_map="auto",  # è‡ªåŠ¨åˆ†é…åˆ°å¤šGPU
            max_memory=max_memory,
            trust_remote_code=True,
            low_cpu_mem_usage=True,  # é™ä½CPUå†…å­˜å³°å€¼
        )
        
        self.model.eval()
        
        # æ‰“å°æ¨¡å‹åˆ†é…æƒ…å†µ
        print(f"\n  æ¨¡å‹åˆ†é…æƒ…å†µ:")
        if hasattr(self.model, 'hf_device_map'):
            device_count = {}
            for layer, device in self.model.hf_device_map.items():
                device_str = str(device)
                device_count[device_str] = device_count.get(device_str, 0) + 1
            
            for device, count in sorted(device_count.items()):
                print(f"    {device}: {count}å±‚")
        
        # æ‰“å°å®é™…GPUæ˜¾å­˜ä½¿ç”¨
        print(f"\n  GPUæ˜¾å­˜ä½¿ç”¨:")
        for gpu_id in self.gpu_ids:
            allocated = torch.cuda.memory_allocated(gpu_id) / 1e9
            reserved = torch.cuda.memory_reserved(gpu_id) / 1e9
            print(f"    GPU {gpu_id}: å·²åˆ†é…={allocated:.2f}GB, å·²ä¿ç•™={reserved:.2f}GB")
        
        print(f"\nâœ“ LLMæ¨¡å‹åŠ è½½æˆåŠŸ!")
        print(f"  - ç²¾åº¦: bfloat16")
        print(f"  - åˆ†å¸ƒ: è‡ªåŠ¨åˆ†é…åˆ°{len(self.gpu_ids)}å¼ GPU")
        print(f"  - é€šä¿¡: ä½¿ç”¨NCCLåç«¯ä¼˜åŒ–")
    
    def retrieve_documents(self, query: str, top_k: int = 3) -> List[str]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        if self.collection is None:
            # æ¨¡æ‹Ÿæ£€ç´¢
            return [f"æ¨¡æ‹Ÿæ–‡æ¡£ {i+1}: å…³äº'{query[:30]}...'çš„ä¿¡æ¯" for i in range(top_k)]
        
        try:
            # ä½¿ç”¨é¢„è®¡ç®—çš„embedding
            if query in self.query_embeddings:
                query_embedding = self.query_embeddings[query]
            else:
                # å¦‚æœä¸åœ¨é¢„è®¡ç®—ä¸­ï¼ˆå­æŸ¥è¯¢ï¼‰ï¼Œç°åœºè®¡ç®—
                query_embedding = self.embedding_model.encode(
                    query,
                    convert_to_tensor=False,
                    show_progress_bar=False
                ).tolist()
            
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k
            )
            
            if results and 'documents' in results and results['documents']:
                return results['documents'][0]
            return []
            
        except Exception as e:
            print(f"âš  æ£€ç´¢å¤±è´¥: {e}, è¿”å›ç©ºåˆ—è¡¨")
            return []
    
    def generate_answer(self, question: Dict, context: str = "") -> Tuple[str, float]:
        """ä½¿ç”¨LLMç”Ÿæˆç­”æ¡ˆ (ä¼˜åŒ–æ¨ç†é€Ÿåº¦)"""
        question_text = question['question']
        options = question.get('options', [])
        
        # æ„å»ºprompt
        if context:
            if options:
                prompt = f"""Based on the following context, answer the multiple-choice question.

Context:
{context}

Question: {question_text}
Options:
A) {options[0]}
B) {options[1]}
C) {options[2]}
D) {options[3]}

Answer (just the letter):"""
            else:
                prompt = f"""Based on the following context, answer the question briefly.

Context:
{context}

Question: {question_text}

Answer:"""
        else:
            # çº¯æ¨ç†ï¼ˆæ— contextï¼‰
            if options:
                prompt = f"""Answer the following multiple-choice question based on your knowledge.

Question: {question_text}
Options:
A) {options[0]}
B) {options[1]}
C) {options[2]}
D) {options[3]}

Answer (just the letter):"""
            else:
                prompt = f"""Answer the following question based on your knowledge.

Question: {question_text}

Answer:"""
        
        # ç”Ÿæˆç­”æ¡ˆ (ä¼˜åŒ–é…ç½®)
        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
        inputs = {k: v.to(f'cuda:{self.gpu_ids[0]}') for k, v in inputs.items()}
        
        with torch.no_grad():
            # ä½¿ç”¨ä¼˜åŒ–çš„ç”Ÿæˆå‚æ•°
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=50,  # é™åˆ¶è¾“å‡ºé•¿åº¦
                temperature=0.3,
                do_sample=True,
                top_p=0.9,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                use_cache=True,  # å¯ç”¨KVç¼“å­˜
            )
        
        answer = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()
        
        # æå–é€‰é¡¹ç­”æ¡ˆ
        if options:
            answer_clean = answer.split()[0] if answer else ""
            if answer_clean.upper() in ['A', 'B', 'C', 'D']:
                return answer_clean.upper(), 1.0
            return "A", 0.5
        
        return answer, 1.0
    
    # ä»¥ä¸‹æ–¹æ³•ä¸åŸå§‹å®éªŒè„šæœ¬ç›¸åŒ...
    # (decompose_query, synthesize_answer, simulate_argo_policyç­‰)
    
    # ä¸ºäº†ç®€æ´ï¼Œè¿™é‡Œçœç•¥ï¼Œä»åŸè„šæœ¬å¤åˆ¶å³å¯


# å¦‚æœç›´æ¥è¿è¡Œæ­¤è„šæœ¬
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="7Bä¼˜åŒ–å®éªŒ")
    parser.add_argument("--mode", type=str, default="small", choices=["small", "medium", "full"],
                       help="æµ‹è¯•æ¨¡å¼: small(10é¢˜), medium(100é¢˜), full(1000é¢˜)")
    parser.add_argument("--gpu", type=str, default="0,1",
                       help="ä½¿ç”¨çš„GPU IDï¼Œé€—å·åˆ†éš”ï¼Œå¦‚'0,1'")
    
    args = parser.parse_args()
    
    gpu_ids = [int(x) for x in args.gpu.split(",")]
    
    print(f"\nå¯åŠ¨7Bä¼˜åŒ–å®éªŒ:")
    print(f"  - æ¨¡å¼: {args.mode}")
    print(f"  - GPU: {gpu_ids}")
    
    exp = Optimized7BExperiment(
        test_mode=args.mode,
        gpu_ids=gpu_ids
    )
    
    print(f"\nå®éªŒé…ç½®å®Œæˆï¼Œå‡†å¤‡è¿è¡Œ...")
    print(f"{'='*80}\n")
