#!/usr/bin/env python3
"""
æ·±å…¥åˆ†æ Reward Shaping å¯¹ MDP çš„å½±å“
æ£€æŸ¥ç†è®ºä¸å®è·µçš„ä¸€è‡´æ€§
"""

import sys
import os
import numpy as np

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'ARGO_MDP', 'src')))
from mdp_solver import MDPSolver


def analyze_shaping_effect():
    """åˆ†æ reward shaping çš„å½±å“"""
    
    print("=" * 80)
    print("Reward Shaping ç†è®ºåˆ†æ")
    print("=" * 80)
    
    # é…ç½®
    config_base = {
        'mdp': {
            'delta_r': 0.25,
            'delta_p': 0.08,
            'c_r': 0.05,
            'c_p': 0.02,
            'p_s': 0.8,
            'gamma': 0.98,
            'U_max': 1.0,
            'mu': 0.6,
            'U_grid_size': 101
        },
        'quality': {
            'mode': 'linear',
            'k': 1.0
        },
        'reward_shaping': {
            'enabled': False,
            'k': 1.0
        },
        'solver': {
            'max_iterations': 1000,
            'convergence_threshold': 1e-6,
            'verbose': False
        }
    }
    
    # æµ‹è¯•ä¸åŒçŠ¶æ€ä¸‹çš„ Q å€¼
    test_U = 0.0  # åˆå§‹çŠ¶æ€
    
    print(f"\nåˆ†æçŠ¶æ€ U = {test_U}")
    print("-" * 80)
    
    # æ—  shaping
    print("\nã€æ—  Reward Shapingã€‘")
    solver_no_shaping = MDPSolver(config_base)
    
    # è®¡ç®— Qå€¼
    actions = ["Retrieve", "Reason", "Terminate"]
    
    for action_idx, action_name in enumerate(actions):
        if action_idx == 2:  # Terminate
            q_val = solver_no_shaping.quality_function(test_U)
            print(f"  Q({test_U}, {action_name}) = {q_val:.4f}")
        else:
            next_states, probs = solver_no_shaping.transition(test_U, action_idx)
            immediate_reward = solver_no_shaping.reward(test_U, action_idx)
            
            print(f"  {action_name}:")
            print(f"    å³æ—¶å¥–åŠ±: {immediate_reward:.4f}")
            print(f"    ä¸‹ä¸€çŠ¶æ€: {next_states} (æ¦‚ç‡: {probs})")
            print(f"    Î´U: {next_states - test_U}")
    
    # æœ‰ shaping (k=1.0)
    print("\nã€æœ‰ Reward Shaping (k=1.0)ã€‘")
    config_with_shaping = config_base.copy()
    config_with_shaping['reward_shaping'] = {'enabled': True, 'k': 1.0}
    solver_with_shaping = MDPSolver(config_with_shaping)
    
    for action_idx, action_name in enumerate(actions):
        if action_idx == 2:  # Terminate
            q_val = solver_with_shaping.quality_function(test_U)
            print(f"  Q({test_U}, {action_name}) = {q_val:.4f}")
        else:
            next_states, probs = solver_with_shaping.transition(test_U, action_idx)
            immediate_reward = solver_with_shaping.reward(test_U, action_idx)
            
            # è®¡ç®— shaping reward
            shaping_rewards = [solver_with_shaping.shaping_reward(test_U, u_next) 
                             for u_next in next_states]
            expected_shaping = sum(p * s for p, s in zip(probs, shaping_rewards))
            
            print(f"  {action_name}:")
            print(f"    å³æ—¶å¥–åŠ±: {immediate_reward:.4f}")
            print(f"    Shapingå¥–åŠ±: {shaping_rewards} (æœŸæœ›: {expected_shaping:.4f})")
            print(f"    æ€»å¥–åŠ±: {immediate_reward + expected_shaping:.4f}")
            print(f"    ä¸‹ä¸€çŠ¶æ€: {next_states} (æ¦‚ç‡: {probs})")
    
    # ç†è®ºåˆ†æ
    print("\n" + "=" * 80)
    print("ç†è®ºåˆ†æ")
    print("=" * 80)
    
    print("\n1. Retrieve åŠ¨ä½œ:")
    print("   æˆåŠŸ (p=0.8): U' = 0.25")
    print("   å¤±è´¥ (p=0.2): U' = 0.00")
    print("   ")
    print("   æ— shaping:")
    print("     R = -0.05")
    print("   ")
    print("   æœ‰shaping (k=1.0, Î¦(U)=U):")
    print("     F_success = 0.98 * 0.25 - 1.0 * 0.0 = 0.245")
    print("     F_fail = 0.98 * 0.0 - 1.0 * 0.0 = 0.0")
    print("     E[F] = 0.8 * 0.245 + 0.2 * 0.0 = 0.196")
    print("     R' = R + E[F] = -0.05 + 0.196 = 0.146")
    
    print("\n2. Reason åŠ¨ä½œ:")
    print("   ç¡®å®šæ€§: U' = 0.08")
    print("   ")
    print("   æ— shaping:")
    print("     R = -0.02")
    print("   ")
    print("   æœ‰shaping (k=1.0):")
    print("     F = 0.98 * 0.08 - 1.0 * 0.0 = 0.0784")
    print("     R' = R + F = -0.02 + 0.0784 = 0.0584")
    
    print("\n3. å…³é”®æ´å¯Ÿ:")
    print("   Shapingä½¿å¾—Retrieveçš„æœ‰æ•ˆå¥–åŠ±ä» -0.05 å˜ä¸º +0.146")
    print("   Shapingä½¿å¾—Reasonçš„æœ‰æ•ˆå¥–åŠ±ä» -0.02 å˜ä¸º +0.0584")
    print("   ")
    print("   è¿™æ”¹å˜äº†åŠ¨ä½œçš„ç›¸å¯¹å¸å¼•åŠ›:")
    print("   - æ— shaping: Reasonæ›´ä¼˜ (-0.02 > -0.05)")
    print("   - æœ‰shaping: Retrieveæ›´ä¼˜ (0.146 > 0.0584)")
    print("   ")
    print("   âš ï¸  è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆé˜ˆå€¼ä¼šæ”¹å˜!")
    print("   è™½ç„¶ç†è®ºä¸Špotential-based shapingåº”è¯¥ä¿æŒæœ€ä¼˜ç­–ç•¥ï¼Œ")
    print("   ä½†è¿™é‡Œçš„Î¦(U)=kUä¸çŠ¶æ€è½¬ç§»Î”r, Î”päº¤äº’ï¼Œ")
    print("   å®é™…ä¸Šæ”¹å˜äº†ä¸åŒåŠ¨ä½œçš„ç›¸å¯¹ä»·å€¼ã€‚")
    
    print("\n4. ç†è®ºéªŒè¯:")
    print("   æ ‡å‡†çš„potential-based shapingç†è®ºå‡è®¾:")
    print("   - F(s,a,s') = Î³Î¦(s') - Î¦(s)")
    print("   - è¿™ä¿è¯äº†Q*(s,a)çš„æœ€ä¼˜åŠ¨ä½œä¸å˜")
    print("   ")
    print("   ä½†åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹:")
    print("   - Î¦(U) = kU æ˜¯çº¿æ€§çš„")
    print("   - ä¸åŒåŠ¨ä½œå¯¼è‡´ä¸åŒçš„Î”U (Î´_r=0.25 vs Î´_p=0.08)")
    print("   - å› æ­¤shapingä¼šåå‘äº§ç”Ÿæ›´å¤§Î”Uçš„åŠ¨ä½œ")
    print("   - è¿™å®é™…ä¸Šæ”¹å˜äº†ç­–ç•¥ï¼")
    
    print("\n5. ç»“è®º:")
    print("   åœ¨æˆ‘ä»¬çš„ARGO MDPä¸­ï¼Œä½¿ç”¨Î¦(U)=kUçš„shapingä¼š:")
    print("   âœ… å¯èƒ½åŠ é€Ÿæ”¶æ•› (è™½ç„¶å½“å‰æµ‹è¯•ä¸­æ²¡æœ‰æ˜æ˜¾æ•ˆæœ)")
    print("   âš ï¸  æ”¹å˜æœ€ä¼˜ç­–ç•¥ (åå‘äºé«˜ä¿¡æ¯å¢ç›Šçš„åŠ¨ä½œ)")
    print("   ")
    print("   è¿™å¯èƒ½æ˜¯:")
    print("   - ğŸ”´ Bug: å¦‚æœæˆ‘ä»¬å¸Œæœ›ä¿æŒç­–ç•¥ä¸å˜")
    print("   - ğŸŸ¢ Feature: å¦‚æœæˆ‘ä»¬å¸Œæœ›é¼“åŠ±é«˜ä¿¡æ¯å¢ç›Šçš„åŠ¨ä½œ")
    print("   ")
    print("   å»ºè®®: å¦‚æœè¦ä¿æŒç­–ç•¥ä¸å˜ï¼Œåº”è¯¥ä½¿ç”¨:")
    print("   - Î¦(U) = Ïƒ(U) (è´¨é‡å‡½æ•°æœ¬èº«)")
    print("   - æˆ–è€… disabled reward shaping")


if __name__ == "__main__":
    analyze_shaping_effect()
