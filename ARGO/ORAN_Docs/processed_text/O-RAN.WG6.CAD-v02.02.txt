O-RAN.WG6.CAD-v02.02
  Technical R eport 
Cloud Architecture and Deployment Scenarios 
 for O-RAN Virtualized RAN 
This is a re-published version of the attached final specification. 
For this re-published version, the prior versions of the IPR Policy will apply, except that the previous 
requirement for Adopters (as defined in the earlier IPR Policy) to agree to an O-RAN Adopter License 
Agreement to access and use Final Specifications shall no longer apply or be required for these Final 
Specifications after 1st July 2022.
The copying or incorporation into any other work of part or all of the material available in this 
specification in any form without the prior written permission of O-RAN ALLIANCE e.V.  is prohibited, 
save that you may print or download extracts of the material on this site for your personal use, or copy 
the material on this site for the purpose of sending to individual third parties for their information 
provided that you acknowledge O-RAN ALLIANCE as the source of the material and that you inform 
the third party that these conditions apply to them and that they must comply with them.

                                                                                                            
 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
                                                     O-RAN.WG6.CAD-v02.02 
                                                                                                                         Technical Report  
 
 
Cloud Architecture and Deployment Scenarios 
 for O-RAN Virtualized RAN 
  
 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
By using, accessing or downloading any part of this O -RAN specification document, including by copying, saving,       
distributing, displaying or preparing derivatives of, you agree to be and are bound to the terms of the O-RAN Adopter License 
Agreement contained in the Annex ZZZ of this specification. All other rights reserved. 
O-RAN ALLIANCE e.V. 
Buschkauler Weg 27, 53347 Alfter, Germany 
Register of Associations, Bonn VR 11238 
VAT ID DE321720189 
  1 


                                                                                                            
 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 2 

                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
3 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
Revision History 3 
Date Revision Company Description 
2019.01.18 V0000.00 AT&T, Orange, 
Lenovo, … 
Template with initial scenarios. 
2019.01.29 V00.00.01 Editor (AT&T) Updates to terminology, miscellaneous other updates 
2019.02.07 V00.00.02 Editor (AT&T)  More definitions in 2.1, New Sec 4 on Overall Architecture, 
expansion/ updates of sec 5 Profiles, added Sec 6 OAM 
placeholder.  
2019.03.18 V00.00.03 Editor (AT&T) Many additions in content and section structure. 
2019.04.01 V00.00.04 Editor (AT&T) Some restructuring and combining of early sections, and more 
discussion on scope and context.  Addition of implementation 
consideration section, including performance.  Added optional 
Fronthaul GW. Provided framework discussion in each 
scenario’s subsection.  Other updates.   
2019.04.10 V00.00.05 Aricent, Red Hat, 
KDDI, Ciena 
Updates to include comments before April 11 review.  
Comments from RaviKanth (Aricent), Pasi (Red Hat), Shinobu 
(KDDI), and Lyndon (Ciena).  
2019.04.15 V00.00.06 Editor (AT&T) Updates to include some updates from comments from April 11 
review. 
2019.04.24 V00.00.07 Editor (AT&T) Updates of diagrams to address comments, additional figures on 
scope, and other changes to address April 11 review comments. 
2019.05.01 V00.00.08 KDDI Updates to diagrams for Scenarios A and B.  Modifications per 
KDDI regarding C.2.  
2019.05.12 V00.00.09 KDDI, Red Hat, 
Editor (AT&T) 
Updates based on meeting discussions, subsection additions 
based on proposals. 
2019.05.15 V00.00.10 Editor (AT&T) Clean-up in preparation of creating a baseline document – 
marking of many comments as done, adding editor notes where 
needed, and other clarifications. 
2019.05.20 V00.00.11 Editor (AT&T) Continued clean-up in preparation of a baseline. 
2019.05.29 V00.00.12 Editor (AT&T) Continued clean-up in preparation of a baseline. 
2019.06.04 V00.00.13 Wind River, China 
Mobile 
Major additions to the Cloud requirements in section 5.4 and 
Appendix B by Wind River, plus updates to the Fronthaul 
section from China Mobile. Various additional minor updates. 
2019.06.13 V00.01.00 Editor (AT&T) This is the same as V00.00.13, but with renumbering to indicate 
this is the initial baseline for comment, V00.01.00  
2019.06.14  V00.01.01 Wind River, AT&T This includes updates from CRs discussed and agreed to on the 
June 13 call:   
• Wind River contributions on adding a figure for NUMA 
illustration and a major enhancement of Sec 9.1 on cache 
• AT&T contribution to add material on centralization of O-
DU/O-CU resources, to Sections 5.1 and 6.2   
• Update of figures to address Open Fronthaul comments 
(discussed June 6)  

                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
4 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
2019.07.05 V00.01.02 Editor (AT&T), 
based on meeting 
discussion 
Updates to address several CRs: 
• Multiple editorial items: 
o Draft text to address 5G/4G scope in Sec 1.2 – further 
discussion via separate CR 
o Statement in 5.2 about performance to focus on delay 
o Statement in 5.7 about transport 
o 5.8; update of Figure 13 to indicate cloud locations.  
Added MEC text that to address MEC comment 
during call. 
o Delay and loss table updates in 6, and statement in 
5.2 
• Former 9.1 and 9.3 sections of Appendix B (on cache and 
storage details) will be transferred to Tong’s document 
(Reference Design).   
• Update the O-DU pooling analysis in Section 5.1.3. 
2019.07.18 V00.01.03 AT&T, Red Hat, 
TIM, Intel, 
Ericsson 
Updates to address multiple CRs, through July 18: 
• Address NSA aspects in scope 
• Addition of 5.3 (Acceleration) 
• Removal of Scale up/down appendix, and note for future 
study 
• Update of delay figure in 5.2. 
• Update of Figure 4 
• Replacement of Zbox concept with O-Cloud, and all 
related updates. 
2019.08.02 V00.01.04 AT&T, Wind 
River, Red Hat 
Updates to address multiple CRs, discussed on Aug 1: 
• Update Section 5.6, merge in sec 7, explain some 
fundamental operations concepts. 
• Update the sync section to point to work in other WGs, 
and say that text will wait until CAD version 2. 
• Update the delay section (5.2.1)  
• Remove notes that refer to items that will not receive 
contributions in version 1.  Remove comments that are no 
longer relevant. 
• Remove Appendix A 
2019.08.09 V00.01.05 Red Hat, TIM, DT, 
Editor (AT&T) 
Updates to address multiple CRs and DT review comments, 
discussed on Aug 8.   
• Update 5.2.1 to address non-optimal fronthaul, and to 
correct some equations 
• Update 5.6 to add a figure showing the O1* interface 
• Addressed a range of comments by DT, some editorial, 
some more involved. 
2019.08.16 V00.01.06 Ericsson, Wind 
River, AT&T 
Updates to address multiple CRs and DT review comments, 
discussed on Aug 15.   
• Updates to address Ericsson’s comments 
• Update to address DT’s request to define vO-DU tile 
• Update of the Cloud Considerations section (5.4), mostly 
for restructuring to remove duplication, but to also add 
material for VMs or Containers where necessary to 
provide balanced coverage. 
• Additional updates:  Many resolved and obsolete Word 
comments have been removed in anticipation of 
finalization. 
• References to documents that are not finalized have been 
removed. 
2019.08.23 V00.01.07 AT&T Updates to reflect:  
• Updates of the O-DU pooling section based on Aug 20 
discussion 
• Management section updates are to address comments 
made on Aug 15 discussion, particularly regarding the use 

                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
5 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
of the term domain manager and its role in an ME, and the 
location of O1 terminations 
• Edits to remove references to O-RAN WGs, and make 
updates of the revision history. 
• Addition of standard O-RAN Annex ZZZ 
2019.08.26 V00.01.08 Editor (AT&T) • Clean up of references and cross references to them 
• Removed Word comments 
• Removed cardinality questions in Scenarios A (removed 
6.1.1) and Scenario B 
2019.08.26 V00.01.09 Editor (AT&T) Final minor comments during Aug 27 WG6 call, in preparation 
for vote. 
2019.10.01 V01.00.00 Editor (AT&T) Update of Annex ZZZ, page footers, and addition of title page 
disclaimer.  
2020.01.17 V01.00.01 Editor (AT&T) Merged the following CRs, but with  
• ATT-2019-11-19 CADS-C CR ATT-CAD-010 
acceleration 01.00.00 
• WRS 2019-12-04 CAD-C 01.00.00 rev 1 
2020.02.09 V01.00.02 Editor (AT&T) Simplified 5.6.  
• Removed 5.6.1, 5.6.2 – replaced it with pointers to O1, 
and O2 specification. 
• Incorporated NVD comments on 5.3 and 5.4 addressing 
inline acceleration as an option 
2020.03.03 V01.00.03 Editor (AT&T) • Updated 4, 4.1 to reflect the latest O-RAN architecture  
• Incorporated comments on 5.6 to include O1, O2 
references. 
• Updated 4.3 with O-Cloud description and definitions 
of key components of O-Cloud 
• Updated 5.3, Figure 15 to reflect O-Cloud reference 
figure in 4.3 
2020.03.09 V01.00.04 Orange • Various minor editorial modifications, take them as 
suggestions for better readability… 
2020.03.10 V01.05 Editor (AT&T) • Incorporated Ericsson comments provided on v01.00.02 
• Updated 1.1 to include O-RAN Architecture description 
• Added definitions for O-RAN Physical NF, O-RAN 
Cloudified NF 
2020.03.14 V01.06 AT&T, Orange Minor editorial modifications, make this version ready 
for WG6 internal review and voting 
2020.03.20 V02.00 AT&T, Orange Minor editorial, make this version ready for TSC 
review and voting 
2020.07.04 V02.01 AT&T Incorporated the following CRs: 
• TIM.AO-2020.05.18-WG6-CR-0001-CADS Scenario B 
Extension-v05  
• WRS-2020-04-24 CAD-v02.00 CR for PTP 
Notifications v05 
2020.07.06 V02.01 AT&T Added architecture of O-Cloud Resource Pool 
2020.07.14 V02.01 AT&T Addressed minor editorial comments received, making it ready 
for TSC review and voting 
2021.07.01 V02.02 Editor (AT&T) Incorporated the following CRS:  
• DIS-2020-08-03 CAD-v02.01 CR for OMAC v02 
• WRS-2020-09-15 CAD 2.01 CR for LLS-C1 Time 
Sync Requirements 
• JNPR-2021.05.13-WG6-CR-0001-
VLAN_Based_Networking-v04 
Editorial changes:  
• Rename references to O-vDU as vO-DU 
2021.07.15 V02.02 Editor (AT&T) Fixed broken reference for O-RAN WG4, Control, User and 
Synchronization Plane Specification. 
  4 

                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
6 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
Table of Contents 5 
Revision History ................................................................................................................................................. 3 6 
Table of Contents ............................................................................................................................................... 6 7 
Table of Figures .................................................................................................................................................. 7 8 
Table of Tables ................................................................................................................................................... 8 9 
1 Scope ........................................................................................................................................................ 9 10 
1.1 Context; Relationship to Other O-RAN Work ................................................................................................... 9 11 
1.2 Objectives .......................................................................................................................................................... 9 12 
2 References .............................................................................................................................................. 11 13 
3 Definitions and Abbreviations ............................................................................................................... 12 14 
3.1 Definitions ....................................................................................................................................................... 12 15 
3.2 Abbreviations ................................................................................................................................................... 13 16 
4 Overall Architecture ............................................................................................................................... 15 17 
4.1 O-RAN Functions Definitions ......................................................................................................................... 15 18 
4.2 Degree of Openness ......................................................................................................................................... 16 19 
4.3 Decoupling of Hardware and Software ............................................................................................................ 16 20 
 The O-Cloud............................................................................................................................................... 17 21 
 Key O-Cloud Concepts .............................................................................................................................. 18 22 
5 Deployment Scenarios:  Common Considerations ................................................................................. 21 23 
5.1 Mapping Logical Functionality to Physical Implementations ......................................................................... 21 24 
 Technical Constraints that Affect Hardware Implementations................................................................... 21 25 
 Service Requirements that Affect Implementation Design ........................................................................ 21 26 
 Rationalization of Centralizing O-DU Functionality ................................................................................. 22 27 
5.2 Performance Aspects ....................................................................................................................................... 25 28 
 User Plane Delay ........................................................................................................................................ 25 29 
5.3 Hardware Acceleration and Acceleration Abstraction Layer (AAL) ............................................................... 28 30 
 Accelerator Deployment Model ................................................................................................................. 29 31 
 Acceleration Abstraction Layer (AAL) Interface ....................................................................................... 29 32 
 Accelerator Management and Orchestration Considerations ..................................................................... 30 33 
5.4 Cloud Considerations ....................................................................................................................................... 30 34 
 Networking requirements ........................................................................................................................... 30 35 
5.4.1.1 Support for Multiple Networking Interfaces ................................................................................... 30 36 
5.4.1.2 Support for High Performance N-S Data Plane .............................................................................. 30 37 
5.4.1.3 Support for High-Performance E-W Data Plane ............................................................................. 31 38 
5.4.1.4 Support for Service Function Chaining .......................................................................................... 32 39 
5.4.1.5 Support for VLAN based networking ............................................................................................. 32 40 
 Assignment of Acceleration Resources ...................................................................................................... 32 41 
 Real-time / General Performance Feature Requirements ........................................................................... 32 42 
5.4.3.1 Host Linux OS ................................................................................................................................ 32 43 
5.4.3.1.1 Support for Pre-emptive Scheduling .................................................................................. 32 44 
5.4.3.2 Support for Node Feature Discovery .............................................................................................. 33 45 
5.4.3.3 Support for CPU Affinity and Isolation .......................................................................................... 33 46 
5.4.3.4 Support for Dynamic HugePages Allocation .................................................................................. 33 47 
5.4.3.5 Support for Topology Manager ...................................................................................................... 33 48 
5.4.3.6 Support for Scale In/Out ................................................................................................................. 34 49 
5.4.3.7 Support for Device Plugin .............................................................................................................. 34 50 
5.4.3.8 Support for Direct IRQ Assignment ............................................................................................... 35 51 
5.4.3.9 Support for No Over Commit CPU ................................................................................................ 35 52 
5.4.3.10 Support for Specifying CPU Model ................................................................................................ 35 53 
 Storage Requirements ................................................................................................................................ 35 54 
 Notification Subscription Framework ........................................................................................................ 35 55 
5.4.5.1 O-Cloud Notification Subscription Requirements .......................................................................... 35 56 
5.5 Sync Architecture ............................................................................................................................................ 36 57 
 Cloud Platform Time Synchronization Architecture .................................................................................. 37 58 

                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
7 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
5.5.1.1 Edge Cloud Site Level – LLS-C3 Synchronization Topology ........................................................ 37 59 
5.5.1.1.1 LLS-C3 Synchronization Topology Edge Site Time Synchronization Architecture .......... 37 60 
5.5.1.1.2 LLS-C3 Synchronization Topology Edge Site Requirements ............................................ 38 61 
5.5.1.1.2.1 Software .................................................................................................................................... 38 62 
5.5.1.1.2.2 Hardware ................................................................................................................................... 39 63 
5.5.1.2 Edge Cloud Site Level – LLS-C1 Synchronization Topology ........................................................ 39 64 
5.5.1.2.1 LLS-C1 Synchronization Topology Edge Site Time Synchronization Architecture .......... 39 65 
5.5.1.2.2 LLS-C1 Synchronization Topology Edge Site Requirements ............................................ 40 66 
5.5.1.2.2.1 Software .................................................................................................................................... 40 67 
5.5.1.2.2.2 Hardware ................................................................................................................................... 40 68 
 Loss of Synchronization Notification ......................................................................................................... 40 69 
5.6 Operations and Maintenance Considerations ................................................................................................... 41 70 
5.7 Transport Network Architecture ...................................................................................................................... 42 71 
 Fronthaul Gateways ................................................................................................................................... 42 72 
5.8 Overview of Deployment Scenarios ................................................................................................................ 42 73 
6 Deployment Scenarios and Implementation Considerations .................................................................. 44 74 
6.1 Scenario A ....................................................................................................................................................... 44 75 
 Key Use Cases and Drivers ........................................................................................................................ 44 76 
6.2 Scenario B ........................................................................................................................................................ 44 77 
 Key Use Cases and Drivers ........................................................................................................................ 46 78 
6.3 Scenario C ........................................................................................................................................................ 46 79 
 Key Use Cases and Drivers ........................................................................................................................ 47 80 
 Scenario C.1, and Use Case and Drivers .................................................................................................... 47 81 
 Scenario C.2, and Use Case and Drivers .................................................................................................... 48 82 
6.4 Scenario D ....................................................................................................................................................... 50 83 
6.5 Scenario E ........................................................................................................................................................ 50 84 
 Key Use Cases and Drivers ........................................................................................................................ 51 85 
 Scenario E.1 vO-DU with O-RU ................................................................................................................ 51 86 
6.6 Scenario F ........................................................................................................................................................ 51 87 
 Key Use Cases and Drivers ........................................................................................................................ 52 88 
6.7 Scenarios of Initial Interest .............................................................................................................................. 52 89 
7 Appendix A (informative):  Extensions to Current Deployment Scenarios to Include NSA ................. 52 90 
7.1 Scenario A ....................................................................................................................................................... 53 91 
7.2 Scenario B ........................................................................................................................................................ 53 92 
7.3 Scenario C ........................................................................................................................................................ 53 93 
7.4 Scenario C.2 ..................................................................................................................................................... 53 94 
7.5 Scenario D ....................................................................................................................................................... 54 95 
Annex ZZZ:  O-RAN Adopter License Agreement ......................................................................................... 55 96 
 97 
Table of Figures  98 
Figure 1:  Relationship of this Document to Scenario Documents and O-RAN Management Documents ........................ 9 99 
Figure 2:  Major Components Related to the Orchestration and Cloudification Effort  .................................................... 10 100 
Figure 3:  Different Clouds/ Sites ..................................................................................................................................... 11 101 
Figure 4: High Level Architecture of O-RAN .................................................................................................................. 15 102 
Figure 5:  Logical Architecture of O-RAN ....................................................................................................................... 16 103 
Figure 6:  Decoupling, and Illustration of the O-Cloud Concept ...................................................................................... 17 104 
Figure 7:  Relationship between RAN Functions and Demands on Cloud Infrastructure and Hardware  ......................... 18 105 
Figure 8: Key Components Involved in/with an O-Cloud ................................................................................................ 18 106 
Figure 9: O-Cloud Node Roles and Deployment Examples ............................................................................................. 19 107 
Figure 10: Edge O-Cloud Architecture (Hub) .................................................................................................................. 20 108 
Figure 11: Edge O-Cloud Architecture (Single server) .................................................................................................... 20 109 
Figure 12:  Simple Centralization of O-DU Resources .................................................................................................... 23 110 
Figure 13:  Pooling of Centralized O-DU Resources........................................................................................................ 23 111 
Figure 14:  Comparison of Merit of Centralization Options vs. Number of Cell Sites in a Pool  ...................................... 24 112 
Figure 15:  Major User Plane Latency Components, by 5G Service Slice and Function Placement  ................................ 26 113 
Figure 16: Hardware Abstraction Considerations ............................................................................................................. 29 114 

                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
8 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
Figure 17: Accelerator APIs/Libraries in Container and Virtual Machine Implementations  ............................................ 29 115 
Figure 18:  Illustration of the Network Interfaces Attached to a Pod, as Provisioned by Multus CNI ............................. 30 116 
Figure 19:  Illustration of the Userspace CNI Plugin ........................................................................................................ 31 117 
Figure 20:  Example Illustration of Two NUMA Regions ............................................................................................... 34 118 
Figure 21: O-Cloud Notification Framework Architecture .............................................................................................. 36 119 
Figure 22: Edge Cloud Site Time Sync Architecture for LLS-C3 .................................................................................... 38 120 
Figure 23: Hyperconverged Edge Cloud Time Sync Architecture for LLS -C3 ................................................................ 38 121 
Figure 24: Edge Cloud Site Time Sync Architecture for LLS-C1 .................................................................................... 39 122 
Figure 25: Hyperconverged Edge Cloud Time Sync Architecture for LLS -C1 ................................................................ 40 123 
Figure 26: vO-DU Subscribes to PTP Notification .......................................................................................................... 41 124 
Figure 27:  High-Level Comparison of Scenarios ............................................................................................................ 43 125 
Figure 28:  Scenario A ...................................................................................................................................................... 44 126 
Figure 29:  Scenario B – NR Stand-alone ......................................................................................................................... 45 127 
Figure 30: Scenario B – MR-DC (inter-RAT NR and E-UTRA) regardless of the Core Network type (either EPC or 128 
5GC) ................................................................................................................................................................................. 45 129 
Figure 31:  Scenario C ...................................................................................................................................................... 46 130 
Figure 32:  Treatment of Network Slices:  MEC for URLLC at Edge Cloud, Centralized Control, Single vO -DU ........ 47 131 
Figure 33:  Scenario C.1 ................................................................................................................................................... 48 132 
Figure 34:  Treatment of Network Slice: MEC for URLLC at Edge Cloud, Separate vO -DUs ....................................... 49 133 
Figure 35:  Single O-RU Being Shared by More than One Operator ............................................................................... 49 134 
Figure 36:  Scenario C.2 ................................................................................................................................................... 50 135 
Figure 37:  Scenario D ...................................................................................................................................................... 50 136 
Figure 38:  Scenario E ...................................................................................................................................................... 51 137 
Figure 39: Scenario E.1 .................................................................................................................................................... 51 138 
Figure 40:  Scenario F ....................................................................................................................................................... 52 139 
Figure 41:  Scenario A, Including NSA ............................................................................................................................ 53 140 
Figure 42:  Scenario C, Including NSA ............................................................................................................................ 53 141 
Figure 43:  Scenario C.2, Including NSA ......................................................................................................................... 54 142 
Figure 44:  Scenario D, Including NSA ............................................................................................................................ 54 143 
Table of Tables 144 
Table 1:  Service Delay Constraints and Major Delay Contributors ................................................................................. 26 145 
Table 2:  Cardinality and Delay Performance for Scenario B........................................................................................... 46 146 
Table 3:  Cardinality and Delay Performance for Scenario C........................................................................................... 47 147 
Table 4:  Cardinality and Delay Performance for Scenario C.1........................................................................................ 48 148 
 149 
150 

                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
9 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
1 Scope  151 
This Technical Report has been produced by the O-RAN Alliance. 152 
The contents of the present document are subject to continuing work within O -RAN and may change following formal 153 
O-RAN approval. Should O-RAN modify the contents of the present document, it will be re-released by O-RAN with an 154 
identifying change of release date and an increase in version number as follows: 155 
Version x.y.z 156 
where: 157 
x the first digit is incremented for all changes of substance, i.e. technical enhancements, corrections, updates, 158 
etc. (the initial approved document will have x=01). 159 
y the second digit is incremented when editorial only changes have been incorporated in the document.  160 
z the third digit included only in working versions of the document indicating incremental changes during the 161 
editing process. 162 
1.1 Context; Relationship to O ther O-RAN Work 163 
This document introduces and examines different scenarios and use cases for O -RAN deployments of N etwork 164 
Functionality into Cloud Platforms , O -RAN Cloudified NFs  and O-RAN Physical NFs .  D eployment scenarios are 165 
associated with meeting customer and service requirements, while considering technological constraints and the need to 166 
create cost-effective solutions. It will also reference management considerations covered in more depth elsewhe re. 167 
The following O-RAN documents will be referenced (see Section 5.6): 168 
• OAM architecture specification [8] 169 
• OAM interface specification (O1) [9] 170 
• O-RAN Architecture Description [10] 171 
The details of implementing each identified scenario will be covered in separate Scenario documents, shown in green in 172 
Figure 1.   173 
 174 
Figure 1:  Relationship of this Document to Scenario Documents and O-RAN Management Documents 175 
This document also draws on some other work from other O-RAN working groups, as well as sources from other industry 176 
bodies.   177 
1.2 Objectives  178 
The O-RAN Alliance seeks to improve RAN flexibility and deployment velocity, while at the same time reducing the 179 
capital and operating costs through the adoption of cloud architectures. The structure of the Orchestration and 180 
Cloudification work is shown graphically below.  This document focuses on the Cloudification deployment aspects as 181 
indicated.  182 


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
10 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
Editor’s note: O-RU cloudification and O-RU AAL are future study items.  183 
  184 
Figure 2:  Major Components Related to the Orchestration and Cloudification Effort 185 
A key principle is the decoupling of RAN hardware and software for all components including near -RT RIC, O-CU (O-186 
CU-CP and O -CU-UP), O -DU, and O- RU, and the deployment of software components on commodity server 187 
architectures supplemented with programmable accelerators where necessary.   188 
Key characteristics of cloud architectures which we will reference in this document are:  189 
a) Decoupling of hardware from software.  This aims to improve flexibility and choice for operators by decoupling 190 
selection and deployment of hardware infrastructure from software selection, 191 
b) Standardization of hardware specifications across software implementations , to simplify physi cal deployment 192 
and maintenance.  This aims to promote the availability of a multitude of software implementation choices for a 193 
given hardware configuration.   194 
c) Sharing of hardware.  This aims to promote the availability of a multitude of hardware  implementation choices 195 
for a given software implementation. 196 
d) Flexible instantiation and lifecycle management through orchestration automation.   This aims to reduce 197 
deployment and ongoing maintenance costs by promoting simplification and automation throughout the 198 
hardware and software lifecycle through common chassis specifications and standardized orchestration 199 
interfaces.   200 
This document will define various deployment scenario s that can be supported by the O -RAN specifications and are of 201 
either current or relatively near -term interest.  Each scenario is identified by a specific grouping of functionality a t 202 
different key locations (Cell Site, Edge Cloud, and Regional Cloud, which will be defined shortly), and an identification 203 
of whether functionality  at a given location  is provided by a n O-RAN Physical NF based solution where software and 204 
hardware are tightly integrated and sharing a single identity, or by a cloud architecture that meets the above requirements. 205 
The scope of this work clearly includes supporting all 5G technologies, i.e. E-UTRA and NR with both EPC-based Non-206 
Standalone (NSA) and 5GC architectures. This impl ies that cloud/orchestration aspects of NSA (E -UTRA) are also 207 
supported. However, this version primarily addresses 5G SA deployments. 208 
This technical report examines the constraints that drive a specific solution, and discuss the hierarchical properties of each 209 
solution, including a rough scale of the size of each cloud and a sense of the number of sub clouds expected to be served 210 
by a higher cloud.  Figure 3 shows as example of how multiple cell sites feed into a smaller number of Edge Clouds, and 211 
how in turn multiple Edge Clouds feed into a Regional Cloud.  For a given scenario, the Logical Functions are distributed 212 
in a certain way among each type of cloud, and the “cardinality” of the different functions will be discussed.   213 
T
his has implications on the processing power needed in each type of cloud, as well as implications on the environmental 214 
requirements.  This document will also discuss considerations of hardware chassis and components that are reasonable in 215 
each scenario, and the implications of managing such a cloud.   216 
 217 


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
11 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 218 
Figure 3:  Different Clouds/ Sites 219 
Additional major areas for this document are listed below:   220 
• Mapping of logical functions to physical elements and locations, and implications of that mapping.  221 
• High-level assessment of critical performance requirements, and how that influences architecture. 222 
• Processor and accelerator options (e.g., x86, FPGA, GPU).  In order to determine whether a Network Function 223 
is a candidate for openness, there need s to be the possibility to have m ultiple suppliers of software for given 224 
hardware, and multiple sources of required chip/accelerators.   225 
• The Hardware Abstraction Layer, aka “Acceleration Abstraction Layer” needs to be addressed in light of 226 
various hardware options that could be used. 227 
• Cloud infrastructure makeup.  This includes considerations such as: 228 
• Deployments are allowed to use VMs, Containers in VMs, or just Containers.  229 
• Multiple Operating Systems are expected to be supported; e.g., open source Ubuntu, CentOS Linux, or 230 
Yocto Linux-based distributions, or selected proprietary OSs.   231 
• Management of a cloudified RAN introduces some new management considerations, because the mapping 232 
between Network Functionality and cloud platforms  can be done in multiple ways, depending on the scenario 233 
that is chosen.  Thus, management of aspects that are related to platform  aspects rather than RAN functional  234 
aspects need to be designed with flexibility in mind from the start.  For example, logging of physical functions, 235 
scale out actions, and survivability considerations are affected.   236 
• These management considerations are introduced in this document, but  management documents will 237 
address the solutions. 238 
• The transport layer will be discussed, but only to the extent that it affects the architecture and design of the 239 
network.  For example, the chosen L1 technology may affect the performance of transport.  As another example, 240 
the use of a Fronthaul Gateway will affect economics as well as the placement options of certain Network 241 
Functions.  And of course, t he existence of L2 swi tches in a cloud platform deployment will be required for 242 
efficient use of server resources. 243 
Additional areas could be considered in the future.   244 
2 References 245 
The following documents contain provisions which, through reference in this text, constitute provisions of this report. 246 
[1] 3GPP TS 38.470, NG-RAN; F1 general aspects and principles. 247 
[2] 3GPP TR 21.905, Vocabulary for 3GPP Specifications. 248 
[3] eCPRI Interface Specification V1.2, Common Public Radio Interface:  eCPRI Interface Specification. 249 


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
12 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
[4] eCPRI Transport Network V1.2, Requirements Specification, Common Public Radio Interface:  250 
Requirements for the eCPRI Transport Network. 251 
[5] IEEE Std 802.1CM-2018,  Time-Sensitive Networking for Fronthaul.  252 
[6] ITU-T Technical Report, GSTR-TN5G - Transport network support of IMT-2020/5G.  253 
[7] O-RAN WG4, Control, User and Synchronization Plane Specification, Technical Specification.  See 254 
https://www.o-ran.org/specifications. 255 
[8] O-RAN WG1, Operations and Maintenance Architecture – v03.00, Technical Specification.  See 256 
https://www.o-ran.org/specifications. 257 
[9] O-RAN WG1, Operations and Maintenance Interface Specification – v3.0, Technical Specification.  See 258 
https://www.o-ran.org/specifications.  259 
[10] O-RAN WG1, O-RAN Architecture Description – v02.00, Technical Specification. See https://www.o-260 
ran.org/specifications.  261 
[11] 3GPP TS 28.622, Telecommunication management; Generic Network Resource Model (NRM) Integration 262 
Reference Point (IRP); Information Service (IS). 263 
[12] O-RAN WG6, Cloud Platform Reference Design for Deployment Scenario B, Technical Specification.  See 264 
https://www.o-ran.org/specifications. 265 
[13] O-RAN WG7 OMAC HAR 0-v01.00 O-RAN White Box Hardware Working Group Outdoor Macrocell 266 
Hardware Architecture and Requirements (FR1) Specification. 267 
[14] O-RAN WG1, Use Cases Detailed Specifications – v05.00, Technical Specification. See https://www.o-268 
ran.org/specifications. 269 
3 Definitions and Abbreviations 270 
3.1 Definitions 271 
For the purposes of the present document, the terms and definitions given in 3GPP TR 21.905 [2] and the following apply. 272 
A term defined in the present document takes precedence over the definition of the same term, if any, in 3GPP 273 
TR 21.905 [2].  274 
Cell Site This refers to the  location of Radio Units (RUs); e.g., placed on same structure as the Radio 275 
Unit or at the base.  The Cell Site in general will support multiple sectors and hence multiple 276 
O-RUs. 277 
Edge Cloud This is a location that supports virtualized RAN functions for multiple Cell Sites, and provides 278 
centralization of functions for those sites and associated economies of scale.  An Edge Cloud 279 
might serve a large physical area or a relatively small one close to its cell sites, depending on 280 
the Operator’s use case.  However, the sites served by the Edge Cloud must be near enough to 281 
the O-RUs to meet the network latency requirements of the O-DU functions. 282 
F1 Interface  The open interface between O-CU and O-DU in this document is the same as that defined by  283 
the CU and DU split in 3GPP TS 38.473.  It consists of an F1-u part and an F1-c part. 284 
Managed Element  The definition of a Managed Element (ME) is given in 3GPP TS 28.622 [11]  section 285 
4.3.3.  The ME supports communication over management interface(s) to the manager for 286 
purposes of control and monitoring.  287 
Managed Function  The definition of a Managed Function (MF) is given in 3GPP TS 28.622 [11] section 4.3.4.  An 288 
MF instance is managed using the management interface(s) exposed by its containing ME 289 
instance. 290 
Network Function The near-RT RIC, O -CU-CP, O -CU-UP, O -DU, and O -RU logical functions that can be 291 
provided either by virtualized or non-virtualized methods.  292 
Regional Cloud This is a location that supports virtualized RAN functions for many Cell Sites in multiple Edge 293 
Clouds, and provides high centralization of functionality. The sites served by the Regional 294 

                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
13 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
Cloud must be near enough to the O-DUs to meet the network latency requirements of the O-295 
CU and near-RT RIC.  296 
O-Cloud This refers to a collection of O-Cloud Resource Pools at one or more location and the software 297 
to manage Nodes and Deployments hosted on them.  An O-Cloud will include functionality to 298 
support both Deployment-plane and Management services . The O -Cloud provides a single 299 
logical reference point for all O-Cloud Resource Pools within the O-Cloud boundary. 300 
O-RAN Physical NF  A RAN NF software deployed on tightly integrated hardware sharing a single Managed 301 
Element identity. 302 
O-RAN Cloudified NF  A RAN NF software deployed on an O -Cloud with its own Managed Element identity , i.e., 303 
separate from the identity of the O-Cloud. 304 
3.2 Abbreviations 305 
For the purposes of this  document, the abbreviations given in 3GPP TR  21.905 [2] and the following apply.  306 
An abbreviation defined in the present document takes precedence over the definition of the same abbreviation, if any, in 307 
3GPP TR 21.905 [2]. 308 
3GPP Third Generation Partnership Project 309 
5G Fifth-Generation Mobile Communications 310 
AAL Acceleration Abstraction Layer 311 
API Application Programming Interface 312 
ASIC Application-Specific Integrated Circuit  313 
BBU BaseBand Unit 314 
BS Base Station 315 
CI Cloud Infrastructure 316 
CoMP   Co-Ordinated Multi-Point transmission/reception 317 
CNF Cloud-Native Network Function  318 
CNI Container Networking Interface 319 
CPU Central Processing Unit 320 
CR Cell Radius 321 
CU Centralized Unit as defined by 3GPP 322 
DFT Discrete Fourier Transform 323 
DL Downlink 324 
DPDK Data Plan Development Kit  325 
DMS Deployment Management Services 326 
DU Distributed Unit as defined by 3GPP 327 
eMBB enhanced Mobile BroadBand 328 
EPC Evolved Packet Core 329 
E-UTRA Evolved UMTS Terrestrial Radio Access 330 
FCAPS Fault Configuration Accounting Performance Security  331 
FEC  Forward Error Correction 332 
FFT Fast Fourier Transform 333 
FH Fronthaul 334 
FH GW Fronthaul Gateway 335 
FPGA Field Programmable Gate Array 336 
GNSS Global Navigation Satellite System 337 
GPP General Purpose Processor 338 
GPS Global Positioning System 339 
GPU Graphics Processing Unit  340 
HARQ Hybrid Automatic Repeat ReQuest 341 
HW Hardware 342 
IEEE Institute of Electrical and Electronics Engineers 343 
IM Information Modelling, or Information Model 344 
IMS Infrastructure Management Services 345 
IRQ Interrupt ReQuest  346 
ISA Instruction Set Architecture 347 
ISD Inter-Site Distance 348 
ITU International Telecommunications Union 349 
KPI Key Performance Indicator 350 
LCM Life Cycle Management 351 

                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
14 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
LDPC  Low-Density Parity-Check 352 
LLS Lower Layer Split   353 
LTE Long Term Evolution 354 
LVM Logic Volume Manager 355 
MEC Mobile Edge Computing 356 
mMTC massive Machine Type Communications 357 
MNO Mobile Network Operator 358 
NF Network Function 359 
NFD Node Feature Discovery 360 
NFVI Network Function Virtualization Infrastructure 361 
NIC Network Interface Card 362 
NMS Network Management System  363 
NR  New Radio 364 
NSA Non-Standalone 365 
NTP Network Time Protocol 366 
NUMA Non-Uniform Memory Access  367 
NVMe Non-Volatile Memory Express 368 
O-Cloud O-RAN Cloud Platform 369 
OCP  Open Compute Project 370 
O-CU O-RAN Central Unit  371 
O-CU-CP O-CU Control Plane 372 
O-CU-UP O-CU User Plane 373 
O-DU O-RAN Distributed Unit (uses Lower-level Split) 374 
O-RU O-RAN Radio Unit 375 
OTII Open Telecom IT Infrastructure 376 
OWD One-Way Delay 377 
PCI Peripheral Component Interconnect 378 
PNF Physical Network Function 379 
PoE Power over Ethernet 380 
PoP Point of Presence 381 
PRTC Primary Reference Time Clock 382 
PTP Precision Time Protocol 383 
QoS  Quality of Service  384 
RAN Radio Access Network 385 
RAT Radio Access Technology 386 
RIC RAN Intelligent Controller  387 
RT Real Time 388 
RTT Round Trip Time 389 
RU Radio Unit  390 
SA Standalone 391 
SFC Service Function Chaining  392 
SMO Service Management and Orchestration 393 
SMP Symmetric MultiProcessing 394 
SoC System on Chip 395 
SR-IOV Single Root Input/ Output Virtualization 396 
SW Software 397 
TCO Total Cost of Ownership 398 
TNE Transport Network Element 399 
TR Technical Report 400 
TRP Transmission Reception Point 401 
TS Technical Specification 402 
TSC (T-TSC) Telecom Slave Clock 403 
Tx Transmitter 404 
UE User Equipment 405 
UL Uplink 406 
UMTS Universal Mobile Telecommunications System 407 
UP User Plane 408 
UPF User Plane Function 409 
URLLC Ultra-Reliable Low-Latency Communications 410 
vCPU virtual CPU 411 
VIM Virtualized Infrastructure Manager 412 
VM Virtual Machine  413 

                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
15 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
VNF Virtualized Network Function 414 
vO-CU Virtualized O-RAN Central Unit  415 
vO-CU-CP Virtualized O-CU Control Plane 416 
vO-CU-UP Virtualized O-CU User Plane 417 
vO-DU Virtualized O-RAN Distributed Unit 418 
4  Overall Architecture  419 
This section address es the overall architecture in terms of the Network Functions and infrastructure ( O-RAN Physical 420 
NFs, servers, and clouds) that are in scope. Figure 4 provides a high-level view of the O-RAN architecture as depicted in 421 
[10].  422 
 423 
Figure 4: High Level Architecture of O-RAN 424 
4.1 O-RAN Functions Definitions 425 
This section reviews key O-RAN functions definitions in O-RAN.  426 
• The O-DU/ O-RU split is defined as using Option 7-2x.  See [7].  427 
• The O-CU/ O-DU split is defined as using the CU/ DU split F1 as defined in 3GPP TS 38.470 [1].    428 
This document assumes these two splits.  429 
Figure 5 shows the logical architecture of O-RAN (as depicted in [10]) with O-Cloud platform at the bottom, where any 430 
given O-RAN function could be supported by O-Cloud, depending on the deployment scenario.  For example, the figure 431 
here illustrates a case where the O-RU is implemented as an O -RAN Physical NF, and the other functions within the 432 
dashed line are supported by O-Cloud.   433 


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
16 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 434 
Figure 5:  Logical Architecture of O-RAN 435 
4.2 Degree of Openness 436 
In theory, every architecture component could be open in every sense imaginable, but in practice it is likely that different 437 
components will have varying degrees of openness due to economic and other implementation considerations.   Some 438 
factors are significantly affected by the deployment scenario; for example, what might be viable in an indoor deployment 439 
might not be viable in an outdoor deployment.   440 
Increasing degrees of openness for a n O-RAN Physical Network Function or O-RAN Cloudified Network Function(s) 441 
are: 442 
A. Interfaces among Network Functions are open; e.g., E2, F1, and O pen Fronthaul are used. Therefore, Network 443 
Functions in different O-RAN Physical NFs/clouds from different vendors can interconnect. 444 
B. In addition to having open c onnections as described above , the chassis of servers in a cloud are open and can 445 
accept blades/sleds from multiple vendors.  However, the blades/sleds have RAN software that is not decoupled 446 
from the hardware. 447 
C. In addition to having open connections and an open chassis, a specific blade/sled uses software that is decoupled 448 
from the hardware.  In this scenario, the software could be from one supplier, the blade/sled could be from another, 449 
and the chassis could be from another.   450 
Categories A and B have O-RAN Physical NFs/clouds, while Category  C is an open solution that we are calling a n O-451 
Cloud, and is subject to the cloudification discussion and requirements. 452 
In this document, the degree of openness for each O-RAN Physical NF/cloud can vary by scenario. The question of which 453 
Network Functions should be split vs. combined, and the degree of openness in each one, is addressed in the discussion 454 
of scenarios.  455 
4.3 Decoupling of Hardware and Software  456 
Editor’s note: O-RU AAL is a future study item.  457 
There are three layers that we must consider when we discuss decoupling of hardware and software:  458 
• The hardware layer, shown at the bottom in Figure 6.  (In the case of a VM deployment, this maps basically to 459 
the ETSI NFVI hardware sub-layer.) 460 
• A middle layer that includes Cloud Stack functions as well as Acceleration Abstraction Layer functions.  (In the 461 
case of a VM deployment, these map to the ETSI NFVI virtualization sub-layer + VIM.) 462 
OFH CUS-Plane 
F1-c
A1
F1-u
E1
E2
Service Management and Orchestration Framework
Non-Real Time RIC 
Near-Real Time RAN 
Intelligent Controller (RIC)
O-DU
NG-c
NG-u
X2-c
Xn-u
O-Cloud
O2
E2
E2
E2
X2-u
Xn-c
O-CU-CP
O-CU-UP
O-eNB
O1
O-RU
O1
OFH M-Plane
Open 
Fronthaul 
M-Plane

                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
17 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
• A top layer that supports the virtual RAN functions.  463 
Each layer can come from a different supplier.  The first aspect of decoupling has to do with ensuring that a Cloud Stack 464 
can work on multiple suppliers’ hardware; i.e., it does not require vendor-specific hardware.   465 
The second aspect of decoupling has to do with ensuring that a Cloud Platform can support RAN virtualized functions 466 
from multiple RAN software suppliers.  If this is possible, then we say that the Cloud Platform (which includes the 467 
hardware that it runs on) is an O-RAN Cloud Platform, or “O-Cloud”.  See Figure 6 below.   468 
 469 
Figure 6:  Decoupling, and Illustration of the O-Cloud Concept 470 
 The O-Cloud  471 
The general definition of the O-Cloud Cloud Platform includes the following characteristics: 472 
1. The Cloud Platform is a set of hardware and software components that provide cloud computing capabilities to 473 
execute RAN network functions. 474 
2. The Cloud Platform hardware include s compute, networking and storage components, and may also include 475 
various acceleration technologies required by the RAN network functions to meet their performance objectives.  476 
3. The Cloud Platform software exposes open and well-defined APIs that enable the management of the entire life 477 
cycle for network functions.  478 
4. The Cloud Platform software is decoupled from the Cloud Platform hardware (i.e., it can typically be sourced 479 
from different vendors). 480 
The management aspects of the O -Cloud platform are discussed in 5.6. The scope of this document includes listing 481 
specific requirements of the Cloud Platform to support execution of the various O-RAN Network Functions. 482 
An example of a Cloud Platform is an OpenStack and/or a Kubernetes deployment on a set of COTS servers (including 483 
FPGA and GPU cards), interconnected by a spine/leaf networking fabric. 484 
There is an important int erplay between specific virtualized RAN functions and the hardware that is needed to meet 485 
performance requirements and to support the functionality economically .  Therefore, a hardware/ cloud platform 486 
combination that can support, say, a vO -CU function might not be appropri ate to adequately support a vO-DU function.  487 
When RAN functions are combined in different ways in each specific deployment scenario, these aspects must be 488 
considered. 489 
Below is a high -level conceptual example of how different accelerators,  along with their associated cloud capabilities, 490 
can be required for different RAN functions.  Although we do not specify any particular hardware requirement or cloud 491 
capability here, we can note some general themes.  For example, any RAN function that involves real-time movement of 492 
user traffic will require the cloud platform to control for delay and jitter, which may in turn require features such as real -493 
time OSs, avoidance of frequent interrupts, CPU pinning, etc.   494 


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
18 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 495 
Figure 7:  Relationship between RAN Functions and Demands on Cloud Infrastructure and Hardware  496 
Please note that any cloud that has features required for a given function (e.g., for O -DU) can also support functions that 497 
do not require such features.  For example, a cloud that can support O -DU can also support functions such as O-CU-CP.  498 
 499 
 500 
 Key O-Cloud Concepts  501 
Figure 8 illustrates key components of an O-Cloud and its management. 502 
 503 
Figure 8: Key Components Involved in/with an O-Cloud 504 
Key terms in this figure are defined below: 505 
• An O-Cloud instance refers to a collection of O-Cloud Resource Pools at one or more location and the software 506 
to manage Nodes and Deployments hosted on them.  An O -Cloud will include functionality to support both 507 
Deployment-plane (aka. user-plane) and Management services. The O-Cloud provides a single logical reference 508 
point for all O-Cloud Resource Pools within the O-Cloud boundary. 509 


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
19 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
• An O-Cloud Resource Pool is a collection of O-Clouds nodes with homogeneous profiles in one location which 510 
can be used for either Management  services or Deployment Plane functions. The allocation of NF deployment 511 
to a resource pool is determined by the SMO. 512 
• An O-Cloud Node is a collection of CPUs, Mem, Storage, NICs, Accelerators, BIOSes, BMCs, etc., and can be 513 
thought of as a server. Each O-Cloud Node will support one or more “roles”, see next. 514 
• O-Cloud Node Role refers to the functionalities that a given node may support. These include Compute, Storage, 515 
Networking for the Deployment -plane (i.e., user -plane related functions such as the  O- RAN NF ), they may 516 
include optional acceleration functions, and they may also include the appropriate Management  services. 517 
• O-Cloud Deployment Plane is a logical construct representing the O -Cloud Nodes across the Resource Pools 518 
which are used to create NF Deployments. 519 
• An O-Cloud NF Deployment  is a deployment of a cloud native Network Function (all or partial), resources 520 
shared within a NF Function, or resource shared across network functions. The NF Deployment configures and 521 
assembles user-plane resources required for the cloud native construct used to establish the NF Deployment and 522 
manage its life cycle from creation to deletion. 523 
• The O2 Interface is a collection of services and their associated interfaces that are provided by the O -Cloud 524 
platform to the SMO. The services are categorized into two logical groups: (i) Infrastructure Management 525 
Services (IMS), which include the subset of O2 functions that are responsible for deploying and managing cloud 526 
infrastructure. (ii) Deployment Management Services (DMS), which include the subset of O2 functions that 527 
are responsible for managing the lifecycle of virtualized/containerized deployments on the cloud infrastructure.  528 
The O2 services and their associated interfaces shall be specified in the upcoming O2 specification. Any 529 
definitions of SMO functional elements needed to consume these services shall be described in OAM 530 
architecture. 531 
 532 
Figure 9 illustrates several deployment examples to show the different O-Cloud Node Roles. Note that the O-Cloud Node 533 
Roles and the O-Cloud Node names are mentioned here as examples and are neither exhaustive nor standardized.  534 
  535 
Figure 9: O-Cloud Node Roles and Deployment Examples 536 
An O- Cloud Resource Pool comprises of  one or more O-Cloud nodes, each with one or more network cards and 537 
optionally, one or more accelerator cards.  If the Resource Pool contains multiple compute nodes, it may also include an 538 
O-Cloud Switch Fabric consisting of leaf and spine switches that interconnect the O-Cloud nodes. The Switch Fabric may 539 
provide connectivity between the servers , to the O-RU through an O- RAN 7.2x compliant Fronthaul transport and to a 540 
regional O-Cloud through a B ackhaul or Midhaul transport. The Switch Fabric may also be shared across mu ltiple O-541 
Cloud Resource Pools or across multiple O-Cloud instances (via separate network domains). The O-Cloud Switch Fabric 542 
is managed by the Infrastructure Management Services (IMS) described earlier. Figure 10 shows the architecture of an 543 
O-Cloud Resource Pool configured as a hub, comprising of multiple servers.  544 
 545 


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
20 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 546 
Figure 10: Edge O-Cloud Architecture (Hub) 547 
An O-Cloud Resource Pool may also comprise of a single server without any associated Switch Fabric, e.g., infrastructure 548 
deployed at a cell site. In such a scenario where a S witch Fabric is not present, the C ompute Fabric may be directly 549 
connected to the O -RU through an O -RAN compliant front haul connection.  Figure 11 shows the architecture of an O-550 
Cloud Resource Pool in such a configuration without the O-Cloud Switch Fabric.  551 
 552 
 553 
Figure 11: Edge O-Cloud Architecture (Single server) 554 
The requirements on the O -Cloud Switch Fabric such as clock/sync requirements, latency and jitter recommendations 555 
shall be described in a future version of this document. We note that the architecture of a regional cloud may be  similar 556 
to that of an edge cloud but may not include some requirements such as time source.  557 
 558 


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
21 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
5  Deployment Scenarios:  Common Considerations 559 
In any implementation of logical network functionality, decisions need to be made regarding which logical functions are 560 
mapped to which Cloud Platforms, and therefore which functions are to be co -located with other logical functions .  In 561 
this document we do not prescribe one specific implementation, but we do understand that in order to establish agreements 562 
and requirements, the manner in which the Network Functions are mapped to the same or different Cloud Platforms must 563 
be considered.   564 
We refer to each specific mapping as a “deployment scenario”.  In this section, we examine the deployment scenarios that 565 
are receiving the most consideration.  Then we will select the one or ones that should be the focus of initial scenario 566 
reference design efforts. 567 
5.1 Mapping Logical Functionality to Physical Implementations 568 
There are many aspects that need to be considered when deciding to implement logical functions in distinct O-Clouds.  569 
Some aspects have to do with fundamental technical constraints  and economic considerations , while others have to do 570 
with the nature of the services that are being offered.   571 
 Technical Constraints that Affect Hardware Implementations   572 
Below are some factors that will affect the cost of implementations, and can  drive a carrier to require separation of or 573 
combining of different logical functions.   574 
• Environment:  Equipment may be deployed in indoor controlled environments (e.g., Central Offices), semi -575 
controlled environments (e.g., cabinets with fans and heaters), and exposed environments (e.g., Radio Units on 576 
a tower).  In general, the less  controlled the environment, the more difficult and expensive  the equipment will 577 
be.  The required temperature range is a key design factor, and can drive higher power requirements.   578 
• Dimensions:  The physical dimensions can also drive deployment constraints – e.g., the need to fit into a tight 579 
cabinet, or to be placed safely on a tower or pole.   580 
• Transport technology:   The transport technology used for Fronthaul , Midhaul, and Backhaul is often fibe r, 581 
which has an extremely low and acceptable loss rate.  However, there are options  other than fiber, in particular 582 
wireless/ microwave, where the potential for data loss must be considered.  This will be discussed further  in the 583 
next section. 584 
• Acceleration Hardware:  The need for acceleration hardware can be driven by the need to meet basic 585 
performance requirements, but can also be tied to some of the above considerations.  For example, a hardware 586 
acceleration chip (COTS or proprietary) can result in lower power use, less generated heat, and smaller physical 587 
dimensions than if acceleration is not used.  On the other hand, some types of hardware acceleration chips might 588 
not be “hardened” (i.e., they might only operate properly in a restricted environment), and could require a more 589 
controlled environment such as in a central office. 590 
The acceleration hardware most often referred to includes: 591 
• Field Programmable Gate Arrays (FPGAs) 592 
• Graphical Processing Units (GPUs) 593 
• System on Chip (SoC) 594 
• Standardized H ardware:  Use of standardized hardware designs and standardized form factors  can have 595 
advantages such as helping to reduce operations complexity, e.g., when an operator makes  periodic technology 596 
upgrades of selected components.  An example would be to use an Open Compute Project (OCP)  or Open 597 
Telecom IT Infrastructure (OTII) –based design.   598 
 Service Requirements that Affect Implementation Design  599 
RANs can serve a wide range of services and customer  requirements, and each market can drive  some unique  600 
requirements.  Some examples are below. 601 
• Indoor or outdoor deployment:  Indoor deployments (e.g., in a public venue like a sports stadium, train station, 602 
shopping mall, etc.) often enjoy a controlled environment for all elements, including the Radio Units.  This can 603 
improve the economics of some indoor deployment scenarios.  The distance between Network Functions tends 604 

                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
22 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
to be much lower, and the devices that support O -RU functionality may be much easier and cheaper to install 605 
and maintain. This can affect the density of certain deployments, and the frequency that certain scenarios are 606 
deployed.   607 
• Bands supported, and Macro cell vs. Small cell :  The choice of bands (e.g., Sub-6 GHz vs. mmWave) might 608 
be driven by whether the target customers are mobile vs. fixed, and whether a clear line of sight to the customer 609 
is available or is needed. The bands to be supported will of course affect O -RU design.  In addition, because 610 
mmWave carriers can support much higher channel width (e.g., 400 MHz vs. 20 MHz), mmWave deployments 611 
can require a great deal more O -DU and O -CU processing power.   And of course the operations costs of  612 
deploying Macro cells vs. Small cells differ in other ways.   613 
• Performance requirements of the Application /  Network Slice:   Ultimately, user applications drive 614 
performance requirements, and RANs are expected to  support a very wide range of applications.  For example, 615 
the delay requirements to support a Conne cted Car application using Ultra Reliable Low Latency 616 
Communications (URLLC) will be more demanding than the delay requirements for other types of applications.  617 
In our discussion of 5G, we can start by consider ing requirements separately for URLLC, enhanced Mobile 618 
Broadband (eMBB), and massive Machine Type Communications (mMTC). 619 
The consideration of performance requirements is a primary one, and is the subject of Section 5.2.  620 
 Rationalization of Centralizing O-DU Functionality 621 
Almost all Scenarios to be discussed in this document involve a degree of centralization of O -DU.  In this section it is  622 
assumed that O-DU resources for a set of O-RUs are centralized at the same location.   623 
Editor’s Note:  While most Scenarios also centralize O-CU-CP, O-CU-UP, and near-RT RIC in one form or another, 624 
the benefits of centralizing them are not discussed in this section.  625 
Managing O-DU in equipment at individual cell sites (via on-site BBUs today) has multiple challenges, including: 626 
• If changes are needed at a site (e.g., adding radio carriers), then adding equipment is a coarse -grained activity – 627 
i.e., one cannot generally just add “another 1/5 of a box”, if that is all that is needed.  Adding the minimum 628 
increment of additional capacity might result in poor utilization and thereby prevent expansion at that site.   629 
• Cell sites are in many separate locations, and each requires  establishment and maintenance of an acceptable 630 
environment for the equipment.  In turn this requires separate visits for any physical operations.  631 
• Micro sites tend to have much lower average utilization than macro sites, but each can experience considerabl e 632 
peaks. 633 
• “Planned obsolescence” occurs, due to ongoing evolution of smartphone capabilities and throughput 634 
improvements, as well as introduction of new features and services.  It is common practice today to upgrade 635 
(“forklift replace”) BBUs every 36-60 months. 636 
These factors motivate the centralization of resources where possible.  For the O-DU function, we can think of two types 637 
of centralization: simple centralization and pooled centralization.   638 
If the equipment uses O -DU centralization in an Edge Cloud, at an y given hour an O-RU will be using a single specific 639 
O-DU resource that is assigned to it (e.g. via Kubernetes).  On a broad time scale, traffic from any cell site can be rehomed, 640 
without any physical work, to use other/additional resources that are availa ble at that Edge Cloud location.  This would 641 
likely be done infrequently; e.g., about as often as cell sites are expanded.   642 
Centralization can have some additional benefits, such as only having to maintain a single large controlled environment 643 
for many ce ll sites rather than creating and maintaining many distributed locations that might be less controlled (e.g., 644 
outside cabinets or huts).  Capacity can be added at the central site and assigned to cell sites as needed.  Note that simple  645 
centralization still assigns each O-RU to a single O-DU resource1, as shown below, and that traffic from one O-RU is not 646 
split into subsets that could be assigned to different O -DUs.  Also n ote that a Fronthaul (FH) Gateway ( GW) may exist 647 
between the cell site and the centralized resources, not only to improve economics but also to enable traffic re -routing 648 
when desired.  649 
 
1 In this figure, each O-DU block can be thought of as a unit of server resources that includes a hardware accelerator, a GPP, memory and any other 
associated hardware. 

                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
23 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 650 
Figure 12:  Simple Centralization of O-DU Resources 651 
By comparison, with pooled centralization, traffic from an O-RU (or subsets of the O-RU’s traffic) can be assigned more 652 
dynamically to any of several shared O -DU resources.  So if one cell site is mostly idle and another experiences high 653 
traffic demand, the traffic can be routed to the appropriate O-DU resources in the shared pool.  The total resources of this 654 
shared pool can be smaller than resources of distributed locations, because the peak of the sum of the traffic will be 655 
markedly lower than the sum of the individual cell site traffic peaks.   656 
 657 
Figure 13:  Pooling of Centralized O-DU Resources 658 
We note that being able to share O -DU resources somewhat dynamically is expected to be a solvable problem, although 659 
we understand that it is by no means a trivial problem.  There are management considerations, among others.  There may 660 
be incremental steps toward true shared pooling, where rehoming of O -RUs to different O-DUs can be performed more 661 
dynamically, based on traffic conditions. 662 
It is noted that O-DU centralization benefits the most dense networks where several cell sites are within the O -RU to O-663 
DU latency limits.  Sparsely populated areas most probably will be addressed by vO -CU centralization only.   664 
Figure 14 shows the results of an analysis of a simulated  greenfield deployment as an attempt to visualize the relative 665 
merit of simple centralization of O -DU (“oDU”) vs. pooled centralization of O -DU (“poDU”) vs. legacy DU (“BBU”), 666 
plotted against the realizable Cell Site pool size.  667 


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
24 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 668 
Figure 14:  Comparison of Merit of Centralization Options vs. Number of Cell Sites in a Pool  669 
An often-used measure is related to the power require d to support a given number of carrier MHz.  The lower the power 670 
used per carrier, the more efficient is the implementation.  In Figure 14, the values of each curve are normalized to the 671 
metric of Watts/MHz for distributed legacy BBUs, normalized to equal 1.  Please note that in this diagram, a lower value 672 
is better.  The following assumptions apply to the figure:   673 
• A legacy BBU processes X MHz (for carriers) and consumes Y watts.  For example, a specific BBU might 674 
process 1600 MHz and consume 160 watts.  675 
• N legacy BBUs will process N x X  MHz and consume N x Y watts and  have a merit figure of 1, per 676 
normalization.  If a given site requires less than X MHz, it will still be necessary to deploy an X MHz BBU.  For 677 
example, we may need only 480 MHz but still deploy a 1600 MHz BBU.  678 
• Simple Centralization (the “oDU” line):  In this case, active TRPs are statically mapped to specific VMs and 679 
vO-DU tiles2.  Fewer vO-DU tiles are required to support the same number of  TRPs, because MHz per site is 680 
not a constant. 681 
• Independent of resources to support active user traffic, a fixed power level is required to power Ethernet 682 
“frontplane” switches and hardware to support management and orchestration processes.  683 
• In a pool, processing capacity will be added over time as required. 684 
• Due to mobility traffic behavior, tiles will not be fully utilized, although centralization of resources will 685 
improve utilization when compared with a legacy BBU approach.   686 
• Centralization with more dynamic pooling (the “poDU ” line): In addition to active load balancing,  individual 687 
traffic flows (which can last from a few hundreds of msecs  to several seconds) will be  routed to the least used 688 
tile, further optimizing (reducing) vO-DU tile requirements.   689 
• As in the simple centra lization approach above, there is a fixed power level required for hardware that 690 
supports switching, management and orchestration processes. 691 
As a final note, any form of centralization requires efficient transport between the O-RU and the O-DU resources.  When 692 
O-RU functionality is distributed over a relatively large area (e.g., not concentrated in a single large building), the 693 
existence of a Fronthaul Gateway is a key enabler.   694 
 
2 A “vO-DU tile” refers to a chip or System on Chip (SoC) that provides hardware acceleration for math-intensive functionality such as that required 
for Digital Signal Processing.  With the Option 7.2x split, acceleration of Forward Error Correction (FEC) functionality is required (FEC is 
optional for e.g. low band.), and other functionality could be considered for acceleration if desired.  


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
25 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
5.2 Performance Aspects 695 
Performance requirements drive architectural and design considerations.   Performance can include attributes such as 696 
delay, packet loss, transmission loss, and delay variation (aka “jitter”).   697 
Editor’s Note:  While all aspects are of interest, delay has the largest impact on network design and will be th e 698 
focus of this version.  Future versions can address other performance aspects if desired and is FFS.   699 
 User Plane Delay 700 
This section discusses the framework for discussing delay of user-plane packets3, and also general delay numbers that it 701 
can be agreed that apply across all scenarios.  Details relevant to a specific Scenario will be discussed  in each Scenario’s 702 
subsection, as applicable. The purpose of these high- level targets is to act as a baseline for allocating the total latency 703 
budget to subsystems that are on the path of each constraint, as required for system engineering and dimensioning 704 
calculations, and to assess the impact on the function placement within the specific network site tiers.   705 
The goal is to establish reasonable maximum delay targets, as well as to identify and document the major infrastructure 706 
as well as O -RAN NF-specific delay contributing components. For each service or element, minimum delay should be 707 
considered to be zero. The implication of this is that any of the elements can be moved towards the Cell Site (e.g. in a 708 
fully distributed Cloud RAN configuration, all of O-CU-UP, O-DU and O-RU would be distributed to Cell Site).  709 
In real network deployments, the expectation is that, depending on the operator-specific implementation constraints such 710 
as location and fiber availability, deployment area density, etc., deployments result in anything between the fully 711 
distributed and maximally centralized configuration. Even on one operator’s network, it is common that there are many 712 
different sizes of Edge Cloud instances, and combinations of Centralized and Distributed architectures in same network 713 
are also common (e.g. network operator may choose to centralize the deployments on dense Metro areas to the extent 714 
possible and distribute the configurations on suburban/rural areas with larger cell sizes / cell density that do not translat e 715 
to pooling benefits from more centralized architecture). However, the maximum centralization within the constraints of 716 
latencies that can be tolerable is useful for establishing the basis for dimensioning of the maximum sizes, especially for 717 
the Edge and Regional cloud PoPs. Figure 15 below illustrates the relationship among some key delay parameters.   718 
 
3 Delay of control plane or OAM traffic is not considered in this section.  

                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
26 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 719 
Figure 15:  Major User Plane Latency Components, by 5G Service Slice and Function Placement  720 
Please note the following: 721 
• NOTE 1: If the T2 or/and T3 transport network(s) is/are Packet Transport Network(s), then time allocation for 722 
the transport network elements  processing and queuing delays will require some portion of maximum latency 723 
allocation, and will require reduction of the maximum area accordingly. 724 
• NOTE 2: Site Internal / fabric networks are not shown for clarity, but need some latency allocation (effectively 725 
extensions or part of transport delays; per PoP tier designations TE1, TE2, TE3 and TC). 726 
• NOTE 3: To maximize the potential for resource pooling benefits, minimize network function redundancy cost, 727 
and minimize the amount of hardware / power in progressively more distributed sites (towards UEs), target 728 
design should attempt to maximize the distances and therefore latencies available for transport networks within 729 
the service- and RAN-specific time constraints, especially for TT1. 730 
• NOTE 4: UPF, like EC /MEC, is outside of the scope of O -RAN, so UPF shown as  a “black box” to illustrate 731 
where it needs to be placed in context of specific services to be able to take advantage of the RAN service -732 
specific latency improvements. 733 
Figure 15 represents User Equipment locations on the right, and network tiers towards the left, with increasing latency 734 
and increasing maximum area covered per tier towards the left. These Mobile Network Operator’s (MNO’s) Edge tiers 735 
are nominated as Cell Site, Edg e Cloud, and Regional Cloud, with one additional tier nominated as Core Cloud in the 736 
figure. 737 
The summary of the associated latency constraints as well as major latency contributing components as depicted in Figure 738 
15 above is given in Table 1, below. 739 
Table 1:  Service Delay Constraints and Major Delay Contributors 740 
RAN Service-Specific User Plane Delay Constraints 
Identifier Brief Description 
Max. OWD 
(ms) 
Max. RTT 
(ms) 
URLLC Ultra-Reliable Low Latency Communications (3GPP) 0.5 1 
URLLC Ultra-Reliable Low Latency Communications (ITU) 1 2 


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
27 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
eMBB enhanced Mobile Broadband 4 8 
mMTC massive Machine Type Communications 15 30 
Transport Specific Delay Components 
TAIR Transport propagation delay over air interface     
TE1 Cell Site Switch/Router delay     
TT1 Transport delay between Cell Site and Edge Cloud 0.1 0.2 
TE2 Edge Cloud Site Fabric delay     
TT2 Transport delay between Edge and Regional Cloud 1 2 
TE3 Regional Cloud Site Fabric delay     
TT3 Transport delay between Regional  and Core Cloud 10 20 
TC Core Cloud Site Fabric delay     
Network Function Specific Delay Components 
TUE Delay Through the UE SW and HW stack     
TRU Delay Through the O-RU User Plane     
TDU Delay Through the O-DU User Plane     
TCU-UP Delay Through the O-CU User Plane     
 741 
The transport network delays are specified as maximums, and link speeds are considered to be symmetric for all 742 
components with exception of the air interface (TAIR).  For the S-Plane services utilizing PTP protocol, it is a requirement 743 
that the link lengths, link speeds and forward-reverse path routing for PTP are all symmetric. 744 
Radios (O-RUs) are always located in the Cell Site tier, while O-DU can be located “up to” Edge Cloud tier. It is possible 745 
to move any of the user pl ane NF instances closer towards the cell site, as implicitly they would be inside the target 746 
maximum delay, but it is not necessarily possible to move them further away from the Cell Sites while remaining within 747 
the RAN internal and/or RAN service-specific timing constraints.  A common expected deployment case is one where O-748 
DU instances are moved towards or even to the Cell Site and O -RUs (e.g. in Distributed Cloud-RAN configurations), or 749 
in situations where the Edge Cloud needs to be located closer to the  Cell Site due to fiber and/or location availability , or 750 
other constraints. While this is expected to work well from the delay constraints perspective,  the centralization and 751 
pooling-related benefits will be potentially reduced or even eliminated in the context of such deployment scenarios.  752 
The maximum transport network latency between the site hosting O -DU(s) and sites hosting associated O -RU(s) is 753 
primarily determined by the RAN internal processes time constraints (such as HARQ loop, scheduling, etc., time-sensitive 754 
operations). For the purposes of this document, we use 100us latency, which is commonly used as a target maximum 755 
latency for this transport segment i n related industry specifications for user -plane, specifically “High100” on E -CPRI 756 
transport requirements [4] section 4.1.1, as well as “Fronthaul” latency requirement in ITU technical report GSTR-TN5G 757 
[6], section 7 -2, and IEEE Std 802.1CM -2018 [5], section 6.3.3.1.  Based on the 5us/km fiber propagation delay, this 758 
implies that in a 2D Manhattan tessellation model, which is a common simple topology model for dense urban area fiber 759 
routing, the maximum area that can be covered from a single Edge Cloud tier site hosting  O-DUs is up to a 400km2  area 760 
of Cell Sites and associated RUs .  Based on the radio inter -site distances, number of bands and other radio network 761 
dimensioning specific parameters, this can be used to estimate the maximum number of Cell Sites and cell sectors that 762 
can be covered from single Edge Cloud tier location, as well as maximum number of UEs in this coverage area. 763 
The maximum transport network latencies towards the entities located at higher tiers are constrained by the lower of F1 764 
interface latency (max 10 ms as per GSTR -TN5G [6], section 7.2), or alternatively service -specific latency constraints, 765 
for the edge -located services that are positioned to take advantage of improved latencies.   For eMBB, UE -CU latency 766 
target is 4m s one -way delay, while for the U RLLC it is 0.5ms as per 3GPP (or 1ms as per ITU requirements). The 767 
placement of the O-CU-UP as well as associated UPF, to be able to provide URLLC services would have to be at most at 768 
the Edge Cloud tier to satisfy the service latency constraint. For the eMBB services with 4ms OWD target,  it is possible 769 
to locate O-CU-UP and UPF on next higher latency location tier, i.e. Regional Cloud tier. Note that while not shown i n 770 
the picture, Edge compute / Multi-Access Edge Compute (MEC) services for a  given RAN service type are expected to 771 
be collocated with the associated UPF function to take advantage of the associated service latency reduction potential.  772 
For the services that do not have specific low-latency targets, the associated O-CU-UP and UPF can be located on higher 773 
tier, similar to deployments in typical LTE network designs. This is designated as Core Cloud tier in the example in Figure 774 

                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
28 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
15 above.  For eMBB services, if there are no local service instances in the Edge or Regional clouds to take advantage of 775 
the 4ms OWD enabled by eMBB service definition, but the associated services are provided from either core clouds, 776 
external networks or from other Edge Cloud / RAN instances (in case of user -to-user traffic), the associated non -777 
constrained (i.e. over 4ms from subscriber) eMBB O -CU-UP and UPF instances can be located in Core Cloud sites 778 
without perceivable impact to the service user, as in such cases the transport and/or service-specific latencies are dominant 779 
latency components.  780 
The intent of this section is not to micromanage the latency budget, but to rather establish a reasonable baseline for 781 
dimensioning purposes, particularly to provide basic assessment to enable sizing of the cloud t iers within the context of 782 
the service-specific constraints and transport allocations. As such, we get the following “allowances” for the aggregate 783 
unspecified elements: 784 
• URLLC3GPP: 0.5ms - 0.1ms (TT1) = 0.4ms  ≥ TUE + TAIR + TE1 + TRU + 2(TE2) + TDU + TCU-UP 785 
• URLLCITU: 1ms - 0.1ms (TT1) = 0.9ms  ≥ TUE + TAIR + TE1 + TRU + 2(TE2) + TDU + TCU-UP 786 
• eMBB: 4ms - 0.1ms (TT1) - 1ms (TT2) = 2.9ms ≥ TUE + TAIR + TE1 + TRU + 2(TE2) + TDU + TE3 + TCU-UP 787 
• mMTC15: 15ms - 0.1ms (TT1) - 1ms (TT2) - 10ms (TT3) = 3.9ms ≥ TUE + TAIR + TE1 + TRU + 2(TE2) + TDU + TE3 + 788 
TCU-UP + TC 789 
 790 
If required, we may provide more specific allocations in later versions of the document, as we gain more implementation 791 
experience and associated test data, but at this stage it is considered to be premature to do so. It should also be noted that 792 
the URLLC specification is still work in progress at this stage in 3GPP , so likely first implementations will focus on 793 
eMBB service, which leaves 2.9ms for combined O-RAN NFs, air interface, UE and cloud fabric latencies. 794 
It is possible that network queuing delays may be the dominant delay contributor for some service classes. However, these 795 
delay components should be understood to be  in context of the most latency -sensitive services, particu larly on RU-DU 796 
interfaces, and relevant to the system level dimensioning. It is expected that if we will have multiple QoS classes, then 797 
the delay and loss parameters are specified on per-class basis, but such specification is outside of scope of this section.  798 
The delay components in this section are based on presently supported O -RAN splits, i.e. 3GPP reference split 799 
configurations 7-2 & 8 for the RU-DU split (as defined in O-RAN), and 3GPP split 2 for F1 (as defined in O -RAN) and 800 
associated transport allocations, and constraints are based on the 5G service requirements from ITU & 3GPP .  801 
Other extensions have been approved and included in version 2.0 of the O-RAN Fronthaul specification [7], which allow 802 
for so called “non -ideal” Fronthaul. It should be noted that while they allow substantially larger delays (e.g. 10 ms FH 803 
splits have been described and implemented outside of O-RAN), they cannot be considered for all possible 5G use cases, 804 
as for example it is clearly impossible to meet the 5G service- specification requirements over such large delay values 805 
over the FH for URLLC or even 4 ms eMBB services. In addition, in specific scenarios (e.g. high- speed users), adding 806 
latency to the fronthaul interface can result in reduced performance, and lower potential benefits, e.g. in Co- Ordinated 807 
Multi-Point (CoMP) mechanisms. 808 
5.3 Hardware Acceleration and Acceleration Abstraction Layer 809 
(AAL) 810 
As stated in Section 4.3.2, an O-Cloud Node is a collection of CPUs, Memory, Storage, NICs, BIOSes, BMCs, etc. , and 811 
may include hardware accelerators to offload computational-intense functions with the aim of optimizing the performance 812 
of the O-RAN Cloudified NF (e.g., O-RU, O-DU, O-CU-CP, O-CU-UP, near-RT RIC).  There are many different types 813 
of hardware accelerators, such as FPGA, ASIC, DSP, GPU, and many different types of acceleration functions, such as 814 
Low-Density Parity -Check (LDPC) , Forward Error Correction (FEC),  end-to-end high- PHY for O -DU, security 815 
algorithms for O -CU, and Artificial Intelligence for RIC .  The combination of hardware accelerator and acceleration 816 
function, and indeed the option to use hardware acceleration, is the vendor’s choice; however , all types of hardware 817 
acceleration on the cloud platform should ensure the decoupling of software from hardware. This decoupling implies the 818 
following key objectives:  819 
• Multiple vendors of hardware GPP CPUs and accelerators (e.g., FGPA, ASIC, DSP, or GPU) can be used in O-820 
Cloud platforms (including agreed- upon Acceleration Abstraction Layer as defined in an upcoming 821 
specification) from multiple vendors, which in turn can support the software providing RAN functionality.  822 
• A given hardware and cloud platform shall support RAN software (including near -RT RIC, O-CU-CP, O-CU-823 
UP, O-DU, and possibly O-RU functionality in the future) from multiple vendors.  824 
There are different concepts that should be considered for the hardware acceleration abstraction layer on the cloud 825 
platform; these are usually the following:  826 

                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
29 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
• Accelerator Deployment Model  827 
• Acceleration Abstraction Layer (AAL) Interface (i.e., the APIs used by the NFs)  828 
 829 
Figure 16: Hardware Abstraction Considerations 830 
 Accelerator Deployment Model  831 
Figure 16 above presents two common hardware accelerator deployment models  as examples : an abstracted 832 
implementation utilizing a vhost_user and virtIO type deployment, and a pass -through model using SR -IOV. While the 833 
abstracted model allows a full decoupling of the Network Function (NF) from the hardware accelerator, this model may 834 
not suit real-time latency sensitive NFs such as the O-DU. For better acceleration capabilities, SR-IOV pass through may 835 
be required, as it is supported in both VM and container environments.  836 
 Acceleration Abstraction Layer (AAL) Interface 837 
To allow multiple NF vendors to utilize a given accelerator through its Acceleration Abstraction Layer (AAL)  interface, 838 
the a ccelerators must provide an open -sourced API.  Likewise, this API shall allow NFs applications to discover , 839 
configure, select and use (one or more) acceleration functions provided by a given accelerator  on the cloud platform . 840 
Moreover, this API shall also support different offload architectures including look aside , inline and any combination of 841 
both. Examples of open APIs include DPDK’s CryptoDev, EthDev, EventDev, and Base Band Device (BBDEV).  842 
When delivering an NF to an Operator, it is assumed that the supplier of that Network Function will provide not only the 843 
Network Function, but it will also package the appropriate Accelerator Driver (possibly provided by a 3rd party) and will 844 
indicate the corresponding AAL profile  needed in the Operator’s O -Cloud. Figure 17 illustrates this for both Container 845 
and Virtual Machine (VM) deployments.  846 
  847 
Figure 17: Accelerator APIs/Libraries in Container and Virtual Machine Implementations 848 


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
30 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 Accelerator Management and Orchestration Considerations 849 
Note that Figure 17 shows the APIs/Libraries as used by the NF application running in a Container or a VM, but there are 850 
several entities that require management. Accordingly,  t he figure also shows the Accelerator Management and 851 
Accelerator Driver in the O -Cloud.  As will be discussed in Section 5.6 , these entities (in addition to  any hardware 852 
accelerator considerations) will be managed via O2, specifically the Infrastructure Management Services.  Figure 17 also 853 
shows that the Accelerator Driv er (e.g., the PMD driver) needs to be supported both by the O -Cloud Platform, by the 854 
Guest OS in case of VMs, and by the NF packaged into a container.   855 
In general, t he hardware accelerators shall be capable of being managed and orchestrated. In particular, hardware 856 
accelerators shall support feature discovery and life cycle management.  Existing Open Source solutions may be leveraged 857 
for both VMs and containers as defined in an upcomingO2 specification.  Examples include OpenStack Nova and Cyborg, 858 
while in Kubernetes we can leverage the device plugin framework for vendors to advertise their device and associated 859 
resources for the accelerator management.   860 
5.4 Cloud Considerations 861 
In this section we talk about the list of cloud platform capabilities which is expected to be provided by the cloud platform 862 
to be able to support the deployment of the scenarios which are covered by this document.   863 
It is assumed that some or all deployment scenarios may be using VM orchestrated/managed by OpenStack and / or 864 
Container managed/orchestrated by Kubernetes, and therefore this section will cover both options. 865 
The discussion in most sub-sections of this section is structured into (up to) three parts:  (1) Common, (2) Container 866 
only, and (3) VM only.  867 
 Networking requirements 868 
A Cloud Platform should have the ability to support high performance N – S and E – W networking, with high throughput 869 
and low latency.  870 
5.4.1.1 Support for Multiple Networking Interfaces 871 
Common:  In the different scenarios, near -RT RIC, vO -CU, and vO -DU all depend on having support for multiple 872 
network interfaces. The Cloud Platform is required to support the ability to assign multiple networking interfaces to a 873 
single container or VM instance, so that the cloud platform could support successful deployment for the different 874 
scenarios.  875 
Container-only:  For example, the cloud platform can achieve this by supporting the implementation of Multus Container 876 
Networking Interface (CNI) Plugin. For more details, please see https://github.com/intel/multus-cni. 877 
 878 
Figure 18:  Illustration of the Network Interfaces Attached to a Pod, as Provisioned by Multus CNI  879 
VM-only:  OpenStack provides the Neutron component for networking. For more details, please see 880 
https://docs.openstack.org/neutron/stein/  881 
5.4.1.2 Support for High Performance N-S Data Plane 882 
Common:  The Fronthaul connection between the O -RU/RU and vO -DU requires high performance and low latency. 883 
This means handling packets at high speed and low latency. As per the different scenarios covered in this document, 884 


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
31 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
multiple vO-DUs may be running on the same physical cloud platform, which will result in the need for sharing the same 885 
physical networking interface with multiple functions. Typically, the SR-IOV networking interface is used for this. 886 
The cloud platform will need to provide support for assigning SR -IOV networking interfaces to a container or VM 887 
instance, so the instance can use the network interface (physical function or virtual function) directly without using a 888 
virtual switch.  889 
If only one container needs to use the networking interface, the PCI pass -through network interface can provide high 890 
performance and low latency without using a virtual switch. 891 
In general, the following two items are needed for high performance N-S data throughput: 892 
• Support for SR-IOV; i.e., the ability to assign SR-IOV NIC interfaces to the containers/ VMs 893 
• Support for PCI pass-through for direct access to the NIC by the container/ VM  894 
Container-only:  When containers are used, the cloud platform can achieve this by supporting the implementation of SR-895 
IOV Network device plugin for Kubernetes. For more details, please refer to https://github.com/intel/sriov-network-896 
device-plugin  897 
VM-only: OpenStack provides the Neutron component for networking. For more details, please see 898 
https://docs.openstack.org/neutron/stein/admin/config-sriov.html . 899 
5.4.1.3 Support for High-Performance E-W Data Plane 900 
Common:  High-performance E-W data plane throughput is a requirement for the implementation of the different near -901 
RT RIC, vO-CU, and vO-DU scenarios which are covered in this document.  902 
One of commonly used options for E-W high-performance data plane is the use of a virtual switch which provides basic 903 
communication capability for instances deployed at either the same machine or different machines. It provides L2 and L3 904 
network functions.  905 
To get the high performance required, one of the options  is to use a Data Plan Development Kit (DPDK) -based virtual 906 
switch.  Using this method, the packets will not go into Linux kernel space networking, and instead will implement 907 
userspace networking which will improve the throughput and latency. To support this, the container or VM instance will 908 
need to use DPDK to accelerate packet handling.  909 
The cloud platform will need to provide the mechanism  to support the implementation of userspace networking for 910 
container(s) / VM(s). 911 
Container-only:  As an example, the  cloud platform can achieve this by supporting implementation of Userspace CNI 912 
Plugin. For more details, please refer to https://github.com/intel/userspace-cni-network-plugin. 913 
 914 
Figure 19:  Illustration of the Userspace CNI Plugin 915 
VM-only:  OVS DPDK is an example of a Host userspace virtual switch and could provide high performance L2/L3 916 
packet receive and transmit.   917 


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
32 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
5.4.1.4 Support for Service Function Chaining  918 
Common:  Support for a Service Function Chaining (SFC) capability requires the ability to create a service function 919 
chain between multiple VMs or containers. In the virtualization environment, multiple instances will usually be deployed, 920 
and being able to efficiently connect the instances to provide service will be a fundamental requirement.  921 
The ability to dynamically configure traffic flow will provide flexibility to Operators.  When the service requirement or 922 
flow direction needs to be changed, the Service Function Chaining capability can be used to easily implement it instead 923 
of having to restart and reconfigure the services, networking configuration and Containers/VMs.  924 
Container-only: An example of SFC functionality is found at: https://networkservicemesh.io/ 925 
VM only:  The OpenStack Neutron SFC and OpenFlow -based SFC are examples of solutions that can implement the 926 
Service Function Chaining capability. 927 
5.4.1.5 Support for VLAN based networking 928 
Common:  VLAN based networking is the most common and fundamental form of networking. VLANs are typically 929 
used to provide the isolation of various types of traffic in cloud environments. Cloud platforms must support the traffi c 930 
isolation requirements of the application.  931 
The O-RAN slicing use cases specified in [14] require the use of VLANs by O-RAN NFs to distinguish traffic belonging 932 
to different slices. To support this requirement,  the O-Cloud platform must provide support for trunked VLAN network 933 
interfaces to be made available to Cloudified NFs (VMs and Containers) so that packets tagged with different VLANs 934 
can be transported on the same virtual network interface. 935 
 936 
VLANs may also be used to differentiate slices in the transport network  once the appropriate VLAN tags are applied by 937 
Cloudified NFs in the Data Center as specified in [14] . Therefore, the O -Cloud must also ensure that any VLAN tags 938 
applied by the O-RAN NFs are carried over to the transport network. 939 
 940 
Container-only: For example, the cloud platform can achieve this by supporting the implementation of Multus Container 941 
Networking Interface (CNI) Plugin. For more details, please see https://github.com/k8snetworkplumbingwg/multus -cni 942 
VM only:  OpenStack provides the Neutron component for networking. For more details, please see 943 
https://docs.openstack.org/neutron/stein/  944 
 945 
 Assignment of Acceleration Resources 946 
Common:  For both container and VM solutions, specific devices such as accelerator (e.g., FPGA, GPU) may be needed. 947 
In this case, the cloud platform needs to be able to assign the specified device to container instance or VM instance.  948 
For example, some L1 protocols require an FFT algorithm (to compute the DFT) that could be implemented in an FPGA 949 
or GPU, and the vO-DU would need the PCI Pass-Through to assign the accelerator device to the vO-DU for access and 950 
use. 951 
 Real-time / General Performance Feature Requirements 952 
5.4.3.1 Host Linux OS 953 
5.4.3.1.1 Support for Pre-emptive Scheduling  954 
Support may be required to support Pre -emptive Scheduling (real time Linux uses the preempt_rt  patch). Generally, 955 
without real time features, it is very difficult for an application to get deterministic response times for events, interrupts 956 
and other reasons4. In addition, during the housekeeping processes in Linux system, the application also cannot guarantee 957 
the running time (CPU cycle), so from the wireless application design perspective, it needs the real time feature. In 958 
 
4 Other options include things such as Linux signal, softwareirq, and perhaps using a common process. Because the pre-emptive kernel could 
interrupt the low priority process and occupy the CPU, it will get more chance to run the high priority process. Then through proper application 
design, it will have guaranteed time/resource and can have deterministic performance. 

                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
33 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
addition, to support the requirements of high throughput, multiple accesses and low latency, some wireless applications 959 
need the priority-based OS environment.  960 
5.4.3.2 Support for Node Feature Discovery 961 
Common:  Automated and dynamic placement of Cloud-Native Network Functions (CNFs) / microservices and VMs is 962 
needed, based on the hardware requirements imposed on the vO -DU, vO-CU and near-RT RIC functions.  This requires 963 
the cloud platform to support the ability to discover the hardware capabilities on each node and advertise it via labels vs. 964 
nodes, and allows O- RAN Cloudified NFs’  descriptions to have hardware requirements via labels . This mechanism is 965 
also known as Node Feature Discovery (NFD). 966 
Container-only:  For example, the cloud platform can achieve this by supporting implementation of NFD for Kubernetes. 967 
For more details, please see https://github.com/kubernetes-sigs/node-feature-discovery. 968 
VM-only:  VMs can use OpenStack mechanisms.  For example, the OpenStack Nova filter, host aggregates and 969 
availability zones can be used to implement the same function. 970 
5.4.3.3 Support for CPU Affinity and Isolation 971 
Common:  The vO-DU, vO-CU and even the near-RT RIC are performance sensitive and require the ability to consume 972 
a large amount of CPU cycles to work correctly.  They depend on the ability of the cloud platform to provide a mechanism 973 
to guarantee performance determinism even when there are noisy neighbors.  974 
Container-only:  This requires the cloud platform to support using affinity and isolation of cores, so high performance 975 
Kubernetes Pod cores also can be dedicated to specified tasks.  For example, the cloud platform can achieve this by 976 
implementing CPU Manager for Kubernetes. For more details, please refer to https://github.com/intel/CPU-Manager-for-977 
Kubernetes . 978 
VM-only:  For example the modern Linux operating system uses the Symmetric MultiProcessing  (SMP) mode, so the 979 
system process and application will be located at different CPU cores. To run the VM and guarantee the VM performance, 980 
the capability to assign the specific CPU cores to a VM is the way to do that. And at the same time, CPU isolation will 981 
reduce the inter-core affinity.  Please refer to https://docs.openstack.org/senlin/pike/scenarios/affinity.html  982 
5.4.3.4 Support for Dynamic HugePages Allocation 983 
Common:  When an application requires high performance and performance determinism, the reduction of paging is very 984 
helpful. vO-DU, vO-CU and even near -RT RIC can require performance determinism. The cloud platform needs to be 985 
able to support the ability to provide this mechanism to applications that require it. 986 
This requires the cloud platform to support ability to dynamically allocate the necessary amount of the faster memory 987 
(a.k.a. HugePages) to the container or VM as necessary, and also to relinqui sh this memory allocation in the event of 988 
unexpected termination.  989 
Container-only:  For example, the cloud platform can achieve this by supporting implementation of Manage HugePages 990 
in Kubernetes. For more details please refer to https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-991 
hugepages/ . 992 
VM-only:  For example, the OpenStack Nova flavor setting can be used to configure the HugePage size for a VM instance.  993 
See https://docs.openstack.org/nova/pike/admin/huge-pages.html  994 
5.4.3.5 Support for Topology Manager 995 
Common:  Some of the cloud infrastructure which is targeted in the scenarios in this document may have servers which 996 
utilize a multiple-socket configuration which comes with multiple memory regions. Each core5 is connected to a memory 997 
region. While each CPU on one socket can access the memory region of the CPUs on another socket of the same board, 998 
the access time is significantly slower when crossing socket boundaries, and this will affect performance significa ntly.  999 
The configuration of hardware with multiple memory regions is also known as Non- Uniform Memory Access (NUMA) 1000 
regions. To support automated and dynamic placement of CNFs/microservices or VMs based on cloud infrastructure that 1001 
 
5 In this document, we use the terms core and socket in the following way.  A socket, or more precisely the multichip platform that fits into a server 
socket, contains multiple cores, each of which is a separate CPU.  Each core in a socket has some dedicated memory, and also some shared 
memory among other cores of the same socket, which are within the same NUMA zone.  

                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
34 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
has multiple NUMA regions and guarantee the response time of the application (especially for vO-DU), it is critical to be 1002 
able to ensure that all the containers/VMs are associated with  core(s) which are connected to the same NUMA region. In 1003 
addition, if the application relies on  access to hardware accelerators and/or I/O which uses memory as a way to interact 1004 
with the application, it is also critical that those also use the same NUMA region that the application uses.  1005 
The cloud platform will need to provide the mechanism to enable managing the NUMA topology to ensure the placement 1006 
of specified containers/VMs on cores which are on the same NUMA region, as well as making sure that the devices which 1007 
the application uses are also connected to the same NUMA region.  1008 
 1009 
Figure 20:  Example Illustration of Two NUMA Regions 1010 
5.4.3.6 Support for Scale In/Out 1011 
Common:  The act of scaling in/out of containers/ VMs can be based on triggers such as CPU load, network load, and 1012 
storage consumption. The network service usually is not just a single container or VM, and in order to leverage the 1013 
container/ VM benefit, the netwo rk service usually will have multiple containers/ VMs. But if demand is changing 1014 
dynamically, especially for the O -CU, the service needs to be scaled in/out according to service requirements such as 1015 
subscriber quantity.  1016 
For example, when the number of subscribers increases, the system needs to start more container/ VM instances to ensure 1017 
the service quality. From the cloud platform perspective, it could monitor the CPU load; if the load reaches a level such 1018 
as 80%, it needs to scale out. If the CPU load drops 40%, it could then scale in. 1019 
Different services can scale in/out depending on different criteria, such as the CPU load, network load and storage 1020 
consumption.  Support for scale in/out can be helpful in implementing on-demand services.  1021 
Editor’s Note:  Support for scale up/down is not discussed at this time, but may be revisited in the future.   1022 
5.4.3.7 Support for Device Plugin 1023 
Common:  For vO -DU, vO-CU and near -RT RIC applications, hardware accelerators such as SmartNICs, FPGAs and 1024 
GPUs may be required to meet performance objectives that can’t be met by using software only implementations.  In 1025 
other cases, such accelerators can be useful as an option to reduce the consumption of CPU cycles to achieve better cost 1026 
efficiency. 1027 
The cloud platform will need to provide the mechanism to support those accelerators. This in turn requires support the 1028 
ability to discover, advertise, schedule and manage devices such as SR-IOV, GPU, and FPGA.   1029 
Container-only:  For example, the cloud platform can achieve this by supporting implementation of Device Plugins in 1030 
Kubernetes. For more details please check: https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-1031 
net/device-plugins/. 1032 
VM-only:  The PCI passthrough feature in OpenStack allows full access and direct control of a physical PCI device in 1033 
guests. This mechanism is generic for any kind of PCI device, and runs with a Network Interface Card (NIC), Graphics 1034 
Processing Unit (GPU), or any other devices that can be attached to a PCI bus.  Correct driver installation is the only 1035 
requirement for the guest to properly use the devices. 1036 


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
35 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
Some PCI devices provide Single Root I/O Virtualization and Sharing (SR -IOV) capabilities. When SR -IOV is used, a 1037 
physical device is virtualized and appears as multiple PCI devices. Virtual PCI devices are assigned to the same or 1038 
different guests. In the case of PCI passthrough, the full physical device is assigned to only one guest and cannot be shared. 1039 
See https://wiki.openstack.org/wiki/Cyborg 1040 
5.4.3.8 Support for Direct IRQ Assignment 1041 
VM-only:  The general -purpose platform has many devices that will generate the IRQ to the system. To develop a 1042 
performance-sensitive application, inclusion of low-latency and deterministic timing features, and assigning the IRQ to a 1043 
specific CPU core, will reduce the impact of housekeeping processes and decrease the response time to desired IRQs.  1044 
5.4.3.9 Support for No Over Commit CPU 1045 
VM-only:  The “No Over Commit CPU” VM creation option is able to guarantee VM performance with a “dedicated 1046 
CPU” model. 1047 
In traditional telecom equipment design, this will maintain the level of CPU utilization to avoid burst and congestion 1048 
situations. In a virtualization environment, performance-sensitive applications such as vO-DU, vO-CU, and near-RT RIC 1049 
will need the platform to provide a mechanism to secure the CPU resource.  1050 
5.4.3.10 Support for Specifying CPU Model 1051 
VM-only:  OpenStack can use the CPU model setting to configure the vCPU for a VM.  For example, QEMU allows the 1052 
CPU options to be “Nehalem”, “Westmere”, “SandyBridge” or “IvyBridge”, or alternatively it could be configured as 1053 
“host-passthrough”. This allows VMs to leverage advanced features of selected CPU architectures. For the vO -CU and 1054 
vO-DU design and implementation, there will be some algorithm and computing functi ons that can leverage host CPU 1055 
instructions to realize some benefits such as performance. The cloud platform needs to provide this capability to VMs.  1056 
 Storage Requirements 1057 
The storage requirements are the same for both VM and Container based implementations.  1058 
For O-RAN components, the O-RAN Cloudified NF needs storage for the image and for the O-RAN Cloudified NF itself.  1059 
It should support different scale , e.g., for a  Regional Cloud vs. an  Edge Cloud.  The cloud platform needs to  support a 1060 
large-scale storage solution with redundancy, medium and small -scale storage solution s for two or  more servers, and a 1061 
very small-scale solution for a single server.  1062 
 Notification Subscription Framework 1063 
Applications should have the ability to retrieve notifications that are necessary for their functionality. For example, vO-1064 
DU needs to know that the node that it starts on has a PTP clock in sync with the master clock.  1065 
Rationale – Application functionality often relies on but is not limited to O-Cloud platform HW resources such as FPGA, 1066 
GPU, PHC. Hence, these application(s) should have the ability to select the resources that will provide them notifications 1067 
about the status of these resources, initial state and changing state. This requires the applications to use a privilege mode 1068 
in order to access the O -Cloud platform drivers and retrieve the status . H owever, in a Cloud Native environment , 1069 
applications should not have a privilege mode for accessing the O -Cloud resources. This framework allows applications 1070 
to subscribe for their necessary  notifications without claiming a privilege mode and comply with O -Cloud Native 1071 
requirements.    1072 
5.4.5.1 O- Cloud Notification Subscription Requirements 1073 
Tracking function: 1074 
• tracks for resource(s) state of relevant data (for example, change in class of a master clock)    1075 
• tracking function can be configured with tracking frequency per the resource being tracked (default value will 1076 
be defined) 1077 
Registration function: 1078 
• allows application(s) and/or SMO (or other entities) to query for the resources that provide notifications  1079 

                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
36 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
• allows application(s) and/or SMO (or other entities) to subscribe to receiving notifications from the selected 1080 
resource(s) 1081 
• allows application(s) and/or SMO (or other ent ities) to subscribe to pulling notifications/data from the selected 1082 
resource(s) 1083 
• allows application(s) and/or SMO (or other entities) to unsubscribe to notification(s) which were previously 1084 
subscribed to for either receiving or pulling notifications 1085 
• The registration function updates the notification function about the state of the subscription and its request type 1086 
(receiving or pulling notifications) 1087 
Notification function: 1088 
• used by the tracking function to message registered listeners of the resource state an d/or its relevant data 1089 
• pulls the tracking function per the application and/or SMO request  1090 
• as soon as an application and/or SMO registers it receives a notification of the resource(s) status it is subscribed 1091 
to 1092 
Figure 21 illustrates the architecture for implementing a framework for notification subscription. This diagram shows the 1093 
functionally and interaction from a logical perspect ive, however, where these functions reside or how they are 1094 
implemented is not in scope of this document and will be described by Cloud Platform Reference Design [12].  1095 
 1096 
Figure 21: O-Cloud Notification Framework Architecture 1097 
5.5 Sync Architecture 1098 
Synchronization mechanisms and options are receiving significant attention in the industry.   1099 
Editor’s Note :  O -RAN W orking Groups 4 and 5 are addressing some aspects of synchronization, and more 1100 
discussion of Sync is expected in future versions of this document.   1101 
Version 2 of the Control, User and Synchronization (CUS) Plane Specification [7] discusses, in chapter 9.2.2, “Clock 1102 
Model and Synchronization Topology”, four topology configuration options Lower Layer Split Control Plane 1 – 4 (LLS-1103 
C1 – LLS-C4) that are required to support different O-RAN deployment scenarios.  Configuration LLS-C3 is seen as the 1104 
most likely initial option for deployment and is discussed below.  This section will provide a summary of what is required 1105 
to support the LLS-C1 and LLS-C3 synchronization topology from the cloud platform perspective. 1106 
Note that in  chapter 6 “Deployment Scenarios and Implementation Considerations” of this document , we call the site 1107 
which runs the vO -DU the “Edge Cloud”, while the Control, User and Synchronization (CUS) Plane Specification [7] 1108 
calls it the “Central Site”.  However, the meaning is the same. 1109 


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
37 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 Cloud Platform Time Synchronization Architecture 1110 
The Time Sync deployment architecture which is described below relies on usage of Precision Time Protocol (PTP) IEEE 1111 
1588-2008 (a.k.a. IEEE 1588 Version 2) to synchronize clocks throughout the Edge Cloud site.  1112 
For LLS-C3 in the CUS specification [7], vO-DU may act as a Telecom Slave Clock (T-TSC) and select the time source 1113 
the same SyncE and PTP distribution from fronthaul as O -RU.  For vO-DU, only the ITU-T G.8275.1 type T-TSC will 1114 
be addressed; others are for Further Study.  Please note that the following synchronization topology for LLS -C3 will 1115 
address only the case where O -DU and O -RU are synchronized from the same time source connected to the fronthaul 1116 
network, other cases are for Further Study. 1117 
For LLS -C1, the O -Cloud running the vO -DU acts as synchronization master towards the fronthaul interface to 1118 
synchronize the O -RU. Please note that the following synchronization topology for LLS -C1 will address only the case 1119 
where O-DU synchronization source is from a local PRTC (GNSS receiver), other cases are for Further Study.  1120 
 1121 
5.5.1.1 Edge Cloud Site Level – LLS-C3 Synchronization Topology 1122 
This section outlines what the time synchronization architecture  should be  from the cloud platform perspective , and 1123 
identifies requirements that the Cloud Platform and Edge Site need to support in order to support the O-RAN deployment 1124 
scenarios that use the LLS-C3 synchronization topology described in CUS specification [7]. 1125 
5.5.1.1.1 LLS-C3 Synchronization Topology Edge Site Time Synchronization Architecture 1126 
The deployment architecture at the Edge Cloud site level includes: 1127 
• Primary Reference Time Clock (PRTC)-traceable time source (i.e., Grandmaster Clocks):  1128 
o External precision time source for the PTP networks, usually based on Global Navigation Satellite 1129 
System/Global Positioning System (GNSS/GPS) 1130 
• Compute Nodes:  1131 
o Compute Nodes synchronize their clocks to a Grandmaster Clock via the Fronthaul Network 1132 
• Controller Nodes: 1133 
o Controller Nodes synchronize their clocks to the Network Time Protocol (NTP) via the Management 1134 
Network 1135 
 1136 
Figure 22 illustrates the relationship of these entities where the Controller functions are hosted on separate nodes from 1137 
the Compute nodes.  Figure 23 illustrates the relationships where each Compute node also includes the Controller 1138 
functions (i.e., the hyperconverged case). 1139 
  1140 

                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
38 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
  1141 
Figure 22: Edge Cloud Site Time Sync Architecture for LLS-C3 1142 
  1143 
Figure 23: Hyperconverged Edge Cloud Time Sync Architecture for LLS-C3 1144 
5.5.1.1.2 LLS-C3 Synchronization Topology Edge Site Requirements 1145 
To support time synchronization at the Edge site, the cloud platform (O -Cloud) used at the Edge site needs to support 1146 
implementation of the PTP IEEE 1588-2008 (a.k.a. IEEE 1588 Version 2) standard. The following software and hardware 1147 
capabilities are required: 1148 
5.5.1.1.2.1 Software 1149 
Support for PTP will be needed in all the Edge Site O-Cloud nodes that support compute roles and will run vO-DU service 1150 
operating as a Slave Clock. The following PTP configuration options should be provided:  1151 
o Network Transport – G.8275.1 sync over Ethernet (Layer 2) 1152 
o Delay Measurement Mechanism – utilize E2E to measure the delay 1153 
o Time Stamping – support for hardware time stamping 1154 
 1155 
For example: in the case when an O- Cloud is based on the Linux OS , this will require support for Linux PTP ( see 1156 
http://linuxptp.sourceforge.net) with the following: 1157 


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
39 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
o ptp4l – implementation of PTP (Ordinary Clock, Boundary Clock), HW timestamping, E2E delay measurement 1158 
mechanism. 1159 
o phc2sys – Synchronization of two clocks, PHC and system clock (Linux clock) when using HW timestamping  1160 
 1161 
5.5.1.1.2.2 Hardware  1162 
Use of High speed, low latency Network Interface Card (NIC) with support for PTP Hardware Clock (PHC) subsystem 1163 
for the data interface (fronthaul) on all the compute node(s) that will run the vO-DU function. 1164 
5.5.1.2 Edge Cloud Site Level – LLS-C1 Synchronization Topology 1165 
This section outlines what the time synchronization architecture  should be  from the cloud platform perspective , and 1166 
identifies requirements that the Cloud Platform and Edge Site need to support in order to support the O-RAN deployment 1167 
scenarios that use the LLS-C1 synchronization topology described in CUS specification [7]. 1168 
5.5.1.2.1 LLS-C1 Synchronization Topology Edge Site Time Synchronization Architecture 1169 
The deployment architecture at the Edge Cloud site level includes: 1170 
• Primary Reference Time Clock (PRTC)-traceable time source (i.e., Grandmaster Clocks):  1171 
o External precision time source for the PTP networks, usually based on Global Navigation Satellite 1172 
System/Global Positioning System (GNSS/GPS) 1173 
• Compute Nodes:  1174 
o Compute Node as acts synchronization master towards the fronthaul interface  1175 
• Controller Nodes: 1176 
o Controller Nodes synchronize their clocks to the Network Time Protocol (NTP) via the Management 1177 
Network 1178 
 1179 
Figure 24 illustrates the relationship of these entities where the Controller functions are hosted on separate nodes from 1180 
the Compute nodes. Figure 25 illustrates the relationships where each Compute node also includes the Controller functions 1181 
(i.e., the hyperconverged case). 1182 
 1183 
Figure 24: Edge Cloud Site Time Sync Architecture for LLS-C1 1184 


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
40 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 1185 
Figure 25: Hyperconverged Edge Cloud Time Sync Architecture for LLS-C1 1186 
5.5.1.2.2 LLS-C1 Synchronization Topology Edge Site Requirements 1187 
To support time synchronization at the Edge site, the cloud platform (O -Cloud) used at the Edge site needs to support 1188 
implementation of the PTP IEEE 1588-2008 (a.k.a. IEEE 1588 Version 2) standard. The following software and hardware 1189 
capabilities are required: 1190 
5.5.1.2.2.1 Software 1191 
Support for PTP will be needed in all the Edge Site O-Cloud node that supports compute role and will run vO-DU service 1192 
operating as a Master Clock. The following PTP configuration options should be provided:  1193 
o Network Transport – G.8275.1 sync over Ethernet (Layer 2) 1194 
o Delay Measurement Mechanism – utilize E2E to measure the delay 1195 
o Time Stamping – support for hardware time stamping 1196 
 1197 
5.5.1.2.2.2 Hardware  1198 
Use of High speed, low latency Network Interface Card (NIC) with support for PTP Hardware Clock (PHC) subsystem 1199 
for the data interface (fronthaul) on all the compute node(s) that will run the vO-DU function. 1200 
When vO-DU requires SyncE, the NIC must support it. 1201 
 Loss of Synchronization Notification 1202 
Applications that rely on a Precision Time Protocol for synchronization (such as vO -DU but not limited to) should have 1203 
the ability to retrieve the relevant data that can indicate the status of the PHC clock related to the worker node that the 1204 
application is running on (for example a source clock class). Once an application subscribes to PTP notifications it 1205 
receives the initial data which shows the PHC synchronization state and it will receive notifications when there is a state  1206 
change to the sync status and/or per request for notification (pull), please refer to the notification subscription framework 1207 
(section 5.4.5) how to subscribe for a PTP Notification.   1208 
Rationale - The CUS specification [7]  section 9.4.2, specifies various behavio urs related to the state of the vO -DU and 1209 
O-RU time synchronization.  1210 


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
41 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
• For example, if an vO -DU transits to the FREERUN state, because the synchronizing network delivers 1211 
unacceptable synchronization quality, the vO-DU shall disable RF transmission on all connected O -RUs, and 1212 
keep it turned off until synchronization is reacquired again.  1213 
It should be noted that since vO-DU may need to take an action upon the synchronization notification (see example above) 1214 
it is required to handle these notifications at the scope of the edge cloud (at the site location where the vO-DU is running) 1215 
for two main reasons:  ensuring that the vO-DU receives the notifications regardless of the communication state of its 1216 
backhaul link and reducing the round trip delay for notifying the vO -DU. 1217 
Figure 26 illustrates an vO-DU subscribes to retrieve PTP Notification based on the subscription framework described at 1218 
section 5.4.5.  1219 
 1220 
Figure 26: vO-DU Subscribes to PTP Notification 1221 
 1222 
5.6 Operations and Maintenance Considerations 1223 
Management of cloudified RAN  Network Functions  introduces some new management considerations, because the 1224 
mapping between Network Functionality and physical hardware can be done in multiple ways, depending on the Scenario 1225 
that is chosen.  Thus, management of aspects that are related to physical aspects rather than logical aspects need to be 1226 
designed with flexibility in mind from the start.  For example, logging of physical functions, scale out actions, and 1227 
survivability considerations are affected.   1228 
The O-RAN Alliance has defined key fundamentals of the OAM framework (see [8] and [9], and refer to Figure 1). Given 1229 
the number of deployment scenario options and possible variations of O -RAN Managed Functions (MFs) being mapped 1230 
into Managed Elements (MEs) in different ways, it is important for all MEs to support a consistent level of visibility and 1231 
control of their contained Managed Functions to the Service Management & Orchestration Framework.  This consistency 1232 
will be enabled by support of the common OAM Interface Specification [9] for Fault Configuration Accounting 1233 
Performance Security (FCAPS) and Life Cycle Management (LCM) functionality, and a common Information Modelling 1234 
Framework that will provide underlying information models used for the MEs and MFs in a particular deployment. 1235 
 The O1 Interface 1236 
As described in [8] , t he O1 is an interface between management entities in Service Management and Orchestration 1237 
Framework and O-RAN managed elements, for operation and management, by which FCAPS management, Softw are 1238 
management, File management shall be achieved.  1239 


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
42 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 The O2 Interface 1240 
The O2 Interface is a collection of services and their associated interfaces that are provided by the O -Cloud platform to 1241 
the SMO.  The services are categorized into two logical groups: 1242 
• Infrastructure Management Services: which include  the subset of O2 functions that are responsible for 1243 
deploying and managing cloud infrastructure. 1244 
• Deployment Management Services:  which include the subset of O2 functions that are responsible for 1245 
managing the lifecycle of virtualized/containerized deployments on the cloud infrastructure.  1246 
The O2 services and their associated interfaces shall be specified in the upcoming O2 specification. Any definitions of 1247 
SMO functional elements needed to consume these services shall be described in OAM architecture.  O2 interface would 1248 
also address the management of hardware acceleration and supporting software in the O-Cloud platform. 1249 
5.7 Transport Network Architecture 1250 
While a Transport Network is a necessary foundation upon which to build any O -RAN deployment, a great many of the 1251 
aspects of transport do not have to be addressed or specified in O -RAN Alliance documents.  For example, any location 1252 
with cloud servers will be connected by layer 2 or layer 3 switches, but we do not need to specify much if anything about 1253 
them in this document.   1254 
The transport media used, particularly for fronthaul, can have an effect on aspects such as performance.  However, in the 1255 
current version of this document we have been assuming that fiber transport is used.   1256 
Editor’s Note:  Other transport technologies (e.g., microwave) are also possible, and could be addressed at a later 1257 
date.  1258 
That said, the use of an (optional) Fronthaul Gateway (FH GW) will have noteworthy effects on any O-RAN deployment 1259 
that uses it. 1260 
 Fronthaul Gateways 1261 
In the deployment scenarios that follow, when the O-DU and O-RU functions are not implemented in the same physical 1262 
node, a Fronthaul Gateway is shown as an optional  element between them.  A Fronthaul Gateway can be motivated by 1263 
different factors depending on a carrier’s deployment, and may perform different functions.   1264 
The O-RAN Alliance does not currently have a single definition of a Fronthaul Gateway, and this document does not 1265 
attempt to define one.  However, the Fronthaul Gateway is included in the diagrams as an optional implementation to 1266 
acknowledge the fact that carriers are considering Fronthaul Gateways in their plans. Below are some examples of the 1267 
functionality that could be provided: 1268 
• A FH GW can convert CPRI connections to the node supporting the O-RU function to eCPRI connections to the 1269 
node that provides O-DU functionality.   1270 
• Note that when there is no FH GW, it is assumed that the Open Fronthaul interface between the O-RU and 1271 
O-DU uses Option 7-2, as mentioned earlier in Section 4.1.  When there is a FH GW, it may have an Option 1272 
7-2 interface to both the O -DU and the O -RU, but it is also possible for the FH GW to have a different 1273 
interface to the O-RU/RU; for example, where CPRI is supported.   1274 
• A FH GW can support the aggregation of fiber pairs. 1275 
• A FH GW must support the following forwarding functions: 1276 
• Downlink:  Transport the traffic from O-DU to each O-RU (and cascading FH GW, if present) 1277 
• Uplink:  Summation of traffic from O-RUs 1278 
• A FH GW can provide power to the NEs supporting the O -RU function, e.g. via Power over Ethernet (PoE) or 1279 
hybrid cable/fibers 1280 
5.8 Overview of Deployment Scenarios 1281 
The description of logical functionality in O -RAN includes the definition of key interfaces E2, F1, and Open Fronthaul.  1282 
However, as noted earlier, this does not mean that each Network Function block must be implemented in a separate O -1283 

                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
43 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
RAN Physical NF/O-RAN Cloudified NF.  Multiple logical functions can be implemented in a single O -RAN Physical 1284 
NF/O-RAN Cloudified NF (for example O-DU and O-RU may be packaged as a single appliance).  1285 
We assume that when Network F unctions are implemented as different O -RAN Physical NFs/O-RAN Cloudified NFs , 1286 
the interfaces between them  must conform to the O -RAN specifications.  However, when multiple Network F unctions 1287 
are implemented by a single O-RAN Physical NF/O-RAN Cloudified NF , it is up to the operator to decide whether to 1288 
enforce the O -RAN interfaces between the embedded Network Functions .  However,  note that the OAM re quirements 1289 
for each separate Network Function will still need to be met.   1290 
The current deployment scenario s for discussion are summarized in the figure below.  This includes options that are 1291 
deployable in both the short and long term .  Each will be discussed in some detail in the following sections, followed by 1292 
a summary of which one or ones are candidates for initial focus. Please note that, to help ease the high-level depiction of 1293 
functionality, a single O-CU box is shown with an F1 interface, but in detailed discussions of specific scenarios, this will 1294 
need to be discussed properly as composed of an O -CU-CP function with an  F1-c interface and an O -CU-UP function 1295 
with an F1-u interface.  Furthermore, there would in general be an unequal number of O-CU-CP and O-CU-UP instances.   1296 
Figure 27 below shows the Network F unctions at the top, and each identified scenario shows how these Network 1297 
Functions are deployed as O-RAN Physical NFs or as O-RAN Cloudified NFs running on an O-RAN compliant O-Cloud.  1298 
The term O-Cloud is defined in Section 4 .  Please note that the requirements  for an O-Cloud are driven by the Network 1299 
Functions that need to be supported by the hardware, so for instance an O -Cloud that supports an O-RU function would 1300 
be different from an O-Cloud that supports O-CU functionality.   1301 
Finally, note that in the high-level figure below, the User Plane (UP) traffic is shown being delivered to the UPF.  As will 1302 
be discussed, in specific scenarios it is sometimes possible for UP traffic to be delivered to edge applications that are 1303 
supported by Mobile Edge Computing (MEC).  However, note that the specification of MEC itself is out of scope of this 1304 
document. 1305 
Note that vendors are not required to support all scenarios – it is a business decision to be made by each vendor.  Similarly, 1306 
each operator will decide which scenarios it wishes to deploy.   1307 
 1308 
Figure 27:  High-Level Comparison of Scenarios 1309 
Each scenario is discussed in the next section.   1310 
 1311 


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
44 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
6 Deployment Scenarios and Implementation 1312 
Considerations 1313 
This section reviews each of the deployment scenario s in turn.  For a given scenario, the requirements that apply to the 1314 
O-RAN Physical NFs, O-RAN Cloudified NFs or O-Cloud platforms may become more specific and unique, while many 1315 
of the logical Network Function requirements will remain the same.   1316 
Please note that in all of the scenario figures of this section, the interfaces are logical interfaces (e.g., F1, E2, etc.) .  This 1317 
has a couple of implications.  First, the two functions on each side of an interface could be on different devices separated 1318 
by physical transport connections (e.g., fiber or Ethernet transport connections), could be on different devices within the 1319 
same cloud platform, or could even exist within the same server.  Second, the functions on each side of an interface could 1320 
be from the same vendor or different vendors.  1321 
In addition, please note that all User Plane interfaces are shown with a solid lines, and all Control Pl ane interfaces use 1322 
dashed lines.  1323 
Editor’s note: The terms vO-CU and vO-DU represent virtualized or containerized O-CU and O-DU, and are used 1324 
interchangeably with O-CU and O-DU in these scenarios (with the exception when the O-DU is explicitly stated 1325 
as a non-virtualized O-DU). 1326 
 1327 
6.1 Scenario A  1328 
In this scenario, the near -RT RIC, O -CU, and O -DU functions are all virtualized on the same cloud platform, and 1329 
interfaces between those functions are within the same cloud platform.    1330 
This scenario supports deployments in dense urban areas with an abundance of fronthaul capacity that allows BBU  1331 
functionality to be pooled in a central location with sufficiently low latency to meet the O -DU latency req uirements. 1332 
Therefore, i t does not attempt to centralize the near -RT RIC more than  the limit that O -DU functionality can be 1333 
centralized.  1334 
 1335 
Figure 28:  Scenario A 1336 
Also please note that if the optional FH GW is present, the interface between it and the Radio Unit might not meet the O-1337 
RAN Fronthaul requirements (e.g., it might be an Option 8 interface), in which case the Ra dio Unit could be referred to 1338 
as an “RU”, not an “O-RU”.  However, if FH GWs are defined to support an interface such as Option 8, it could be argued 1339 
that the O-RU definition at that time will support Option 8.   1340 
 Key Use Cases and Drivers 1341 
Editor’s Note:  This section is FFS.  1342 
6.2 Scenario B 1343 
In this scenario, the near -RT RIC Network Function is virtualized on a Regional Cloud Platform, and the O -CU and O-1344 
DU functions are virtualized on an Edge Cloud hardware platform that in general will be at a different location.  The 1345 


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
45 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
interface between the Near-RT RIC network function and the O-CU/O-DU network functions is E2.  Interfaces between 1346 
the O-CU and O-DU Network Functions are within the same Cloud Platform.  1347 
 1348 
Figure 29:  Scenario B – NR Stand-alone 1349 
This scenario addresses deployments in locations with limited remote fronthaul capacity and O-RUs spread out in an area 1350 
that limits the number of O-RUs that can be supported by pooled vO-CU/vO-DU functionality while still meeting the O-1351 
DU latency requirements.  The use of a FH GW in the architecture allows significant savings in providing transport 1352 
between the O-RU and vO-DU functionality. 1353 
 1354 
Figure 30: Scenario B – MR-DC (inter-RAT NR and E-UTRA) regardless of the Core Network type (either EPC 1355 
or 5GC) 1356 
An Alternative to NR Standalone  scenario B is given by the MR -DC (inter-RAT NR/E-UTRA) scenarios which extend 1357 
requirements on the cloud platform to additionally support E -UTRA network functions (subscript E)  and required 1358 
interfaces Xn, open fronthaul and W1. The W1 interface, defined in 3GPP TS 37.470, only applies to E -UTRA nodes 1359 
connected to 5G Core Network, i.e. ng-eNB as defined in 3GPP TS 38.300 and TS 38.401. Moreover, the foreseen MR -1360 
DC (inter-RAT NR/E-UTRA) scenarios also include  the EPC-connected E-UTRA-NR Dual Connectivity (EN -DC) by 1361 
properly replacing the Xn interface with the X2 interface interconnecting E-UTRA nodes (eNBs) and NR ones (en-gNBs), 1362 
with the possibility to exploit vO-CU/vO-DU functional split only for the en-gNBs6.  1363 
As discussed earlier in Section 5.1.3, the O-CU and O-DU functions can be virtualized using either simple centralization 1364 
or pooled centralization.  The desire is to have support for pooled centralization, although we need to under stand what 1365 
needs to be developed to enable such sharing.  Perhaps pooling will be a later feature, but any initial solution should not 1366 
preclude a future path to a pooled solution.    1367 
 
6 O-eNB vO-CUE/vO-DUE split (foreseen in 3GPP), is pending O-RAN architecture alignment in wg1. 


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
46 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 Key Use Cases and Drivers 1368 
In this case, there are multiple O -RUs distributed in an area served by a  centralized vO-DU functionality that can meet 1369 
the latency requirements.  Depending on the concentration of the O -RUs, N could vary, but in general is expected to be 1370 
engineered to support < 64 TRPs per O-DU.7  The near-RT RIC is centralized further to allow for optimization based on 1371 
a more global view (e.g., a single large metropolitan area), and to reduce the number of separate near -RT RIC instances 1372 
that need to be managed.   1373 
The driving use case for this is to support an outdoor  deployment of a mix of Small Cells and Macro cells in a relatively 1374 
dense urban setting.  This can support mmWave as well as Sub-6 deployments. 1375 
In this scenario, a given “virtual BBU” supports both vO -CU and vO -DU functions, and can connect many O- RUs.  1376 
Current studies show that savings  from pooling are significant but level off  once more than 64 Transmission Reception 1377 
Points (TRPs) are pooled.  This would imply N would be around 32-64. This deployment should support tens of thousands 1378 
of O-RUs per near-RT RIC, so L could easily exceed 100.   1379 
Below is a summary of the cardinality requirements assumed for this scenario.  1380 
  Table 2:  Cardinality and Delay Performance for Scenario B 1381 
 
Attribute RIC – O-CU O-CU – O-DU O-DU – O-RU/RU 
 
Example Cardinality L = 100+ M=1 N = 1-64  
6.3 Scenario C 1382 
In this scenario, the near -RT RIC and O -CU Network Functions are virtualized on a Regional Cloud Platform with a 1383 
general server hardware platform, and the O-DU Network Functions are virtualized on an Edge Cloud hardware platform 1384 
that is expected to include significant hardware accelerator capabilities.  Interfaces between the near-RT RIC and the O-1385 
CU network functions are within the same Cloud Platform.  The interface between the Regional Cloud and the Edge cloud 1386 
is F1, and an E2 interface from the near-RT RIC to the O-DU must also be supported.  1387 
 1388 
Figure 31:  Scenario C 1389 
This scenario is to support deployments in locations with limited remote Fronthaul capacity and O -RUs spread out in an 1390 
area that limits the number of O-RUs that can be pooled while still meeting the O-DU latency requirements. It also applies 1391 
to some whitebox macrocell deployments. The O-CU Network Function is further pooled to increase the efficiency of the 1392 
hardware platform which it shares with the near-RT RIC Network Function.   1393 
However, note that if a service type has tighter O-CU delay requirements than other services, then that may either severely 1394 
limit the number of O -RUs supported by the Regional cloud, or a method will be needed to separate the processing of 1395 
such services.  This will be discussed further in the following C.1 and C.2 Scenarios.   1396 
 
7 It is assumed that one O-RU is associated with one TRP.  For example, if a cell site has three sectors, then each sector would have at least one TRP 
and hence at least three O-RUs.  


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
47 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
The use of a FH GW in the architecture allows significant savings in providing transport between the O -RU and vO-DU 1397 
functionality.   1398 
 Key Use Cases and Drivers 1399 
In this case, there are multiple O -RUs distributed in an area where each O -RU can meet the latency requir ement for the 1400 
pooled vO-DU function.  The near -RT RIC and O -CU Network Functions are further centralized to realize additional  1401 
efficiencies.   1402 
A use case for this is to support an outdoor deployment of a mix of Small Cells and Macro cells in a relatively dense 1403 
urban setting.  This can support mmWave as well as Sub-6 deployments. 1404 
In this scenario, as in Scenario B, the Edge Cloud is expected to support roughly 32- 64 O-RUs. This deployment should 1405 
support tens of thousands of O-RUs per near-RT RIC.  1406 
Below is a summary of the cardinality and the distance/delay requirements assumed for this scenario.   1407 
Table 3:  Cardinality and Delay Performance for Scenario C   1408 
 
Attribute RIC – O-CU O-CU – O-DU O-DU – O-RU/RU 
 
Example Cardinality L= 1 M=100+  N=Roughly 32-64 
 Scenario C.1, and Use Case and Drivers 1409 
This is a variation of Scenario C, driven by the fact that different types of traffic (network slices) have different latency  1410 
requirements.  In particular, URLLC  has more demanding user -plane latency requirements, and Figure 32 below shows 1411 
how the vO- CU User P art (vO-CU-UP) could be terminated in different places for different network  slices.  Below, 1412 
network slice 3 is terminated in the Edge Cloud.  This scenario is also suitable in case there isn’t enough space or power 1413 
supply to install all vO-CUs and vO-DUs in one Edge Cloud site.  1414 
 1415 
Figure 32:  Treatment of Network Slices:  MEC for URLLC at Edge Cloud, Centralized Control, Single vO-DU 1416 
In Scenario C.1, all O -CU control is placed in the Regional Cloud, and there is a single vO -DU for all Network Slices.  1417 
Only the placement of the vO -CU-CP differs, depending on the network slice.  Below is the diagram of this scenario, 1418 
using the common diagram conventions of all scenarios.  1419 


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
48 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 1420 
Figure 33:  Scenario C.1 1421 
Below is a summary of the cardinality and the distance/delay requirements assumed for this scenario.  The URLLC user 1422 
plane requirements are what drive the placement of the vO-CU-UP function to be in the Edge cloud.   1423 
Table 4:  Cardinality and Delay Performance for Scenario C.1 1424 
 
Attribute RIC – O-CU O-CU – O-DU O-DU – O-RU/RU 
 
Example Cardinality L= 1 M=320 N=100 
Delay Max  
1-way (distance)    mMTC NA 625 μs (125 km) 100 μs (20 km)  
   eMBB NA 625 μs (125 km) 100 μs (20 km) 
   URLLC (user/control) NA 100 μs (20 km)/625 μs (125 
km) 
100 μs (20 km) 
 1425 
 Scenario C.2, and Use Case and Drivers 1426 
This is a second variation of Scenario C, which utilizes the same method of placing some vO-CU user plane functionality 1427 
in the Edge Cloud, and some in the Regional Cloud.  However, instead of having one vO-DU for all network slices, there 1428 
are different vO-DU instances in the Edge Cloud.  1429 
It is driven by factors including the following two use cases: 1430 
• One driver is RAN (O-RU) sharing among operators. In this use case, any operator  can flexibly launch vO-CU 1431 
and vO-DU instances at Edge or Regional Cloud site.  For example, as shown in Figure 34, Operator #1 wants 1432 
to launch the vO-CU1 instance in the Regional Cloud, and the vO-DU1 instance at subtending Edge Cloud sites. 1433 
On the other hand, Operator #2 wants to install both the vO-CU2 and vO-DU2 instances at the same Regional 1434 
Cloud site.  Note that both operators will share the O-RU).  1435 
• Another driver is that, even within a single operator, that operator can customize scheduler functions depending 1436 
on the network slice types, and can place the vO-CU and vO-DU instances depending on the network slice types. 1437 
For example, an operator may launch both vO-CU and vO-DU at the edge cloud site (see Operator #2 below) to 1438 
provide a URLLC service.   1439 


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
49 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 1440 
Figure 34:  Treatment of Network Slice: MEC for URLLC at Edge Cloud, Separate vO -DUs 1441 
The multi-Operator use case has the following pros and cons: 1442 
Pros: 1443 
• O-RU sharing can reduce TCO 1444 
• Flexible CU/DU location allows deployments to consider not only service  requirements but also limitation s of 1445 
space or power in each site 1446 
Cons: 1447 
• Allowing multiple operators to share O -RU resources is expected to require changes to the Open Fronthaul 1448 
interface (especially the handshake among more than one vO-DU and a given O-RU).   1449 
• This change seems likely to have  M-plane specification impact.  Therefore, this approach would need O-RAN 1450 
buy-in and approval.   1451 
Figure 35 below illustrates how different Component Carriers can be allocated to different operators, at the same O -RU 1452 
at the same time.  Note that some updates of not only M -plane but also CUS-plane specifications will be required when 1453 
considering frequency resource sharing among DUs. 1454 
 1455 
Figure 35:  Single O-RU Being Shared by More than One Operator 1456 
The diagram of how Network Functions map to Networks Elements for Scenario C.2 is shown below .  1457 


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
50 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 1458 
Figure 36:  Scenario C.2 1459 
The performance requirements are the same as those discussed earlier for Scenario C.1 in Section 6.3.2.  1460 
6.4 Scenario D  1461 
This scenario is a variation on Scenario C , but in this case the O-DU functionality is supported by a n O-RAN Physical 1462 
NF rather than an O-Cloud.  1463 
The general assumption is that Scenario D has the same use cases and performance requirements as Scenario C, and the 1464 
primary difference is in the business decision of how the O-RAN Physical NF based solution compares with the O-RAN 1465 
compliant O-Cloud solution.  Implementation considerations (discussed in Section 5.1) could lead a carrier to decide that 1466 
an acceptable O-Cloud solution is not available in a deployment’s timeframe.   1467 
 1468 
Figure 37:  Scenario D 1469 
6.5 Scenario E  1470 
In contrast to Scenario D, this scenario assumes that not only can the O -DU be virtualized as in Scenario C, but that the 1471 
O-RU can also be successfully virtualized.  Furthermore, the O -RU and O -DU would be implemented in the same O -1472 
Cloud, which has acceleration hardware if required by either or both the O-RU and O-DU.   1473 
Note, this seems to be a future scenario, and is not part of our initial focus.   1474 


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
51 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 1475 
Figure 38:  Scenario E 1476 
 Key Use Cases and Drivers 1477 
Because the O -DU and O- RU are implemented in the same O -Cloud in this Scenario , it seems that the O -DU 1478 
implementation must meet the environmental and accessibility requirements typically associated with an O -RU.  1479 
Therefore, an indoor use case seems most appropriate.  1480 
 Scenario E.1 vO-DU with O-RU 1481 
For Macrocell deployment with the Open Hardware approach that is used in WG7, the O -DU 7-2 of O-RAN WG7 1482 
OMAC HAR 0-v01.00 [13] can be a virtual function. In this small-scale scenario, HW acceleration is optional. The 1483 
Cloud platform could be physically located near or at the bottom of the tower and be associated with a number of O -1484 
RUs implemented with the Open HW design, possibly but not necessarily in the same chassis.  1485 
 1486 
Figure 39: Scenario E.1 1487 
 1488 
6.6 Scenario F  1489 
This is a variation on Scenario E in which the O-DU and O-RU are both virtualized, but in different O-Clouds. This means 1490 
that: 1491 
• The O-DU function can be placed in a more convenient location in terms of accessibility for maintenance and 1492 
upgrades. 1493 
• The O-DU function can be placed in an environment that is semi -controlled or controlled, which reduces some 1494 
of the implementation complexity.  1495 
 1496 


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
52 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 1497 
Figure 40:  Scenario F 1498 
 Key Use Cases and Drivers 1499 
Because this assumes that the O-RU is virtualized, this is a future use case. 1500 
This use case seems to be better suited for outdoor deployments (e.g., pole mounted) than Scenario E. 1501 
6.7 Scenarios of Initial Interest 1502 
More scenarios have been identified than can be addressed in the initial release of this document.  Scenario B has been 1503 
selected as the one to address initially, and to be the subject of detailed treatment in a Scenario document (refer back to 1504 
Figure 1).  Other scenarios are expected to be addressed in later work.   1505 
 1506 
7 Appendix A (informative):  Extensions to Current 1507 
Deployment Scenarios to Include NSA 1508 
In this appendix, some extensions to (some of) the current deployment scenarios are proposed with the aim of introducing 1509 
Non-Standalone (NSA) in the pictures, consistently with the scope O -RAN cloud architecture. These extensions will be 1510 
the basis of the discussion for next version of the present document. In the following charts the subscript ‘N’ is indicating 1511 
blocks related to NR, while the subscript ‘E’ is indicating blocks related to E-UTRA.8  For E-UTRA, the W1 interface is 1512 
indicated. Its definition is ongoing in a 3GPP work item. 1513 
 
8 No UPF or MEC blocks are explicitly indicated in the figures of this appendix, as the focus of this appendix is on the radio part. 


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
53 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
7.1 Scenario A 1514 
 1515 
Figure 41:  Scenario A, Including NSA 1516 
7.2 Scenario B 1517 
Editor’s Note: Scenario B, Including NSA has been incorporated into 6.2.  1518 
7.3 Scenario C 1519 
 1520 
Figure 42:  Scenario C, Including NSA 1521 
7.4 Scenario C.2 1522 
The scenario addresses both the single and multi-operator cases. To reduce the complexity in the figure the multi operator 1523 
case is considered, so no X2/Xn interface is present between CUN1 and CUE2 or between CUE1 and CUN2. 1524 


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
54 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 1525 
Figure 43:  Scenario C.2, Including NSA 1526 
7.5 Scenario D 1527 
 1528 
Figure 44:  Scenario D, Including NSA  1529 


                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
55 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
Annex ZZZ:  O-RAN Adopter License Agreement 1530 
BY DOWNLOADING, USING OR OTHERWISE ACCESSING ANY O-RAN SPECIFICATION, ADOPTER AGREES TO 1531 
THE TERMS OF THIS AGREEMENT.   1532 
This O-RAN Adopter License Agreement (the “Agreement”) is made by and between the O-RAN Alliance and 1533 
the entity that downloads, uses or o therwise accesses any O -RAN Specification, including its Affiliates (the 1534 
“Adopter”). 1535 
This is a license agreement for entities who wish to adopt any O-RAN Specification. 1536 
SECTION 1:  DEFINITIONS 1537 
 1538 
1.1 “Affiliate” means an entity that directly or indirectly controls, is controlled by, or is under common 1539 
control with another entity, so long as such control exists.  For the purpose of this Section, “Control” 1540 
means beneficial ownership of fifty (50%) percent or more of the voting stock or equity in an entity. 1541 
 1542 
1.2 “Compliant Portion” means only those specific portions of products (hardware, software or 1543 
combinations thereof) that implement any O-RAN Specification. 1544 
 1545 
1.3  “Adopter(s)” means all entities, who are not Members, Contributors or Academic Contributors, 1546 
including their Affiliates, who wish to download, use or otherwise access O-RAN Specifications. 1547 
 1548 
1.4 “Minor Update” means an update or revision to an O-RAN Specification published by O-RAN Alliance 1549 
that does not add any significant new features or functionality and remains interoperable with the prior 1550 
version of an O-RAN Specification.  The term “O-RAN Specifications” includes Minor Updates. 1551 
 1552 
1.5 “Necessary Claims” means those claims of all present and future patents and patent applications, 1553 
other than design patents and des ign registrations, throughout the world, which (i) are owned or 1554 
otherwise licensable by a Member, Contributor or Academic Contributor during the term of its Member, 1555 
Contributor or Academic Contributorship; (ii) such Member, Contributor or Academic Contributor has the 1556 
right to grant a license without the payment of consideration to a third party; and (iii) are necessarily 1557 
infringed by implementation of a Final Specification (without considering any Contributions not included 1558 
in the Final Specification). A claim is necessarily infringed only when it is not possible on technical (but not 1559 
commercial) grounds, taking into account normal technical practice and the state of the art generally 1560 
available at the date any Final Specification was published by the O -RAN Alliance or the date the patent 1561 
claim first came into existence, whichever last occurred, to make, sell, lease, otherwise dispose of, repair, 1562 
use or operate an implementation which complies with a Final Specification without infringing that claim. 1563 
For the avoidance of doubt in exceptional cases where a Final Specification can only be implemented by 1564 
technical solutions, all of which infringe patent claims, all such patent claims shall be considered Necessary 1565 
Claims. 1566 
 1567 
1.6 “Defensive Suspension” means for the purposes of any license grant pursuant to Section 3, Member, 1568 
Contributor, Academic Contributor, Adopter, or any of their Affiliates, may have the discretion to include 1569 
in their license a term allowing the licensor to suspend the license against a licensee who brings a patent 1570 
infringement suit against the licensing Member, Contributor, Academic Contributor, Adopter, or any of 1571 
their Affiliates. 1572 
 1573 
SECTION 2: COPYRIGHT LICENSE 1574 
 1575 
2.1 Subject to the terms and conditions of this Agreement, O-RAN Alliance hereby grants to Adopter a 1576 
nonexclusive, nontransferable, irrevocable, non-sublicensable, worldwide copyright license to obtain, use 1577 

                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
56 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
and modify O-RAN Specifications, but not to further distribute such O-RAN Specification in any modified 1578 
or unmodified way, solely in furtherance of implementations of an O-RAN Specification. 1579 
 1580 
2.2 Adopter shall not use O -RAN Specifications except as expressly set forth in this Agreement or in a 1581 
separate written agreement with O-RAN Alliance. 1582 
 1583 
SECTION 3: FRAND LICENSE 1584 
 1585 
3.1 Members, Contributors and Academic Contributors and their Affiliates are prepared to grant based 1586 
on a separate Patent License Agreement to each Adopter under Fair, Reasonable And Non-Discriminatory 1587 
(FRAND) terms and conditions with or without compensation (royalties) a nonexclusive, non-transferable, 1588 
irrevocable (but subject to Defensive S uspension), non -sublicensable, worldwide license under their 1589 
Necessary Claims to make, have made, use, import, offer to sell, lease, sell and otherwise distribute 1590 
Compliant Portions; provided, however, that such license shall not extend: (a) to any part or function of a 1591 
product in which a Compliant Portion is incorporated that is not itself part of the Compliant Porti on; or 1592 
(b) to any Adopter if that Adopter is not making a reciprocal grant to Members, Contributors and Academic 1593 
Contributors, as set forth in Section 3.3.  For the avoidance of doubt, the foregoing license includes the 1594 
distribution by the Adopter’s distri butors and the use by the Adopter’s customers of such licensed 1595 
Compliant Portions. 1596 
 1597 
3.2  Notwithstanding the above, if any Member, Contributor or Academic Contributor, Adopter or their 1598 
Affiliates has reserved the right to charge a FRAND royalty or other fee for its license of Necessary Claims 1599 
to Adopter, then Adopter is entitled to charge a FRAND royalty or other fee to such Member, Contributor 1600 
or Academic Contributor, Adopter and its Affiliates for its license of Necessary Claims to its licensees. 1601 
 1602 
3.3 Adopter, on behalf of itself and its Affiliates, shall be prepared to grant based on a separate Patent 1603 
License Agreement to each Members, Contributors, Academic Contributors, Adopters and their Affiliates 1604 
under FRAND terms and conditions with or without compensation (royalties) a nonexclusive, non -1605 
transferable, irrevocable (but subject to Defensive Suspension), non -sublicensable,  worldwide license 1606 
under their Necessary Claims to make, have made, use, impo rt, offer to sell, lease, sell and otherwise 1607 
distribute Compliant Portions; provided, however, that such license will not extend: (a) to any part or 1608 
function of a product in which a Compliant Portion is incorporated that is not itself part of the Compliant 1609 
Portion; or (b) to any Members, Contributors, Academic Contributors, Adopters and their Affiliates that is 1610 
not making a reciprocal grant to Adopter , as set forth in Section 3.1.  For the avoidance of doubt, the 1611 
foregoing license includes the distribution by the Members’, Contributors’, Academic Contributors’, 1612 
Adopters’ and their Affiliates’ distributors and the use by the Members’, Contributors’, Academic 1613 
Contributors’, Adopters’ and their Affiliates’ customers of such licensed Compliant Portions. 1614 
 1615 
SECTION 4:  TERM AND TERMINATION 1616 
 1617 
4.1 This Agreement shall remain in force, unless early terminated according to this Section 4. 1618 
 1619 
4.2 O-RAN Alliance on behalf of its Members, Contributors and Academic Contributors may terminate 1620 
this Agreement if Adopter materially breaches this Agreement and does not cure or is not capable of 1621 
curing such breach within thirty (30) days after being given notice specifying the breach. 1622 
 1623 
4.3 Sections 1, 3, 5 -  11 of this Agreement shall survive any termination of this Agreement.  Under 1624 
surviving Section 3, after termination of this Agreement, Adopter will continue to grant licenses (a) to 1625 
entities who become Adopters after the date of termination; and (b) for future versions of O -RAN 1626 

                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
57 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
Specifications that are backwards compatible with the versi on that was current as of the date of 1627 
termination. 1628 
 1629 
SECTION 5: CONFIDENTIALITY 1630 
 1631 
Adopter will use the same care and discretion to avoid disclosure, publication, and dissemination of O -1632 
RAN Specifications to third parties, as Adopter employs with its own confidential information, but no less 1633 
than reasonable care.  Any disclosure by Adopter to its Affiliates, contractors and consultants should be 1634 
subject to an obligation of confidentiality at least as restrictive as those contained in this Section.  The 1635 
foregoing obligation shall not apply to any information which is: (1) rightfully known by Adopter without 1636 
any limitation on use or disclosure prior to disclosure; (2) publicly available through no fault of Adopter; 1637 
(3) rightfully received without a duty of confidentiality ; (4) disclosed by O -RAN Alliance or a Member, 1638 
Contributor or Academic Contributor to a third party without a duty of confidentiality on such third party; 1639 
(5) independently developed by Adopter; (6) disclosed pursuant to the order of a court or other authorized 1640 
governmental body, or as required by law, provided that Adopter provides reasonable prior written notice 1641 
to O-RAN Alliance, and cooperates with O -RAN Alliance and/or the applicable Member, Contributor or 1642 
Academic Contributor to have the opportunity to oppose any such order; or (7) disclosed by Adopter with 1643 
O-RAN Alliance’s prior written approval.  1644 
 1645 
SECTION 6:  INDEMNIFICATION 1646 
 1647 
Adopter shall indemnify, defend, and hold harmless the O -RAN Alliance, its Members, Contributors or 1648 
Academic Contributors, and their employees, and agents and their respective successors, heirs and 1649 
assigns (the “ Indemnitees”), against any liability, damage, loss, or expense (including reasonable 1650 
attorneys’ fees and expenses) incurred by or imposed upon any of the Indemnitees in connection  with 1651 
any claims, suits, investigations, actions, demands or judgments arising out of Adopter’s use of the 1652 
licensed O -RAN Specifications or Adopter’s commercialization of products that comply with O -RAN 1653 
Specifications. 1654 
 1655 
SECTION 7:  LIMITATIONS ON LIABILITY; NO WARRANTY 1656 
 1657 
EXCEPT FOR BREACH OF CONFIDENTIALITY, ADOPTER’S BREACH OF SECTION 3, AND ADOPTER’S 1658 
INDEMNIFICATION OBLIGATIONS, IN NO EVENT SHALL ANY PARTY BE LIABLE TO ANY OTHER PARTY OR 1659 
THIRD PARTY FOR ANY INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE OR CONSEQUENTIAL D AMAGES 1660 
RESULTING FROM ITS PERFORMANCE OR NON-PERFORMANCE UNDER THIS AGREEMENT, IN EACH CASE 1661 
WHETHER UNDER CONTRACT, TORT, WARRANTY, OR OTHERWISE, AND WHETHER OR NOT SUCH PARTY 1662 
HAD ADVANCE NOTICE OF THE POSSIBILITY OF SUCH DAMAGES. 1663 
 1664 
O-RAN SPECIFICATIONS ARE PROVIDED “AS IS” WITH NO WARRANTIES OR CONDITIONS WHATSOEVER, 1665 
WHETHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE.  THE O- RAN ALLIANCE AND THE MEMBERS, 1666 
CONTRIBUTORS OR ACADEMIC CONTRIBUTORS EXPRESSLY DISCLAIM ANY WARRANTY OR CONDITION OF 1667 
MERCHANTABILITY, SECURITY, SATISFACTORY QUALITY, NONINFRINGEMENT, FITNESS FOR ANY 1668 
PARTICULAR PURPOSE, ERROR- FREE OPERATION, OR ANY WARRANTY OR CONDITION FOR O- RAN 1669 
SPECIFICATIONS. 1670 
 1671 
SECTION 8:  ASSIGNMENT 1672 
 1673 
Adopter may not assign the Agreement or any of its rights or obligations under this Agreement or make 1674 
any grants or other sublicenses to this Agreement, except as expressly authorized hereunder , without 1675 
having first received the prior, written consent of the O-RAN Alliance, which consent may be withheld in 1676 
O-RAN Alliance’s sole discretion.  O-RAN Alliance may freely assign this Agreement. 1677 
 1678 

                                                                                                                      O-RAN.WG6.CAD-v02.02 TR 
 
58 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 1679 
 1680 
SECTION 9:  THIRD-PARTY BENEFICIARY RIGHTS 1681 
 1682 
Adopter acknowledges and agrees that Members, Contributors and Academic Contributors  (including 1683 
future Members, Contributors and Acade mic Contributors ) are entitled to rights as a third -party 1684 
beneficiary under this Agreement, including as licensees under Section 3. 1685 
 1686 
SECTION 10:  BINDING ON AFFILIATES 1687 
 1688 
Execution of this Agreement by Adopter in its capacity as a legal entity or association constitutes that legal 1689 
entity’s or association’s agreement that its Affiliates are likewise bound to the obligations that are 1690 
applicable to Adopter hereunder and are also entitled to the benefits of the rights of Adopter hereunder. 1691 
 1692 
SECTION 11:  GENERAL 1693 
 1694 
This Agreement is go verned by the laws of Germany without regard to its conflict or choice of law 1695 
provisions.   1696 
 1697 
This Agreement constitutes the entire agreement between the parties as to its express subject matter and 1698 
expressly supersedes and replaces any prior or contemporan eous agreements between the parties,  1699 
whether written or oral, relating to the subject matter of this Agreement. 1700 
 1701 
Adopter, on behalf of itself and its Affiliates, agrees to comply at all times with all applicable laws, rules 1702 
and regulations with respect to its and its Affiliates’ performance under this Agreement, including without 1703 
limitation, export control and antitrust laws.  Without limiting the generality of the foregoing, Adopter 1704 
acknowledges that this Agreement prohibits any communication that would violate the antitrust laws. 1705 
 1706 
By execution hereof, no form of any partnership, joint venture or other special relationship is created 1707 
between Adopter, or O-RAN Alliance or its Members, Contributors or Academic Contributors.  Except as 1708 
expressly set forth in this Agreement, no party is authorized to make any commitment on behalf of 1709 
Adopter, or O-RAN Alliance or its Members, Contributors or Academic Contributors. 1710 
 1711 
In the event that any provision of this Agreement conflicts with governing law or if any provision is held 1712 
to be null, void or otherwise ineffective or invalid by a court of competent jurisdiction, (i) such provisions 1713 
will be deemed stricken from the contract, and (ii) the remaining terms, provisions, covenants and 1714 
restrictions of this Agreement will remain in full force and effect. 1715 
 1716 
Any failure by a party or third party beneficiary to insist upon or enforce performance by another party of 1717 
any of the provisions of this Agreement or to exercise any rights or remedies under this Agreement or 1718 
otherwise by law shall not be construed as a waiver or relinquishment to any extent of the other parties’ 1719 
or third party beneficiary’s right to assert or rely upon any such provision, right or remedy in that or any 1720 
other instance; rather the same shall be and remain in full force and effect. 1721 
 1722 