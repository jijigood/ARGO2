O- RAN.WG6.CLOUD-REF-B-v01.01
Technical Specification 
Cloud Platform Reference Design 
for Deployment Scenario B 
This is a re-published version of the attached final specification. 
For this re-published version, the prior versions of the IPR Policy will apply, except that the previous 
requirement for Adopters (as defined in the earlier IPR Policy) to agree to an O-RAN Adopter License 
Agreement to access and use Final Specifications shall no longer apply or be required for these Final 
Specifications after 1st July 2022.
The copying or incorporation into any other work of part or all of the material available in this 
specification in any form without the prior written permission of O-RAN ALLIANCE e.V.  is prohibited, 
save that you may print or download extracts of the material on this site for your personal use, or copy 
the material on this site for the purpose of sending to individual third parties for their information 
provided that you acknowledge O-RAN ALLIANCE as the source of the material and that you inform the 
third party that these conditions apply to them and that they must comply with them.

 
Copyright © 2020 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 
 
 
  O-RAN.WG6.CLOUD-REF-B-v01.01 
Technical Specification 
 
 
 
 
 
 
 
 
Prepared by the O-RAN Alliance. Copyright © 2020 by the O-RAN Alliance.  
By using, accessing or downloading any part of this O -RAN specification document, including by copying, saving, 
distributing, displaying or preparing derivatives of, you agree to be and are bound to the terms of the O -RAN Adopter License 
Agreement contained in Annex ZZZ of this specification. All other rights reserved . 
 
 
 
 
Cloud Platform Reference Design  
for Deployment Scenario B  

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
2 
Revision History 1 
Date Revision Author Description 
2019.06.02 01.00.00 Tong Li (Lenovo) Initial skeleton. 
2019.08.05 01.00.01 Tong Li (Lenovo) Initial content with contributions from Intel, Lenovo, 
Mavenir, and Wind River. 
2019.11.05 01.00.03 Guy Turgeon (Wind River) Initial Cloud Platform Requirements 
2019.11.05 01.00.04 Guy Turgeon (Wind River) Cloud platform requirements additions 
2019.11.05 01.00.05 Arjun Nanjundappa 
(Mavenir) 
vBBU initial requirements 
2019.11.06 01.00.06 Guy Turgeon (Wind River) Cloud Platform Requirements architecture diagrams 
modifications and text 
2019.11.08 01.00.07 Arjun Nanjundappa 
(Mavenir) 
vBBU Stack Requirements updated Sections 3.3.8 
(RAN Nodes Traffic Types and Acceleration)  and 3.3.9 
(Cloud Transport Protocol). 
2019.11.08 01.00.08 Guy Turgeon (Wind River) Merge 01.00.06 and 01.00.007 
2019.11.14 01.00.09 Guy Turgeon (Wind River) Genericize Cloud Platform Requirements  
2019.11.18 01.00.10 Guy Turgeon (Wind River) Update Scenario B: high- level diagram with LLS -C3 
and SMO. Modify Chapter 4 VM and Container based 
platform diagrams 
2019.11.26 01.00.11 Arjun Nanjundappa 
(Mavenir) 
1. Made the Traffic Types Generic as AAL instead of 
BBDEV.2. Included different types of Acceleration: 
Look-aside, In -line and Hybrid -Mode Acceleration.3. 
Replaced S1 -MME by S1 -AP4. Updated CU -UP with 
S1-U and NG-U traffic type. 
2019.12.30 01.00.12 Tong Li (Lenovo) Re-structure doc, merge vendor & operator survey 
results, merge INT-004 rev 1 from Niall Power  (Intel), 
and various edits  
2020.01.15 01.00.13 Guy Turgeon (Wind River) Specify CR -IO 4.2.2, add Wireless cipher 4.2.3, clean 
up Kubernetes architecture components figure in 5.2.3 
2020.01.24 01.00.14 Niall Power (Intel) Specify AAL Requirements for the Hardware 
Acceleration section 
2020.01.29 01.00.14 
(bis) 
Arjun Nanjundappa 
(Mavenir) 
Added content for Time Synchronization section and 
re-worked sections on Acceleration to capture different 
acceleration needs (i.e., traffic and algos) 
2020.01.31 Baseline 
01.00.01 
Lucian Suciu (Orange) Integrated contributions from Intel and Mavenir, 
implemented agreed resolution s for several comments 
received from RH, ERI, NVD (a few are still open…) , 
generated the Baseline document 
2020.02.12 Baseline 
01.00.02 
Tong Li (Lenovo) Revision based on WG6 group discussion and various 
edits 
2020.02.12 Baseline 
01.00.03 
Arjun Nanjundappa 
(Mavenir) 
Time Synchronization Requirements and Figure 
Caption update 
2020.02.26 Baseline-
v01.00.05 
Lopamudra Kundu (Nvidia) Revised section 4.3.2, appended lookaside and in -line 
acceleration dataflows and associated Fig. in section 
4.3.1, added/revised figs. in section 5.1.6.2 related to 

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
3 
  2 
GPU based inline PHY acceleration reference design 
and high- level edge cloud platform for inline 
acceleration, provided additional Kubernetes plugins in 
section 5.2.3. 
2020.03.04 Baseline-
v01.00.07 
Arjun Nanjundappa 
(Mavenir) 
Included both In -line and Look -aside acceleration 
diagrams for O -DU and Accelerators to other nodes 
have been kept generic for section 4.3.2. 
2020.03.12 Baseline-
v01.00.11 
Marge Hillis (Nokia), 
Lopamudra Kundu (Nvidia), 
Tong Li (Lenovo), Arjun 
Nanjundappa (Mavenir), 
Niall Power (Intel), Lucian 
Suciu (Orange), Guy 
Turgeon (Wind River), 
Xiaogang Yan (China 
Mobile), Tao Yang (China 
Telecom) 
Various edits. 
2020.03.17 V00.16 Lenovo, Red Hat, Orange Time Sync modifications, minor editorial, make this 
version ready for WG6 internal review and voting 
2020.03.19 V00.17 Nokia, Red Hat Time Sync section modifications proposal from Nokia 
(mostly editorial), and clarifications from Red Hat. 
 
2020.03.20 
 
V01.00 
  
Version ready for TSC review and voting 
2020.07.10 V01.01 Udi Schwager (Wind River) Added support for container.d 
2020.07.10 V01.01 Niall Power (Intel) Added reference accelerator cards for acceleration 
2020.07.10 V01.01 Lopamudra Kundu (Nvidia) Added reference design for edge cloud server with GPU 
based acceleration 

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
4 
Table of Contents  3 
Revision History ................................................................................................................................................. 2 4 
Chapter 1 Introductory Material ......................................................................................................................... 6 5 
 Scope  ........................................................................................................................................................................... 6 6 
 References .................................................................................................................................................................... 6 7 
 Abbreviations ................................................................................................................................................................ 7 8 
Chapter 2 Deployment Scenario ......................................................................................................................... 9 9 
Chapter 3 Operator Requirements .................................................................................................................... 10 10 
 Typical Use Cases ....................................................................................................................................................... 10 11 
 Cloud Deployment Requirements ............................................................................................................................... 11 12 
 Regional Cloud ........................................................................................................................................................ 11 13 
 Edge Cloud .............................................................................................................................................................. 11 14 
Chapter 4 O-Cloud Requirements .................................................................................................................... 12 15 
 O-Cloud Architectural Components and Requirements ............................................................................................. 12 16 
 Hardware Requirements .......................................................................................................................................... 12 17 
 Operating System Requirements.............................................................................................................................. 13 18 
 Cloud Platform Runtime Requirements ................................................................................................................... 13 19 
 Generic Requirements for Cloud Platform Management ......................................................................................... 13 20 
 VM/Container Management and Orchestration Requirements ................................................................................ 14 21 
 Time Synchronization Requirements .......................................................................................................................... 15 22 
 G.8275.1 Full Timing Support (FTS) ...................................................................................................................... 15 23 
 G.8275.2 Partial Timing Support (PTS) .................................................................................................................. 17 24 
 Hardware Acceleration Requirements ........................................................................................................................ 17 25 
 Background on Acceleration Types ......................................................................................................................... 18 26 
 Hardware Acceleration Abstraction Layer Requirements ....................................................................................... 20 27 
Chapter 5 Cloud Platform Reference Design Example .................................................................................... 23 28 
 Cloud Platform Hardware Reference Design .............................................................................................................. 23 29 
 Regional Cloud Server ............................................................................................................................................. 23 30 
 Edge Cloud Server ................................................................................................................................................... 24 31 
 BIOS/UEFI Configurations ..................................................................................................................................... 25 32 
 Firmware Versions ................................................................................................................................................... 27 33 
 Accelerator Hardware .............................................................................................................................................. 27 34 
 Cloud Platform Software Reference Design ............................................................................................................... 30 35 
 Common Reference Configurations for all Profiles .................................................................................................  30 36 
 Regional Cloud Reference Configuration Profile .................................................................................................... 32 37 
 Edge Cloud Reference Configuration Profile .......................................................................................................... 32 38 
 Example Real-time Linux Configurations ............................................................................................................... 33 39 
Annex ZZZ: O-RAN Adopter License Agreement .......................................................................................... 35 40 
Section 1: DEFINITIONS ................................................................................................................................................ 35 41 
Section 2: COPYRIGHT LICENSE .................................................................................................................................  35 42 
Section 3: FRAND LICENSE .......................................................................................................................................... 35 43 
Section 4: TERM AND TERMINATION ........................................................................................................................ 36 44 
Section 5: CONFIDENTIALITY ..................................................................................................................................... 36 45 
Section 6: INDEMNIFICATION ..................................................................................................................................... 36 46 
Section 7: LIMITATIONS ON LIABILITY; NO WARRANTY .................................................................................... 37 47 
Section 8: ASSIGNMENT ............................................................................................................................................... 37 48 
Section 9: THIRD-PARTY BENEFICIARY RIGHTS .................................................................................................... 37 49 
Section 10: BINDING ON AFFILIATES ........................................................................................................................ 37 50 
Section 11: GENERAL ..................................................................................................................................................... 37 51 
 52 
 53 
 54 

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
5 
Table of Figures 55 
Figure 2-1. Cloud deployment scenario B. ......................................................................................................................... 9 56 
Figure 2-2. Scenario B – O-Cloud high-level deployment diagram. .................................................................................. 9 57 
Figure 4-1. Architecture component requirements for the cloud platform. ...................................................................... 12 58 
Figure 4-2. Key synchronization chain elements in an example implementation in O-Cloud for LLS-C3. ..................... 16 59 
Figure 4-3. O-DU interface and algorithm acceleration. .................................................................................................. 18 60 
Figure 4-4. O-CU-CP interface and algorithm acceleration. ............................................................................................ 19 61 
Figure 4-5. O-CU-UP interface and algorithm acceleration. ............................................................................................ 20 62 
Figure 4-6. AAL support for multiple devices. ................................................................................................................. 21 63 
Figure 4-7. AAL look-aside acceleration model. .............................................................................................................. 21 64 
Figure 4-8. AAL inline acceleration model. ..................................................................................................................... 22 65 
Figure 4-9. Dataflow paths in look-aside and inline acceleration architectures. .............................................................. 23 66 
Figure 5-1. Hardware-accelerated FEC functions with look-aside model. ....................................................................... 28 67 
Figure 5-2. Hardware-accelerated front-end functions with inline model. ....................................................................... 28 68 
Figure 5-3. Hardware-accelerated end-to-end high-PHY functions with inline model. ................................................... 28 69 
Figure 5-4. Edge cloud platform for look-aside acceleration of O-DU and O-CU. .......................................................... 29 70 
Figure 5-5. Edge cloud platform for O-DU inline acceleration and O-CU look-aside acceleration. ................................ 29 71 
Figure 5-6. Edge cloud platform for O-DU inline acceleration of end-to-end high-PHY. ............................................... 30 72 
 73 
Table of Tables 74 
Table 3-1. AT&T Carrier aggregation requirements. ....................................................................................................... 10 75 
Table 3-2. Deutsche Telekom carrier aggregation requirements. ..................................................................................... 10 76 
Table 3-3. Orange carrier aggregation requirements. ....................................................................................................... 10 77 
Table 3-4. BBU pooling requirements. ............................................................................................................................. 10 78 
Table 3-5. Edge cloud environmental characteristics. ...................................................................................................... 11 79 
Table 4-1. Requirements for the cloud platform hardware. .............................................................................................. 13 80 
Table 4-2. Requirements for the cloud platform operating system. .................................................................................. 13 81 
Table 4-3. Requirements for the cloud platform runtime. ................................................................................................ 13 82 
Table 4-4. Cloud platform management requirements. .................................................................................................... 14 83 
Table 4-5. Requirements for VM/container management and orchestration. ................................................................... 14 84 
Table 4-6. O-DU acceleration types. ................................................................................................................................ 18 85 
Table 4-7. O-CU-CP acceleration types. .......................................................................................................................... 19 86 
Table 4-8. O-CU-UP acceleration types. .......................................................................................................................... 20 87 
Table 5-1. Regional cloud server example reference design. ........................................................................................... 24 88 
Table 5-2. Edge cloud server example reference design with FPGA acceleration. .......................................................... 24 89 
Table 5-3. Edge cloud server example reference design with GPU acceleration. ............................................................ 25 90 
Table 5-4. BIOS configurations for strong real-time systems. ......................................................................................... 27 91 
Table 5-5. Firmware for example reference design based on Intel server board S2600WF. ............................................ 27 92 
Table 5-6. Common cloud platform reference configurations for all profiles. .................................................................  32 93 
Table 5-7. Regional cloud reference configuration profile. .............................................................................................. 32 94 
Table 5-8 Edge Cloud Reference Configuration Profile ................................................................................................... 33 95 
Table 5-9. Real-time Linux configurations. ...................................................................................................................... 34 96 
 97 
 98 
 99 
100 

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
6 
Chapter 1 Introductory Material 101 
Scope 102 
This Technical Specification has been produced by the O-RAN Alliance. 103 
The contents of the present document are subject to continuing work within O -RAN and may change following formal 104 
O-RAN approval. Should the O-RAN Alliance modify the contents of the present document, it will be re -released by O-105 
RAN with an identifying change of release date and an increase in version number as follows: 106 
Release x.y.z 107 
where: 108 
x the first digit is incremented for all changes of substance, i.e. technical enhancements, corrections, updates, 109 
etc. (the initial approved document will have x=01). 110 
y the second digit is incremented when editorial only changes have been incorporated in the document.  111 
z the third digit included only in working versions of the document indicating incremental changes during the 112 
editing process. 113 
The present document focuses on cloud deployment scenario B , as defined in the O -RAN WG6 Cloud Architecture 114 
document [1], and specifies requirements and reference designs for the cloud platform hardware and software . 115 
References 116 
The following documents contain provisions which, through reference in this text, constitute provisions of th is 117 
specification. 118 
[1] O-RAN WG6, Cloud Architecture and Deployment Scenarios for O-RAN Virtualized RAN. 119 
[2] O-RAN WG4, Control, User and Synchronization Plane Specification. 120 
[3] GR-63-CORE for Network Switching Systems (NEBS) Seismic Rated Enclosures, Telecordia. 121 
[4] GR-1089-CORE Electromagnetic Compatibility and Electrical Safety, Telcordia. 122 
[5] OTII Server Technical Specification, 2019. 123 
[6] OCP openEDGE Project, https://www.opencompute.org/wiki/Telcos/openEDGE. 124 
[7] ETSI EN300 019 -1-3 Class 3.2 Equipment Engineering (EE) Environmental Conditions and E nvironmental 125 
Tests for T elecommunications Equipment Part 1- 3: Classification of E nvironmental Conditions Stationary U se 126 
at Weather-protected Locations. 127 
[8] ETSI EN300386 (v1.6.1) Terrestrial Trunked Radio (TETRA); Direct Mode Operation (DMO); Part 6: Security 128 
[9] FCC CFR47 15 (class A), FCC, Rules and Regulations for Title 47 Telecommunications. 129 
[10] CISPR 22/32 (Class A) International Electr otechnical Commission (IEC) International  Special Committee on 130 
Radio Interference, Radiated Emission Limits. 131 
[11] CISPR 24, IEC Information Technology Equipment – Immunity Characteristics. 132 
[12] TEC/EMI/TEL-001/01/FEB-09 – Information Technology Equipment. 133 
[13] TEC/IR/SWN-2MB/07/MAR-10 – Information Technology Equipment. 134 
[14] IEC 62368-1:2014, Safety Standard for Information Technology Equipment and Audio Video Equipment.  135 
[15] O-RAN WG4 Management Plane Specification. 136 
[16] 3GPP TS 23.501: “System architecture for the 5G System (5GS)”. 137 
[17] O-RAN WG8, Base Station O-DU and O-CU Software Architecture and APIs. 138 
[18] O-RAN Infrastructure Project, https://wiki.o-ran-sc.org/display/IN/Infrastructure+Home. 139 

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
7 
Abbreviations 140 
For the purposes of the present document, the following abbreviations apply.  141 
AAL Acceleration Abstraction Layer 142 
API Application Programming Interface 143 
ASIC Application-specific Integrated Circuit 144 
BIOS Basic Input Output System 145 
BBU Base Band Unit 146 
CNF Containerized Network Function 147 
CPU Central Processing Unit 148 
DSP Digital Signal Processing 149 
FPGA Field Programmable Gate Array 150 
DPDK Data Plane Development Kit 151 
eNB eNodeB (applies to LTE)   152 
EPA Enhanced Platform Awareness 153 
FEC Forward Error Correcting 154 
FFS For Further Study 155 
FFT Fast Fourier Transform 156 
FHGW Front-haul Gateway 157 
FTS Full Timing Support 158 
gNB gNodeB (applies to NR) 159 
GNSS Global Navigation Satellite System 160 
GPU Graphics Processing Unit 161 
HA High Availability 162 
HW Hardware 163 
LLS-C3 Lower Layer Split Configuration 3 164 
NFV Network Functions Virtualization 165 
NFVI Network Functions Virtualization Infrastructure 166 
NIC Network Interface Card 167 
NSA Non-standalone 168 
NUMA Non-uniform Memory Access 169 
O-CU O-RAN Centralized Unit 170 
O-DU O-RAN Distributed Unit 171 
O-RU O-RAN Radio Unit 172 
OS Operating System 173 
PHC PTP Hardware Clock 174 
PIM Physical Infrastructure Manager 175 
PRACH Physical Random Access Channel 176 
PRTC Primary Reference Time Clock 177 
PSU Power Supply Unit 178 
PTP Precision Time Protocol 179 

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
8 
PTS Partial Timing Support 180 
RAID Redundant Array of Inexpensive Disks 181 
RIC RAN Intelligent Controller 182 
RT Real-time 183 
SA Standalone 184 
SMO Service Management and Orchestration 185 
SR-IOV Single Root I/O Virtualization 186 
SW Software 187 
T-BC Telecom Boundary Clock 188 
T-GM Telecom Grand Master 189 
T-TSC Telecom Time Slave Clock 190 
UEFI Unified Extensible Firmware Interface 191 
vBBU Virtual Baseband Unit 192 
VF Virtual Function 193 
VIM Virtualized Infrastructure Manager 194 
VM Virtual Machine 195 
VNF Virtualized Network Function 196 
  197 

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
9 
Chapter 1 Deployment Scenario 198 
This document focuses on cloud deployment scenario B, as defined in the O -RAN Cloud Architecture document  [1]. 199 
The current version of this document focuses on SA NR gNBs deployed on the O-Cloud. The extensions of O-Cloud, if 200 
required, to support NSA scenarios indicated in the annexes of the O-RAN Cloud Architecture document [1] are FFS. 201 
Figure 1-1 illustrates scenario B, where the near-RT RIC is virtualized or containerized on a r egional O-Cloud, and the 202 
O-CU and O -DU functions are virtualized or containerized on an e dge O-Cloud that typically resides at a different 203 
location. The terms, vO -CU and vO -DU, represent virtualized or containerized  O-CU and O -DU, and are used 204 
interchangeably with O-CU and O-DU in this document. 205 
The interface that the near-RT RIC uses between the regional O-Cloud and the edge O-Cloud is E2. Interfaces  between 206 
the O -CU and O -DU managed functions are either within the same single m anaged element, or among multiple 207 
managed elements within the same edge O-Cloud. 208 
 209 
Figure 1-1. Cloud deployment scenario B. 210 
This scenario is to support deployments in locations with limited remote front -haul capacity and a set of O -RUs spread 211 
out in a limited  area, supported by pooled vO -CU/vO-DU functionality while still meeting the O -DU latency 212 
requirements. The optional use of a Front -haul Gateway ( FHGW) in the architecture allows significant savings in 213 
providing transport between the O-RU and vO-DU functionalities. 214 
Figure 1-2 shows the O -Cloud high- level deployment diagram. One vO -CU and one vO -DU together form a virtual 215 
BBU (vBBU), which connects to N O-RUs (typically up to 64, see [1] ). This deployment should support tens of 216 
thousands of O-RUs per near-RT RIC, and therefore L could easily exceed 100.  217 
O-Clouds are deployed at both regional and edge locations. At a given location, the edge O -Cloud can scale from a 218 
single server to a clustered configuration with possibly hundreds of servers. For the time synchronization aspects, this 219 
document focuses on the LLS-C3 configuration [2]. 220 
 221 
Figure 1-2. Scenario B – O-Cloud high-level deployment diagram. 222 


                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
10 
Chapter 2 Operator Requirements 223 
Typical Use Cases 224 
Typical use cases include mobile wireless coverage for large metropolitan areas  with operator requirements as shown in 225 
Table 2-1 to Table 2-4. 226 
Carrier Type Component Carrier 
Bandwidth 
Number of Layers 
(Downlink) 
Number of 
Aggregated Carriers 
Total Aggregated 
Bandwidth 
LTE 20 MHz 4 layers 5 carriers 400 MHz 
NR Sub-6 GHz 20 MHz 16 layers 5 carriers 1600 MHz 
mmWave 100 MHz 2 layers 8 carriers 1600 MHz 
Table 2-1. AT&T Carrier aggregation requirements. 227 
 228 
Carrier Type Component Carrier 
Bandwidth 
Number of Layers Number of 
Aggregated Carriers 
Total Aggregated 
Bandwidth 
LTE 5-20 MHz 4 layers 3-5 carriers 240-320 MHz 
NR Sub-6 GHz 100 MHz 8-16 layers 1 carrier Up to 1600 MHz 
mmWave FFS FFS FFS FFS 
Table 2-2. Deutsche Telekom carrier aggregation requirements. 229 
 230 
Carrier Type Component Carrier 
Bandwidth 
Number of Layers Number of 
Aggregated Carriers 
Total Aggregated 
Bandwidth 
LTE 20 MHz 4 layers 4 carriers 320 MHz 
NR Sub-6 GHz 100 MHz 8 or 16 layers 1 carrier Up to 1600 MHz 
mmWave FFS FFS FFS FFS 
Table 2-3. Orange carrier aggregation requirements. 231 
 232 
Active UEs (95 th percentile) supported per macro sub -6 GHz 
TRP 
256 (AT&T, others FFS) 
Active UEs (95th percentile) supported per mmWave TRP 64 (AT&T, others FFS) 
Carrier capacity (MHz) per O-DU pool 57,600 MHz (AT&T, others FFS) 
O-DU pooling model  Pooled centralization [1] 
Minimum number of TRPs that an O-DU pool supports 32 (AT&T and Orange, others FFS) 
Need for a fronthaul gateway (FHGW) Yes 
Fronthaul interface to O-DU O-RAN 7.2x, with low phy in O-RU or FHGW 
Physical interface to the cloud platform supporting O -DU (e.g., 
from FHGW) 
At least 2x 100 Gbps links to minimize 
switching latency 
Table 2-4. BBU pooling requirements. 233 

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
11 
Cloud Deployment Requirements 234 
As Figure 1-1 shows, deployment of a cloud platform occurs in tw o locations: regional and edge clouds. The physical 235 
environment of a regional cloud is typically a modern data center, hosting standard server racks with a 19- inch wide 236 
rack enclosure, 42U height, and external depth between 1 and 1.2 meters. In this environment, the cloud platform 237 
hardware can use standard rack -mount servers and switches, and does not impose special requirements on the hardware 238 
form factors. 239 
The edge cloud often resides in a traditional telecommunications equipment room with limited space, cooling capacity, 240 
floor loading, and so forth.  The limitations can impose special requirements on the cloud platform  hardware that may 241 
require telecommunications  standard certifications, such as the Network Equipment Building System (NEBS) 242 
requirements fou nd in GR -63-CORE [3] and GR-1089-CORE [4]. For example, the Open Telecom IT Infrastructure 243 
(OTII) Project specifies a 2U rack server with depth less than 450 mm  [5][3]. In addition, the Open Compute Project 244 
(OCP) operates an openEDGE project  [6], which specifie s a compact cloud platform that  has the extended 245 
environmental requirements often needed in edge deployments. 246 
Regional Cloud 247 
A regional cloud hosts the near -RT RIC, which includes Radio Connection Management, Mobility Management, QoS 248 
Management, Interference Management, Radio -Network Information Base , and possibly 3rd-party applications . A 249 
typical regional cloud has the following characteristics: 250 
• Standard Internet Data Center (IDC) environment; 251 
• At least one standard 19-inch rack with a top-of-rack (TOR) switch and five servers; 252 
• Highly available cloud platform software; 253 
• Highly available network topology; 254 
• Highly available storage system in the form of tradi tional storage area network (SAN), software -defined 255 
storage (SDS), or combination of the two. 256 
 257 
Edge Cloud 258 
The edge cloud is often deployed in a relatively rough physical environment with typical characteristics in Table 2-5.  259 
Rack Form Factors 19-inch width, 600mm depth 
Temperature Range Long-term: -5°C to 45°C (ETSI EN300 019-1-3 Class 3.2 [7]) 
Short-term: -5°C to 55°C (GR-63-CORE [3]) 
Humidity Operating humidity: 5% to 95% 
Long-term: 5% to 85% 
Short-term: 5% to 90% 
EMC EN300386 (v1.6.1) [8] 
FCC CFR47 15 (class A) [9], CISPR 22/32 (class A) [10], CISPR 24 [11] 
TEC/EMI/TEL-001/01/FEB-09 [12] and TEC/IR/SWN-2MB/07/MAR-10 [13] 
GR-1089-CORE [4] 
Equipment Center Class A (Telecom center); Class B (Non-telecom center) 
Seismic GR-63-CORE (Zone 4) [3] 
Safety IEC 62368-1:2014 [14] 
GR-63-CORE (Electrical safety, grounding and bonding) [3] 
Acoustic Noise GR-63-CORE (equipment room criteria) [3] 
Fire Resistance GR-63-CORE (shelf level criteria) [3] 
Power Supply 208V AC, 220V AC, or -48V DC (-60V DC in Germany) 
Table 2-5. Edge cloud environmental characteristics. 260 

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
12 
Chapter 3 O-Cloud Requirements 261 
This chapter describes design requirements for the  O-Cloud. While most requirements apply to both the regional and 262 
edge O-Clouds, the edge O -Cloud may impose unique requirements, e.g., hardware accelerators, which will be noted 263 
specifically. In addition, d epending on the implementation choices for O -CU, O-DU, and near -RT RIC, the O -Cloud 264 
needs to provide support for virtual machines or containers. 265 
O-Cloud Architectural Components and Requirements 266 
 267 
Figure 3-1. Architecture component requirements for the cloud platform. 268 
Figure 3-1depicts the high- level architecture components for deploying VM/container -based O-RAN functions in the 269 
regional and edge clouds. 270 
The Virtualized/Containerized O -RAN Functions  include the O -DU, O-CU and other workloads deployed on the O -271 
Cloud via the O2 interface. 272 
The VM/Container Management and Orchestration is typically implemented with Open Stack for VM deployments and 273 
Kubernetes for container deployments. 274 
The Cloud Platform Management represen ts the O -Cloud infrastructure management functions required for life cycle 275 
management, high availability, fault management and configuration of the O -Cloud platform. 276 
The Cloud Platform Runtime  represents the additional cloud platform run- time capabilities required to host O -RAN 277 
functions as compared to 5G core network functions. 278 
Hardware Requirements 279 
Table 3-1 shows requirements for the cloud platform hardware. 280 
Components Description 
Server • Regional cloud: standard COTS server 
• Edge cloud: depth ≤ 450mm, height ≤ 2RU, width ≤ 19in 
• Minimum 16-core CPU at 2.2 GHz base frequency and 128 GB DDR4 
• At least 4x 10/25GbE ports for front -haul interface, 2 x 10 GbE ports  for backhaul 
interface, and 1x 1GbE out-of-band management port 
• SR-IOV-capable NICs with network link aggregation enabled 


                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
13 
• For single-socket, at least 3 PCI-E slots for  3x HHHL or 1x FHFL + 2x HHHL cards; 
for dual-socket, at least 4 slots for 4x HHHL or 1x FHFL DW + 3x LP cards 
• All hard disk drives require hot-pluggable 
• Management interfaces support IPMI, SNMP, and Redfish (preferred) 
• UEFI secure boot enabled 
Hardware Accelerator • Required for O-DU in the edge cloud in forms such as FPGA, ASIC, and GPU  
• At least 2 ports at 10 or 25 Gbps eCPRI/RoE open front-haul interface 
• Acceleration for functions such as LDPC encoding and decoding, and end- to-end high 
PHY 
Switch • Standard COTS TOR switch 
Storage • Software-defined storage (e.g., Ceph) based on COTS servers 
Table 3-1. Requirements for the cloud platform hardware. 281 
Operating System Requirements 282 
Table 3-2 shows requirements for the cloud platform operating system. 283 
Operating System Description 
Linux 
 
• Linux kernel with real -time pre emption patches  for O-DU workloads  (i.e., SMP 
PREEMPT RT). Real-time kernel patch for O-CU is optional. 
• Deterministic interrupt handling with  a maximum latency of 20 us  as measured by 
cyclictest for system interrupts (e.g., externa l I/O) and interrupt- based O -CU and O -
DU implementations (e.g., those that reply on OS timers) 
• CRI-O and/or CRI plugin containerd support 
• QEMU/KVM (or other types of hypervisor) for virtual machines 
Table 3-2. Requirements for the cloud platform operating system. 284 
Cloud Platform Runtime Requirements 285 
Table 3-3 shows requirements for the cloud platform runtime. 286 
Runtime Components Description 
Accelerator Driver 
 
• Edge cloud: driver for loading, configuring, managing and interfacing with accelerator 
hardware providing offload functions for O-DU container or VM 
Crypto Driver • Crypto offload driver for networking and O-CU wireless cipher (optional) 
Network Driver • Network driver(s) for  front-haul, back-haul, mid -haul, inter container or VM 
communication, management and storage networks 
Board Management • Board management for interfacing with server hardware and sensors 
PTP • Precision time protocol for distributing phase, time and synchronization over a packet- 
based network 
Software-defined 
Storage (SDS) 
• Software implementation of block storage running on COTS servers, optional for edge 
cloud and required for regional cloud 
• If used in edge cloud, required to be hyperconverged;  in regional cloud, either 
hyperconverged or deployed on dedicated storage nodes 
Container Runtime • Executes and manages container images on a node 
Hypervisor • Allows host to run multiple isolated VMs 
• Low performance overhead, compared with non- virtualized execution of specified 
benchmarks, e.g., SPECcpu, fio, L2Fwd, etc. (quantitative requirements FFS) 
Table 3-3. Requirements for the cloud platform runtime. 287 
Generic Requirements for Cloud Platform Management  288 
Table 3-4 shows the generic requirements for management of the cloud platform.  289 

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
14 
Components Description 
Configuration 
Management 
 
• Auto-discovery of new servers added to an O-Cloud  
• Management of installation parameters such as console, root disks 
• Configuration of core, memory, huge-page assignments on each server node 
• Network interfaces and storage assignments 
• Hardware discovery of CPUs, cores, storage, network ports, hardware accelerators, etc. 
• Virtual-to-physical CPU core and NUMA node pinning 
• Virtual NUMA topology configuration 
• Huge-page memory 
• Interrupt and thread pinning to virtual and physical cores 
Host Management 
 
• Life-cycle and availability management of the server 
• Host failure detection and recovery 
• Fault monitoring and reporting 
• Cluster connectivity failure detection and recovery 
• VM/container failure detection and recovery 
• Critical process failure detection and recovery 
• Resource utilization thresholds (e.g., CPU, memory, storage limits) 
• Network interface states 
• Hardware sensors and faults 
• Watchdog 
• Board management interfaces and out-of-band server management 
• Hardware sensor monitoring 
Service Management • Monitoring of critical platform infrastructure services and processes 
• Automatic recovery of failed platform infrastructure services and processes  
HA Management • High-availability services for supporting cloud platform redundancy 
Fault management • Cloud platform infrastructure alarm and fault management. 
• Ability to set, clear, query, filter, suppress, log and collect alarms 
Software Management 
 
• Ability to deploy software updates 
• Support for rolling updates across all nodes of the local O-Cloud 
• Support for migrating VMs and/or containers within the local O -Cloud for software 
upgrade procedures  
User Management • User authentication and authorization 
• Isolation of control and resources among different users 
Node Feature 
Management 
• Detection and set ting of  node-level policies to align resource allocation choices (i.e. 
NUMA, SR-IOV, CPU, etc.) 
HW Accelerator 
Management 
• Support for m anaging hardware accelerator s, mapping them to O -RAN applications 
VMs and/or containers, and updating accelerator firmware 
Table 3-4. Cloud platform management requirements. 290 
VM/Container Management and Orchestration Requirements 291 
Table 3-5 shows requirements for VM/container management and orchestration as part of the cloud platform.  292 
Components Description 
VM/Container 
Management 
 
• VM management and scheduling with OpenStack Nova 
• Container management and scheduling with Kubernetes kube -apiserver, kube -
scheduler, etcd, kube-controller-manager 
VM/Container 
Orchestration 
• VM orchestration with Heat templates 
• Container orchestration with HELM charts 
VM/Container Storage • VM storage and image services with OpenStack Glance and Cinder 
• Container persistent volume claims (PVCs) 
• VM/Container Ceph backends for persistent storage (optional) 
VM/Container 
Networking 
• VM networking with OpenStack Neutron 
• Container networking with kube-proxy and CNI 
Table 3-5. Requirements for VM/container management and orchestration. 293 

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
15 
Time Synchronization Requirements 294 
The synchronization requirements relevant to this document cover only the synchronization aspects of the O -Cloud 295 
nodes that are part of the edge cloud. Specifically, we need to comply with the synchronization requirements associated 296 
with the O-DU instances when instantiated on the O -Cloud nodes, as in CUS specification [2] LLS-C3 synchronization 297 
configuration with G.8275.1 full timing support profile or G.8275.2 parti al timing support profile.  298 
The primary synchronization point in an O -RAN system is in the radio units, i.e. O -RUs, and those requirements drive 299 
the performance aspects of many associated entities in the synchronization transfer path. The end- to-end performance 300 
requirements for absolute and relative timing at the O -RU are use-case and system-design specific, and are specifically 301 
outside of the scope of this section. The performance aspects with specific use cases in the context of O -RU 302 
requirements are specified in the CUS specification [2]. 303 
This section assumes that the O-DU and O-RU requirements are decoupled and independent, which is the case when O -304 
DU instances are not in the synchronization transfer path from a synchronization source, i.e. PRTC/T -GM instance and 305 
O-RU. The LLS-C3 configuration meets these criteria, as the synchronization source is decoupled from O -DU instances, 306 
and placed in the front-haul network.  307 
In terms of  the synchronization transfer chain, both the O -DU and O- RU act as slaves with respect to the physical 308 
network elements (such as switches and/or routers) in the synchronization transfer path, which act as masters in 309 
interfaces towards the O -DU (and O- RU) slave instances, and Boundary Clocks, i.e. slave towards the synchronization 310 
source elements and masters towards slave and/or downstream switches when they are responsible for timing transfer 311 
to/from other physical network elements (i.e. other switches and/or Grandmaster clocks). 312 
The performance requirements in the CUS specification [2] are end-to-end performance requirements. Specifically, in 313 
the context of this specification and use case, the CUS specification  [2] specifies an absolute end -to-end performance 314 
applicable to O-DU instances, the applicable requirements are the requirements associated with the LLS -C3 case in the 315 
CUS specification [2]. There are multiple performance classes for all of  the instances involved in the CUS specification  316 
[2] and the associated standards specified therein. The end -to-end performance must be met with the maximum required 317 
number of the nodes on the path, which is dependent on a combination of the network design , each element’s 318 
performance class, and location of the desired synchronization source (PRTC/T -GM) in the topology.  319 
Implementers are encouraged to work on improving the performance of all parts of the elements (e.g. through designing 320 
individual components  to more demanding classes), which allows more  flexibility on the synchronization source 321 
placement with respect to the clients (e.g. through leveraging redundant PRTC/T -GM sources from surrounding edge 322 
cloud sites and/or from regional cloud sites), and mor e intermediate nodes in the synchronization transfer path and/or 323 
more accurate synchronization performance at the T-TSC levels. 324 
Since the synchronization functions are largely decoupled from the O -DU instances in the cloud implementation (i.e. 325 
most synchronization functions are provided as infrastructure services), the associated synchronization management and 326 
monitoring interfaces in the cloud are for further study. In the interim, implementers should target at least semantic 327 
equivalence to the synchroniza tion configuration management, performance monitoring metrics and events (such as 328 
synchronization state transitions) as described in the CUS specification [2] , M-plane specification [15] , and references 329 
therein for synchronization management aspects as relevant to the LLS-C3 case. 330 
G.8275.1 Full Timing Support (FTS) 331 
Figure 3-2 illustrates an example of the redundant synchronization network for an edge site, whic h is aligned with the 332 
LLS-C3 synchronization configuration.  333 
 334 
 335 
 336 
 337 
 338 
 339 
 340 

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
16 
 341 
Figure 3-2. Key synchronization chain elements in an example implementation in O -Cloud for LLS-C3. 342 
The salient features of this design example are: 343 
(1) The synchronization reference sources (PRTC/T -GM) are redundant and located in the edge cloud. In this 344 
example, the references are derived from the GNSS system, but any other implementation compliant with the 345 
CUS specification [2] requirements (and references therein), which acts as G.82751 T-GM, is acceptable. 346 
(2) The synchronization referenc e sources are connected to the physical fabr ic in the edge cloud, with redundant 347 
Ethernet interconnects. Specifically, each grandmaster connects to at least two leaf switches. 348 
(3) Each switch within the synchronization tr ansfer path implements  the G.8275.1-compliant Boundary Clock 349 
functionality. 350 
(4) For larger site configurations (i.e.  in which the switch capacity and port  count requirements exceed the 351 
capabilities of the single physical switch), the structure is expanded in a “leaf-spine” fabric configuration, as 352 
illustrated in Figure 3-2. When spine switches are used/required, they also need to be T -BC capable, or 353 
alternative path for the synchronization transfer that meets the requirements of G.8275.1 profile needs to be 354 
provided. For most e dge site configurations, it is expected that a two-tier structure is sufficient, which implies 355 
that the maximum length of any active synchronization transfer path from co- located sources is three boundary 356 
clocks between the grandmaster  and any node in the e dge cloud. Therefore, the synchronization path “length” 357 
in terms of number of boundary clocks that needs  to be supportable in this configuration is typically from one 358 
to three. Capability to support more nodes in the path is desirable to allow for configuratio ns where the 359 
grandmasters are not co-located at the edge cloud. 360 
(5) All edge O-Cloud nodes are attached to at least two physically separated, independent leaf switches to provide 361 
network redundancy and tolerance against a single point of failure that can affect more than one node (i.e. 362 
switch failures). In addition to normal network f ault tolerance, this is also required /useful for maintaining the 363 
synchronization network fault tolerance. 364 
(6) All leaf switch ports which are facing the nodes are configured as masters in synchronization transfer, and all 365 
node ports are configured as slaves. I t is not a requirement or expectation for any of the node s to assume a role 366 
of the master (nor be able to meet the performance specifications of the T-GM). 367 
(7) All of the edge O-Cloud nodes implement PTP Hardware  Clock (PHC) function on all of the NICs that ar e 368 
associated with timing transfer. Nodes may have other interfaces, such as management interfaces, backhaul 369 
network interfaces and so forth, and associated switching infrastructure, but those are omitted from illustration 370 
for clarity. Such interfaces are n ot required to implement synchronization support features, but if they choose 371 
to do so, the synchronization-capable interface requirements apply to those interfaces as well. 372 
(8) All of the edge O-Cloud nodes implement T-TSC functionality in full compliance with the G.8275.1 profile. 373 
(9) The operating system clock on each node is synchronized from the NIC T-TSC clock(s). The cloud platform 374 
software on each node  is responsible for PTP protocol implementation, including adjustments to associated 375 


                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
17 
PHC clock(s). Whether the direct read -only access is required to the PHC counters from the i nstances is for 376 
further study. Write access is not allowable, as that would interfere with the PHC management from the cloud 377 
platform software, and as there are limited number of PHC ins tances in typical NICs, usually associated with 378 
the number of ports, the number of PHC instances can also in many cases be substantially less than the number 379 
of running application instances such as O-DU instances, in the node. 380 
(10) Nodes (or switches) are not required to implement the S yncE capabilities, as long as the accuracy requirements 381 
associated with O-DU instances as specified in the  CUS specification [2] for O-DU are met. Note that the O -382 
RU implementations may require the switches also to implement SyncE, but that is a network and 383 
implementation-level detail that is out of scope of this document. 384 
  385 
Many alternative variations and  physical realizations of the above design are possible (e.g. PRTCs/GMs can be 386 
combined with switches or routers, leaf -spine structure can be integrated onto physical box, sizes of elements can vary, 387 
etc.), but functionally with respect to the synchronization transfer chain, all of the entities are expected to be present (i.e. 388 
PRTC/T-GMs, Telecom Boundary Clock nodes, as well Telecom Slave Clock nodes).  389 
All physical network elements that are in the synchronization transf er path (i.e. PRTC/T-GM, switches that act as T-BC, 390 
and all O -Cloud node slaves that act as T -TSC) should actually implement a  1PPS output interface to access the 391 
synchronization reference signal to facilitate connection to external performance test equipm ent for performance 392 
validation purposes. 393 
The physical switches (leaf –spine) can be considered to be an integr al part or an extension of any f ront-haul transport 394 
network, i.e. G.8275.1 synchronization requirements apply to these elements as well, as outline d above. Further, for 395 
G.8275.1 profile and CUS specification [2] compliance, it is required that at least the Synchronization Plane aspects of 396 
these networks support untagged L2 Ethernet transport for the S -Plane traffic (even if C - and/or U-planes may util ize 397 
VLAN tags and/or IP encapsulations, potentially over the same interfaces with respect to O -DU front-haul traffic). 398 
For the purposes of this document, the O -DU synchronization performance requirements as specified in the CUS 399 
specification [2] for the O-DU LLS -C3 configuration should be met with the following set of elements in the 400 
synchronization transfer path: 401 
• a T-GM clock with the least demanding performance class (i.e. G.8272 PRTC/T-GM type PRTC-A);  402 
• at least three interim network elements acting as T -BCs in the path, with the least demanding T -BC 403 
performance class (i.e. G.8273.2 T-BC Class A); 404 
• T-TSC implementation with the least demanding performance class (i.e. G.8273.2, Annex C, T -TSC Class A). 405 
 406 
The initial requirements as outlined above are conservative and , to some extent , take advantage of the expected 407 
proximity of the synchronization sources with respect to the e dge O-Cloud nodes. Also, if the O -RUs are synchronized 408 
from the same sources, which is expected to be a typical case, O -RU rather than O -DU related synchronization 409 
requirements may determine the performance classes of the PRTC/T-GM and T-BCs, which are on the synchronization 410 
distribution path towards the O-RUs. 411 
G.8275.2 Partial Timing Support (PTS) 412 
As specified in the CUS  specification [2], DU synchronization requirements a re more relaxed than the requirements 413 
associated with the RU, and CUS specification [2] also allows separate PRTC sources to be used for the 414 
synchronization of the RUs and DUs. The use of G.8275.2 Partial Timing Support is allowed for the synchronization of 415 
O-Cloud nodes (and instantiated O -DU's), provided that the synchronization accuracy re quirements for the O -DU as 416 
specified in CUS specification [2] are met in such implementation. The detailed implementation of this configuration is 417 
subject for the further study. 418 
Hardware Acceleration Requirements 419 
The O-RAN Cloud Architecture document [1]  discusses the hardware  abstraction consider ations for the O -Cloud. It 420 
lists two levels of abstraction referring to the deployment model and APIs , and  also discusse s the management and 421 
orchestration considerations that should be supported by such hardware accelerators deployed on the cloud platform. 422 
This section describes the types of acceleration as background information and then specifies requirements applicable to 423 
the O-Cloud for acceleration hardware abstraction. 424 

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
18 
Background on Acceleration Types 425 
Hardware acceleration for the managed functions, such as O-DU and O-CU, can be classified into two types: 426 
• Interface acceleration: acceleration of I/O data transfers on any interface  between 3GPP or O -RAN network 427 
functions [16]; 428 
• Algorithm acceleration: acceleration of any algorithm processing within an O-RAN network function. 429 
 430 
Interface acceleration mostly exhibits as acceleration of network traffic transmission and reception, for which SR -IOV 431 
NICs, combined with software mechanisms such as DPDK, is commonly used. 432 
Algorithm acceleration includes acceleration of compute -intensive algorithm s in the O -DU and O- CU, utilizing 433 
accelerator hardware such as FPGA, GPU, DSP, and ASIC. 434 
Acceleration for the O-DU 435 
Figure 3-3 illustrates acceleration of the interfaces between the O -DU and other O -RAN network functions and 436 
acceleration of the high-PHY algorithms within the O-DU. Table 3-6 shows in more detail the accelerated functions. 437 
 438 
Figure 3-3. O-DU interface and algorithm acceleration. 439 
 440 
Accelerated Function Interface Name Acceleration Type 
I/O between O-DU and O-CU-CP F1-C Interface acceleration (optional) 
I/O between O-DU and O-CU-UP F1-U Interface acceleration  
I/O between O-DU and SMO O1 Interface acceleration (optional) 
I/O between O-DU and Near-RT RIC E2 Interface acceleration (optional) 
I/O between O-DU and O-RU FH7.2 Interface acceleration 
High-PHY algorithms N/A Algorithm acceleration 
Table 3-6. O-DU acceleration types. 441 


                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
19 
Acceleration for the O-CU-CP 442 
Figure 3-4 illustrates acceleration of the interfaces between the O-CU-CP and other O-RAN network functions and 443 
acceleration of the security algorithms within the O-CU-CP. Table 3-7 shows in more detail the accelerated functions. 444 
 445 
Figure 3-4. O-CU-CP interface and algorithm acceleration. 446 
 447 
Accelerated Function Interface Name Acceleration Type 
I/O between O-CU-CP and O-DU F1-C Interface acceleration (optional) 
I/O between O-CU-CP and SMO O1 Interface acceleration (optional) 
I/O between O-CU-CP and Near-RT RIC E2 Interface acceleration (optional) 
I/O between O-CU-CP and O-CU-UP E1 Interface acceleration (optional) 
I/O between O-CU-CP and O-CU-CP Xn Interface acceleration (optional) 
I/O between O-CU-CP and O-CU X2 Interface acceleration (optional) 
I/O between O-CU-CP and MME S1AP Interface acceleration (optional) 
I/O between O-CU-CP and AMF NG-C Interface acceleration (optional) 
Security algorithms N/A Algorithm acceleration 
Table 3-7. O-CU-CP acceleration types. 448 
Acceleration for the O-CU-UP 449 
Figure 3-5 illustrates acceleration of the interfaces between the O-CU-UP and other O-RAN network functions and 450 
acceleration of the security algorithms within the O-CU-UP. Table 3-8 shows in more detail the accelerated functions. 451 
 452 


                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
20 
 453 
Figure 3-5. O-CU-UP interface and algorithm acceleration. 454 
 455 
Accelerated Function Interface Name Acceleration Type 
I/O between O-CU-UP and O-DU F1-U Interface acceleration 
I/O between O-CU-UP and SMO O1 Interface acceleration (optional) 
I/O between O-CU-UP and Near-RT RIC E2 Interface acceleration (optional) 
I/O between O-CU-UP and O-CU-CP E1 Interface acceleration (optional) 
I/O between O-CU-UP and SAE-GW S1U Interface acceleration 
I/O between O-CU-UP and UPF NG-U Interface acceleration 
Security algorithms N/A Algorithm acceleration 
Table 3-8. O-CU-UP acceleration types. 456 
Hardware Acceleration Abstraction Layer Requirements  457 
The Acceleration Abstraction Layer (AAL) provides a set of APIs to VNF/CNF software  for offloading certain 458 
functions that, for instance , are compute- and power-intensive, to hardware accelerators supported on the O -Cloud 459 
platform. The objective of the AAL is to hide the different implementations of  hardware accelerators and present API s 460 
to VNF/CNF applications to enable portability and software -and-hardware decoupling. Full listing of the functions  that 461 
can be offloaded through the AAL is outside the scope of this document. 462 
Generic Requirements  463 
Extensibility  464 
O-RAN has defined the functions that can be accelerated by the cloud platform based on 3GPP s pecifications and O -465 
RAN deployment scenarios. However, the AAL API should not limit innovation of future implementations and s hould 466 
evolve as the specification requires . In this way , the API shall be exten sible to accommodate future revisions of the 467 
specification. 468 


                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
21 
Interrupt and Poll Mode 469 
The API shall allow multiple design choices for application vendors and shall not preclude a vendor from adopting an 470 
interrupt-driven design or poll -mode design or any combination of both. As such,  the API shall support both interrupt 471 
mode and poll mode for the data-path application interface.   472 
Discovery and Configuration 473 
The API shall support application  software to discover and configure the accelerator hardware. The API shall allow an 474 
application to discover what physical resources have been assigned to it from the upper layers and then to configure said 475 
resources for offload operations.  476 
Multiple Device Support  477 
There may be scenarios where multiple hardware accelerators (either implementing the same acceleration function or 478 
different) are assigned to a single application , which uses one or more of these accelerators as needed. The API shall 479 
support an application using one or more accelerator devices at the same time, as Figure 3-6 illustrates. 480 
 481 
Figure 3-6. AAL support for multiple devices. 482 
API offload requirements 483 
The API in supporting different implementations shall support different offload architectures including look- aside, 484 
inline, and any combination of both.  485 
Look-aside Acceleration Model  486 
The API shall support look-aside acceleration model where the host CPU invokes an accelerator for data processing and 487 
receives the result after processing is complete.  A look- aside architecture, illustrated in Figure 3-7, allows the 488 
application to offload work to a hardware accelerator and continue to perform other work in parallel—this could be to 489 
continue to execute other software  tasks in parallel or to sleep and wait for the accelerator hardware to complete. This 490 
model requires the API to support two operations , one for initiating the offload and another for retrievin g the operation 491 
once complete. 492 
 493 
Figure 3-7. AAL look-aside acceleration model. 494 


                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
22 
Inline Acceleration Model 495 
The API shall support inline acceleration model where acceleration by function and I /O-based acceleration are 496 
performed on the physical interface as the packet ingresses/egresses the platform. Figure 3-8 shows one possible 497 
implementation of an inline acceleration model. 498 
 499 
Figure 3-8. AAL inline acceleration model. 500 
In Figure 3-8, a “Func” block refers to some algorithmic function that is accelerated in the device before the data 501 
egresses or ingresses from the acceleration device. “Tx”  refers to the transmission of the  data from the acceleration 502 
device to an Ethernet interface, while ‘Rx’ refers to the reception of data from the Ethernet interface to the acceleration 503 
device.  504 
While the look-aside architecture shall  support dataflow from the CPU to the accelerator and back to the CPU before 505 
being sent to the front-haul interface, the inline architecture shall support data flow from the CPU to the accelerator and 506 
directly from the accelerator to the front-haul interface, instead of being sent back to the CPU. The typical data  flows 507 
for accelerating the O-DU high-PHY functions for the look-aside and inline architectures are as follows. 508 
Look-aside architecture dataflow 509 
CPU ↔ accelerator ↔ CPU ↔ front-haul: for a set of consecutive PHY functions offload (e.g., FEC) 510 
CPU ↔ accelerator ↔ CPU ↔ accelerator ↔ …… ↔ CPU ↔ front-haul: for a set of non-consecutive PHY functions 511 
offload 512 
Inline architecture dataflow 513 
CPU ↔ accelerator ↔ front-haul: for a set of consecutive PHY functions offload (up to the end of the PHY pipeline)  514 
Figure 3-9 illustrates one possible implementation of the look-aside and inline architectures. While a set of PHY -layer 515 
functions are offloaded to the accelerator hardware for look-aside acceleration, the entire end-to-end high PHY pipeline 516 
is offloaded to the accelerator for inline acceleration.  517 


                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
23 
L
2
+
P
H
Y
F
H
O-DU
L2+ on 
CPU
Func. 2 on 
accelerator
L2+ on 
CPU
 
RRU
RRU
Fronthaul 
interface
Func. (n-1) on 
accelerator
Look-aside 
model
Inline 
model
Func. 
1
Func.
3
Func. 
4
Func. 
n
AAL
L1/L2 
interface*
High PHY on CPU
High PHY on accelerator
Func. 
n-1
Func. 
2
Func. 
3
Func. 
1
Accelerator interface on CPU
Fronthaul 
interface
I/O data flow PHY downlink
PHY uplink
L1/L2 
interface*
Func. 
n
*e.g. FAPI
...
...
 518 
Figure 3-9. Dataflow paths in look-aside and inline acceleration architectures. 519 
API Concurrency and Parallelism   520 
To enable greater flexibility and design choice by application vendors, the API shall support multi- threading 521 
environment allowing an application to offload acceleration requests in parallel from several threads.  522 
Chapter 4 Cloud Platform Reference Design Example 523 
Based on the requirements, this chapter  specifies reference designs for the regional and edge clouds f or both the cloud 524 
platform hardware and software.  These reference designs are examples, not normative , and focus on sub- 6GHz 525 
scenarios. Support for mmWave and quanti fication of capacities of these reference designs (such as the number of cells, 526 
data rates, and so forth) will be further studied in the next release of this document. 527 
Cloud Platform Hardware Reference Design 528 
This section specifies the cloud platform hardware with a focus on server and accelerator hardware. These reference 529 
designs are examples, not normative. This document focuses on x86 processor-based cloud server design; it will address 530 
ARM processor-based designs in a future release. 531 
Regional Cloud Server 532 
Table 4-1 shows an example reference design for the regional cloud servers. 533 
Processor 2x 20-core CPUs at 2.4 GHz, e.g., 2nd-generation Intel Xeon Scalable Processor  
Memory 12x 32 GB, 2400 MHz TruDDR4 
Expansion Slots 6x PCIe 3.0  
Drive Bays 14x 3.5” or 24x 2.5” hot-swap bays or 24x NVMe bays 
RAID support RAID 1, 12 Gb/s 

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
24 
Network 
Interface 
4x 10GbE/25GbE ports, 1 dedicated 1GbE management port , 1x Fortville 40GbE Ethernet PCIe 
XL710-QDA2 Dual Port QSFP+ (PCI-E card direct to CPU 0) 
Network link 
aggregation 
Enabled  
Power 2x hot swap/redundant; support 220V/380V AC, 480V and -48V DC 
Security TPM or TCM 
Availability  Hot-swap fans/PSUs; 45℃ continuous operation; light path diagnostic LEDs; front -access 
diagnostics via dedicated USB port 
Manageability Support for IPMI, SNMP, and Redfish 
Table 4-1. Regional cloud server example reference design. 534 
Edge Cloud Server 535 
Table 4-2 shows an example reference design for the edge cloud servers with FPGA based acceleration. 536 
Form factors Depth ≤ 450mm, height ≤ 2RU, width ≤ 19in  
Processor 2nd-generation Intel Xeon Gold (e.g., 6210U) or Xeon D-2187NT (16-core) or Xeon D -2177NT 
(14-core); hyperthreading enabled for O-CU, disabled for O-DU 
Note: the NT models have Intel QAT integrated with the CPU and can perform PDCP crypto 
acceleration in the vO-CU. 
Memory 12x 32 GB, 2400 MHz TruDDR4 
Expansion Slots At least 2x half-height half-length (HHHL)  or 1x HHHL and 1x FHFL 
Drive Bays 6x hot -swappable 2.5 -inch SATA or SAS drives ; 2x NVMe drives; all drives accessible from  
front panel 
RAID support RAID1 
Network 
Interface 
4x 10GbE/25GbE ports, 1 dedicated 1GbE management port , 1x Fortville 40GbE Ethernet PCIe 
XL710-QDA2 Dual Port QSFP+ (PCI-E card direct to CPU 0), network cabling at front panel 
PSU • 1+1 redundant 
• Hot-swappable 
• Support for 220V AC or -48V DC 
• PSUs are  connected through the rear panel (AT&T, China Mobile) or front panel ( China 
Telecom, DT, KDDI, Orange) 
Temperature -5 °C ~ 55 °C (-10 °C to 60 °C preferred) 
Security TPM or TCM 
Availability  Hot-swap fans/PSUs; 45℃ continuous operation; light path diagnostic LEDs; front -access 
diagnostics via dedicated USB port 
Manageability Support for IPMI, SNMP, and Redfish 
Acceleration 
(look-aside,  
FEC profile) 
Intel® Programable Acceleration Card N3000, PCIe x16 Gen3, FHHL, I/O 2x25Gbe per 
QSFP28, FEC  Profile 15Gbps Uplink, 5Gbps Downlink, Up to 8 VFs.     
Table 4-2. Edge cloud server example reference design with FPGA acceleration. 537 
Table 4-3 shows an example reference design for the edge cloud servers with GPU based acceleration.  538 
Form factors Depth ≤ 450mm, height ≤ 2RU, width ≤ 19in 
Processor 12C x86 CPU complex or better (e.g. Xeon)  

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
25 
Memory 64 GB, 2400 MHz; or higher 
Expansion Slots At least 1x HHHL and 1x FHFL Dual Slot 
Drive Bays 6x hot -swappable 2.5 -inch SATA or SAS drives ; 2x NVMe drives; all drives accessible from 
front panel 
RAID support RAID1 
Network 
Interface 
2x 100/50/25/10 GbE ports, 1 dedicated 1GbE management port, 1x Mellanox ConnectX®-6 Dx 
(with crypto offload support, eCPRI offload with 5T for 5G technology) 
Efficient peer-to-peer support, either through root complex, or external PCIe switch (such as 
ExpressLane™ PEX 8747) recommended for high bandwidth use cases (like multi-carrier 
100MHz massive MIMO systems) 
PSU • 1+1 redundant 
• Hot-swappable 
• Support for 220V AC or -48V DC 
• PSUs are  connected through the rear panel (AT&T, China Mobile) or front panel (China 
Telecom, DT, KDDI, Orange) 
Temperature -5 °C ~ 55 °C (-10 °C to 60 °C preferred) 
Security TPM 
Availability  Hot-swap fans/PSUs; 45℃ continuous operation; light path diagnostic LEDs; front -access 
diagnostics via dedicated USB port 
Manageability Support for IPMI, SNMP, and Redfish 
Acceleration 
(inline, end-to-
end HIGH PHY 
profile) 
NVIDIA® T4, PCIe x16/x8 Gen3, HHHL (for smaller capacity, low band deployments). 
NVIDIA® Tesla® V100, PCIe x16 Gen3, FHFL (for higher capacity, to enable support for  multi-
sector massive MIMO (100 MHz and above) with advanced beam-forming techniques). 
Table 4-3. Edge cloud server example reference design with GPU acceleration.   539 
BIOS/UEFI Configurations 540 
Table 4-4 shows recommended settings for the various edge cloud deployment types.  541 
BIOS Setting Description Regional 
Cloud Settings 
for Maximum 
Performance 
(with Turbo 
mode enabled) 
Regional & 
Edge Cloud 
with Real 
Time  
Edge Cloud 
with Strict 
Real Time 
Configuration 
for O-DU 
System Power 
and Performance 
Policy  
Favor high performance over low power 
consumption for the entire system (e.g., CPU 
and memory). 
Optional Enabled  Enabled 
Intel Hyper-
Threading 
Technology (HT) 
Enabling HT doubles the number of (logical)  
CPU cores. But thread s on the same physical 
core share various core data  structures and 
caches, causing performance interference. It is 
recommended to enable HT to ensure high 
system throughput, but for applications such 
as O -DU in the edge cloud, careful ly pin 
latency-sensitive threads to dedicated cores to 
ensure real-time performance. 
Enabled  Optional   Optional  

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
26 
Prefetching Memory cache prefetching technologies allow  
prefetching data and instruction s into the 
cache before they are used. In general, 
prefetching reduces cache misses  and 
increases application performance. 
Enabled Enabled Enabled 
Intel 
virtualization 
support (VT-x) 
CPU hardware support for virtualization. Enabled Enabled Enabled 
Intel VT for 
direct I/O (VT-d) 
Support for PCI-E passthrough and SRIOV. Enabled Enabled Enabled 
Intel Turbo Boost 
Technology 
Enabling this feature allows individual CPU 
cores to run at frequencies higher than the 
base frequency when the processor has 
sufficient thermal headroom. Frequent 
transition between normal and turbo modes, 
however, can cause non -predictive 
performance and impact system real -time 
characteristics. 
Enabled  Disabled  Enabled  
Enhanced Intel 
SpeedStep 
Technology 
Enhanced SpeedStep is a series of dynamic 
frequency scaling techno logies built into 
modern Intel microprocessors that allow  the 
clock speed of the processor to be 
dynamically changed (to different P -states) by 
software. This allows the processor to meet 
instantaneous performance needs of the 
operation being performed, while minimizing 
power draw and heat generation. In general, 
disabling this feature  avoids 
CPU frequency 
changing dynamically and thus  facilitates 
deterministic performance. Enabling this but 
setting the configurable TDP level in the next 
setting allows finer control on the frequency 
that the CPU will run at. 
Enabled  Disabled Enabled 
Intel 
Configurable 
TDP 
Nominal Nominal Level 2 
Energy-efficient 
Turbo 
Transitions in and out of turbo mode incur 
extra power and overly frequent transitions are 
energy inefficient. This feature enables more 
energy-efficient turbo operation. 
Disabled Disabled  Disabled 
 
Hardware P-
States 
Disabling hardware-controlled P-states avoids 
unpredicted frequency change and helps 
achieve deterministic performance. 
Disabled C0/C1 state C0/C1 state 
Hardware PM 
Interrupt 
Disabled Disabled Disabled 
EPP Enable Disabled Disabled Disabled 
APS Rocketing Disabled Disabled Disabled 
Scalability  Disabled Disabled Disabled 
PP0-Budget Disabled Disabled Disabled 
CPU C State 
Control & C1E & 
Processor C6 
Disabling C -state transitions prevents CPU 
cores from entering sleep states when there is 
no task to execute. C -states that are often seen 
in BIOS settings include C1E, C3, and C6 
states. As C- state transitions introduce delay, 
disabling them helps real-time performance, 
Enabled  Disabled Disabled 

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
27 
though at the cost of higher power 
consumption. 
Direct Cache 
Access (DCA) & 
Data Direct I/O 
(DDIO) 
DCA can only reduce packet reception delay. 
Another technology, DDIO, reduces delay for 
both transmission and rec eption. It is 
recommended to enable DDIO instead of 
DCA when both settings are present. 
Enabled Enabled Enabled 
Uncore 
Frequency 
Scaling 
Allow the voltage and frequency of the uncore 
to be programmed independently. U ncore 
activities are monitored to optimize the 
frequency in real time. 
Enabled  Disabled Enabled  
 
 
Memory RAS 
and Performance 
Configuration & 
NUMA 
Optimized  
Enable ACPI tables that are required for 
NUMA-aware operating systems. 
 
Optional Optional Optional  
Table 4-4. BIOS configurations for strong real-time systems. 542 
Firmware Versions 543 
Table 4-5 shows firmware versions for the example reference design based on the Intel Server Board S2600WF family. 544 
BIOS/IFWI SE5C620.86B.00.01.0013.030920180427  
Baseboard Management Controller (BMC) 1.43.91f76955  
Download link  https://downloadcenter.intel.com/download/27632/Intel-
Server-Board-S2600WF-Family-BIOS-and-Firmware-
Update-Package-for-UEFI-?product=89005  
Table 4-5. Firmware for example reference design based on Intel server board S2600WF. 545 
Accelerator Hardware 546 
For deployment scenario B  [1], both the O -DU and O- CU are deployed as VNF(s)  or CNF(s)  on the edge c loud 547 
platform and the Near -RT RIC is deployed as a VNF  or CNF on the regional c loud platform. This section details the 548 
hardware acceleration options for these VNFs/CNFs that the cloud platforms shall provide.  549 
Regional Cloud Platform 550 
In deployment scenario B , the region al cloud platform provides  support for the Near -RT RIC. There is  currently no 551 
hardware acceleration options required for the Near-RT RIC at this stage of definition but this may evolve in future.  552 
Edge Cloud Platform  553 
The O-CU and O-DU may follow the software architecture as defined in the O-RAN WG8 specification [17]. 554 
O-CU Acceleration 555 
The O -CU, depending on performance r equirements, may choose to use hardware  acceleration (e.g., Intel QAT) for 556 
wireless cipher processing, namely 128-EEA3, 128-EIA3, UEA2 and UIA2—depending on configurations. 557 
O-DU Acceleration 558 
The O-DU is assumed to use the 7.2 split with the front -end processing (FFT, PRACH) implemented in the O -RU. The 559 
remaining O -DU functions are implemented as software  running on the edge cloud platform. Depending on the 560 
performance requirements , the compute -intense workloads may be accelerated in hardware using l ook-aside 561 
acceleration or inline acceleration or any combination of both. 562 

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
28 
Figure 4-1 shows a high- level architecture  of look-aside acceleration  with proposed Forward Error Correcting (FEC) 563 
functions to be hardware-accelerated for the O-DU in deployment scenario B.  564 
 
Figure 4-1. Hardware-accelerated FEC functions with look-aside model. 565 
Figure 4-2 shows an example of an inline solution for the O -DU that accelerates the front -end compression and 566 
decompression (e.g., Mu-Law and BFP) functions in accelerator hardware before sending to the O-RU.  567 
 568 
 569 
Figure 4-2. Hardware-accelerated front-end functions with inline model. 570 
Figure 4-3 shows a high- level architecture of inline acceleration with end -to-end high PHY functions to be hardware -571 
accelerated for deployment scenario B. 572 
CRC 
Generation + 
segmentation
LDPC 
Encoder
Rate 
Matching Scrambler Mod. 
Mapper Precoder RE 
Mapper RRU
CRC 
check
Downlink PDSCH pipeline
LDPC 
Decoder
De-Rate 
Matching
De-
Scrambler Demod. Channel 
Estimator
MIMO 
Equalizer
RE 
Demapper RRU
Uplink PUSCH pipeline
GPU
R
L
C
M
A
C
R
L
C
M
A
C
L1/L2 
interface
Layer 
Mapper
CPU573 
 574 
Figure 4-3. Hardware-accelerated end-to-end high-PHY functions with inline model. 575 
High-level Platform Architecture Examples  576 
Based on the hardware acceleration options above, Figure 4-4 to Figure 4-6 illustrate the system -level platform 577 
components required for the cloud platform to support  various acceleration architectures for  the O-DU and O- CU. A 578 
cloud platform can also support a combination of accelerator functions , such as inline fron t-end acceleration and FEC 579 
look-aside acceleration. Note that hardware acceleration for O -RAN network fu nctions is desired but a software-only 580 
implementation is also possible.  581 


                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
29 
Example 1: O-DU and O-CU look-aside acceleration  582 
Figure 4-4 shows an example cloud platform and its data  flows with look-aside acceleration for both the O -DU (FEC) 583 
and O-CU (wireless cipher).  584 
 
Figure 4-4. Edge cloud platform for look-aside acceleration of O-DU and O-CU. 
Example 2: O-DU inline and O-CU look-aside acceleration 
Figure 4-5 shows an example cloud platform with mixed acceleration models, where the O -DU uses the 
inline model for front-haul processing and the O-CU adopts look-aside acceleration for wireless ciphering. 
 
Figure 4-5. Edge cloud platform for O-DU inline acceleration and O-CU look-aside acceleration. 
Example 3: O-DU end-to-end high-PHY inline acceleration and O-CU look-aside acceleration.  585 
Figure 4-6 shows an example cloud platform, where the O-DU uses the inline model for the entire high-PHY processing 586 
and the O-CU adopts look-aside acceleration for wireless ciphering. 587 


                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
30 
 588 
Figure 4-6. Edge cloud platform for O-DU inline acceleration of end-to-end high-PHY. 589 
Cloud Platform Software Reference Design 590 
This section shows an example reference design for the regional and edge cloud platform software. A single installable 591 
package supporting various profiles of the platform software runs in both the regional and edge clouds, and can adjust 592 
to the different physical environments and scale requirements. This  document focuses on a container-based cloud 593 
platform design and will address VM- based designs in the next release. 594 
Kubernetes is a  common implementation for container orchestration and management . Platform infrastructure 595 
management and additional components , such as Kubernetes plugins, must be added for hosting accelerated O -DU and 596 
O-CU network functions. 597 
The regional and edge cloud platform requirements also differ based on the network functions t hey host. Generally, the 598 
edge cloud platform requirements are a superset of the functional requirements of the regional cloud platform. The 599 
primary differences relate to hardware acceleration features, real -time Linux, precision time protocol and server node 600 
scalability down to a single node for cell-site deployments. 601 
This reference design focuses on deployment scenario B, LLS-C3 clock synchronization, and FPGA and GPU hardware  602 
acceleration at the edge cloud. It defines two reference configuration profiles, for the regional cloud and edge cloud. 603 
Common Reference Configurations for all Profiles 604 
Table 4-6 lists the primary platform software components  that are common across the two cloud platform reference 605 
configuration profiles. A complete list of packages, version information and build recipes to create a cloud platform 606 
meeting the requirements of Section 0 for container deployment are available in O -RAN SC INF project release B 607 
(Bronze release). See [18] for further details. 608 
Platform 
Components 
Description 
Ansible Orchestration engine for automating configuration management and application deployment. 
 ansible.com 
Ceph Provides object storage, block storage and POSIX-compliant network file system. See ceph.io. 
Collectd Collects system and application performance metrics and provides mechanisms to store values. 
collectd.org 
Dhcp Assigns IP address and other network information to hosts. 
Dnsmasq Provides Domain Name System forwarder. 


                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
31 
en.wikipedia.org/wiki/Dnsmasq 
docker-distribution Docker registry for storing and distributing images. 
github.com/docker/distribution 
Drbd Replicated storage. 
Etcd Distributed key-value store. 
etcd.io 
Helm Kubernetes package manager. 
helm.sh 
helm-charts Tool for managing charts. Charts are packages of pre-configured Kubernetes resources. 
github.com/helm/charts 
ima Detects if files have been accidentally or maliciously altered, both remotely and locally . 
linux-ima.sourceforge.net 
iptables Configures the tables from the Linux kernel firewall and the chains and rules it sores. 
en.wikipedia.org/wiki/Iptables 
Kubernetes Automates deployment, scaling and management of containerized applications. 
kubernetes.io 
mariadb Relational databases. 
mariadb.org 
nginx High-performance HTTP server, reverse proxy and load balancer. 
pxe-network-
installer 
Provides PXE (preboot execution environment) install server. 
rabbit-mq Highly-available and scalable message broker. 
rabbitmq.com 
redfishtool Industry standard protocol providing a RESTful interface for the management of servers.  
tpm2 Provides trusted platform module support. 
Kubernetes 
Components 
Description 
etcd Distributed key-value store used as Kubernetes backing store for cluster data. 
etcd.io 
coredns Kubernetes cluster DNS service. 
armada Manages helm chart dependences. 
calico Enables ne tworking and network policy in K ubernetes clusters. Calico uses a pure IP 
networking fabric to provide high-performance networking. 
nginx-ingress-
controller 
Enables Kubernetes to configure nginx for load balancing Kubernetes based services 

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
32 
tiller Service that communicates with the Kubernetes API to manage helm packages . 
Table 4-6. Common cloud platform reference configurations for all profiles. 609 
Regional Cloud Reference Configuration Profile 610 
The regional edge cloud profile is a cluster -based implementation with two  redundant cloud platform controller nodes 611 
and worker node scal ability from one to 200 nodes. In addition to providing a cloud platform for the Near-RT RIC, the 612 
regional cloud platform provides zero-touch deployment and provisioning and management of the edge clouds.  613 
The SMO interfaces to the regional cloud platform for deployment of new edge clouds as well as updating and 614 
upgrading of the edge cloud platform software. The regional controller functionality is provided by the distributed cloud 615 
component. The regional clou d platform requires the common components in Table 4-6, as well as the components  in 616 
Table 4-7. 617 
Components Description 
distributed-cloud Provides centralized controller functions for management (e.g., lifecycle) of edge clouds. 
Table 4-7. Regional cloud reference configuration profile. 618 
Edge Cloud Reference Configuration Profile 619 
Based on the ha rdware accelerati on choice, the L inux kernel can be “standard” for in line GPU-based acceleration or 620 
“real-time” for look-aside FPGA-based acceleration. See Section 0 for the real-time Linux kernel configuration details. 621 
In addition, the edge cloud platform requires the common components in  Table 4-6, as well as the components in Table 622 
4-8. 623 
Platform 
Components 
Description 
distributed-cloud-
client 
Provides edge cloud client services for distributed cloud function. 
e1000e-kmod E1000e driver for Intel® Gigabit NIC  (optional – leveraged only if specific Intel hardware 
support is required). 
gdrcopy Provides GPU memory copy library (optional – leveraged only if specific Nvidia hardware 
support is required). 
i40e-kmod I40e driver for Intel® 40G NIC  (optional – leveraged only if specific Intel hardware support 
is required). 
iavf-kmod Driver for Intel® Ethernet adaptive virtual function  (optional – leveraged only if specific 
Intel hardware support is required). 
integrity-kmod Kernel module for IMA 
ixgbe-kmod Ixgbe driver for Intel® 82598 and 82599 based 10G NIC  (optional – leveraged only if 
specific Intel hardware support is required). 
ixgbevf-kmod Driver for Intel® Ethernet adaptive virtual function for 10 Gig NIC  (optional – leveraged 
only if specific Intel hardware support is required). 
libibverbs-31mlnx1-
ofed 
Allows user -space processes to use RDMA verbs directly  (optional – leveraged only if 
specific Mellanox hardware support is required). 
mlnx-ofa-kernel Driver for Mellanox NICs  (optional – leveraged only if specific Mellanox hardware support 
is required). 
opae-intel-fpga-
driver-kmod 
Open Programmable Acceleration Engine (OPAE) Kernel module for Intel FPGA . 

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
33 
qat17 Driver for Intel® QuickAssist Technology (optional – leveraged on ly if specific Intel 
hardware support is required). 
rdma-core-45mlnx1 Driver for Mellanox RDMA -based interconnect (optional – leveraged only if specific 
Mellanox hardware support is required) 
Linuxptp Provides Precision Time Protocol according to IEEE standard 1588 and telecom profiles 
G.8265.1, G.8275.1 and G.8275.2 
linuxptp.sourceforge.net 
Kubernetes 
Plugin/Operators 
Description 
Multus Multus is a Multi CNI plugin to support the Multi Networking feature in Kubernetes using 
CRD-based network objects in Kubernetes.  The Multus plugin should be used on the master 
on all nodes to enable the use of multiple network interfaces inside a K8 pod.  
github.com/intel/multus-cni. 
FPGA FPGA Device plugin for FPGA passthrough  (optional – leveraged only if specific Intel 
hardware support is required). 
github.com/intel/intel-device-plugins-for-kubernetes#fpga-device-plugin 
QAT QAT Device plugin for Intel® QAT adapters  (optional – leveraged only if specific  Intel 
hardware support is required). 
github.com/intel/intel-device-plugins-for-kubernetes/blob/master/cmd/qat_plugin/ 
SR-IOV The SR -IOV plug in is required to enable the assignment of VFs to containers. SR -IOV 
provides a low-latency interface for both packet I/O and acceleration interfaces. 
github.com/hustcat/sriov-cni 
CPU Manager The CPU manager for Kubernetes can be used to pr ovide basic core affinity for VNFs and 
CNFs. It is required to pin real - time threads to specific CPU cores in order to improve 
performance and meet real-time latency.  
github.com/intel/CPU-Manager-for-Kubernetes. 
Node Feature 
Discovery 
The Node Feature D iscovery plugin enables discovering server hardware  node features for 
Kubernetes. It detects hardware features available on each node in a Kubernetes cluster, and 
advertises those features using node labels.  
github.com/kubernetes-sigs/node-feature-discovery 
Node Topology 
Manager 
Node Topology Manager can be used to support NUMA awareness and pinning via the 
scheduler. 
kubernetes.io/docs/tasks/administer-cluster/topology-manager/ 
GPU Operator The GPU operator manages NVIDIA GPU and automates tasks related to boostrapping GPU 
enabled nodes (optional – leveraged only if specific Nvidia hardware support is required). 
nvidia.github.io/gpu-operator/ 
Table 4-8 Edge Cloud Reference Configuration Profile 624 
Example Real-time Linux Configurations 625 
Table 4-9 shows the reference configurations for the real-time Linux OS. 626 
Linux Kernel  3.10.0-957.21.3.rt56.935.el7.x86_64 

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
34 
Host or Guest Linux 
Kernel Parameters 
rcu_nocbs=1-N rcu_nocb_poll 
isolcpus=1-N irqaffinity=0 tsc=perfect 
nohz_full=1-N idle=poll noswap 
N is the total number of logical CPU cores. 
Use the “tuna” tool, provided in some Linux 
distributions, to configure run-time CPU 
isolation and thread  priorities, if permanent 
isolation with isolcpus kernel option  is not 
desired. 
selinux=0 enforcing=0 Disable SELinux policies can help improve 
latency but should only be done if the security 
considerations are fully understood.  
default_hugepagesz=1G hugepagesz=1G 
hugepages=21 
Use huge pages to minimize TLB miss 
overhead; use 2MB huge pages when system 
memory is limited. 
usbcore.autosuspend=-1 
nmi_watchdog=0 softlockup_panic=0 
cgroup_disable=memory skew_tick=1 
nosoftlockup 
General configurations for real time.  
kthread_cpus=0 Limit host kernel threads to the OS core. Used 
also in guest assumin g a pure user space 
application design.  
intel_pstate=disable 
intel_idle.max_cstate=0 
processor.max_cstate=1 
Disabling P-states and limiting C-states in the 
kernel helps improve real time performance.  
Host Linux GRUB 
Parameters 
intel_iommu=on iommu=pt 
pci=pcie_bus_perf 
Enable SR-IOV and optimal PCI performance. 
Host and Guest 
Tuning 
Turn off graphical desktop and 
unnecessary daemons in the system 
 
If ext3 or ext4 file system is used, 
consider using noatime in fstab 
 
Host Kernel 
Command Line 
example  
BOOT_IMAGE=/vmlinuz-3.10.0-957.21.3.rt56.935.el7.x86_64 root=/dev/mapper/centos -
root ro crashkernel=auto nomodeset rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet 
hugepagesz=1G hugepages=35 default_hugepagesz=1G int
el_iommu=on iommu=pt 
LANG=en_GB.UTF-8 tboot=false biosdevname=0 usbcore.autosuspend= -1 selinux=0 
enforcing=0 nmi_watchdog=0 softlockup_panic=0 cgroup_disable= memory skew_tick=1 
isolcpus=1-N rcu_nocbs=1 -N kthread_cpus=0 irqaffinity=0 nohz_full=1- N nopti clo ck=pit 
no_timer_check pci=pcie_bus_perf clocksource=tsc tsc=perfect idle=poll 
intel_pstate=disable intel_idle.max_cstate=0 processor.max_cstate=1 nohz=on mce=off 
nosoftlockup 
Table 4-9. Real-time Linux configurations. 627 
  628 

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
35 
Annex ZZZ: O-RAN Adopter License Agreement 629 
BY DOWNLOADING, USING OR OTHERWISE ACCESSING ANY O- RAN SPECIFICATION, ADOPTER 630 
AGREES TO THE TERMS OF THIS AGREEMENT. 631 
This O -RAN Adopter License Agreement (the “Agreement”) is made by and between the O -RAN Alliance and the 632 
entity that downloads, uses or otherwise accesses any O-RAN Specification, including its Affiliates (the “Adopter”). 633 
This is a license agreement for entities who wish to adopt any O-RAN Specification. 634 
Section 1: DEFINITIONS 635 
1.1 “Affiliate” means an entity that directly or indirectly controls, is controlled by, or is under common control with 636 
another entity, so long as such control exists. For the purpose of this Section, “Control” means beneficial ownership of 637 
fifty (50%) percent or more of the voting stock or equity in an entity. 638 
1.2 “Compliant Implementation” means any system, device, method or operation (whether implemented in hardware, 639 
software or combinations thereof) that fully conforms to a Final Specification. 640 
1.3 “Adopter(s)” means all entities, who are not Members, Contributors or Academic Contributors, including their 641 
Affiliates, who wish to download, use or otherwise access O-RAN Specifications. 642 
1.4 “Minor Update” means an update or revision to an O -RAN Specification published by O -RAN Alliance that does 643 
not add any significant new features or functionality and remains interoperable with the prior version of an O -RAN 644 
Specification. The term “O-RAN Specifications” includes Minor Updates. 645 
1.5 “Necessary Claims” means those claims of all present and future patents and patent applications, other than design 646 
patents and design registrations, throughout the world, which (i) are owned or otherwise licensable by a Member, 647 
Contributor or Academic Contributor during the term of its Member, Contributor or Academic Contributorship; (ii) 648 
such Member, Contributor or Academic Contributor has the right to grant a license without the payment of 649 
consideration to a third party; and (iii) are necessarily infringed by a Complia nt Implementation (without considering 650 
any Contributions not included in the Final Specification). A claim is necessarily infringed only when it is not possible 651 
on technical (but not commercial) grounds, taking into account normal technical practice and the state of the art 652 
generally available at the date any Final Specification was published by the O -RAN Alliance or the date the patent 653 
claim first came into existence, whichever last occurred, to make, sell, lease, otherwise dispose of, repair, use or operate 654 
a Compliant Implementation without infringing that claim. For the avoidance of doubt in exceptional cases where a 655 
Final Specification can only be implemented by technical solutions, all of which infringe patent claims, all such patent 656 
claims shall be considered Necessary Claims. 657 
1.6 “Defensive Suspension” means for the purposes of any license grant pursuant to Section 3, Member, Contributor, 658 
Academic Contributor, Adopter, or any of their Affiliates, may have the discretion to include in their license a t erm 659 
allowing the licensor to suspend the license against a licensee who brings a patent infringement suit against the 660 
licensing Member, Contributor, Academic Contributor, Adopter, or any of their Affiliates.  661 
Section 2: COPYRIGHT LICENSE 662 
2.1 Subject to the terms and conditions of this Agreement, O -RAN Alliance hereby grants to Adopter a nonexclusive, 663 
nontransferable, irrevocable, non -sublicensable, worldwide copyright license to obtain, use and modify O -RAN 664 
Specifications, but not to further distribute such O-RAN Specification in any modified or unmodified way, solely in 665 
furtherance of implementations of an ORAN 666 
Specification. 667 
2.2 Adopter shall not use O -RAN Specifications except as expressly set forth in this Agreement or in a separate written 668 
agreement with O-RAN Alliance. 669 
Section 3: FRAND LICENSE 670 
3.1 Members, Contributors and Academic Contributors and their Affiliates are prepared to grant based on a separate 671 
Patent License Agreement to each Adopter under Fair Reasonable And Non - Discriminatory (FRAND) term s and 672 
conditions with or without compensation (royalties) a nonexclusive, non- transferable, irrevocable (but subject to 673 
Defensive Suspension), non-sublicensable, worldwide patent license under their Necessary Claims to make, have made, 674 
use, import, offer t o sell, lease, sell and otherwise distribute Compliant Implementations; provided, however, that such 675 
license shall not extend: (a) to any part or function of a product in which a Compliant Implementation is incorporated 676 

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
36 
that is not itself part of the Compliant Implementation; or (b) to any Adopter if that Adopter is not making a reciprocal 677 
grant to Members, Contributors and Academic Contributors, as set forth in Section 3.3. For the avoidance of doubt, the 678 
foregoing licensing commitment includes the distribution by the Adopter’s distributors and the use by the Adopter’s 679 
customers of such licensed Compliant Implementations. 680 
3.2 Notwithstanding the above, if any Member, Contributor or Academic Contributor, Adopter or their Affiliates has 681 
reserved the right to charge a FRAND royalty or other fee for its license of Necessary Claims to Adopter, then Adopter 682 
is entitled to charge a FRAND royalty or other fee to such Member, Contributor or Academic Contributor, Adopter and 683 
its Affiliates for its license of Necessary Claims to its licensees. 684 
3.3 Adopter, on behalf of itself and its Affiliates, shall be prepared to grant based on a separate Patent License 685 
Agreement to each Members, Contributors, Academic Contributors, Adopters and their Affiliates under Fair 686 
Reasonable And Non -Discriminatory (FRAND) terms and conditions with or without compensation (royalties) a 687 
nonexclusive, non-transferable, irrevocable (but subject to Defensive Suspension), non- sublicensable, worldwide patent 688 
license under their Necessary Claims to m ake, have made, use, import, offer to sell, lease, sell and otherwise distribute 689 
Compliant Implementations; provided, however, that such license will not extend: (a) to any part or function of a 690 
product in which a Compliant Implementation is incorporated t hat is not itself part of the Compliant Implementation; or 691 
(b) to any Members, Contributors, Academic Contributors, Adopters and their Affiliates that is not making a reciprocal 692 
grant to Adopter, as set forth in Section 3.1. For the avoidance of doubt, the  foregoing licensing commitment includes 693 
the distribution by the Members’, Contributors’, Academic Contributors’, Adopters’ and their Affiliates’ distributors 694 
and the use by the Members’, Contributors’, Academic Contributors’, Adopters’ and their Affiliate s’ customers of such 695 
licensed Compliant Implementations. 696 
Section 4: TERM AND TERMINATION 697 
4.1 This Agreement shall remain in force, unless early terminated according to this Section 4.  698 
4.2 O-RAN Alliance on behalf of its Members, Contributors and Academic C ontributors may terminate this Agreement 699 
if Adopter materially breaches this Agreement and does not cure or is not capable of curing such breach within thirty 700 
(30) days after being given notice specifying the breach. 701 
4.3 Sections 1, 3, 5 - 11 of this Agreement shall survive any termination of this Agreement. Under surviving Section 3, 702 
after termination of this Agreement, Adopter will continue to grant licenses (a) to entities who become Adopters after 703 
the date of termination; and (b) for future versions of ORAN Specifications that are backwards compatible with the 704 
version that was current as of the date of termination. 705 
Section 5: CONFIDENTIALITY 706 
Adopter will use the same care and discretion to avoid disclosure, publication, and dissemination of O -RAN 707 
Specifications to third parties, as Adopter employs with its own confidential information, but no less than reasonable 708 
care. Any disclosure by Adopter to its Affiliates, contractors and consultants should be subject to an obligation of 709 
confidentiality at least as  restrictive as those contained in this Section. The foregoing obligation shall not apply to any 710 
information which is: (1) rightfully known by Adopter without any limitation on use or disclosure prior to disclosure; 711 
(2) publicly available through no fault of Adopter; (3) rightfully received without a duty of confidentiality; (4) disclosed 712 
by O -RAN Alliance or a Member, Contributor or Academic Contributor to a third party without a duty of 713 
confidentiality on such third party; (5) independently developed by A dopter; (6) disclosed pursuant to the order of a 714 
court or other authorized governmental body, or as required by law, provided that Adopter provides reasonable prior 715 
written notice to O -RAN Alliance, and cooperates with O -RAN Alliance and/or the applicable Member, Contributor or 716 
Academic Contributor to have the opportunity to oppose any such order; or (7) disclosed by Adopter with O -RAN 717 
Alliance’s prior written approval. 718 
Section 6: INDEMNIFICATION 719 
Adopter shall indemnify, defend, and hold harmless the O -RAN Alliance, its Members, Contributors or Academic 720 
Contributors, and their employees, and agents and their respective successors, heirs and assigns (the “Indemnitees”), 721 
against any liability, damage, loss, or expense (including reasonable attorneys’ fees and expenses) incurred by or 722 
imposed upon any of the Indemnitees in connection with any claims, suits, investigations, actions, demands or 723 
judgments arising out of Adopter’s use of the licensed O -RAN Specifications or Adopter’s commercialization of 724 
products that comply with O-RAN Specifications. 725 

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.01 TS  
          
________________________________________________________________________________________________ 
Copyright © 2020 by the O-RAN Alliance. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           
37 
Section 7: LIMITATIONS ON LIABILITY; NO WARRANTY 726 
EXCEPT FOR BREACH OF CONFIDENTIALITY, ADOPTER’S BREACH OF SECTION 3, AND ADOPTER’S 727 
INDEMNIFICATION OBLIGATIONS, IN NO EVENT SHALL ANY PARTY BE LIABLE TO ANY OTHER 728 
PARTY OR THIRD PARTY FOR ANY INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE OR CONSEQUENTIAL 729 
DAMAGES RESULTING FROM ITS PERFORMANCE OR NON- PERFORMANCE UNDER THIS AGREEMENT, 730 
IN EACH CASE WHETHER UNDER CONTRACT, TORT, WARRANTY, OR OTHERWISE, AND WHETHER OR 731 
NOT SUCH PARTY HAD ADVANCE NOTICE OF THE POSSIBILITY OF SUCH DAMAGES. O- RAN 732 
SPECIFICATIONS ARE PROVIDED “AS IS” WITH NO WARRANTIES OR CONDITIONS WHATSOEVER, 733 
WHETHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE. THE O -RAN ALLIANCE AND THE 734 
MEMBERS, CONTRIBUTORS OR ACADEMIC CO NTRIBUTORS EXPRESSLY DISCLAIM ANY WARRANTY 735 
OR CONDITION OF MERCHANTABILITY, SECURITY, SATISFACTORY QUALITY, NONINFRINGEMENT, 736 
FITNESS FOR ANY PARTICULAR PURPOSE, ERROR -FREE OPERATION, OR ANY WARRANTY OR 737 
CONDITION FOR O-RAN SPECIFICATIONS. 738 
Section 8: ASSIGNMENT 739 
Adopter may not assign the Agreement or any of its rights or obligations under this Agreement or make any grants or 740 
other sublicenses to this Agreement, except as expressly authorized hereunder, without having first received the prior, 741 
written consent of the O -RAN Alliance, which consent may be withheld in O -RAN Alliance’s sole discretion. O -RAN 742 
Alliance may freely assign this Agreement. 743 
Section 9: THIRD-PARTY BENEFICIARY RIGHTS 744 
Adopter acknowledges and agrees that Members, Contributors and Academic Contributors (including future Members, 745 
Contributors and Academic Contributors) are entitled to rights as a third- party beneficiary under this Agreement, 746 
including as licensees under Section 3. 747 
Section 10: BINDING ON AFFILIATES 748 
Execution of this Agree ment by Adopter in its capacity as a legal entity or association constitutes that legal entity’s or 749 
association’s agreement that its Affiliates are likewise bound to the obligations that are applicable to Adopter hereunder 750 
and are also entitled to the benefits of the rights of Adopter hereunder. 751 
Section 11: GENERAL 752 
This Agreement is governed by the laws of Germany without regard to its conflict or choice of law provisions.  753 
This Agreement constitutes the entire agreement between the parties as to its expres s subject matter and expressly 754 
supersedes and replaces any prior or contemporaneous agreements between the parties, whether written or oral, relating 755 
to the subject matter of this Agreement.  756 
Adopter, on behalf of itself and its Affiliates, agrees to compl y at all times with all applicable laws, rules and 757 
regulations with respect to its and its Affiliates’ performance under this Agreement, including without limitation, export 758 
control and antitrust laws. Without limiting the generality of the foregoing, Adopter acknowledges that this Agreement 759 
prohibits any communication that would violate the antitrust laws. 760 
By execution hereof, no form of any partnership, joint venture or other special relationship is created between Adopter, 761 
or O -RAN Alliance or its Member s, Contributors or Academic Contributors. Except as expressly set forth in this 762 
Agreement, no party is authorized to make any commitment on behalf of Adopter, or O -RAN Alliance or its Members, 763 
Contributors or Academic Contributors. 764 
In the event that any pr ovision of this Agreement conflicts with governing law or if any provision is held to be null, 765 
void or otherwise ineffective or invalid by a court of competent jurisdiction, (i) such provisions will be deemed stricken 766 
from the contract, and (ii) the remain ing terms, provisions, covenants and restrictions of this Agreement will remain in 767 
full force and effect. 768 
Any failure by a party or third party beneficiary to insist upon or enforce performance by another party of any of the 769 
provisions of this Agreement or  to exercise any rights or remedies under this  Agreement or otherwise by law shall not 770 
be construed as a waiver or relinquishment to any extent of the other parties’ or third party beneficiary’s right to assert 771 
or rely upon any such provision, right or remedy in that or any other instance; rather the same shall be and remain in full 772 
force and effect. 773 