Copyright © 2022 by the O-RAN ALLIANCE e.V.
The copying or incorporation into any other work of part or all of the material available in this specification in any form without the prior written permission of O-RAN ALLIANCE e.V.  is prohibited, save that you may print or download extracts of the material of this specification for your personal use, or copy the material of this specification for the purpose of sending to individual third parties for their information provided that you acknowledge O-RAN ALLIANCE as the source of the material and that you inform the third party that these conditions apply to them and that they must comply with them.
O-RAN ALLIANCE e.V., Buschkauler Weg 27, 53347 Alfter, Germany
Register of Associations, Bonn VR 11238, VAT ID DE321720189
Contents
Introduction
Scope
This Technical Specification has been produced by the O-RAN.org.
The contents of the present document are subject to continuing work within O-RAN WG6 and may change following formal O-RAN approval. Should the O-RAN.org modify the contents of the present document, it will be re-released by O-RAN Alliance with an identifying change of release date and an increase in version number as follows:
Release x.y.z
where:
x	the first digit is incremented for all changes of substance, i.e., technical enhancements, corrections, updates, etc. (the initial approved document will have x=01).
y	the second digit is incremented when editorial only changes have been incorporated in the document.
z	the third digit included only in working versions of the document indicating incremental changes during the editing process.
This document defines O-RAN O-Cloud DMS interface functions and protocols for the O-RAN O2 interface. The document studies the functions conveyed over the interface, including management functions, procedures, operations, and corresponding solutions, and identifies existing standards and industry work that can serve as a basis for O-RAN work.
References
The following documents contain provisions which, through reference in this text, constitute provisions of this specification (see also https://www.o-ran.org/specifications).
3GPP TR 21.905, Vocabulary for 3GPP Specifications
O-RAN WG6, Orchestration Use Case and Requirements for O-RAN Virtualized RAN
O-RAN WG6, Cloud Architecture and Deployment Scenarios for O-RAN Virtualized RAN
O-RAN WG6, O-RAN O2 General Aspects and Principles Specification
Kubernetes® API, online https://kubernetes.io/docs/reference/kubernetes-api/
Kubernetes® API Conventions, online https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md
NOTE:	Kubernetes® and K8s® are registered trademarks of the Linux Foundation, in the United States and other countries. O-RAN is not affiliated with, endorsed, or sponsored by the Linux Foundation.
Definitions and Abbreviations
Definitions
Abbreviations
For the purposes of the present document, the abbreviations given in 3GPP TR 21.905 [1] and the following apply. An abbreviation defined in the present document takes precedence over the definition of the same abbreviation, if any, in 3GPP TR 21.905 [1].
DMS	Deployment Management Services
API	Application Programming Interface
SMO	Service Management and Orchestration
NFO	Network Function Orchestration
O2dms Service Overview
container-based NF and other workloadsOverview
This document deals with the containerized workload lifecycle management services offered over the O2dms interface using the O2dms profile for Kubernetes. The relationship with O2ims services is also described whenever applicable (e.g., provisioning of Kubernetes labels in the O-Cloud Kubernetes cluster by SMO to be able to match them against the O-RAN CNF deployment requirements).
This profile exposes the full set of capabilities of Kubernetes native “API Server” function to the SMO and enables the reuse of Kubernetes native API procedures and data model (Kubernetes resource manifests). As depicted in Figure 2.1-1, the O2dms profile for Kubernetes exposes one O2dms interface instance per one Kubernetes cluster to the SMO and the interface is terminated by the API Server component of Kubernetes native control plane. The address and access credentials required for communication with the Kubernetes API Server component are assumed to be available to the SMO (for example via O2-IMS, from the Kubernetes cluster instantiation time).
Figure 2.1-1
Supported Use Cases
The list of orchestration use-cases captured in [2] needs to be supported using O2dms capabilities for containerized workloads (NFs and other workloads). This version of the O2dms profile for Kubernetes supports the following use cases aligned with [2] based on Kubernetes native APIs and resource objects.
Instantiation of containerized workloads on Kubernetes cluster: This O2dms capability enables the SMO to instantiate a containerized workload on a target Kubernetes cluster using the Kubernetes native resource manifests (Deployments, StatefulSets, ConfigMaps, etc.). The complete set of Kubernetes native resource objects may be used for workload instantiation including network, compute, storage, and configuration related objects.
Termination of containerized workloads on Kubernetes cluster: This O2dms capability enables the SMO to terminate a containerized workload on a target Kubernetes cluster by deleting all the Kubernetes native resource manifests related to that workload from the target Kubernetes cluster.
NF Deployment level healing: This O2dms capability enables the SMO to recover an NF deployment from failures on a Kubernetes cluster. The SMO may trigger the NF deployment recovery by creating, modifying, and/or deleting K8s native resource manifests belonging to that NF deployment.
Note: Support for other workload LCM use cases is FFS.
O2dms Service Definitions
General
This section provides details of the O2dms interface capabilities for containerized workload management exposed to the SMO using the O2dms Kubernetes profile. The O2dms Kubernetes profile exposes a REST based interface to the SMO from the API Server function of Kubernetes native control plane. The protocol stack used by O2dms Kubernetes profile for secure communication between the SMO and the target Kubernetes cluster is depicted in Figure 3.1-1. This protocol stack enables a secure transfer of the Kubernetes native API objects from the SMO to the target Kubernetes cluster.
Figure 3.1-1: Illustration of the protocol stack used by O2dms Kubernetes profile
The O2dms Services Supported
In this specification, the Kubernetes native APIs provide all the required capabilities to the SMO for containerized workload lifecycle management in a Kubernetes cluster. The Kubernetes native APIs are exposed by the API Server function of the Kubernetes native control plane and is the common denominator for the SMO to manage containerized workloads on Kubernetes based O-Clouds.
The Kubernetes API Sever exposes a REST based interface, offering CRUD (Create, Read, Update, Delete) operations using Kubernetes native resource manifests. Aligned with this REST interface, the ORAN WG6 use cases captured in [1] for lifecycle management of containerized workload are realized by using the API Server’s supported operations (CRUD) and data model (Kubernetes native resource manifests).
Instantiate NF Deployment
The instantiation use case requires the SMO to start the execution of a containerized workload on a Kubernetes cluster that meets the execution requirements of that workload. In this O2dms profile, the instantiation use case is realized by secured transfer of the Kubernetes native resource objects from the SMO to the API Server function in the target Kubernetes cluster. A containerized network function usually consists of several Kubernetes resource object manifests for different types of resources and a secured transfer of all these manifests is required for successful compeletion of the instantiation use case.
Figure 3.1.2-1 exemplifies the instantiation flow in O2dms Kubernetes profile aligned with the NF instantiation use case in [1].
@startuml
Autonumber
skinparam sequenceArrowThickness 1
skinparam ParticipantPadding 5
skinparam BoxPadding 10
participant User
Box "SMO" #gold
Participant NFO as NFO
end box
Box "Kubernetes® based O-Cloud" #lightseagreen
participant apiserver as "K8s® API Server"
participant worker as "K8s® worker"
End box
Note over NFO, apiserver
PRECONDITIONS:
SMO is active and running normally
K8s® cluster is active and running normally
SMO and K8s® API Server connectivity is established
End Note
== Use case: Instantiate containerized workload on K8s® cluster ==
User -> NFO : Instantiate CNF (Id)
Note over NFO
SMO gets the deployment artifacts for
containerized workload (e.g., ASD, Helm Charts)
from a repository
end Note
Note over NFO
If unspecified, the SMO performs homing to select
a suitable K8s® cluster and adds information in
the inventory for the workload being deployed
end Note
Note over NFO
SMO generates parameterized K8s® manifests
for the containerized workload
end Note
loop Until all K8S resource manifests are sent to K8s® cluster
NFO -> apiserver : **[O2dms]** <<HTTP POST>> Create K8s® resource object
apiserver -> apiserver: Create K8s® resource
apiserver -> NFO : **[O2dms]** <<HTTP 201>> CREATED
end
Note over apiserver
K8s® API Server notifies the worker
node(s) to start workload execution
end note
Note over worker
K8s® worker fetches the container
images and starts their execution
end note
note over NFO
SMO acquires the workload instantiation status
via O2dms
end note
NFO --> User : Outcome notification
== End Instantiate containerized workload on K8s® Cluster ==
@enduml
Figure 3.1.2-1: NF Deployment Instantiation
Table 3.1.2-1: Instantiation Use Case
Terminate NF Deployment
The termination use case requires the SMO to stop the execution of a containerized workload on a Kubernetes cluster and release all resources that are being used by that workload. In this O2dms profile, the termination use case is realized by deleting the Kubernetes native resource objects from the Kubernetes cluster that is running the workload. Figure 3.1.3-1 exemplifies the termination use case flow in O2dms Kubernetes profile aligned with the NF termination use case in [1].
@startuml
Autonumber
skinparam sequenceArrowThickness 1
skinparam ParticipantPadding 5
skinparam BoxPadding 10
participant User
Box "SMO" #gold
Participant NFO as NFO
end box
Box "Kubernetes® based O-Cloud" #lightseagreen
participant apiserver as "K8s® API Server"
participant worker as "K8s® worker"
End box
Note over NFO, apiserver
PRECONDITIONS:
SMO is active and running normally
K8s® cluster is active and running normally
SMO and K8s® API Server connectivity is established
End Note
== Use Case: Terminate containerized workload on K8s® cluster ==
User -> NFO : Terminate CNF (ID)
Note over NFO
The SMO determines the K8s® cluster
running the containerized workload
The SMO determines the K8s® resources
installed in the cluster for that workload
end note
loop Until all K8s® resources are deleted from the cluster
NFO -> apiserver : **[O2dms]** <<HTTP DELETE>> Delete K8s® resource object
apiserver -> apiserver: Delete K8s® resource
apiserver -> NFO : **[O2dms]** <<HTTP 200>> OK
end
Note over apiserver
K8s® API Server notifies the worker
node(s) to terminate workload execution
end note
Note over worker
K8s® worker stops the container
process and releases the resources
end note
note over NFO
SMO acquires the workload termination status
via O2dms
end note
NFO --> User : Outcome notification
== End Terminate containerized workload on K8s® cluster ==
@enduml
Figure 3.1.3-1: NF Deployment Termination
Table 3.1.3-1: Termination Use Case
Heal NF Deployment
Recovery of an NF deployment from failures is documented in [2] clause 3.6.2, which describes an NF deployment level healing use case based on either auto-healing support in the O-Cloud platform or SMO triggered healing over O2dms interface. Based on these healing triggers, the use case describes corrective actions that can be taken for NF deployment healing in the O-Cloud such as restarting, replacing and/or reallocating O-Cloud resources belonging to the NF deployment. This section describes the NF deployment level healing use case for healing containerized workloads on a K8s cluster using O2dms K8s profile.
Auto Healing of NF Deployment in a Kubernetes cluster
Auto-healing of NF Deployment refers to the intrinsic capabilities of the O-Cloud platform to monitor the workloads and initiate corrective actions automatically when runtime errors/faults are detected by the O-Cloud platform. The Kubernetes platform provides auto-healing features based on continuous monitoring of the running containerized workloads (e.g., Pods) using liveness checks at the application layer. A workload designed for Kubernetes is expected to pass these liveness checks to be considered as healthy workload running in the K8s cluster. If the liveness checks fail for a workload, the Kubernetes control plane automatically replaces the failing workload with a new copy.
When the restart and replacement mechanics of Kubernetes auto-healing process are not sufficient for the full recovery of the NF deployment, the SMO may trigger NF deployment healing using O2dms interface.
SMO triggered Healing of NF Deployment in a Kubernetes cluster
Based on monitoring of the NF workloads using O1 and/or O2 interface, the SMO may detect the need and decide to heal an NF Deployment using O2dms interface. This SMO triggered healing over O2dms may occur independently from any auto-healing features running in the O-Cloud platform for healing NF workloads. In the O2dms K8s profile, the SMO triggered NF deployment healing is realized by creating, modifying and/or deleting K8s native resource objects belonging to the NF deployment facing failures. Figure 3.1.4-1 exemplifies the SMO triggered healing of NF Deployment using the O2dms K8s profile.
@startuml
Autonumber
Box "SMO" #gold
Participant NFO as NFO
end box
Box "Kubernetes based O-Cloud" #lightseagreen
participant apiserver as "K8s API Server"
End box
== Use case: SMO triggered NF Deployment Healing on a K8s® cluster ==
Note over NFO
The SMO actively monitors the NF deployments
for failure detection using O1/O2 interface
end Note
NFO -> NFO: SMO detects or receives a NF healing trigger
Note over NFO
SMO identifies the resources to be healed
and generates updated K8s resource manifests
end Note
group opt [K8s resources to be created]
loop Until all required K8s resources are created
NFO -> apiserver : **[O2dms]** <<HTTP POST>> Create K8s resource object
apiserver -> apiserver: Create K8s resource
apiserver -> NFO : **[O2dms]** <<HTTP 201>> CREATED
end
end
group opt [K8s resources to be modified]
loop Until all required K8s resources are updated
NFO -> apiserver : **[O2dms]** <<HTTP PUT/PATCH>> Update K8s resource object
apiserver -> apiserver: Update K8s resource
apiserver -> NFO : **[O2dms]** <<HTTP 200>> OK
end
end
group opt [K8s resources to be deleted]
loop Until all required K8s resource are deleted
NFO -> apiserver : **[O2dms]** <<HTTP DELETE>> Delete K8s resource object
apiserver -> apiserver: Delete K8s resource
apiserver -> NFO : **[O2dms]** <<HTTP 200>> OK
end
end
Note over apiserver
K8s API Server notifies the worker nodes to process the
updated resources and heal the NF deployment workload
end note
note over NFO
SMO acquires the workload status via O2dms
end note
== End: SMO triggered NF Deployment Healing on a K8s® Cluster ==
@enduml
Figure 3.1.4-1 NF Deployment Healing
Table - NF Deployment Healing Use Case
O2dms Service Definitions for Kubernetes
General
The O2dms profile for Kubernetes is based on the usage of Kubernetes native APIs and data model as O2dms interface. Kubernetes offers a native REST based, secured interface with a large set of cluster-scoped and namespace-scoped resource objects as data model. This chapter provides the details of these K8s native APIs and data model.
Referenced Cloud-Native APIs and Data Model Solutions
Kubernetes API overview
At high-level, the Kubernetes API [4] structure has a grouping of the managed K8s resource objects which represent K8s resource categories. The generic concepts are introduced regarding the data model of the managed K8s resource objects in Kubernetes.
API Structure
The Kubernetes API [4] managed objects represent a concrete instance of a K8s resource type on the Kubernetes cluster. Kubernetes leverages standard RESTful terminology to describe the API concepts:
A K8s resource is a single instance of a K8s resource type (Note: Throughout this document, to distinguish a "resource" in context of Kubernetes from the "resources" used in a different O-RAN context, this will now be called a "K8s resource")
A K8s resource type is the name used in the URL
All K8s resource types have a representation in JSON (their object schema) which is called a kind
A list of instances of a K8s resource type is known as a collection
All K8s resource types are either scoped by the Kubernetes cluster (e.g., /apis/GROUP/VERSION/*) or to a namespace (e.g., /apis/GROUP/VERSION/namespaces/NAMESPACE/*).
API Verbs
API verbs GET, CREATE, UPDATE, PATCH, DELETE and PROXY support single K8s resources only. These verbs with single K8s resource support have no support for submitting multiple K8s resources together in an ordered or unordered list or transaction. API verbs LIST and WATCH support getting multiple K8s resources, and DELETECOLLECTION supports deleting multiple K8s resources.
K8s resource Objects
The Kubernetes API [4] supports read and write operations on the K8s resource objects via a Kubernetes API endpoint. Kubernetes differentiates the following categories of K8s resource objects managed via their APIs:
Workloads: objects used to manage and run OS containers on the Kubernetes cluster.
Discovery & Loadbalancing: objects used to inter-connect the workloads into externally accessible, load-balanced services.
Configuration & Storage: objects used to inject initialization data into the containerized applications, and to persist data that is external to the OS containers.
Cluster: objects define how the Kubernetes cluster itself is configured.
Metadata: objects used to configure the behavior of other K8s resources within the Kubernetes cluster.
Note: There are some Kubernetes resource objects (e.g., ClusterRoles, CustomResourceDefinitions, StorageClasses etc.) which the SMO may manage separately from the containerized workload resource management. These are FFS.
Data model concepts
The K8s resource objects are modelled with individual object schemas. All K8s resource objects typically have 3 components:
K8s resource ObjectMeta: The metadata about the K8s resource object, such as its name, type, api version, annotations, and labels. This schema, which is common to all K8s resource types, contains fields that maybe updated both by the SMO and the Kuberneters cluster’s internal control plane.
K8s resourceSpec: It is defined by the SMO and describes the desired state of system concerning the K8s resource object. Specified when creating or modifying a K8s resource object is requested.
K8s resourceStatus: Provided by the Kubernetes cluster’s internal control plane and represents the current state of the system concerning the K8s resource object.
O2dms Procedures
The following are the list of Kubernetes procedures supported by the O2dms:
CREATE: The CREATE procedure enables the SMO to instantiate a workload on the target Kubernetes cluster. This procedure requires a namespace and a specified K8s resource type.
READ:
GET: The GET procedure enables the SMO to retrieve and read a specified workload from a target Kubernetes cluster. This procedure requires a namespace, a specified K8s resource type, and the name of the K8s resource.
LIST: The LIST procedure enables the SMO to list or watch a specified workload type from a target Kubernetes cluster. This procedure requires a namespace and a specified K8s resource type.
UPDATE:
PATCH: The PATCH procedure enables the SMO to partially update a workload in a Kubernetes cluster. This procedure requires a namespace, a specified K8s resource type, and the name of the K8s resource.
REPLACE: The REPLACE procedure enables the SMO to replace a workload in a target Kubernetes cluster. This procedure requires a namespace, a specified K8s resource type, the name of the K8s resource.
DELETE: The DELETE procedure enables the SMO to terminate a workload(s) in a target Kubernetes cluster. This procedure requires a namespace, a specified K8s resource type, and the name of the K8s resource.
Referenced K8s resource objects
The Kubernetes APIs and resource objects are fully specified by the open-source Kubernetes project and this document adopts those definitions, APIs, and resource objects [5]. For the purposes of this document, the terms and definitions for Kubernetes API resource objects given in [5] would apply. This section provides details of relevant K8s APIs and resource objects along with recommended use and constraints, if any.
Kubernetes Native Namespace Scoped Resource Objects
Kubernetes native workload resources
Pod
A Pod is the smallest and most basic deployable unit of functionality in a K8s cluster. A Pod resource represents a single instance of one or multiple containers that are scheduled and run on a single worker node in a K8s cluster. The containers in a Pod are managed as a single unit and share the Pod’s networking and storage resources.
Table 4.3.1.1.1-1 provides details of the standard K8s Pod resource specification.
Table 4.3.1.1.1-1: Pod resource specification [5]
Table 4.3.1.1.1-2 provides relevant K8s Pod resource URIs and supported HTTP methods using O2dms interface. For the SMO to lifecycle manage a K8s Pod resource in a chosen K8s cluster using O2dms K8s profile, it shall use the URIs, and HTTP methods listed in Table 4.3.1.1.1-2 with K8s standard query and body parameters for Pod resource included in the HTTP request [5].
Table 4.3.1.1.1-2: Pod resource URIs and HTTP methods supported using O2dms K8s profile
NOTE:	A Pod resource should not be created directly via O2dms because a Pod resource created directly is not managed by K8s control plane and it cannot repair or replace itself. A Pod resources should be created through K8s controller resources instead (e.g., Deployment, Job, Statefulset).
Deployment
A K8s deployment resource is used for running one or multiple identical stateless Pods in a K8s cluster. A Deployment resource ensures that the desired number of replicas of a Pod are running at any given time by replacing any failed or unresponsive Pod instances. The Deployment resource object contains the Pod resource template which specifies the details of the Pods to be created and instantiated on K8s worker node/s. Deployment resource is generally used for running stateless applications in K8s clusters since it does not provide a unique identity to the Pods it manages.
Table 4.3.1.1.2-1 provides details of the standard K8s Deployment resource specification.
Table 4.3.1.1.2-1: Deployment resource specification [5]
Table 4.3.1.1.2-2 provides relevant K8s Deployment resource URIs and supported HTTP methods using O2dms interface. For the SMO to lifecycle manage a K8s Deployment resource in a chosen K8s cluster using O2dms K8s profile, it shall use the URIs, and HTTP methods listed in Table 4.3.1.1.2-2 with K8s standard query and body parameters for Deployment resource included in the HTTP request [5].
Table 4.3.1.1.2-2: Deployment resource URIs and HTTP methods supported using O2dms K8s profile
NOTE:	The creation of a Deployment resource in a K8s cluster also results in the creation of the K8s ReplicaSet resource in the same cluster. The ReplicaSet resource ensures that the number of Pods running in the cluster are equal to the number requested in the Deployment resource. A Deployment resource should be used instead of creating the ReplicaSet resource directly. The K8s Deployment resource fully substitutes the ReplicaSet functions and should be used in most cases.
StatefulSet
A K8s statefulset resource is used for running one or multiple stateful Pods that maintain unique and persistent identities and hostnames during their lifetime. The Pods belonging to a StatefulSet resource are created from the same Pod spec but maintain their individual identities across any rescheduling events caused by Pod failures. In addition, statefulsets provide an ordered and graceful deployment and scaling of Pods in K8s clusters. By default, the Pod deployment for statefulsets is done in sequential order and termination in reverse order but this behavior can be overridden.  For state persistency, StatefulSet resources are generally paired with persistent storage volumes and this paired relationship is maintained across Pod rescheduling events.
Table 4.3.1.1.3-1 provides details of the standard K8s StatefulSet resource specification.
Table 4.3.1.1.3-1: StatefulSet resource specification [5]
Table 4.3.1.1.3-2 provides relevant K8s StatefulSet resource URIs and supported HTTP methods using O2dms interface. For the SMO to lifecycle manage a K8s StatefulSet resource in a chosen K8s cluster using O2dms K8s profile, it shall use the URIs, and HTTP methods listed in Table 4.3.1.1.3-2 with K8s standard query and body parameters for StatefulSet resource included in the HTTP request [5].
Table 4.3.1.1.3-2: StatefulSet resource URIs and HTTP methods supported using O2dms K8s profile
DaemonSet
A K8s DaemonSet resource is used for running a Pod instance on all or a subset of the worker nodes in a K8s cluster. If the cluster scales up or down i.e., worker nodes are added ore removed from the cluster, the daemonset Pods are scaled accordingly to ensure that one Pod per worker node requirement is met.
Table 4.3.1.1.4-1 provides details of the standard K8s DaemonSet resource specification.
Table 4.3.1.1.4-1: DaemonSet resource specification [5]
Table 4.3.1.1.4-2 provides relevant K8s DaemonSet resource URIs and supported HTTP methods using O2dms interface. For the SMO to lifecycle manage a K8s DeamonSet resource in a K8s cluster using O2dms K8s profile, it shall use the URIs, and HTTP methods listed in Table 4.3.1.1.4-2 with K8s standard query and body parameters for DeamonSet resource included in the HTTP request [5].
Table 4.3.1.1.4-2: DaemonSet resource URIs and HTTP methods supported using O2dms K8s profile
Job
A K8s Job resource is used for running workloads in K8s clusters that run to completion and are terminated after completion. A Job resource can create one or more Pods and execute them until a specified number of Jobs are successfully terminated. A Job resource tracks successful completions of Pod executions and upon reaching the desired number of completions, the Job is considered complete.
Table 4.3.1.1.5-1 provides details of the standard K8s Job resource specification.
Table 4.3.1.1.5-1: Job resource specification [5]
Table 4.3.1.1.5-2 provides relevant K8s Job resource URIs and supported HTTP methods using O2dms interface. For the SMO to lifecycle manage a Job resource in a chosen K8s cluster using O2dms K8s profile, it shall use the URIs, and HTTP methods listed in Table 4.3.1.1.5-2 with K8s standard query and body parameters for Job resource included in the HTTP request [5].
Table 4.3.1.1.5-2: Job resource URIs and HTTP methods supported using O2dms K8s profile
CronJob
A K8s CronJob resource is used for running K8s Jobs on a pre-defined time schedule provided in Linux/Unix Cron format. The time schedule of the CronJob resource follows the timezone set for the K8s native kube-controller-manager function.
Table 4.3.1.1.6-1 provides details of the standard K8s CronJob resource specification.
Table 4.3.1.1.6-1: CronJob resource specification [5]
Table 4.3.1.1.6-2 provides relevant K8s CronJob resource URIs and supported HTTP methods using O2dms interface. For the SMO to lifecycle manage a CronJob resource in a chosen K8s cluster using O2dms K8s profile, it shall use the URIs, and HTTP methods listed in Table 4.3.1.1.6-2 with K8s standard query and body parameters for CronJob resource included in the HTTP request [5].
Table 4.3.1.1.6-2: CronJob resource URIs and HTTP methods supported using O2dms K8s profile
Kubernetes native discovery and load balancing resources
Service
A K8s Service resource is used for exposing a containerized workload/application running in a K8s cluster to internal and external clients. The Service resource also abstracts the ephemeral nature of Pods and containers by creating a constant point of entry to the group of Pods that provide the same functionality. Each Service gets an IP address and port number that does not change during the lifetime of the Service resource in the K8s cluster. In addition, the Service resource provides load-balancing across all Pod instances that provide the same functionality. Kubernetes supports five types of Service resources identified as ClusterIP, NodePort, LoadBalancer, ExternalName and Headless Service with ClusterIP being the default Service type.
Table 4.3.1.2.1-1 provides details of the standard K8s Service resource specification.
Table 4.3.1.2.1-1: K8s Service resource specification [5]
Table 4.3.1.2.1-2 provides relevant K8s Service resource URIs and supported HTTP methods using O2dms interface. For the SMO to lifecycle manage a Service resource in a chosen K8s cluster using O2dms K8s profile, it shall use the URIs, and HTTP methods listed in Table 4.3.1.2.1-2 with K8s standard query and body parameters for Service resource included in the HTTP request [5].
Table 4.3.1.2.1-2: K8s Service resource URIs and HTTP methods supported using O2dms K8s profile
Ingress
A K8s Ingress resource is used for defining HTTP(s) routing rules that control external access to applications running inside the K8s cluster. A K8s Ingress resource can be associated with one or more K8s Service resources (each of which can expose a different set of Pods) and offers load-balancing and SSL termination features. The Ingress resource can assign a specific externally reachable URL to each Service resource associated with it.
Table 4.3.1.2.2-1 provides details of the standard K8s Ingress resource specification.
Table 4.3.1.2.2-1: K8s Ingress resource specification [5]
Table 4.3.1.2.2-2 provides relevant K8s Ingress resource URIs and supported HTTP methods using O2dms interface. For the SMO to lifecycle manage an Ingress resource in a chosen K8s cluster using O2dms K8s profile, it shall use the URIs, and HTTP methods listed in Table 4.3.1.2.2-2 with K8s standard query and body parameters for Ingress resource included in the HTTP request [5].
Table 4.3.1.2.2-2: K8s Ingress resource URIs and HTTP methods supported using O2dms K8s profile
NOTE: 	For an Ingress resource to work, an Ingress controller (i.e., an additional controller to the default K8s controllers) shall be running inside the K8s cluster. K8s does not offer a default ingress controller and leaves the choice of ingress controller to the cluster administrator. In the ORAN context, the K8s cluster may have a default ingress controller installed at the time of cluster creation, or the SMO may decide to extend the K8s cluster with any specific ingress controller after cluster creation using O2dms interface.
NetworkPolicy
A K8s NetworkPolicy resource is used to control traffic flow to/from a K8s workloads at the IP address or port level using ingress and egress rules. These rules may apply to other K8s Pods, Namespaces, or generic IP blocks to restrict or allow traffic from these entities. Selectors and IP blocks (CIDR ranges) are used to enforce the NetworkPolicy constraints specified in the NetworkPolicy resource specification.
Table 4.3.1.2.3-1 provides details of the standard K8s NetworkPolicy resource specification.
Table 4.3.1.2.3-1: NetworkPolicy resource specification [5]
Table 4.3.1.2.3-2 provides relevant K8s NetworkPolicy resource URIs and supported HTTP methods using O2dms interface. For the SMO to lifecycle manage a K8s NetworkPolicy resource in a K8s cluster using O2dms K8s profile, it shall use the URIs, and HTTP methods listed in Table 4.3.1.2.3-2 with K8s standard query and body parameters for NetworkPolicy resource included in the HTTP request [5].
Table 4.3.1.2.3-2: NetworkPolicy resource URIs and HTTP methods supported using O2dms K8s profile
NOTE: 	The functionality of the NetworkPolicy resource depends on the presence of a compatible K8s network plugin that supports K8s network policies. Without a supporting network plugin, the NetworkPolicy resource creation will have no impact on the ingress and egress traffic to/from a K8s workload.
Kubernetes native storage and workload configuration resources
ConfigMap
A K8s ConfigMap resource is used for storing non-confidential data in the form of key-value pairs that can be read as runtime configurations by Pods in a K8s cluster. ConfigMap resource can be used to store configuration files, environment variables or command line arguments for workloads to use at runtime. A ConfigMap resource decouples the workload containers from environment specific configuration data thereby making the K8s Pods easily portable across the worker nodes in a K8s cluster. The data stored in a ConfigMap cannot exceed 1 MiB.
Table 4.3.1.3.1-1 provides details of the standard K8s ConfigMap resource specification.
Table 4.3.1.3.1-1: K8s ConfigMap resource specification [5]
Table 4.3.1.3.1-2 provides relevant K8s ConfigMap resource URIs and supported HTTP methods using O2dms interface. For the SMO to lifecycle manage an ConfigMap resource in a chosen K8s cluster using O2dms K8s profile, it shall use the URIs, and HTTP methods listed in Table 4.3.1.3.1-2 with K8s standard query and body parameters for ConfigMap resource included in the HTTP request [5].
Table 4.3.1.3.1-2: K8s ConfigMap resource URIs and HTTP methods supported using O2dms K8s profile
Secret
A K8s Secret resource is used for storing sensitive or confidential data in key value pairs format. Secrets are like ConfigMaps but specifically intended to be used for storing confidential data such as passwords, OAuth tokens, SSH keys etc. By default, data stored in Secrets is not encrypted in a K8s cluster.
Table 4.3.1.3.2-1 provides details of the standard K8s Secret resource specification.
Table 4.3.1.3.2-1: K8s Secret resource specification [5]
Table 4.3.1.3.2-2 provides relevant K8s Secret resource URIs and supported HTTP methods using O2dms interface. For the SMO to lifecycle manage a Secret resource in a chosen K8s cluster using O2dms K8s profile, it shall use the URIs, and HTTP methods listed in Table 4.3.1.3.2-2 with K8s standard query and body parameters for Secret resource included in the HTTP request [5].
Table 4.3.1.3.2-2: K8s Secret resource URIs and HTTP methods supported using O2dms K8s profile
PersistentVolumeClaim
The K8s PersistentVolumeClaim resource is used for requesting and claiming a K8s PersistentVolume resource. The PersistentVolume may pre-exist in the K8s cluster or created dynamically in response to the creation of PersistentVolumeClaim resource. A PersistentVolumeClaim provides a way to request a persistent storage for Pods without knowing the underlying storage technology. Pods can use PersistentVolumeClaims as volumes and mount them as part of the Pod specification.
Table 4.3.1.3.3-1 provides details of the standard K8s PersistentVolumeClaim resource specification.
Table 4.3.1.3.3-1: K8s PersistentVolumeClaim resource specification [5]
Table 4.3.1.3.3-2 provides relevant K8s PersistentVolumeClaim resource URIs and supported HTTP methods using O2dms interface. For the SMO to lifecycle manage a PersistentVolumeClaim resource in a chosen K8s cluster using O2dms K8s profile, it shall use the URIs, and HTTP methods listed in Table 4.3.1.3.3-2 with K8s standard query and body parameters for PersistentVolumeClaim resource included in the HTTP request [5].
Table 4.3.1.3.3-2: K8s PersistentVolumeClaim resource URIs and HTTP methods supported using O2dms K8s profile
Kubernetes native metadata and cluster configuration resources
LimitRange
A K8s LimitRange resource is used to set constraints on the resource utilization of Pods and containers in a namespace. By default, a container can use as much compute resources as it needs in the namespace and that can lead to resource contention problems among Pods and containers that share the namespace and may belong to different priority classes. A LimitRange resource an enforce defaults or min-max values for different resources in the namespace (e.g., CPU, memory, storage) at Pod or container level.
Table 4.3.1.4.1-1 provides details of the standard K8s LimitRange resource specification.
Table 4.3.1.4.1-1: K8s LimitRange resource specification [5]
Table 4.3.1.4.1-2 provides relevant K8s LimitRange resource URIs and supported HTTP methods using O2dms interface. For the SMO to lifecycle manage a LimitRange resource in a chosen K8s cluster using O2dms K8s profile, it shall use the URIs, and HTTP methods listed in Table 4.3.1.4.1-2 with K8s standard query and body parameters for LimitRange resource included in the HTTP request [5].
Table 4.3.1.4.1-2: K8s LimitRange resource URIs and HTTP methods supported using O2dms K8s profile
ResourceQuota
A K8s ResourceQuota resource is used to set limits on the total/aggregate resource consumption in a namespace. A ResourceQuota object can specify limits for other K8s resource objects (by resource type) and compute resources (e.g., memory, CPU, storage). A single K8s namespace can have multiple ResourceQuota resources with unique names for limiting different types of namespace scoped resources. The ResourceQuota specified limits are enforced at K8s resource creation time and have no impact on exsting Pods and resources in the namespace.
Table 4.3.1.4.2-1 provides details of the standard K8s ResourceQuota resource specification.
Table 4.3.1.4.2-1: ResourceQuota resource specification [5]
Table 4.3.1.4.2-2 provides relevant K8s ResourceQuota resource URIs and supported HTTP methods using O2dms interface. For the SMO to lifecycle manage a K8s ResourceQuota resource in a K8s cluster using O2dms K8s profile, it shall use the URIs, and HTTP methods listed in Table 4.3.1.4.2-2 with K8s standard query and body parameters for ResourceQuota resource included in the HTTP request [5].
Table 4.3.1.4.2-2: ResourceQuota resource URIs and HTTP methods supported using O2dms K8s profile
HorizontalPodAutoscaler
A K8s HorizontalPodAutoscaler resource is used to automatically scale up or down, the number of Pod instances belonging to a workload resource that supports horizontal scaling (e.g., Deployments and StatefulSets). The scale-up and scale-down levels are controlled by defined minimum and maximum values in the HorizontalPodAutoscaler resource. The HorizontalPodAutoscaler resource works by changing the “scale” property of a workload resource e.g., Deployment, based on observed metrics such as CPU and Memory utilization.
NOTE:	For the K8s cluster to horizontally scale a workload based on the metrics specified in the K8s HorizontalPodAutoscaler resource, it should have a metrics collection service running in the cluster.
Table 4.3.1.4.3-1 provides details of the standard K8s HorizontalPodAutoscaler resource specification.
Table 4.3.1.4.3-1: HorizontalPodAutoscaler resource specification [5]
Table 4.3.1.4.3-2 provides relevant K8s HorizontalPodAutoscaler resource URIs and supported HTTP methods using O2dms interface. For the SMO to lifecycle manage a K8s HorizontalPodAutoscaler resource in a chosen K8s cluster using O2dms K8s profile, it shall use the URIs, and HTTP methods listed in Table 4.3.1.4.3-2 with K8s standard query and body parameters for HorizontalPodAutoscaler resource included in the HTTP request [5].
Table 4.3.1.4.3-2: HorizontalPodAutoscaler resource URIs and HTTP methods supported using O2dms K8s profile
PodDisruptionBudget
A K8s PodDisruptionBudget resource is used to ensure that a specific number of Pod instances belonging to a workload resource (e.g., Deployments, StatefulSets) are available in the cluster in a healthy running state in the event of planned or unplanned Pod disruptions. A K8s PodDisruptionBudget resource ensures that the application or service offered by the Pods remains available in the event of concurrent disruptions due to Pod termination events. The number of healthy Pod instances required during disruption events can be specified as a minimum-available or maximum-unavialable number.
Table 4.3.1.4.4-1 provides details of the standard K8s PodDisruptionBudget resource specification.
Table 4.3.1.4.4-1: PodDisruptionBudget resource specification [5]
Table 4.3.1.4.4-2 provides relevant K8s PodDisruptionBudget resource URIs and supported HTTP methods using O2dms interface. For the SMO to lifecycle manage a K8s PodDisruptionBudget resource in a chosen K8s cluster using O2dms K8s profile, it shall use the URIs, and HTTP methods listed in Table 4.3.1.4.4-2 with K8s standard query and body parameters for PodDisruptionBudget resource included in the HTTP request [5].
Table 4.3.1.4.4-2: PodDisruptionBudget resource URIs and HTTP methods supported using O2dms K8s profile
Kubernetes Native Cluster Scoped Resource Objects
This is FFS.
History