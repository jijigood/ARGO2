ORAN-WG9.XPAAS.0-v02.00
02.00RAN.WG9.XPSAAS-v01.00 Technical Specification
O-RAN Open Transport Working Group 9
 Xhaul Packet Switched Architectures and Solutions
This is a re-published version of the attached final specification.
For this re-published version, the prior versions of the IPR Policy will apply, except that the previous requirement
for Adopters (as defined in the earlier IPR Policy) to agree to an O-RAN Adopter License Agreement to access
and use Final Specifications shall no longer apply or be required for these Final Specifications after 1st July 2022.
The copying or incorporation into any other work of part or all of the material available in this specification in
any form without the prior written permission of O-RAN ALLIANCE e.V.  is prohibited, save that you may print
or download extracts of the material on this site for your personal use, or copy the material on this site for the
purpose of sending to individual third parties for their information provided that you acknowledge O-RAN
ALLIANCE as the source of the material and that you inform the third party that these conditions apply to them
and that they must comply with them.

Prepared by the O-RAN Alliance. Copyright © 2021 by the O-RAN Alliance.
By using, accessing or downloading any part of this O -RAN specification document, including by copying, saving,
distributing, displaying or preparing derivatives of, you agree to be and are bound to the terms of the O-RAN Adopter License
Agreement contained in Annex ZZZ of this specification. All other rights reserved.

.
                                     ORAN-WG9.XPAAS.0-v02.00
02.00RAN.WG9.XPSAAS-v01.00 Technical Specification

O-RAN Open Transport Working Group 9

 Xhaul Packet Switched Architectures and Solutions

Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ             1

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        2
1 Revision History 1
Date Revision Author Description
2020/11/11 V01.00 All Authors 1st revision outlining a packet switched O-RAN
solution based on an underlay transport
infrastructure based on MPLS or IPv6 with
Segment Routing (SRv6) with mobile services
provided by Multi-protocol BGP based VPNs.
2021/07/01 V02.00 Simon Spraggs, Krzysztof
Szarkowicz, Luis Miguel
Contrersas Murillo, Ivan
Bykov, Lujing Cai
Update of references as required. Addition of
annex F describing a packet switched slicing
solution appropriate to slicing phase 1
described in O-RAN.WG1.Slicing-
Architecture-v05.00
 2
1.1 Contributors 3
Editors: Simon Spraggs, Krzysztof Szarkowicz 4
Contributors in alphabetical order: Jennifer Andreoli-Fang, Lujing Cai, Francois Fredricx, Ivan 5
Bykov, Kashif Islam, Luis Miguel Contreras Murillo, Toby Rees, Simon Spraggs, Krzysztof 6
Szarkowicz, Reza Vaez-Ghaemi, Nader Zein, Jeffrey Zhang 7
 8
 9
 10
 11
2 Contents 12
1 Revision History ....................................................................................................................................... 2 13
1.1 Contributors ....................................................................................................................................................... 2 14
3 Scope ........................................................................................................................................................ 7 15
4 References ................................................................................................................................................ 9 16
5 Definitions and abbreviations ................................................................................................................. 17 17
5.1 Definitions ....................................................................................................................................................... 17 18
5.2 Abbreviations ................................................................................................................................................... 17 19
6 5G Transport network requirements....................................................................................................... 22 20
7 5G logical connectivity requirements..................................................................................................... 24 21
7.1 Fronthaul .......................................................................................................................................................... 24 22
7.1.1 O-RAN 7.2x Fronthaul ............................................................................................................................... 24 23
7.1.2 O-RAN Fronthaul logical transport requirements ...................................................................................... 25 24
7.2 Non-ORAN Fronthaul ..................................................................................................................................... 26 25
7.2.1 eCPRI based C-RAN solutions .................................................................................................................. 26 26
7.2.2 Radio over Ethernet (RoE) based C-RAN solutions .................................................................................. 26 27
7.2.3 Non O-RAN Fronthaul logical transport requirements .............................................................................. 28 28
7.3 Midhaul logical transport requirements ........................................................................................................... 28 29
7.3.1 Overall Midhaul logical transport requirements ......................................................................................... 29 30
7.4 Backhaul logical transport requirements.......................................................................................................... 29 31
7.4.1 Overall Backhaul logical transport requirements ....................................................................................... 30 32
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        3
8 Operator use cases .................................................................................................................................. 30 1
8.1 Scenario 1: C-RAN architecture with collocated O-DU and O-CU ................................................................ 31 2
8.2 Scenario 2: C-RAN architecture with collocated O-RU and O-DU ................................................................ 32 3
8.3 Scenario 3: C-RAN architecture with coexistence of legacy Backhaul traffic ................................................ 33 4
8.4 Scenario 4: C-RAN architecture with coexistence of legacy Fronthaul traffic ................................................ 34 5
8.5 Scenario 5: C-RAN architecture with further split of O-DU and O-CU .......................................................... 35 6
8.6 Scenario 6: C-RAN architecture with local breakout ...................................................................................... 35 7
8.7 Scenario 7: Transport slicing ........................................................................................................................... 36 8
9 Overall packet switched Open Xhaul architecture ................................................................................. 38 9
9.1 Physical layout and xHaul transport options .................................................................................................... 39 10
9.2 Open Xhaul architecture in revision 1 and 2 .................................................................................................... 40 11
9.3 Technology and architectural choices .............................................................................................................. 41 12
9.4 Standardization ................................................................................................................................................ 41 13
9.5 Document organization .................................................................................................................................... 41 14
10 Physical network design for packet switched Xhaul .............................................................................. 42 15
10.1 Packet over fibre .............................................................................................................................................. 43 16
10.1.1 Access ........................................................................................................................................................ 44 17
10.1.2 Pre-aggregation / Aggregation / Core transport ......................................................................................... 48 18
10.2 Alternative physical transport solutions ........................................................................................................... 49 19
10.2.1 WDM in access network ............................................................................................................................ 50 20
10.2.2 Passive Optical Networks (PONs) ............................................................................................................. 50 21
10.2.3 DOCSIS Networks ..................................................................................................................................... 55 22
10.2.4 Microwave and mmave radio transport technologies ................................................................................. 58 23
10.3 Data Centers..................................................................................................................................................... 65 24
10.3.1 Complete separation between DC and WAN infrastructure....................................................................... 65 25
10.3.2 DC integrated into WAN infrastructure. .................................................................................................... 65 26
11 Packet-switched underlay network – MPLS based ................................................................................ 65 27
11.1 MPLS data plane .............................................................................................................................................. 66 28
11.2 MPLS control plane ......................................................................................................................................... 67 29
11.3 Classic MPLS control plane............................................................................................................................. 68 30
11.4 SR/MPLS control plane ................................................................................................................................... 70 31
11.4.1 Interior Gateway Protocol (IGP) for SR/MPLS ......................................................................................... 70 32
11.4.2 SR/MPLS Traffic Engineering ................................................................................................................... 74 33
11.5 Scaling the MPLS infrastructure ...................................................................................................................... 75 34
11.5.1 Seamless MPLS architecture ...................................................................................................................... 75 35
11.5.2 Controller based network scaling architectures .......................................................................................... 77 36
11.6 MPLS Quality of Service ................................................................................................................................. 83 37
11.7 MPLS OAM..................................................................................................................................................... 83 38
11.8 IP/MPLS service infrastructure........................................................................................................................ 83 39
12 Packet-switched underlay network – SRv6 based .................................................................................. 83 40
12.1 SRv6 data plane ............................................................................................................................................... 84 41
12.2 SRv6 control plane........................................................................................................................................... 84 42
12.2.1 Interior Gateway Protocol (IGP) for SRv6 ................................................................................................. 85 43
12.2.2 SRv6 Traffic Engineering .......................................................................................................................... 89 44
12.2.3 Inter-domain connectivity .......................................................................................................................... 90 45
12.3 Scaling an SRv6 underlay infrastructure ......................................................................................................... 91 46
12.3.1 Route summarization and redistribution .................................................................................................... 91 47
12.3.2 Controller based scaling ............................................................................................................................. 93 48
12.3.3 SRv6 scaling conclusion ............................................................................................................................ 94 49
12.4 IPv6 Quality of Service .................................................................................................................................... 94 50
12.5 SRv6 OAM ...................................................................................................................................................... 94 51
12.5.1 Ping / Traceroute to a remote IPv6 network address .................................................................................. 94 52
12.5.2 Ping / Traceroute to remote SID functions ................................................................................................. 95 53
12.6 SRv6 on-going standardisation ........................................................................................................................ 95 54
12.7 SRv6 Service infrastructure ............................................................................................................................. 95 55
13 Packet-switched Xhaul services Infrastructure ...................................................................................... 96 56
13.1 MP-BGP design ............................................................................................................................................... 96 57
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        4
13.2 Ethernet services .............................................................................................................................................. 96 1
13.2.1 Ethernet services redundancy ..................................................................................................................... 97 2
13.3 IP Services ..................................................................................................................................................... 102 3
13.3.1 Building flexible L3VPN service topologies ........................................................................................... 102 4
13.3.2 Constraints based Traffic Steering in L3VPNs ........................................................................................ 102 5
14 Quality of Service in packet-switched networks .................................................................................. 103 6
14.1 Xhaul transport core interface QoS ................................................................................................................ 103 7
14.1.1 Transport network core interface classification........................................................................................ 104 8
14.1.2 Core interface queue structure .................................................................................................................. 104 9
14.1.3 Transport network core interface marking structure ................................................................................ 106 10
14.1.4 Core interface scheduling model .............................................................................................................. 106 11
14.2 Xhaul transport network edge interface QoS ................................................................................................. 107 12
14.2.1 Transport network domain PE ingress classification of Ethernet frames. ................................................ 107 13
14.2.2 Transport domain PE ingress classification IP packets. ........................................................................... 109 14
14.2.3 Admission control .................................................................................................................................... 110 15
14.2.4 PE Egress scheduling ............................................................................................................................... 110 16
15 Multicast ............................................................................................................................................... 111 17
15.1 Multicast use cases......................................................................................................................................... 111 18
15.1.1 Multicast transport for fixed line services ................................................................................................ 111 19
15.1.2 MBMS/5MBS transport ........................................................................................................................... 111 20
15.2 Overlay and underlay multicast ..................................................................................................................... 112 21
15.3 Recommendation/considerations for multicast solutions ............................................................................... 113 22
16 Packet-switched orchestration and telemetry ....................................................................................... 113 23
17 5G Slicing in a packet switched Xhaul network .................................................................................. 114 24
17.1 Packet-switched underlay network ................................................................................................................ 116 25
17.1.1 Underlay forwarding plane ....................................................................................................................... 116 26
17.1.2 Single forwarding plane for all slices ....................................................................................................... 116 27
17.1.3 Forwarding plane per 5G service ............................................................................................................. 117 28
17.1.4 Forwarding plane per slice customer ........................................................................................................ 117 29
17.2 Quality of Service .......................................................................................................................................... 117 30
17.2.1 Edge QoS ................................................................................................................................................. 117 31
17.2.2 Core QoS .................................................................................................................................................. 117 32
17.3 5G Services and slices ................................................................................................................................... 119 33
18 Supporting mobile scenarios on a packet switched Xhaul network ..................................................... 119 34
18.1 Physical network ............................................................................................................................................ 120 35
18.2 Logical underlay architecture ........................................................................................................................ 121 36
18.2.1 Underlay Quality of Service (QoS) .......................................................................................................... 122 37
18.3 Service architecture........................................................................................................................................ 123 38
18.3.1 Automated VPN Traffic Steering ............................................................................................................. 123 39
18.4 Mobile services .............................................................................................................................................. 123 40
18.4.1 Open Fronthaul ......................................................................................................................................... 123 41
18.4.2 Non O-RAN Fronthaul ............................................................................................................................. 126 42
18.4.3 Midhaul and Backhaul ............................................................................................................................. 126 43
18.5 Scenario 1 and 5 ............................................................................................................................................. 128 44
18.6 Scenario 2 ...................................................................................................................................................... 130 45
18.7 Scenario 3 5G C-RAN with legacy D-RAN .................................................................................................. 131 46
18.7.1 Scenario 3a ............................................................................................................................................... 131 47
18.7.2 Scenario 3b ............................................................................................................................................... 131 48
18.8 Scenario 4 5G C-RAN with RoE mappers ..................................................................................................... 132 49
18.9 Scenario 6 5G C-RAN with distributed UPF ................................................................................................. 133 50
18.10 Scenario 7 Slicing .......................................................................................................................................... 133 51
19 Annex A: Overview of “Segment Routing” (SR) ................................................................................ 133 52
Background ..................................................................................................................................................................... 134 53
Segment Routing ............................................................................................................................................................ 134 54
Segment Routing architectural principle ........................................................................................................................ 135 55
Segment Routing data plane ........................................................................................................................................... 135 56
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        5
Segment Routing control plane ....................................................................................................................................... 137 1
20 Annex B: IETF Ethernet Virtual Private Networks ............................................................................. 139 2
21 Annex C: MP-BGP based L3VPNs ..................................................................................................... 145 3
Building blocks of a L3 VPN service ............................................................................................................................. 146 4
Traffic Steering into an BGP VPN ................................................................................................................................. 148 5
22 Annex D: Quality of Service ................................................................................................................ 149 6
What is Quality of Service? ............................................................................................................................................ 149 7
Why do we need QoS? ................................................................................................................................................... 149 8
QoS functional elements ................................................................................................................................................. 150 9
Network level behaviour ................................................................................................................................................. 150 10
Node level behaviour ...................................................................................................................................................... 151 11
Traffic classification and marking .................................................................................................................................. 151 12
Congestion management ................................................................................................................................................. 154 13
Congestion avoidance ..................................................................................................................................................... 156 14
23 Annex E: Multicast Technologies background .................................................................................... 160 15
Overlay multicast ............................................................................................................................................................ 160 16
PIM-based overlay signalling for IPVPN ....................................................................................................................... 161 17
BGP-based overlay signalling for IPVPN and EVPN .................................................................................................... 161 18
Underlay multicast .......................................................................................................................................................... 162 19
MVPN/EVPN and Seamless MPLS/SR ......................................................................................................................... 162 20
24 Annex F: Transport network slicing solution for WG1 Slicing phase 1 (informational) ..................... 164 21
WG-1 Phase 1 requirements and scope .......................................................................................................................... 164 22
Overall Packet Switched Transport Architecture ............................................................................................................ 166 23
Underlay network for WG-1 slicing phase 1 .................................................................................................................. 167 24
Service Models for WG-1 slicing phase 1 ...................................................................................................................... 167 25
Transport and DC management networks ....................................................................................................................... 167 26
Transport network management network ....................................................................................................................... 168 27
Data Centre (DC) management network ......................................................................................................................... 168 28
O-RAN control and management networks .................................................................................................................... 169 29
O-RAN Fronthaul Management network (M-Plane) ...................................................................................................... 169 30
O-RAN Control and Management network (A1, E2, O1 interfaces) .............................................................................. 170 31
3GPP Control Plane network .......................................................................................................................................... 170 32
O-RAN and 3GPP user planes networks ........................................................................................................................ 171 33
Fronthaul C/U plane network ......................................................................................................................................... 172 34
Midhaul user plane network (F1-u and Xn-u) ................................................................................................................ 172 35
Backhaul user plane network (N3 and N9) ..................................................................................................................... 173 36
Data Network (N6) ......................................................................................................................................................... 175 37
Transport Network Quality of Service architecture  ....................................................................................................... 175 38
Transport QoS considerations in a 5G environment ....................................................................................................... 175 39
3GPP QoS flows and Transport QoS  ............................................................................................................................. 176 40
QoS Architecture for O-RAN slicing phase 1 ................................................................................................................ 178 41
Annex ZZZ : O-RAN Adopter License Agreement ....................................................................................... 185 42
Section 1: DEFINITIONS .............................................................................................................................................. 185 43
Section 2: COPYRIGHT LICENSE ............................................................................................................................... 186 44
Section 3: FRAND LICENSE ........................................................................................................................................ 186 45
Section 4: TERM AND TERMINATION ...................................................................................................................... 186 46
Section 5: CONFIDENTIALITY ................................................................................................................................... 187 47
Section 6: INDEMNIFICATION ................................................................................................................................... 187 48
Section 7: LIMITATIONS ON LIABILITY; NO WARRANTY .................................................................................. 187 49
Section 8: ASSIGNMENT ............................................................................................................................................. 188 50
Section 9: THIRD-PARTY BENEFICIARY RIGHTS .................................................................................................. 188 51
Section 10: BINDING ON AFFILIATES ...................................................................................................................... 188 52
Section 11: GENERAL ................................................................................................................................................... 188 53
 54
 55
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        6
  1
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        7
3 Scope 1
This Technical Specification has been produced by the O -RAN Alliance. The document is intended 2
to describe best practises for O -RAN transport based on end -to-end packet switching technology. It 3
is recognised that other solutions, not based on packet switch ing, could be employed or mixed with 4
a packet switching solution. Beyond the solutions described in this document, other packet 5
switching solutions may be adequate for Xhaul transport networks and can be considered in future 6
versions of this document.   7
 8
This specification defines an architecture for an Open Xhaul transport network based on an end -to-9
end packet switching architecture that utilises statistical multiplexing and a hierarchy of packet 10
switching “Transport Node Equipment” (TNE) starting at the c ell site in the access layer and going 11
to the core layer of the transport network capable of supporting the requirements outlined in the O-12
RAN WG9 Transport Requirements document [18]. 13
 14
Within the transport core, aggregation and pre -aggregation it is assumed the L0/L1 transport 15
technology connecting the packet switches are high capacity, low delay Ethernet point to point 16
circuits. These circuits can be derived from dark fibre, WDM or other technologies capable of 17
presenting Ethernet interfaces and where the delay component primarily consists of light 18
propagation within the fibre.  This technology is clearly very important but out of scope of this 19
document.   20
 21
To allow the operators to offer the most flexibility in designing their RAN infrastructure the access 22
network should utilise the same design paradigm as the transport core, aggregation and pre -23
aggregation. However, in some instances this may not be an option, so this document identifies 24
other potential access technologies, provide s a description and considerations/trade -offs for their 25
usage.   26
  27
Figure 3-1 illustrates the scope of network segments covered by WG9. The area inside the dotted 28
green line characterizes the transport networks composed of a number of Transport Network 29
Elements (TNE) deployed among different components defined in other O-RAN WGs. WG9 does 30
not define the interfaces along the dotted green line. As an example, the fronthaul interface of an O-31
RU or O-DU are defined by WG4.  32
 33
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        8
 1
 2
Figure 3-1 Xhaul Transport Network Overview 3
 4
WG9 focuses on opt ion 7 -2x. Functional elements translating option 7 -2x to non O -RAN lower 5
layer split option (Fronthaul Gateways FHG) are considered to be part of radio network, and beyond 6
the scope of this document. The same applies to Fronthaul Multiplexer (FHM) and casc aded radios. 7
Radio over Ethernet mapping (RoE, IEEE 1419.3) is covered only as a service provided by the 8
packet-based network and is not defined by WG9. The stated functions are logical ones, actual 9
product implementation may combine several of these funct ions. For example, vendors may market 10
a Fronthaul Gateway product that combines different elements such as TNE, FHG and RoE  mapper 11
in one and the same physical box. 12
 13
WG9 sub-teams are working on several solution documents. This document focuses on packet-14
based transport technologies. It also includes sections on some of the technologies used in physical 15
network such as PON, DOCSIS® networks and wireless Xhaul. WDM in the access network 16
providing Fronthaul services is covered in a different document [19]. 17
 18
Revision 1 of this document covers: 19
 20
1. High level transport requirements 21
2. 5G Fronthaul, Midhaul, Backhaul transport requirement 22
3. 5G operator use cases   23
 24
The document then considers packet infrastructure and how the packet infrastructure can support 25
the identified requirements and use cases. It describes a packet architecture consisting of an 26
underlay packet switching infrastructure which supports L2 and L3 services.  27
The document covers two potential underlay solutions; MPLS based or SRv6 based. In both cases 28
the service infrastructure consists of MP-BGP VPN solutions supporting L2 and L3 services. 29

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        9
The last part of the document outlines, using examples, how  the packet switching infrastructure can 1
support the operator use cases outlined earlier in the document. 2
 3
Revision 2 builds on the basic functionality described in Revision 1 and outlines a transport slicing 4
solution capable of supporting phase 1 of WG -1 slicing architecture [23]. This is primarily covered 5
in an informational annex ( Annex F: Transport network slicing solut ion for WG1 Slicing phase 1 6
(informational)).   7
 8
This document does not cover: 9
 10
1. Timing and synchronization –Another O-RAN WG-9 effort is underway. [20] 11
2. WDM in access for Fronthaul services -Another O-RAN WG-9 effort is underway[19].  12
3. Highly specialised URLLC, for example motion control in an indust rial setting. I.E. Private 13
network, tightly constrained, requiring TSN in the Backhaul and RAN looking like a TSN 14
bridge. This can be covered in a later edition. 15
4. OTN or SPN/G.mtn as a transport layer. 16
 17
This document uses information and requirements published by O-RAN, 3GPP, IEEE, ITU-T, 18
IETF, CableLabs, NGMN, MEF, BBF and many other standard bodies and industry associations. 19
 20
4 References 21
The following documents contain provisions which, through reference in this text, constitute 22
provisions of the present document. 23
- References are either specific (identified by date of publication, edition number, version 24
number, etc.) or non-specific. 25
- For a specific reference, subsequent revisions do not apply. 26
- For a non-specific reference, the latest version applies. In the case of a reference to a 3GPP 27
document (including a GSM document), a non-specific reference implicitly refers to the latest 28
version of that document in Release 15. 29
 30
  31
3GPP references  32
[1] 3GPP TS 23.501 v16.4.0(2020-03): “System Architecture for 5G” 33
[2] 3GPP TS 23.203 v17.0.0: Policy Control and Charging Architecture 34
[3] 3GPP TS 38.306: “NR; User equipment (UE) radio access capabilities” 35
[4] 3GPP TS 29.060 V15.0.0: “GPRS Tunnelling Protocol (GTP) across the Gn and 36
Gp interface (Release 15)” 37
[5] 3GPP TS.281 v17.0.0: “General packet Radio System (GPRS) Tunnel Protocol 38
User Plane (GTPv1-U) 39
[6] 3GPP TS 29.274 V16.1.0: “GPRS Tunnelling Protocol (GTP) across the Gn and 40
Gp interface (Release 15)” 41
[7] 3GPP TS 38.401: “NG-RAN; Architecture description” 42
[8] 3GPP TS 36.422 version 15.1.0: “X2 signalling transport (Release 15)” 43
[9] 3GPP TS 36.424 version 15.0.0: “X2 data transport (Release 15)” 44
[10] 3GPP TS 38.474 V15.3.0: “F1 data transport (Release 15)”. 45
[11] 3GPP TS 38.462 V15.5.0: “E1 signalling transport (Release 15)” 46
[12] 3GPP TS 38.422 version 15.0.0 Release 15: “Xn general aspects and principles” 47
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        10
[13] 3GPP TS 38.415 V16.4.0: “PDU Session User Plane Protocol (Release 16)” 1
 2
O-RAN references 3
[14] O-RAN.WG4.IOT.0-v04.00 “Open Fronthaul IOT specification v04.00”, 4
November 2020 5
[15] O-RAN.WG5.Transport.0-v01.00 “Transport Specification v01.00”, April 2020  6
[16] O-RAN.WG4.CUS.0-v06.00 “Open Fronthaul Control, User and 7
Synchronization Plane Version v06.00”, November 2020 8
[17] O-RAN.WG4.MP.0-v06.00: “Open Fronthaul Management plane specification 9
v06.00”, November 2020 10
[18] O-RAN.WG9.XTRP-REQ-v01.00 “Xhaul Transport Requirements v01.00”, 11
November 2020 12
[19] O-RAN.WG9.WDM-v01.00 “WDM-based Fronthaul Transport v01.00”, 13
November 2020  14
[20] O-RAN.WG9.XTRP-SYN.0-v01.00 “Synchronization Architecture and 15
Solution Specification v01.00”, March 2021  16
[21] O-RAN.WG9.XTRP-MGT.0-v01 “Management interfaces for Transport 17
Network Elements v01.00”, March 2021 18
[22] O-RAN.WG4.CTI-TCP.0-v02.00, “Cooperative Transport Interface Transport 19
Control Plane Specification v02.00”, November 2020 20
 21
[23] O-RAN.WG1.Slicing-Architecture-v05.00, July 2021 22
   23
 24
IEEE references 25
[24] IEEE 802.3-2018: “IEEE Standard for Ethernet” 26
[25] IEEE 802.1Q-2018: “IEEE Standard for Local and metropolitan area 27
networks— Bridges and Bridged Networks” 28
[26] IEEE 802.1CM-2018: “Time-Sensitive Networking for Fronthaul” 29
[27] IEEE Std 1914.1TM-2019: “IEEE Standard for Packet-based Fronthaul 30
Transport Network”  31
[28] IEEE 802.3av “Physical Layer Specifications and Management Parameters for 32
10 Gb/s Passive Optical Networks”  33
[29] IEEE 802.3bk “Physical Layer Specifications and Management Parameters for 34
Extended Ethernet Passive Optical Networks”  35
[30] IEEE 802.3ca “Physical Layer Specifications and Management Parameters for 36
25 Gb/s and 50 Gb/s Passive Optical Networks” 37
[31] IEEE Std 1914.3-2018 – IEEE standard for radio over Ethernet Encapsulations 38
and Mappings  39
 40
IETF references 41
[32] IETF RFC 791: “INTERNET PROTOCOL” 42
[33] IETF RFC768: “User Datagram Protocol” 43
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        11
[34] IETF RFC1195: “Use of OSI IS-IS for routing in TCP/IP and dual 1
environments” 2
[35] IETF RFC1771: “A Border Gateway Protocol 4” 3
[36] IETF RFC2205: “Resource ReSerVation Protocol” 4
[37] IETF RFC2209: “Resource ReSerVation Protocol (RSVP) --Version 1 Message 5
Processing Rules” 6
[38] IETF RFC 2210: “The use of RSVP with IETF Integrated Services” 7
[39] IETF RFC2328: “OSPF Version 2” 8
[40] IETF RFC 2283: “Multiprotocol Extensions for BGP-4” 9
[41] IETF RFC 2475: “An Architecture for Differentiated Services” 10
[42] IETF RFC 2474: “Definition of the Differentiated Services Field (DS Field) in 11
the IPv4 and IPv6 Headers” 12
[43] IETF RFC2545: Use of BGP-4 Multiprotocol Extensions for IPv6 Inter-Domain 13
Routing”       14
[44] IETF RFC2597: “Assured Forwarding PHB Group”       15
[45] IETF RFC2598: “An Expedited Forwarding PHB” 16
[46] IETF RFC2961: “RSVP Refresh Overhead Reduction Extensions” 17
[47] IETF RFC2745: “RSVP Diagnostic Messages” 18
[48] IETF RFC2983: “Differentiated Services and Tunnels” 19
[49] IETF RFC3031: “Multiprotocol Label Switching Architecture” 20
[50] IETF RFC3032: “MPLS Label Stack Encoding”  21
[51] IETF RFC3097: “RSVP Cryptographic Authentication—Updated Message 22
Type Value” 23
[52] IETF RFC3209: “Extensions to RSVP for LSP Tunnels” 24
[53] IETF RFC3212: “Constraint-Based LSP Setup using LDP” 25
[54] IETF RFC 3215: “LDP State Machine” 26
[55] IETF RFC3246: “An Expedited Forwarding PHB (Per-Hop Behavior) 27
[56] IETF RFC3107: “Carry Label Information in BGP-4” 28
[57] IETF RFC3443: “Time To Live (TTL) Processing in Multi-Protocol Label 29
Switching (MPLS) Networks” 30
[58] IETF RFC3477: “Unnumbered Links in Resource ReSerVation Protocol - 31
Traffic Engineering (RSVP-TE)” 32
[59] IETF RFC 3478: “Graceful Restart Mechanism for Label Distribution Protocol”, 33
[60] IETF RFC3630: “Traffic Engineering (TE) Extensions to OSPF Version 2” 34
[61] IETF RFC3719: “Recommendations for Interoperability using ISIS” 35
[62] IETF RFC4090: “Fast Reroute Extensions to RSVP-TE for LSP Tunnels” 36
[63] IETF RFC4115: “A Differentiated Service Two-Rate, Three-color Market with 37
Efficient Handling of in-Profile Traffic” 38
[64] IETF RFC4182 “Removing a Restriction on the use of MPLS Explicit NULL” 39
[65] IETF RFC4303: “OSPF Extensions in Support of Generalized Multi-Protocol 40
Label Switching (GMPLS)” 41
[66] IETF RFC4206: Label Switched Paths (LSP) Hierarchy with Generalized Multi-42
Protocol Label Switching (GMPLS) Traffic Engineering (TE)” 43
[67] IETF RFC4443: “ICMPv6 (ICMP for IPv6)” 44
[68] IETF RFC4552: “Authentication/Confidentiality for OSPFv3” 45
[69] IETF RFC4558: “Node-ID Based Resource Reservation Protocol (RSVP) 46
Hello” 47
[70] IETF RFC4561: “Record Route Object (RRO) Node-Id Sub-Object” 48
[71] IETF RFC4594: “Configuration Guidelines for DiffServ Service Classes” 49
[72] IETF RFC4364: “BGP/MPLS IP Virtual Private Networks (VPNs)” 50
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        12
[73] IETF RFC4684: “Constrained Route Distribution for BGP/MPLS IP VPNs” 1
Computation Element Protocol (PCEP)” 2
[74] IETF RFC4760: “Multiprotocol Extensions for BGP-4” 3
[75] IETF RFC4875: “Extensions to Resource Reservation Protocol – Traffic 4
Engineering for Point to Multipoint TE Label Switched Paths”  5
[76] IETF RFC5036: “LDP Specification” 6
[77] IETF RFC5130: “A Policy Control Mechanism in IS-IS Using Administrative 7
Tags” 8
[78] IETF RFC5283: “LDP Extension for Inter-Area Label Switched Paths (LSPs)” 9
[79] IETF RFC5291: “Outbound Route Filtering Capability for BGP-4” 10
[80] IETF RFC5292 “Address-Prefix-Based Outbound Route Filter for BGP-4” 11
[81] IETF RFC5302 “Domain-Wide Prefix Distribution with Two-Level IS-IS” 12
[82] IETF RFC5303: “Three-Way Handshake for IS-IS Point-to-Point Adjacencies” 13
[83] IETF RFC5304: “IS-IS Cryptographic Authentication” 14
[84] IETF RFC 5305: “IS-IS Extensions for Traffic Engineering” 15
[85] IETF RFC5308: “Routing IPv6 with ISIS” 16
[86] IETF RFC5310: “IS-IS Generic Crypto Authentication” 17
[87] IETF RFC5329: “Traffic Engineering Extensions to OSPF Version 3”  18
[88] IETF RFC5340: “OSPF for IPv6” 19
[89] IETF RFC5420: “Encoding of Attributes for MPLS LSP Establishment Using 20
Resource Reservation Protocol Traffic Engineering (RSVP-TE)” 21
[90] IETF RFC5440: “Path Computational Element (PCE) Communications Protocol 22
(PCEP)” 23
[91] IETF RFC5443: “LDP IGP Synchronization” 24
[92] IETF RFC5496: “The Reverse Path Forwarding (RPF) Vector TLV” 25
[93] IETF RFC5512: “The BGP Encapsulation Subsequent Address Family Identifier 26
(SAFI) and the BGP Tunnel Encapsulation Attribute” 27
[94] IETF RFC5561: “LDP capabilities”  28
[95] IETF RFC5838: “Support of Address Families in OSPFv3” 29
[96] IETF RFC6232: “Purge Originator Identification TLV for IS-IS” 30
[97] IETF RFC6388: “Label Distribution Protocol Extensions for Point-to-Multipoint 31
and Multipoint-to-Multipoint Label Switched Paths”  32
[98] IETF RFC6391: “Flow-Aware Transport of pseudowires over an MPLS Packet 33
Switched Network”  34
[99] IETF RFC6512: “Using Multipoint LDP When the Backbone has No Route to 35
the ROOT”  36
[100] IETF RFC6513: “Multicast in MPLS/BGP IP VPNs” 37
[101] IETF RFC6514: “BGP Encodings and procedures for multicast in MPLS/BGP 38
IP VPNs” 39
[102] IETF RFC7432: “BGP MPLS-Based Ethernet VPN” 40
[103] IETF RFC7471: “OSPF Traffic Engineering (TE) Metric Extensions” 41
[104] IETF RFC7534: “Inter-Area Point to Multipoint” 42
[105] IETF RFC7570: “Label Switched Path (LSP) Attribute in the Explicit Route 43
Object (ERO)” 44
[106] IETF RFC7752: “BGP Link State (BGP-LS)” 45
[107] IETF RFC7761: “Protocol Independent Multicast” 46
[108] IETF RFC7810: “IS-IS Traffic Engineering (TE) Metric Extensions” 47
[109] IETF RFC8029: “Detecting Multiprotocol Label Switched (MPLS) Data-plane 48
Failures” 49
[110] IETF RFC8200: “Internet Protocol, Version 6 (IPv6) Specification” 50
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        13
[111] IETF RFC8214: “Virtual Private Wire Service Support in Ethernet VPN” 1
[112] IETF RFC8231: “Path Computational Element Communications Protocol 2
(PCEP) Extensions for Stateful PCE” 3
[113] IETF RFC8277: “BGP and Labeled Address Prefixes” 4
[114] IETF RFC8287: “Label Switched Path (LSP) Ping/Traceroute for Segment 5
Routing (SR) IGP-Prefix and IGP-Adjacency Segment Identifiers (SIDs) with 6
MPLS Data Planes” 7
[115] IETF RFC8317: “Ethernet-Tree (E-Tree) Support in Ethernet VPN (EVPN) and 8
Provider Backbone Bridging EVPN (PBB-EVPN)” 9
[116] IETF RFC8370: “Techniques to Improve the Scalability of RSVP-TE 10
Deployments”  11
[117] IETF RFC8395: “Extension to BGP-Signaled pseudowires to support flow-12
aware transport labels” 13
[118] IETF RFC8402: “Segment Routing Architecture” 14
[119] IETF RFC8476: “Signalling Maximum SID depth using OSPF” 15
[120] IETF RFC8491: “Signalling Maximum SID depth using IS-IS” 16
[121] IETF RFC8571: “BGP – Link State (BGP-LS) Advertisement of IGP Traffic 17
Engineering Performance Metric Extensions” 18
[122] IETF RFC8577: “Signaling RSVP-TE Tunnels on a Shared MPLS Forwarding 19
Plane” 20
[123] IETF RFC8660: “Segment Routing with MPLS Dataplane” 21
[124] IETF RFC8661: “Segment Routing MPLS Interworking with LDP” 22
[125] IETF RFC8664: “PCEP Extensions for Segment Routing” 23
[126] IETF RFC8666: “OSPFv3 Extensions for Segment Routing” 24
[127] IETF RFC8667: “IS-IS Extensions for Segment Routing” 25
[128] IETF RFC8754: “IPv6 Segment Routing Header (SRH)” 26
[129] IETF RFC8814: “Signaling Maximum SID Depth (MSD) Using the BGP – Link 27
State” 28
[130] Draft-ietf-bess-bgp-multicast-controller-06: “Controller based BGP Multicast 29
signalling” 30
[131] Draft-ietf-bess-evpn-bum-procedure-updates-08: “Updates on EVPN BUM 31
procedures” – submitted to IESG for publication  32
[132] Draft-ietf-bess-evpn-vpws-fxc-02: “EVPN VPWS Flexible Cross-Connect 33
Service”  34
[133] Draft-ietf-bess-evpn-mh-pa-04: “EVPN multi-homing port-active load-35
balancing”  36
[134] Draft-ietf-bess-evpn-prefix-advertisement-11: “IP prefix Advertisements in 37
EVPN” 38
[135] Draft-ietf-bess-evpn-pref-df-07: “Preference-based EVPN DF Election” 39
[136] Draft-ietf-bess-srv6-services-07: “SRv6 BGP based Overlay services” 40
[137] Draft-ietf-idr-bgp-ls-segment-routing-ext-18: “BGP Link-State Extensions for 41
Segment Routing” 42
[138] Draft-ietf-lsr-flex-algo-15: “IGP Flexible Algorithm” 43
[139] Draft-ietf-mpls-ri-rsvp-frr-10: “Refresh-interval independent FRR facility 44
protection”  45
[140] Draft-ietf-pim-sr-p2mp-policy-02: “Segment Routing Point-to-Multipoint Policy” 46
[141] Draft-ietf-rtgwg-segment-routing-ti-lfa-06: “Topology Independent Fast Reroute 47
using Segment Routing” 48
[142] Draft-ietf-pce-binding-label-sid-08: “Carrying Binding Label/Segment-ID in 49
PCE-based Networks” 50
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        14
[143] Draft-ietf-lsr-isis-srv6-extensions-14: “IS-IS Extensions to support SR over IPv6 1
dataplane” 2
[144] Draft-ietf-pce-segment-routing-IPv6-08: “PCEP Extensions for Segment 3
Routing leveraging the IPv6 data plane” 4
[145] draft-ietf-pce-segment-routing-policy-cp-04: “PCEP extension to support 5
Segment Routing Candidate Path 6
[146] draft-ietf-spring-segment-routing-policy-11: “Segment Routing Policy 7
Architecture” 8
[147] RFC 8986 : “SRv6 Network Programming” 9
[148] Draft-ietf-lsr-ospfv3-srv6-extensions-02: “OSPFv3 Extensions for SRv6” 10
[149] Draft-ietf-idr-bgpls-srv6-ext-07: “BGP Link State extensions for IPv6 Segment 11
Routing (SRv6)” 12
[150] Draft-ietf-6man-spring-srv6-oam-10: “Operations, Administration, and 13
Maintenance (OAM) in Segment Routing Networks with IPv6 Data plane 14
(SRv6)” 15
[151] Draft-ietf-mpls-ri-rsvp-frr-10: “Refresh-interval Independent FRR Facility 16
Protection” 17
 18
Others 19
[152] NGMN “5G RAN CU-DU network architecture, transport options and 20
dimensioning, version 1.0 12 April 2019” 21
[153] MEF 61.1: “IP Service attributes” 22
[154] MEF 10.3: “Ethernet Service attributes” 23
[155] MEF 6.2: “EVC service definition” 24
[156] Broadband Forum TR-101 “Migration to Ethernet-Based Broadband 25
Aggregation” 26
[157] Broadband Forum TR-156 “Using GPON Access in the context of TR-101“ 27
[158] ITU-R M.2083: “IMT Vision – framework and overall objectives of the future 28
development of IMT for 2020 and beyond. 29
[159] ITU-T GSTR-TN5G – Transport network support of IMT 2020/5G  30
[160] ITU-T G.Sup.66 “5G wireless Fronthaul requirements in a passive optical 31
network context” 32
[161] ITU-T G.9807 series “10-Gigabit-capable symmetric passive optical network”, 33
ITU-T G.989 series “40-Gigabit-capable passive optical networks (NG PON2)“, 34
On-going work ITU-T G.HSP 35
[162] ITU-T G.8271 “Time and phase synchronization aspects of telecommunication 36
networks” 37
[163] ITU-T G.8271.1 “Network limits for time synchronization in packet networks 38
with full timing support from the network” 39
[164] ITU-T G.8273.2 “Timing characteristics of telecom boundary clocks and 40
telecom time slave clocks” 41
[165] ITU-T G.8275.1 “Precision time protocol telecom profile for phase/time 42
synchronization with full timing support from the network” 43
[166] 40m Transmission of OAM mode and Polarization Multiplexing in E-band, 44
Globcom, Dec 2019. M.Hirabe, et. Al 45
[167] CableLabs “Low Latency Mobile Xhaul over DOCSIS Technology” 46
https://www.cablelabs.com/specifications/CM-SP-LLX 47
[168] CableLabs “Synchronization Techniques for DOCSIS Technology Specification” 48
https://www.cablelabs.com/specifications/CM-SP-SYNC 49
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        15
[169] CableLabs “Data-Over-Cable Service Interface Specifications 3.1, MAC and 1
Upper Layer Protocols Interface” 2
https://www.cablelabs.com/specifications/CM-SP-MULPIv3.1 3
[170] CableLabs “Data-Over-Cable Service Interface Specifications 4.0, MAC and 4
Upper Layer Protocols Interface” 5
https://www.cablelabs.com/specifications/CM-SP-MULPIv4.0 6
[171] CableLabs “Remote PHY Specification” 7
https://www.cablelabs.com/specifications/CM-SP-R-PHY 8
[172] “Study on new radio access technology: Radio access architecture and 9
interfaces” 3GPP TR 38.801 Table A-1. 10
[173] Cisco Press, MPLS and VPN Architectures, Volume 1, 420 pages, by Ivan 11
Pepelnjak, and Jim Guichard, 2001 12
[174] Cisco Press, MPLS and VPN Architectures, Volume 2, 470 pages, by Ivan 13
Pepelnjak, Jim Guichard, and Jeff Apcar, 2003 14
[175] O’Reilly, MPLS in the SDN Era, 890 pages, by Antonio Sánchez-Monge, and 15
Krzysztof Grzegorz Szarkowicz, 2015 16
[176] BBF TR-221: Technical Specification for MPLS in Mobile Backhaul Networks, 17
99 pages, Oct 2011 18
[177] BBF TR-221, Amd.1: Technical Specifications for MPLS in Mobile Backhaul 19
Networks, 24 pages, Nov 2013 20
[178] BBF TR-221, Amd.2: Technical Specifications for MPLS in Mobile Backhaul 21
Networks, 22 pages, Sep 2017 22
[179] ETSI GR mWT 012 V1.1.1 (2018-11): 5G Wireless Backhaul/Xhaul 23
[180] Microwave and millimeter-wave technology overview and evolution, Workshop 24
on Evolution of Fixed Service in Backhaul support of IMT 2020 / 5G, Geneva, 25
29 April 2019, https://www.itu.int/en/ITU-R/study-26
groups/workshops/fsimt2020/Pages/default.aspx 27
 28
 29
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        16
 1
 2
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        17
5 Definitions and abbreviations 1
5.1 Definitions 2
The key words “SHALL”, “SHALL NOT”, “SHOULD”, “SHOULD NOT”, “MAY”, and 3
“OPTIONAL” in this document are to be interpreted as described in IETF RFC 2119 [25]. All key 4
words must be in upper case, bold text. 5
Items that are REQUIRED (contain the words SHALL or SHALL NOT) will be labelled as [Rx] 6
for required. Items that are RECOMMENDED (contain the words SHOULD or SHOULD NOT) 7
will be labelled as [Dx] for desirable. Items that are OPTIONAL (contain the words MAY or 8
OPTIONAL) will be labelled as [Ox] for optional.  9
Items, if supported, are not meant to be active at all times, but should be available for use. Their 10
state (active or not active) should be based on configuration. 11
5.2 Abbreviations 12
Abbreviations defined in this document take precedence over the definition of 3GPP  13
 14
Abbreviations Meaning or explanation
3GPP Third Generation Partnership Project – Standards Development Organization
4G Fourth-generation mobile network
5G Fifth-generation mobile network
ABR Area Border Router
Amd Ammendment (term used by BBF)
API Application Programming Interfaces
ARPU Average revenue per user
AS Automated (traffic) steering
AS Autonomous System
ASBR Autonomous System Border Router
BBF Broadband Forum
BBU Baseband unit
BGP Border Gateway Protocol
BGP-LU BGP labelled unicast
BGP-LS BGP link state
BIER Bit Indexed Explicit Replication
BITS Building Integrated Timing System
C-RAN Centralized Radio Access Network
CapEx Capital expenditure
CBWRED Class based Weighted Random Early Detection
CDN Content delivery network
CDMA Code Division Multiple-Access – a mobile radio standard
COTS Commercial off the shelf
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        18
CPE Customer premises equipment
CPRI Common public radio interface
CSP Communications service provider
CSR Cell Site Router
CU Centralized unit
CUPS Control/User Plane Separation
D-RAN Distributed Radio Access Network
DB  Dynamic Bandwidth assignment
DC Data center
DiffServ Differentiated services – a quality-of-service mechanism
DSCP DiffServ Code Point
DWDM Dense Wavelength Division Multiplexing
eMBB Enhanced mobile broadband
ECMP Equal-cost multipath
EPC Evolved packet core
eCPRI Enhanced Common Radio Interface (CPRI)
eNB Enhanced Node B
ESMC Ethernet Synchronization Message Channel
EVPN Ethernet VPN
EXP Experimental
FDD Frequency division duplexing
FHG Fronthaul Gateway
FIB Forwarding Information Base
FIFO First In, First Out
FMC Fixed-mobile convergence
FMC Fixed-mobile convergence
Fronthaul Portion of the mobile network supporting O-RAN 7.2x, eCPRI, RoE or CPRI protocols
FRR Fast Re-Route
Gbps Gigabits per second
GNSS Global Navigation Satellite System (example being GPS)
GPS Global Positioning System
HLS High-level split
HSR Hub Site Router
iBGP internal Border Gateway Protocol
IEC International Electrotechnical Commission
IEEE Institute of Electrical and Electronics Engineers – Standards Development Organization
IETF Internet Engineering Task Force – Standards Development Organization
IGP Interior Gateway Protocol
IoT Internet of Things (see also mMTC)
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        19
IP Internet Protocol
IS-IS Intermediate System to Intermediate System
ISO International Standardization Organization
ITU-T International Telecommunication Union-Telecommunication – Standards Development Organization
ITU-R International Telecommunication Union-Radiocommunication – Standards Development Organization
JSON JavaScript Object Notation
L1 / L2 / L3 Layer 1 / layer 2 / layer 3 of the network protocol stack
L3VPN Layer 3 Virtual Private Network
LDP Label Distribution Protocol
LLS Low-level splits
LSDB Link state database
LSP Label Switched Path
LTE Long Term Evolution (generation of Mobile networks – see 4G)
LTE-A Long Term Evolution – Advanced
MEC Formerly Mobile Edge Compute, now Multi-Access Edge Compute
mMTC Massive machine type communications
MIMO Multiple-Input Multiple-Output (number of antennas)
MNO Mobile Network Operator
MP-BGP Multi-protocol Border Gateway Protocol
MPLS Multiprotocol Label Switching
MSD Maximum Segment Depth
MVNO Mobile virtual network operator
NETCONF Network Configuration Protocol
NLRI Network Layer Reachability Information
NFV Network functions virtualization
NGFI Next-generation Fronthaul interface
NR New radio
NSI Network Slice Instance
NSSI  Network Subnet Slice Instance
O-CU Open Central Unit
O-DU  Open Distributed Unit
O-RU Open Radio Unit
OAM Operations, administration, and maintenance
ODN On-demand next hop
ODN Optical Distribution Network
OLT Optical Line Termination
ONU Optical Network Unit
OpEx Operational expenses
ORF Outbound Route Filter
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        20
OSPF Open Shortest Path First
OTN Optical Transport Networking
P Provider (router)
PCC Path computation client
PCE Path computation element
PCEP Path computation element protocol
PE Provider edge (router)
PIM Protocol for IP Multicast
PON Passive Optical Network
PTP Precision Time Protocol
QoS Quality of Service
RAN Radio access network
RE Radio equipment
REC Radio equipment controller
RoE Radio over Ethernet
RRO Record Route Object
RRU Remote Radio Unit
RSVP Resource Reservation Protocol
RSVP-TE Resource Reservation Protocol – Traffic Engineering
RT Route Target
RTT Round-trip times
RU Remote (radio) unit
SD-WAN Software-defined wide area network
SDH Synchronous Digital Hierarchy – a digital communications system
SDN Software-defined networks
SID Segment Identifier
SLA Service level agreement
SMB Small and medium business
SONET Synchronous Optical Networking – a digital communications system
SPF Shortest Path First
SR Segment Routing
SR-DPM Segment Routing-data plane management
SR-TE Segment Routing – traffic engineering
SR-PCE Segment Routing – path computation element
SRH Segment Routing header
SRLG Shared risk link groups
SSU Synchronization Supply Unit
SyncE Synchronous Ethernet
T-GM Telecom – Grand Master – a PTP clock type
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        21
T-TSC  Telecom – Time Slave Clock – a PTP clock type
TAE  Time Alignment Error
TC Traffic Class
TDD Time-division duplexing – a radio communications technique
TDM Time division multiplexing
TE Traffic engineering
TE  Time Error
TI-LFA Topology-independent loop-free alternative
TNE Transport Network Equipment (an O-RAN term to denote a transport device)
TR Technical report
TTL Time to live
TWDM Time and Wavelength Domain Multiplexing
UE User equipment
URLLC Ultra-reliable low-latency communications
UPF User plane functions
VIM Virtualized infrastructure managers
VNF Virtual network function(s)
VPN Virtual private networks
VRF Virtual Routing and Forwarding
WAN Wide area network
WDM Wavelength Division Multiplexing
Xhaul Collective name for Fronthaul, Midhaul, and Backhaul
XML eXtensible Markup Language
YANG Yet another next generation – data modelling language
 1
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        22
6 5G Transport network requirements 1
For full details of the O -RAN transport  requirements, bandwidth and delay estimates of a 5G 2
network  3
see [18]. 4
  5
Requirements for the transport architecture can be characterized in following categories: 6
 7
1. Latency, Frame Loss Ratio and Bandwidth requirements for Fronthaul, Midhaul, and 8
Backhaul. 9
2. Operability requirements that include fault and performance management. 10
3. Synchronization requirements. 11
4. “ITU-T GSTP-TN5G: Transport support of IMT -2020/5G” [159] identifies the need for the 12
transport to be multi-service in nature. In addition to mobile services, the infrastructure needs 13
to support fixed line consumer and enterprise services. These services are not explicitly 14
covered in the docume nt, but the architecture must enable L2 or L3 services to be created 15
between any two edge TNEs regardless of relative position to each other in the transport 16
network. 17
5. End to end support of 4G/5G mobile infrastructure including Fronthaul / Midhaul / Backhaul.  18
6. Concurrent support for RAN deployment scenarios outlined in “ITU -T GSTP -TN5G: 19
Transport support of IMT -2020/5G” [159] running from a single cell site location. These 20
shown in Figure 6-1 are: 21
 22
a. Co-located O-CU and O-DU – O-RAN split 7.2x from cell site 23
b. Independent O-RU, O-CU, O-DU locations – O-RAN 7.2x from cell site 24
c. O-RU and O-DU integration on cell site – O-RAN split 2 from cell site 25
d. O-RU, O-DU and O-CU integration on cell sites – Split 1 from cell site 26
  27
 28
Figure 6-1: ITU-T 5G use cases 29
 30

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        23
7. Central, distributed or a mix of the two for user plane termination. Central, distributed or a 1
mix of two for 5G control plane placement. (Figure 6-2) 2
   3
 4
Figure 6-2: 5G central and distributed user plane termination 5
 6
8. Concurrent support for 4G and 5G RAN solutions running from a single cell site.  In addition 7
to the use cases outlined in earlier figures, support for 4G radios running alongside 5G radios 8
is required. Figure 6-3 illustrates the two 4G architectures that need to co -exist with the 5G 9
architectures outlined above. In the upper case, the 4G RAN infrastructure utilizes a C-RAN 10
architecture where the RRH and BBU support split 8 which is converted to 7.2x or RoE for 11
transportation across the packet access infrastructure. In the lower case, the 4G RAN 12
infrastructure uses D-RAN or split 1 architecture.    13
  14
 15
Figure 6-3: 4G use cases 16
 17
9. End to end support for 5G slicing and 5G service types. 18
 19

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        24
7 5G logical connectivity requirements  1
 The transport network needs to be very flexible as depending on the use case and the RAN design 2
each part of the physical transport network may need to support multiple slices, multiple 5G 3
services and also different 3GPP interfaces. This section covers the logical transport connectivity 4
requirements of the 5G Fronthaul, Midhaul and Backhaul components. Further details can be found 5
in O-RAN WG9.Transport Requirements document [18]. 6
7.1 Fronthaul  7
The Fronthaul infrastructure potentially needs to support: 8
• O-RAN 7.2x Fronthaul (for 5G NR) 9
• Non-ORAN Fronthaul 10
 11
7.1.1 O-RAN 7.2x Fronthaul 12
The O-RAN 7.2x is a split 7 “Low Level Split” (LLS) that runs between the O-RU and the O-DU 13
(optional more than one for network and DU based redundancy). The associated mobile interfaces 14
for the Fronthaul are the Control, User and Synchronization and Management planes. The 15
synchronization plane is covered in a separate WG-9 Timing and Synchronization Architecture and 16
solutions document [20].   17
 18
 19
 20
  21
Figure 7-1: Fronthaul O-RAN 7.2x control and user plane using an Ethernet encapsulation 22

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        25
   1
Figure 7-2: O-RAN 7.2x Hybrid Management plane  2
 3
• O-RAN 7.2x Control and User planes: These interfaces are described in O-4
RAN.WG4.CUS.0-v3.00: Control, User and Synchronization Planes [16]. These interfaces run 5
between the O-RUs and their serving O-DU.  Ethernet encapsulation is a mandatory 6
requirement and IP encapsulation is optional and applies if the transmitting and receiving nodes 7
support IP capabilities. In both cases the payload is one or more eCPRI transport headers with 8
respective application data. The latency requirements associated with 7.2x control plane and 9
user plane traffic are very low, and the bandwidth requirements are generally high but vary 10
based on the level of user data traffic being transmitted.  11
• O-RAN 7.2x Synchronization plane: This interface is described in O-RAN.WG4.CUS.0-12
v6.00: Control, User and Synchronization Planes [16]. In C-RAN architectures accurate 13
synchronization between the O-DU and O-RUs is required to support “Time Division Duplex” 14
(TDD), Carrier Aggregation (CA) using multiple O-RUs, MIMO and other processes. In an O-15
RAN Fronthaul environment using an Ethernet transport layer, protocols such as PTP and 16
SyncE are used to achieve synchronization between the O-RUs and O-DUs. For more details 17
refer to WG-9 Timing and Synchronization Architecture and solutions document [20].         18
• O-RAN 7.2x Management plane: This interface is described in O-RAN WG4.MP.0-v3.00 19
Management plane specification[17]. Two M-Plane models are defined. 20
o Hierarchical model: In this model, an O-RU is managed by one of more O-DUs. These 21
O-DUs are entirely responsible for sub-ordinate O-RUs, which means the NMS only 22
needs to interact with the O-DU level. In this mode the O-RAN 7.2x M-Plane interface 23
only runs between the O-DU and sub-ordinate O-RUs.   24
o Hybrid model: In this model, an O-RU is managed by one or more NMSs, in addition to 25
the serving O-DU. In this mode the O-RAN 7.2x M-Plane interface runs between the O-26
RUs, O-DUs and the NMS.  27
The O-RAN 7.2x M-plane uses IP/NETCONF and the basic transport requirement is end-to-end 28
IP connectivity between the O-RU and the elements managing it. IPv4 shall be supported as a 29
mandatory transport protocol for M-Plane and IPv6 support is optional.  30
 31
7.1.2 O-RAN Fronthaul logical transport requirements 32
Details of the O-RAN transport requirements are illustrated in Figure 7-1.  33
C/U-Planes:  34
1. Ethernet connectivity from O-RUs to serving O-DU with potential backup to a redundant O-35
DU. 36
2. Optional IP connectivity from O-RUs to serving O-DU with potential backup to a redundant 37
O-DU. 38

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        26
3. O-RUs and serving O-DU in close proximity to each other to meet delay criteria associated 1
with Fronthaul. It is unlikely the Fronthaul components (ie O-RUs and serving O-DU) will 2
extend beyond the access transport network. 3
 4
S-Plane: 5
1. This is covered in the “WG-9 Timing and Synchronization Architecture and Solutions 6
document [20]. 7
 8
M-Plane 9
Details of the O-RAN M-Plane transport running in hybrid mode are illustrated in Figure 7-2. 10
 11
1. IP connectivity allowing NMS to communicate with O-DUs and O-RUs if running the hybrid 12
model. 13
2. IP connectivity allowing serving O-DU to communicate with all its sibling O-RUs if running 14
in either hybrid or hierarchical models. 15
 16
Note: In addition to the 7.2x management plane other management components may need to 17
communicate with entities at the cell site. For example, remote monitoring of sensors and actuators 18
in the cell site. These are not explicitly covered in this document, but the specified transport 19
architecture can cater for scenarios, where management and monitoring, are based on either 20
Ethernet or IP connectivity. 21
7.2 Non-ORAN Fronthaul  22
Legacy Fronthaul scenarios are those C-RAN use cases where the Fronthaul traffic is transported 23
over a packet switch network while not using the O-RAN compliant encapsulation protocol that 24
supports 7.2x split. The two most likely legacy Fronthaul scenario in a packet switched transport 25
network are: 26
 27
• eCPRI based C-RAN solutions: using eCPRI encapsulation protocol not compliant to O-28
RAN WG4 CUS specifications [16]. i.e., a non O-RAN 7.2x split. 29
• RoE based C-RAN solutions:  CPRI encapsulated by the RoE protocol.  30
 31
7.2.1 eCPRI based C-RAN solutions 32
An operator may choose a packet-based C-RAN Fronthaul architecture that is not O-RAN 7.2x 33
compliant. In this case the RU and DU uses eCPRI as the packet encapsulation protocol to 34
packetize the Fronthaul data but implements a non-O-RAN compliant radio message protocol to 35
support a different function split.   36
 37
Following 3GPP recommendation [7], the possible function splits may include 38
• Option 6, split between MAC and PHY layers 39
• Option 7.1, 7.2, or 7.3, splits within PHY layer 40
• Option 8, split between radio and PHY layer 41
 42
7.2.2 Radio over Ethernet (RoE) based C-RAN solutions  43
In this scenario an operator has an existing radio deployed at the cell site and the operator intends to 44
convert these radios to support a C-RAN Fronthaul architecture whilst implementing their 5G RAN 45
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        27
infrastructure.  In this case it is likely the legacy equipment (RRH and BBU) only supports an 1
optical CPRI interfaces.  To implement a C-RAN Fronthaul architecture in a packet switched 2
transport network the CPRI needs to be converted to an Ethernet frame at the cell site, transported 3
over the access network as packet and then get converted back to CPRI at the hub for processing 4
from the BBU. In this scenario a component called a RoE mapper is used to perform a CPRI to 5
Ethernet frame conversion at the cell site and an Ethernet frame to CPRI conversion at the location 6
where the BBU is located.   7
7.2.2.1 RoE Mapper transport 8
The design, implementation, management, and interface from the RoE mapper to the legacy 9
equipment is not in the scope of WG-9 but the expectation is the RoE mapper will present either O-10
RAN 7.2x compliant Ethernet or Ethernet IP packets or IEEE 1914.3 Radio over Ethernet packets 11
[31] to the transport network. At this time O-RAN has not defined a CPRI to O-RAN 7.2x 12
conversion capability so implementations out in the market are based on CPRI to Radio Over 13
Ethernet.  14
 15
It is then the responsibility of the transport network to transport these packets to their destination 16
with appropriate characteristics for the legacy connection to function.  17
7.2.2.2 Radio over Ethernet  18
Radio over Ethernet (RoE) is defined in IEEE Std 1914.3-2018 – IEEE standard for radio over 19
Ethernet Encapsulations and Mappings [31]. Two mapping techniques for supporting CPRI to 20
packet conversion are defined.    21
   22
Structure-agnostic RoE mapper  23
The structure-agnostic RoE mapper captures bits from one end of a constant bit rate link, packetizes 24
the bits into Ethernet frames, sends the frames across the network, and then recreates the bit stream 25
at the far end of the link. While the constant bit-rate data stream is commonly encoded with the 26
CPRI protocol, it could also be of any other protocol, provided it is within the range of data rates 27
supported by that equipment.  28
The structure-agnostic RoE mapper has two main modes of operation: 29
 30
• Tunnelling mode or type 0 works as a simple Ethernet tunnel. It does not remove any line 31
coding bits and does not interpret any special characters (such as K-characters). If the source 32
data is 8b/10b-encoded, the 10-bit symbols present on the line will be tunnelled by this RoE 33
mapper as 10 bits of data. Similarly, 66-bit symbols will be sent for 64b/66b-encoded data as 34
66 bits of data. The entire stream is simply packetized.  35
 36
• Line-coding-aware mode or type 1 removes the line coding bits such as for CPRI encoded 37
with 8b/10b or 64b/66b. If the source data is 8b/10b-encoded, the 8-bit symbols present on 38
the line will be tunnelled by this RoE mapper as 8 bits of data. Similarly, if the source data is 39
64b/66b encoded, the 64-bit symbols present on the line will be tunnelled by this RoE 40
mapper as 64 bits of data. To allow the restoration of the 10-bit or 66-bit symbols at the de-41
mapper, the RoE mapper/de-mapper must have some awareness of the protocol it is 42
mapping/de-mapping; the locations of the special characters must be known a priori relative 43
to an event that is indicated in the RoE frame.  44
 45
Structure-aware RoE mapper  46
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        28
In this mode only the useful information in the CPRI stream is packetized into the RoE frames and 1
different types of data (such as control words and data words) can be encapsulated into separate 2
frames for prioritized processing. Those unused fields within the CPRI stream will be ignored thus 3
bring the benefit of Fronthaul BW reduction. This mode requires full knowledge of the protocol 4
layout of the CPRI and due to the proprietary nature of CPRI will require input from the radio 5
vendor.   6
7.2.3 Non O-RAN Fronthaul logical transport requirements  7
The main use case identified by operators for non O-RAN fronthaul is 4G equipment that uses 8
CPRI between the RRH and the BBU (see section 8.4). To support this over a packet based 9
fronthaul network, the CPRI stream needs to be packetized as it ingresses the packet network and 10
the CPRI stream reconstructed as it egresses the packet network. Bandwidth, delay and jitter 11
characteristics in the packet network clearly depend on the technology used to perform this 12
function, which is outside the scope of O-RAN. To provide some guidance on support of this traffic 13
it has been assumed that it is presented as Ethernet and has similar delay and jitter requirements as 14
7.2x fronhaul traffic. 15
7.3 Midhaul logical transport requirements   16
3GPP TS 38.401 [7] defines the de-aggregated RAN, it’s characteristics and outlines the F1 -U, F1-17
C and E1 interfaces . Figure 7-3, taken from 3GPP TS 38.401 illustrates the components and 18
interfaces. The Midhaul transport infrastructure is responsible for supporting these interfaces.  19
 20
 21
 22
Figure 7-3: Deaggreated gNB 23
 24
The characteristics of a disaggregated gNB are:  25
• A gNB may consist of a gNB-CU-CP, multiple gNB-CU-UPs and multiple gNB-DUs 26
• DUs and CU-UPs are connected to one CU-CP via the E1 interface or the F1-C interface 27
• DUs can connect to multiple CU-UPs 28
• Multiple CU-UPs can connect to one CU-CP 29
• For resiliency reasons, DUs and CU-UPs may connect to multiple CU-CPs  30
 31
The 3GPP interface associated with O-DU and O-CU communication is the F1 interface. It has a 32
control (F1-C) and data (F1-U) plane component.  33
 34
Note: W1 interface is the 4G equivalent of the F1 interface. It will not be discussed further in this 35
document as its characteristics are expected to be like the 5G equivalent.  36
 37

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        29
The 3GPP interfaces associated with intra O-CU communications is the E1 interfaces. It runs 1
between the gNB-CU-CP and a gNB-CU-UP. It allows these two components to run as separate 2
entities and potentially in different locations.     3
7.3.1 Overall Midhaul logical transport requirements  4
1. Control Plane:  5
a. Multi-point at the IP interfaces level (IPv4 or IPv6) between O-CU-CP and multiple 6
O-DUs (F1-C interface).  7
b. Multi-point at the IP interfaces level (IPv4 or IPv6) between O-CU-CP and multiple 8
O-CU-UPs (E1 interface).  9
2. Data Plane: Multi-point at the IP interfaces level (IPv4 or IPv6) between O-CU-UP and 10
multiple O-DUs (F1-U interface).  11
3. IP connectivity between O-CUs for Xn interface. 12
4. Some operators may wish to run the user plane interface (F1-U) separately from the control 13
plane interfaces (E1 and F1-C). 14
5. Some operators may wish to treat Midhaul and Backhaul as a single logical network.  15
6. Some operators may wish to treat Midhaul and Backhaul as discrete logical networks. 16
7.4 Backhaul logical transport requirements  17
Figure 7-4 shows components and the 3GPP interfaces in the mobile Backhaul. It has a control 18
plane and user plane component. It is not uncommon to see the control plane and the data plane 19
divided into separate closed user groups  (VPNs) at the transport layer to ensure a clear demarcation 20
between customer user data and the 3GPP control plane.   21
 22
 23
 24
Figure 7-4: 5G Backhaul components and interfaces Source: Adapted from 3GPP TS 23.501 25
v6.4.0(2020-03): System Architecture for 5G [1] with control plane / user plane shading added 26
by document authors. 27
 28
The 5G 3GPP interfaces associated with Backhaul are:  29
  30
• N1 interface is a logical control plane interface between the mobile core network and the 31
UE. From a physical perspective it flows via the RAN through the Backhaul infrastructure 32
to the AMF. It is a signalling interface between the UE and the AMF.  33
  34
• N2 interface supports control plane signalling between RAN and 5G core. It is primarily 35
concerned with connection management, UE context and PDU session management, and UE 36
mobility management. In addition, Non -Access Spectrum (NAS) signalling between the UE 37

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        30
and the AMF  is transported over the N2 connection for that UE. This signalling includes 1
information regarding access control, authentication and authorization, and session 2
management procedures.  3
 4
• N4 Interface is the bridge between the control plane and the user plan e of the 5GC. It runs 5
between the SMF and the UPF and is responsible for conveying policy rules regarding 6
policy handling, forwarding and usage reporting to the UPF.  7
 8
• N3 interface is the user plane interface between the O -CU component of the (gNB) and the  9
initial UPF.   10
 11
• N9 interface is a user plane interface than runs between two UPFs. (i.e. an intermediate 12
UPF and the UPF session anchor). 13
  14
7.4.1 Overall Backhaul logical transport requirements 15
1. Control Plane: Multi-point at the IP interfaces level (IPv4 or IPv6) between O-CU, UPF and 16
5GC components (N1, N2, N4, Xn-c).  17
2. User Plane: Multi-point at the IP interfaces level (IPv4 or IPv6) between O-CU→UPF 18
(N3), UPF→UPF (N9) and O-CU→O-CU (Xn-u). 19
3. Some operators may wish to run the Backhaul user plane (N3/N9) separate from the 20
Backhaul control plane (N1/N2/N4). 21
4. Some operators may wish to treat Midhaul and Backhaul as a single logical network. 22
5. Some operators may wish to treat Midhaul and Backhaul as discrete logical networks. 23
  24
8 Operator use cases  25
Operators have indicated the following use cases are of interest.  Any combination of these 26
scenarios may apply in practical deployments though they are described individually.  Unless 27
otherwise stated, the eCPRI traffic herein is O-RAN compliant and presents the data according to 28
O-RAN Open Fronthaul CUS plane and management plane specifications [16][17].  29
 30
Multicast use cases are deferred to section 15.1, after logical network and services have been 31
described, so that they can be provided with better background. 32
 33
Before describing the individual deployment scenarios, it is worth clarifying the terminology of C-34
RAN vs. D-RAN. In a 5G Distributed RAN (D-RAN) architecture, the O-CU, O-DU, and O-RU all 35
reside at the cell site (Figure 8-1). 36
 37
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        31
 1
Figure 8-1 5G O-RAN D-RAN architecture  2
 3
5G Centralized RAN (C-RAN) architectures splits the radio components into discrete components 4
which can be located in different locations. In 5G O-RAN model (Figure 8-2), operators may decide 5
to only place the O-RU at the cell site, and centralize the O-DUs or the O-DUs together with O-CU 6
in a central location. The other alternative can be represented by locating the O-RUs and O-DUs at 7
the cell-site and centralizing the O-CUs in a location farther away. 8
 9
 10
 11
 12
Figure 8-2 5G O-RAN C-RAN architectures 13
 14
When considering a C-RAN architecture there are both positive and negative impacts. On the 15
positive side, it can increase component efficiency by pooling RAN elements in a centralised 16
location and improve co-ordination between the radio component. On the negative side a C-RAN 17
architecture can significantly increase the bandwidth required if the Fronthaul protocols are 18
traversing the transport network.  19
 20
Note: The representation of the access and aggregation networks in the use case figures below are 21
not to scale. In scenarios where a Fronthaul component exists, the access network will be 22
geographically constrained due to delay requirements of the Fronthaul protocols. In contrast, the 23
aggregation network in the figures can cover much greater areas due to more relaxed delay 24
requirements of the 5G Midhaul and Backhaul protocols.   25
8.1 Scenario 1: C-RAN architecture with collocated O-DU and O-CU  26
As illustrated in Figure 8-3 for this scenario, O-DU and O-CU are collocated at a Hub site, therefore 27
the Midhaul traffic between O-DU and O-CU is local and not going through the transport network. 28
The eCPRI traffic from the 7.2x split from the O-RU, is transported by a packet switched access 29

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        32
transport network to the C-RAN Hub. At the other end of the C-RAN Hub, the aggregation 1
transport network transports the Backhaul traffic to the mobile core. 2
 3
The O-RU can be a new NR radio as well as a legacy LTE radio, as long as they are made to have 4
the O-RAN compliant Fronthaul interface with 7.2x function split.  5
 6
The Xn traffic that performs the inter-gNB coordination is considered as part of Midhaul traffic, but 7
it will not reach to the Core. Instead, it is routed to another O-CU, either within the same Hub site 8
or between the Hub sites, via the aggregation transport network. 9
 10
There is inter-connection between the access and aggregation transport networks for the passing 11
through services such as management traffic.  In some cases, the two transport networks can share a 12
same edge router that naturally completes the connection.      13
 14
 15
Figure 8-3: Scenario 1 – C-RAN architecture with collocated O-DU and O-CU  16
8.2 Scenario 2: C-RAN architecture with collocated O-RU and O-DU  17
In this scenario, both O-RU and O-DU are deployed at the cell site and O-CU is located at the Hub 18
site.  By 3GPP standard, the O-DU and O-CU is split by the functional split option 2 and connected 19
with F1 interface, which is transported by the access transport network that connects the cell site 20
and the Hub site.  The aggregation transport network carries the N2/N3 Backhaul traffic to the 5G 21
core.  22
 23
O-RU and O-DU may be collocated or be in close proximity around the cell site.  In both cases, O-24
RU and O-DU communicate to each other by the Fronthaul interfaces via the Cell Site Router 25
(CSR), which is located at the cell site and is considered as part of access transport network, as 26
shown in Figure 8-4   27
 28

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        33
 1
Figure 8-4: Scenario 2 – C-RAN architecture with collocated O-RU and O-DU 2
 3
NOTE: In this scenario the location of the CSR, O-DU and O-RU relative to each other may be 4
close or in some scenarios more distant. For example, within a stadium environment, the O-DU 5
could be centralised and the O-RUs distributed around the stadium. In all cases consideration needs 6
to be given to the low latency requirements associated with the Fronthaul interfaces.   7
8.3 Scenario 3: C-RAN architecture with coexistence of legacy Backhaul 8
traffic  9
This scenario is an extension of Scenario 1, or Scenario 2, where 3G / 4G D-RAN deployments 10
coexist with a 5G C-RAN deployment. The access transport network thus carries the Backhaul 11
traffic from LTE (S1) or 3G UMTS (lu/lub) in addition to the Fronthaul traffic from Scenario 1, or 12
Midhaul traffic from Scenario 2, as illustrated in Figure 8-5 and Figure 8-6 respectively.   13
 14
The aggregation transport network is responsible for carrying the Backhaul traffic for both new and 15
legacy mobile services.  16
  17
 18
Figure 8-5 Scenario 3a – 7.2x interface combined with legacy Backhaul at access layer 19
 20
 21

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        34
 1
 2
 3
 4
Figure 8-6: Scenario 3b – Midhaul combined legacy Backhaul at access layer  5
 6
These mixed D-RAN and C-RAN scenarios are important for the brown field deployment. 7
 8
8.4 Scenario 4: C-RAN architecture with coexistence of legacy Fronthaul 9
traffic    10
In this scenario, shown in Figure 8-7, both the 4G and 5G networks utilise a C-RAN Fronthaul 11
deployment in the access transport network. In this scenario it is assumed the 4G radio 12
infrastructure consists of RRHs communicating with their serving BBU using CPRI. To migrate 13
these services to a packet based Fronthaul, the native CPRI is converted to packets as it enters the 14
packet infrastructure and converted from packets back to native CPRI as it egresses the packet 15
infrastructure. This scenario assumes the CPRI to packet conversion function is performed by an 16
RoE Mapper using the IEEE 1914.3 standard [31]. The RoE mapper is out of scope of this 17
specification. The impact on the access transport network is it carries 7.2x and RoE fronthaul 18
traffic.   19
  20
In the Backhaul network, S1 from LTE is transported together with NR Backhaul N2/N3. 21
 22

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        35
  1
 2
Figure 8-7: Scenario 4 – C-RAN architecture with coexistence of legacy Fronthaul traffic 3
 4
This scenario is important to support migration plan from 4G to 5G, such as the NSA architecture. 5
 6
8.5 Scenario 5: C-RAN architecture with further split of O-DU and O-CU 7
This is a C-RAN architecture with two RAN splits with the O-RUs, O-DUs, and O-CUs hosted at 8
separate locations. The O-DU is placed at C-RAN Hub site closer to the cell sites for shorter latency 9
and the O-CU is more centralized at a different location as shown in Figure 8-8. The Midhaul 10
traffic, identified as F1 from the 3GPP option 2 split, is carried by an addition transport network 11
segment (pre-aggregation transport network) that connects the two hub sites. Similar to other use 12
cases, the Backhaul traffic is transported by the aggregation transport network.   13
 14
 15
Figure 8-8: Scenario 5 – Dual split C-RAN architecture  16
8.6 Scenario 6: C-RAN architecture with local breakout  17
In supporting URLLC to reduce the data plane latency, or fixed wireless application to offload large 18
amounts of user data locally, the User Plane Function (UPF) breakouts before the aggregation 19
network.  In this case the aggregation network only carries the control plane traffic N2 (Figure 8-9). 20
 21

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        36
 1
Figure 8-9: Scenario 6 – C-RAN architecture with local user plane breakout 2
8.7 Scenario 7: Transport slicing  3
An O-RAN wide initiative to develop end-to-end O-RAN slicing solutions has started, led by WG-4
1. The architecture and phasing are defined in O-RAN.WG1.Slicing-Architecture-v05.00 [23] and 5
is based on operator use cases and the slicing capability of the O-RAN components. This operator 6
scenario will now track the phasing and slicing use cases developed by WG-1.  7
 Revision 2 of this document now includes Annex F: Transport network slicing solution for WG1 8
Slicing phase 1 (informational), which define an example transport solution for slice phase 1 9
contained in O-RAN.WG1.Slicing-Architecture-v05.00 [23]. 10
From the operator use case given in previous subsection, it is observed that the transport network, 11
especially the access transport network, may experience all types of transport flows simultaneously 12
when operators have the need to engage mixed use cases in their deployment. Possible transport 13
service range widely with:        14
 15
• Fronthaul, Midhaul, and Backhaul 16
• NR, legacy LTE and legacy UMTS 17
• Control plan, User plane, and Management plane 18
• To support transport operation for different operators  19
• To support different types of end-to-end services or applications (such as URLLC and 20
eMBB) 21
   22
Each of them may be multiplexed into a commonly shared transport network with largely different 23
transport requirements, which include latency, throughput, transmission reliability. 24
 25
To reduce complexity and manage network resource more efficiently, these transport flows can be 26
classified into transport slices according to common service requirements. One example is shown in 27
Table 1. 28
   29
 30
Transport
Slices
Description Transport flows Transport BW Transport
timing
sensitivity
Transport
reliability
TS 1 Fronthaul 7.2x CUS-plane,
RoE
High High High

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        37
TS 2_1 Data plane for
Backhaul of
URLLC service of
Operator A
F1-U, S1-U, N3,
X2/Xn-U
Medium High High
TS 2_2 Data plane for
Midhaul,
Backhaul of
Operator B
F1-U, S1-U, N3,
X2/Xn-U
Medium Medium High
TS 3 Control plane for
Midhaul,
Backhaul,
Management
plane
7.2x M-Plane,
F1-C, S1-C, N2,
X2/Xn-C,
Management
Low Low Low
 1
Table 1 Transport slicing example 2
 3
The above transport slice may further split into more sub-slices if necessary, for different end-to-4
end user applications or different operators depending on separate needs of the priority, latency, or 5
bandwidth.    6
 7
These transport slices are designed to meet the objectives:  8
• Provide the transport elements to support network slicing. As part of network sub-network 9
instances, the transport network interfaces with other network segments by the transport 10
slices. 11
• Allow flexible configuration to efficiently support adding/deleting/reconfiguring slices and 12
services 13
• Support protection/isolation/prioritization mechanisms to minimize inter-slice effects 14
• Support monitoring/reporting the slice KPIs  15
  With reference to the networ k slicing use cases described in WG1 network slicing specification 16
document [23],  Figure 8-10 presents a transport realization example in suppor ting the phase 1 17
scope summarized in Annex F: Transport network slicing solut ion for WG1 Slicing phase 1 18
(informational). This is based on the dual split transport architecture (scenario 5) combined with 19
scenario 3b described in previous sections. The purpose of this example is to illustrate a practical 20
scenario of traffic distribution in a slicing transport network. 21
  22
 23
 24
Figure 8-10:  Transport based on dual split architecture  25
 26

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        38
The TNEs in the figure are defined as follows:  1
• CSR:  Cell Site Router, aggregating traffics from collocated O-RUs 2
• HSR:  Hub Site Router, aggregating traffics from multiple cell sites 3
• HSR-F: Hub Site Router that distribute fronthaul traffics to O-DUs  4
• HSR-B/M:  Hub Site Router that aggregate Backhaul or Midhaul traffics 5
• Agg-HSR:  Hub Site Router at centralized O-CU site that aggregate the traffic from multiple 6
O-DU hub sites 7
• Core-HSR: Hub Site Router at Core site that aggregate traffic from multiple O-CU hub sites 8
 9
The logical link across the Access transport network between cell site and C -RAN hub is seen with 10
complex traffic types:  fronthaul, midhaul , and backhaul are mixed. Specifically, following traffic 11
flows are jointly transported cross the transport: 12
 13
UP:  7.2x C/U-P, S1-U, F1-U, X2/Xn-U 14
CP:  S1-C, F1-C, X2/Xn-C 15
MP: 7.2x M-P, O1, E2 16
 17
At the Pre -aggregation transport network, there is no presence  of the fronthaul anymore. Those 18
traffics are merged and carried by the network  19
 20
UP:  S1-U, F1-U, X2/Xn-U 21
CP:  S1-C, F1-C, X2/Xn-C 22
MP: O1, E2 23
 24
At the aggregation transport network, only backhaul traffic are distributed to UPFs that may be 25
geographically separated for eMBB and mMTC/NB-IOT: 26
 27
UP:  N3, N9, S1-U 28
CP:  N2, S1-C 29
MP: O1, E2 30
 31
The transport solutions provided in this specification, as described in section 17 and Annex F: 32
Transport network slicing solution for WG1 Slicing phase 1 (informational) are expected to provide 33
adequate toolbox to achieve prioritization, isolation, QoS, and configuration objectives in managing 34
the above transport scenarios. 35
9 Overall packet switched Open Xhaul architecture  36
There are different ways packet switching could be deployed to support an Open Xhaul 37
architecture. Factors include: 38
• Span of the packet switching components. Packet switching could be deployed from cell 39
site to the transport core or mixed with other technologies, for example WDM in access, to 40
form the end-to-end network. 41
• The nature of the underlying L0/L1transport used by the packet switching equipment. 42
• The network protocols used at the packet switching layer. 43
• How Xhaul services are built on the Xhaul infrastructure.    44
  45
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        39
9.1 Physical layout and xHaul transport options 1
A simple model illustrated in Figure 9-1 relies on an end-to-end packet-based transport architecture 2
that builds on a physical network. In this model, Fronthaul, Midhaul and Backhaul deploy packet-3
based transport technologies. The diagram represents a logical view of the network. The physical 4
network implementation might be different. For example, an O-DU might be connected to a single 5
TNE port but deliver two logical connections to a second TNE in Fronthaul network, and a third 6
TNE in Midhaul network. 7
 8
 9
Figure 9-1 Packet based architecture in front-, mid- and Backhaul networks  10
 11
Unlike the previous architecture some operators may decide to use the packet-based technology 12
only in Mid- and Backhaul, and use simple physical networking to connect the O-DU ports with O-13
RUs (Figure 9-2). In this case the physical network between the O-RUs and O-DUs could, in the 14
simplest case could be dark fiber links. The next choice in terms of simplicity can be a passive 15
WDM system. 16
 17
 18
Figure 9-2: Packet based architecture in mid and Backhaul networks 19
 20
Another architectural variant (Figure 9-3) can be designed by inserting TDM based layer 1 transport 21
technologies between the packet-based and physical networks. OTN and SPN/G.mtn are two 22
choices that allow for aggregation and transparent transport of larger traffic volumes, and design of 23
hard slicing architectures.  In this architecture, the OTN or SPN/G.mtn infrastructure must present 24
Ethernet clients at UNI-C/UNI-N interfaces to the packet switched network. The clients need to be 25
transparent and operate at full Ethernet line rates, because the packet switched network elements are 26
responsible for QoS and need to rely on client signals with proper bandwidth.  27
 28
 29
 30
Figure 9-3 Use of TDM based technologies with Ethernet presentation in Xhaul networks 31

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        40
 1
In cases where there are underlying L0/L1 transport solutions, the synchronization and timing flows 2
must not be impaired by these layers, as any impairment will impact the synchronization function of 3
the packet switched network, and the overall synchronization performance of the end-to-end 4
system.  5
 6
9.2 Open Xhaul architecture in revision 1 and 2 7
Revision 1 and 2 of this document describes a packet switched transport architecture illustrated in 8
Figure 9-4.  9
  10
 11
Figure 9-4 Packet switched transport for mobile Xhaul 12
 13
It is a converged end to end packet switched infrastructure, beginning at the cell site, located in the 14
edge of the access layer and stretching to the core of the transport layer. The packet switching TNEs 15
are QoS enabled, high capacity, low latency devices interconnected by point-to-point Ethernet 16
interfaces running at the full capacity of the Ethernet interface, typically using either point to point 17
fibres or via a WDM infrastructure.  It incorporates data centers suitably placed across the transport 18
network infrastructure to support virtual and physical NFs associated with mobile and fixed services 19
but also potentially the placement of “Application Functions” associated with value-add services 20
and customer specific application. 21
 22
The logical architecture is based on a common underlay packet switching infrastructure based on 23
either MPLS or SRv6 overlaid with a L2 / L3 service infrastructure (VPNs) that uses the 24
capabilities of the underlay packet switched network to support the mobiles interfaces.  25
The underlay packet switching infrastructure provides basic network services such as; any-to-any 26
connectivity between TNEs, scaling, fast convergence, shortest path and traffic engineered 27
forwarding, packet-based Quality of Service (QoS) and timing.  28
 29
The service layer supports native Ethernet services, using EVPN technology and IP VPN services, 30
using MP-BGP based L3VPNs. These services can utilise the facilities offered by the underlay 31
packet switched infrastructure to support the different mobile interfaces in an appropriate fashion. 32
Where possible transport services are built on an end-to-end basis without intermediate stitching / 33
switching points within the transport infrastructure. This approach has been taken to minimise the 34
transport service orchestration overhead.   35

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        41
9.3 Technology and architectural choices 1
As discussed in the introduction to this section there are other packet-switched solutions and 2
potentially many design approaches available that may have the same capabilities of the 3
architecture described in this revision of the document. Future versions of this document may 4
describe alternative technologies and designs based on operator requirements and suitably mature 5
standards.  6
9.4 Standardization  7
The underlay packet-switching technologies described in this document are based on MPLS and 8
IPv6 with an emphasis on the Segment Routing (SR) control and data planes. The overlay service 9
layer uses EVPN and MP-BGP L3VPNs. The IETF is the key standardization body for all these 10
technologies. It has a well-defined process for bringing an idea, in the form of a personal draft, via 11
an IETF standards track draft, to full IETF standards status, in the form of an Internet standard 12
“Request For Comment” (RFC). Depending on subject matter this can be a multi-year process.  13
 14
In order to present architectures suitable for 5G, based on modern packet-switching technologies, 15
both RFCs and IETF adopted drafts are referenced in this document. All forms of personal drafts 16
are completely excluded.   17
 18
Future revisions of this document MUST be updated to reflect the status of IETF drafts which are 19
referenced. If they expire, they MUST be removed and if they complete the IETF standardization 20
process, references and requirements MUST be updated to remove the IETF draft and include the 21
appropriate RFC number.  22
9.5 Document organization 23
The remainder of this document describes the physical and logical architecture of a packet 24
switching Xhaul architecture and how it can support 5G and legacy mobile services. It is arranged 25
as follows:  26
 27
Section 10 describes the end-to-end physical network infrastructure. 28
 29
Sections 11 and 12 describes two underlay packet switching architectures capable of supporting a 30
5G mobile environment. The first is based on MPLS, the second IPv6 with Segment Routing 31
(SRv6). Depending on scale and preference, operators will need to select one or the other.   32
 33
Section 13 describes how IP and Ethernet services suitable for 5G are delivered on a packet 34
switched Xhaul network. The approach is common regardless of the underlay technology deployed.  35
 36
Section 14 describes the QoS architecture for a packet switched Xhaul network. 37
 38
Section 15 describes multicast and consideration for deployment. 39
 40
Section 16 covers packet-switched orchestration and telemetry. Currently it is empty and will be 41
completed in a later revision of this document or in a separate document.    42
 43
Section 17 and 18 describes how the packet switched Xhaul architecture supports 5G and legacy 44
mobile use cases outlined in section 8.  45
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        42
10 Physical network design for packet switched Xhaul  1
Figure 10-1 shows two of the most common physical topologies seen in transport networks between 2
the access and the core  of the transport network . In the first, there are four layers of transport 3
infrastructure; access, pre -aggregation, aggregation, and transport core. In the second, there are 4
three layers of transport infrastructure; access,  aggregation, and transport core. Packet switches are 5
used within each layer and provides connectivity between the layers.  6
 7
The way the mobile RAN and core infrastructure is arranged over these different segments varies  8
based on geography and the MNO’s  RAN and mobile core designs and individual operator’s 9
classification of different components in the physical network. It should be noted that in most cases 10
the transport core doesn’t form part of the 5G RAN infrastructure, which is restricted to the access 11
and aggregation infrastructure (often called the metro network), but it does play an important role in 12
scaling and connecting metro infrastructures together to enable end to end services to be built.  13
 14
 15
Figure 10-1: Physical transport components  16

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        43
  1
 2
Figure 10-2 Hierarchical designed transport network consisting of access / aggregation and 3
transport core.   4
 5
Figure 10-2 shows an example of a logical transport network consisting of a number of access, 6
aggregation domains connected hierarchically to form a single transport  core domain. From a 7
logical architecture perspective, the entire infrastructur e needs t o be considered as a whole , but at 8
the physical layer there are significant differences in these discrete physical segments . These 9
include: 10
 11
1. Distances between sites 12
2. Layout of the physical media 13
3. Technology employed 14
4. Environmental conditions  15
5. Bandwidth requirements 16
6. Cost structures surrounding equipment and real estate. 17
 18
10.1 Packet over fibre  19
This document concentrated primarily on a transport infrastructure built using packet over fibre 20
solutions in all physical segments of the network. 21

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        44
10.1.1 Access   1
 A transport network element (TNE) – Ethernet switch or IP router – resides in the cell site 2
aggregating traffic received on access ports (downlinks) from end-hosts (O-RUs, or O-DUs, as 3
outlined in Section 8, as well as other types of end-hosts, like for example 4G BBUs, business 4
CPEs, etc. in hybrid deployment models), and statistically multiplexing this traffic onto a higher 5
capacity Ethernet uplink, typically established over a dark fibre or WDM lambda, or any other 6
underlying technology (for example microwave radio link). Link speed will depend on the traffic 7
levels required by the services running in the access network. The anticipated ranges will be from 8
1…50 Gbps (downlink) up to bundles of 100 Gbps (uplink). 9
10.1.1.1 Topology  10
The physical topology employed in the transport access network is operator dependent and driven 11
mainly by the three aspects: 12
• fibre topology, its availability 13
• the traffic matrix 14
• latency and time error requirements 15
 16
The main physical architectures anticipated in the transport access are: 17
• ring 18
• chain 19
• hub and spoke (called as well spine and leaf), with redundant CSR connectivity 20
• hub and spoke (called as well spine and leaf), without redundant CSR connectivity 21
 22
as shown in Figure 10-3,  Figure 10-4, Figure 10-5 and Figure 10-6. 23
 24
Figure 10-3: Ring based access physical topology  25
 26
 27
Figure 10-4 Chain based access physical topology 28
 29
CSRCSR
CSR
CSR
CSR
HSR
HSR
CSR
CSR
CSR CSR CSR HSRCSR
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        45
 1
Figure 10-5 Redundant hub and spoke access physical topology (also often called spine and 2
leaf)  3
 4
Figure 10-6 Non-redundant hub and spoke access physical topology (also often call spine and 5
leaf) 6
 7
10.1.1.2 Suitability to support O-RAN Fronthaul/Midhaul/Backhaul 8
As discussed in detail in Section 8, depending on the use case, O-DU function can be placed at the 9
cell site, together with O-RU function, or placed away from the O-RU, at the hub site. The O-DU 10
placement has big influence on the possible physical topology for the access domain of the transport 11
network. 12
 13
Open Fronthaul Interface between O-RU and O-DU has very strict latency and timing budgets  14
(O-RAN.WG9.XTRP-REQ-v01.00 “Xhaul Transport Requirements v01.00”, November 2020 [18]). 15
The implication of these requirements on the transport network design is, the number of Transport 16
Network Elements (TNEs) between O-RU and O-DU should be minimized, as each TNE increases 17
the latency and time error. 18
 19
While each of the depicted access topology could be used for Fronthaul, topologies with limited 20
number of transit transport network elements – hub-and-spoke or spine-and-leaf – leave more 21
latency budget for delay caused by light propagation in the fiber, allowing for extending Fronthaul 22
over larger distances, comparing to the topologies with bigger number of transit transport network 23
elements (ring or chain topologies). 24
 25
Ring or chain topologies can be used for Fronthaul, providing that latency and timing error budgets 26
required for particular Fronthaul deployment are maintained. For example, designing Fronthaul for 27
standard NR performance (O-RAN.WG9.XTRP-REQ-v01.00 “Xhaul Transport Requirements 28
v01.00”, November 2020 [18] Table 3), mandates maximum 100 s one-way latency between O-29
RU and O-DU. This latency budget is consumed by the fiber (~4.9 s/km), as well as by transit 30
transport network elements (~1-20 s per node, depending on hardware capabilities and port 31
speeds). Assuming 10 s latency per transport network element, a chain or ring with 10 transport 32
network elements would completely consume entire Fronthaul latency budget (ring might break, so 33
CSR
CSR
HSR
HSR
CSR
CSR
CSR
CSR
CSR
CSR
HSR
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        46
any Fronthaul design must be prepared for link failures not only from traffic rerouting perspective, 1
but as well must take into account increased number of transit network elements, leading to 2
increased latency, under ring failure condition). 3
 4
Shorter chain or ring, with for example 7 transport network elements, leaves only 30  (6 km) 5
latency budget for fiber. 6
 7
Note: the maximum number of TNEs can be limited due to overall synchronization requirements 8
and the consideration of time error contributions of integrated T-BC/T-TC. Examples are given in 9
CUS specification Annex H, and will be provided in WG9 Timing and Synchronization architecture 10
solution document [17]. 11
 12
For Midhaul/Backhaul interfaces, there are not so strict requirements regarding latency budgets, so 13
any access topology (ring, chain or hub-and-spoke/spine-and-leaf) can be used without special 14
considerations. 15
 16
Note: Hub and spoke architectures are often super-imposed over a physical ring topology using 17
WDM technology. To minimize the Fronthaul latency, direct (dark) fibre should be used in hub and 18
spoke (spine and leaf) access physical architecture carrying Fronthaul traffic. Otherwise (ring 19
topology), detailed latency analysis must be performed to confirm that latency stays within 20
mandated Fronthaul budget. 21
 22
10.1.1.2.1 Time Sensitive Networking (TSN) for Fronthaul 23
IEEE 802.1CM [26] defines two Time Sensitive Networking (TSN) profiles, with main 24
characteristics summarized in Table 2. 25
 26
Characteristic TSN Profile A  TSN Profile B
Max frame size for Fronthaul data 2,000 octets 2,000 octets
Max frame size for non-Fronthaul data 2,000 octets no limit
Differentiated prioritization for Fronthaul data yes yes
Frame pre-emption of non-Fronthaul data no yes
 27
Table 2 TSN Profile A and Profile B comparison 28
 29
The major differences between TSN Profile A and TSN Profile B are therefore the maximum frame 30
size of non-Fronthaul traffic, as well frame pre-emption, as pictured in Figure 10-7. 31
 32
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        47
 1
Figure 10-7 TSN Profile A and TSN Profile B operations 2
 3
IEEE 802.1CM has detailed discussion about transport network element latency calculation 4
(Section 7.2), as well as example end-to-end delay calculations (Annex B), therefore these 5
calculations are not repeated in this document. This document, however, focuses on delay 6
differences of Fronthaul traffic introduced by transport network element operating in accordance 7
with TSN Profile A or TSN Profile B. 8
 9
Following delay components contribute to overall transport network element delay: 10
 11
• Frame transmission delay, which is the time taken to transmit the frame at the 12
transmission rate of the port. Since both TSN Profile A and TSN Profile B mandate for 13
Fronthaul data maximum frame size of 2000 bytes, this delay component is equal for 14
Fronthaul data frames with both TSN Profile A and TSN Profile B. 15
• Self-queueing delay, which is the delay caused by other frames in the same traffic class as 16
frame to be sent (i.e. both frames are for example Fronthaul data frames). The part of the 17
self-queuing delay caused by frames that arrive at more or less the same time from different 18
input ports is referred to as fan-in delay; however, it is simpler to handle fan-in delay as part 19
of the self-queuing delay. Since both TSN Profile A and TSN Profile B mandate that 20
Fronthaul data frames cannot be pre-empted, this delay component is equal for Fronthaul 21
data frames with both TSN Profile A and TSN Profile B. 22
• Queuing delay, which is the delay caused by the frame of which transmission already 23
started in an arbitrarily small time before frame X became eligible for transmission, plus the 24
delay caused by queued-up frames from all flows with higher priority than the traffic class 25
of frame X. Since both TSN Profile A and TSN Profile B mandate that Fronthaul data must 26
be prioritized over non-Fronthaul data, in both profiles only single non-Fronthaul frame can 27
contribute to this delay. In case of TSN Profile A, a non-Fronthaul frame size is limited to 28
2,000 bytes (2,020 bytes including preamble, start of frame delimiter and inter-frame gap). 29
In case of TSN Profile B, a non-Fronthaul frame can be pre-empted, but at least 155 bytes of 30
pre-empted frame are transmitted due to the characteristics of frame preemption. 31
eCPRI
BE
AF1
AF2 1
eCPRI
BE
AF1
AF2
2
1
eCPRI
BE
AF1
AF2
2
1
eCPRI
BE
AF1
AF2
2
T0: packet 1
transmit starts
T1: packet 2
appears in
eCPRI queue
T2: packet 1
transmit finishes
T3: packet 2
transmit starts
T2-T1àpacket 2
serialization delay
(=eCPRI latency/PDV)
eCPRI
BE
AF1
AF2 1
eCPRI
BE
AF1
AF2
2
1
eCPRI
BE
AF1
AF2
2
1
eCPRI
BE
AF1
AF2
2
T0: packet 1
transmit starts
T1: packet 2
appears in
eCPRI queue
T2: max 155 bytes
transmitted
since T1
T3: packet 2
transmit starts
T2-T1à155 bytes of
packet 2 serialization delay
(=eCPRI latency/PDV)
TSN Profile A
(no preemption)
TSN Profile B
(preemption)
~
1
~
Frame preemption
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        48
• Store-and-forward delay, which includes all other elements of the forwarding delay that 1
are a consequence of the internal processing of the transport network element, including the 2
time to select the input for transmission to the egress port, assuming that the input queue 3
under consideration and output queues are empty. This delay factor is highly hardware 4
dependent, but is equal for both TSN Profile A and TSN Profile B on given hardware. 5
 6
Summarizing, the only differentiated delay factor between TSN Profile A and TSN Profile B is 7
queueing delay of non-Fronthaul data (queueing delay of 2,020 octets in TSN Profile A, versus 8
queuing delay of 155 octets in TSN Profile B). 9
 10
Table 3 summarizes this queuing delay for various port speeds. 11
 12
Port
speed
TSN Profile A
Queuing delay of 2,020 bytes
TSN Profile B
Queuing delay of 155 bytes
Difference
1 Gbps 16.160 s 1.240 s 14.920 s 3,045 m
10 Gbps 1.616 s 0.124 s 1.492 s 304 m
25 Gbps 0.646 s 0.050 s 0.596 s 122 m
50 Gbps 0.323 s 0.025 s 0.298 s 61 m
100 Gbps 0.162 s 0.012 s 0.149 s 30 m
200 Gbps 0.081 s 0.006 s 0.075 s 15 m
400 Gbps 0.040 s 0.003 s 0.037 s 8 m
 13
Table 3 Queuing delay of non-Fronthaul data 14
 15
Significant difference between TSN Profile A and TSN Profile B can be observed over port with 1 16
Gbps speed (14.92 s, which is equivalent to ~3 km fiber). For port speeds 10 Gbps or above, the 17
latency difference introduced to Fronthaul traffic (1.494 s, which is equivalent to 305 m fiber, or 18
less) is not so significant for designs targeting standard NR performance (with 100 s overall 19
latency budget). 20
 21
10.1.2 Pre-aggregation / Aggregation / Core transport 22
Pre-aggregation and aggregation and the core transport infrastructure can support Midhaul and 23
Backhaul requirements. Due to the delay requirements associated with Fronthaul, it is unlikely O-24
RAN 7.2x or RoE will be seen in these portions of the network. Additionally, in most scenarios the 25
transport core will not be part of the Midhaul or Backhaul environment but may provide N6 26
connectivity to any of the following:  27
1. Peering points for mobile consumer services. 28
2. Enterprise locations for mobile enterprise solutions.   29
3. Inter region voice communications within the operator.  30
4. Other operators. 31
10.1.2.1 Physical media and topologies 32
The assumption made in this architecture is the connectivity between routers in the pre-aggregation, 33
aggregation and transport core is provided by point-to-point Ethernet connections where the full 34
capacity of the Ethernet interface is available to the router. Link speed will depend on the traffic 35
levels required by the services running and the position in the network. The anticipated ranges will 36
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        49
be from 10Gbps up to bundles of 400Gbps with the capacity provided over dark fibre or some form 1
of WDM solution. 2
 3
The physical topology employed in the pre-aggregation, aggregation and transport core are operator 4
dependent and driven by fibre topology, its availability and the traffic matrix. The main physical 5
architectures seen in these segments of the network are all redundant and include ring, “hub and 6
spoke” and mesh. These are shown in Figure 10-8.  7
 8
Note: This document uses the term “hub and spoke architecture” but it is used inter-changeably 9
with leaf / spine and fabric architectures. 10
 11
Figure 10-8 Physical topologies anticipated in Xhaul pre-aggregation, aggregation and core 12
 13
1. Ring: This architecture is commonly seen in pre-aggregation and aggregation networks 14
where traffic is being collected and aggregated from the access towards components in the 15
aggregation or transport core. Connectivity between routers is either via direct dark fibre 16
connections or via lambdas derived from a WDM system. 17
 18
2. Dual hub and spoke: This architecture is most commonly seen in aggregation and core 19
transport networks but also sometimes in pre-aggregation networks. It is used to collect and 20
aggregate traffic towards common aggregation points. Connectivity between routers is 21
typically via direct dark fibre or via lambdas derived from a WDM system. 22
 23
Note: Hub and spoke architectures are often super-imposed over a physical ring topology 24
using WDM technology.  25
 26
3. Mesh: This is architecture is normally seen in transport core networks but can be seen in 27
aggregation networks. It is used when traffic is flowing in an any-to-any fashion. As drive 28
distances increase and connectivity becomes meshier in nature, so there is increasing use of 29
DWDM and ROADM technology to derive the links between routers.     30
 31
10.2 Alternative physical transport solutions 32
This document concentrates packet switching over a fibre infrastructure. However, there are other 33
physical solutions that can be employed in Xhaul networks. This section outlines some of those 34
technologies and discusses their applicability in different Xhaul roles.   35

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        50
10.2.1 WDM in access network  1
The access network uses optical technology with no packet switching transport functionality. Figure 2
10-9 illustrates a passive WDM Fronthaul design but the same design principle applies regardless of 3
the WDM solution. Each O-RU directly connected to the WDM multiplexer/demultiplexer by a 4
colored optic and an optical cable. At the O-DU side in the central office, the WDM 5
multiplexer/demultiplexer performs wavelength multiplexing/demultiplexing, which realize the 6
one-to-one optical wavelength connection. The WDM solution can be passive, semi active or active.  7
 8
 9
Figure 10-9 WDM Fronthaul design (passive) 10
 11
WG-9 has a work item underway considering WDM-based Fronthaul transport [19], consequently 12
this document does not consider this type of access network design further.   13
 14
10.2.2 Passive Optical Networks (PONs)  15
Passive Optical Networks (PONs) are fiber-based point-multipoint access networks, relying on 16
TDM/TDMA for the exchange of traffic between a central Optical Line Termination (OLT) and 17
multiple Optical Network Units (ONUs). PONs also use WDM for separation of up- and 18
downstream traffic, and for overlaying of multiple different PON technologies over the same fiber 19
infrastructure called Optical Distribution Network (ODN). Note that there are two basic flavours of 20
PON;  21
1. Time or Time +Wavelength Division Multiplexing (TDM or TWDM) PONs apply point-22
multipoint connections (one up/down wavelength pair (called Channel Pair) is shared over 23
multiple ONUs) over a point-multipoint ODN.  24
2. Wavelength Division Multiplexing (WDM) PONs apply point-point connections between 25
each ONU and the OLT (one Channel Pair per ONU) over a point-multipoint ODN.  26
 27
WDM PONs are like dedicated point-point links whereas TDM/TWDM PONs have some 28
fundamental characteristics to bear in mind when considering them for mobile Xhaul, described 29
below. 30
 31

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        51
 1
Figure 10-10: Typical TDM PON network. In TWDM PON multiple Channel Pairs are 2
muxed per ODN 3
 4
TDM/TWDM PONs are standardized technologies (see [161], [28], [29], [30]) and are 5
characterized by generic capabilities (independently of the use case): 6
• PON line rates are shared over the ONUs and existing technologies reach 10 Gbit/s per 7
channel (up to 4x 10 Gbit/s with NG-PON2), with new variants being developed at 25 Gbit/s 8
and 50 Gbit/s. Note that some fraction is consumed by the technology-specific overheads 9
(e.g. by parity bits for Forward Error Correction). 10
• Depending on the technology the maximum distance can reach 20 km or 40 km (or 60 km 11
with a reach extender), the maximum amount of ONUs per PON port can reach 64 or 128. 12
Note that the reach (distance) and scalability (amount of ONUs per PON ODN) are a mutual 13
trade-off bound by the optical budget (many classes of optical budget exist, e.g. Class N1 14
covers optical losses from 14dB to 29dB).  15
• The max amount of traffic flows per PON port can reach 4k (independently of physical layer 16
considerations). 17
• The upstream TDMA multiplexing is controlled by a Dynamic Bandwidth Assignment 18
(DBA) algorithm in the OLT (which is equivalent to upstream shaping over the PON 19
segment). The DBA allocates the bandwidth needs per transport container (each ONU can 20
have one or multiple transport containers) and update its assigned bandwidth. The assigned 21
bandwidth is then converted into a burstiness (rate and size of bursts). Bandwidth can also 22
be (partially or fully) assigned as a fixed value. The upstream latency is influenced by the 23
DBA behaviour, with a trade-off between latency and efficiency (more frequent bursts 24
means less waiting time between bursts but more overhead). 25
• Protection of (parts of) the passive ODN and active OLT equipment by automated switch-26
over from working to protecting parts of the equipment. 27
• Management and configuration of the ONUs. Note that ONUs operate at Layer 2 only (e.g. 28
for Layer 2 flow classification). An ONU can be connected to (or integrated with) a Layer 3 29
device like a residential gateway. 30
 31
PON ports are terminated in the OLT node, which terminates the PON layers and acts as an 32
aggregation and multiplexing node for the multiple PON ports. Contrary to the PON layer, the 33

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        52
OLT’s features are not strictly standardized, although reference deployment models are defined in 1
Broadband Forum documents (See [156], [157]). Typical OLT features include;  2
• QoS-aware packet forwarding between user side and network side (unicast, multicast, Layer 3
2 switching, Layer 3 routing) 4
• Traffic management (classification, (re)marking, queuing and scheduling, policing, 5
shaping…) 6
• User-side protocol interaction (e.g., DHCP relay agent, IGMP proxy, ARP relay, …) or on 7
the contrary protocol transparency 8
• Network-side protocol support (e.g., IP routing protocols, IGMP proxy, MPLS signalling, 9
…) 10
 11
The datagrams (typically IP over Ethernet) undergo several handlings when being transported from 12
end to end over a PON system. The different steps can be summarized as follows:  13
 14
Downstream (OLT SNI ingress to ONU UNI egress)  15
1. Traffic Management (classification, policing, queuing and scheduling) at ingress (OLT SNI), 16
2. Forwarding (Layer 2 switching or Layer 3 routing) towards corresponding PON MAC (PON 17
port), 18
3. Encapsulation in PON MAC (according to standard, ITU-T or IEEE), 19
4. OLT PON port egress queuing and scheduling, 20
5. Fiber propagation, 21
6. ONU (Layer 2 based) forwarding to corresponding UNI, 22
7. Decapsulation of PON MAC layer, 23
8. ONU UNI egress queuing and scheduling (Layer 2) 24
 25
Upstream (ONU UNI ingress to OLT SNI egress) 26
1. DBA processing in OLT for burst-based TDMA transmission (acts as policer and shaper), 27
OLT notifies grants to ONUs by sending a Bandwidth Map to all ONUs, 28
2. Traffic Management (Layer 2 classification, queuing and scheduling) at ingress (ONU UNI), 29
3. Encapsulation in PON MAC (according to standard, ITU-T or IEEE), 30
4. Burst generation as per bandwidth map, 31
5. Fiber propagation, 32
6. Decapsulation of PON MAC layer, 33
7. Forwarding (L2 switching or L3 routing) to right OLT uplink (SNI), 34
8. OLT uplink (SNI) egress queuing and scheduling 35
 36
PONs are widely used for all FTTx use cases (Cabinet, DPU, Office, Home, …), and are designed 37
for multi-service support (residential triple play, business services, Wireless LAN access points). As 38
explained in the next section and [160], PONs have been extended with additional features and are  39
also suitable for multiple Xhaul scenario’s. PONs can act as layer 2 legs in the end-end transport, or 40
participate (from OLT upwards) to layer 3 routing. 41
10.2.2.1 Using PON for Xhaul: specific features 42
As a PON is an asymmetrical (bandwidth and latency) point-to-multipoint medium, time and 43
frequency synchronization have to be transported using native media means using media converters 44
(in OLT and ONU). The PON technology have in-built mechanisms for providing frequency and 45
phase synchronization. This will be described in [20]. 46
  47
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        53
The PON DBA is being extended with a cooperative mode DBA (called Cooperative DBA or CO 1
DBA in short for interaction with O-RAN CTI [22] making it suitable for some Fronthaul needs by 2
improving the multiplexing while keeping transport latency low. 3
 4
A specific side effect of T(W)DM PONs is the need for discovery and ranging of new ONUs by 5
means of short interruptions (up to a couple of 100’s of µs) of upstream traffic. Such interruptions 6
are likely to impact the performance at the mobile radio layer, but several ways to mitigate the 7
impact or reduce or eliminate the interruptions are being developed (ranging notification from OLT 8
to O-DU by CTI, optimization of ranging timing to minimize impact, avoiding ranging on the 9
latency-sensitive channel by means of separate dedicated activation wavelength). 10
 11
10.2.2.2 Xhaul Use cases: topologies 12
PON for Fronthaul 13
 14
 15
Figure 10-11 PON for transport of Fronthaul flows 16
 17
Points of attention for Fronthaul deployments are latency at transport level, capacity per O-RU, and 18
time synchronization accuracy (Time Error between the O-RUs). For Fronthaul the O-RUs are 19
connected to ONUs, the OLT is connected to multiple O-DUs (or O-DU/O-CUs), possibly through 20
intermediate aggregating node(s). The PON system can be on-site (e.g., cell site = event zone) or 21
reach up to the cell site (ONU at cell site).  22
The appropriateness of a PON technology depends on several points: 23
- the considered radio bandwidth and corresponding Fronthaul throughput and the number of 24
O-RUs per PON that can multiplexed per single PON port 25
- the obtained latency at packet transport level which depends on the DBA settings (and use 26
of CTI). The requirements for Fronthaul depend on the combination of O-DU and O-RU 27
categories, which have one-way latency budgets ranging from tens to hundreds of µs for 28
usual Fronthaul, and multiple ms for non-ideal Fronthaul (see [16]). Plain DBA is not suited 29
for usual fronthaul, for which Cooperative DBA with CTI is required. Plain DBA can 30
address non-ideal fronthaul. See 10.2.2.3 for more details. 31
- The time accuracy between O-RUs on PON systems will be described in [20]. 32
 33
PON for Midhaul 34
 35
 36
Figure 10-12: PON for transport of Midhaul flows 37
 38
Points of attention for Midhaul deployments are latency at application level (e.g. for URLLC), QoS 39
differentiation (for achieving better statistical multiplexing), capacity per O-RU. For Midhaul the 40
O-DUs are connected to ONUs, the OLT is connected to multiple O-CUs, possibly through 41

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        54
intermediate aggregating node(s). The PON system (ONU) can reach up to the cell site or to some 1
higher aggregation point.  2
The appropriateness of a PON technology depends on  3
- the considered radio bandwidth and corresponding Midhaul throughput,  4
- the number of O-RUs per PON that can multiplexed per single PON port and the obtained 5
latency at application level (for higher CoS like e.g. URLLC). See 10.2.3.2 for more details. 6
- the requirement for time alignment accuracy of TDD transmitters between O-RU and the 7
PRTC (Class 4A 3µs) is independent of the topology of O-RU clusters (mutual position of 8
O-RUs on the PONs). The time accuracy with PON systems will be described in [20]. 9
 10
PON for Backhaul 11
 12
Figure 10-13 PON for transport of Backhaul flows 13
  14
The points of attention are similar as for Midhaul. For Backhaul the O-CUs are connected to ONUs, 15
the OLT is connected to the mobile core, possibly through intermediate Backhaul aggregating 16
node(s). The PON system (ONU) can reach up to the cell site or to some higher aggregation point. 17
The appropriateness of a PON technology is similar as for Midhaul. 18
 19
PON for carrying traffic mixes 20
 21
Note that PON systems are multi-service, hence mixes of different Xhaul flavours can co-exist on 22
the same PON system (e.g. Midhaul + Backhaul) and can also co-exist with non-mobile traffic (e.g. 23
business services). There is also the possibility of re-using the passive fiber infrastructure for 24
different PON technologies by means of WDM overlay, e.g., one system for mobile and one system 25
for residential services. Traffic characteristics of each PON system are then fully independent of the 26
other system.   27
 28
10.2.2.3 Using PON for Xhaul: trade-offs of the technology  29
PON systems can be used in many situations, but obviously not all. Each Xhaul transport use case 30
poses specific requirements, which must be considered in light of the possible trade-offs inherent to 31
the PON technology: 32
1. Latency 33
a. versus bandwidth efficiency (making bursts more numerous and shorter decreases 34
latency but increases physical layer overhead).  35
b. versus statistical multiplexing (the higher the ratio of variable versus fixed BW 36
allocations, the better the multiplexing but the higher the latency can become). Using 37
CTI allows to reach higher statistical multiplexing while keeping latency limited. 38
c. versus reach (5µs/km one-way propagation time) 39
d. As guideline, using PONs with fixed bitrate allows to support very low latencies 40
(sub-100µs) but this is not efficient for variable rate fronthaul. PONs with plain DBA 41
efficiently follow variable rate traffic but introduce a latency in the order of one to 42
several ms. When combining CTI with Cooperative DBA in the OLT for variable rate 43
fronthaul traffic (like O-RAN 7.2x), the extra latency can be reduced by an order of 44

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        55
magnitude compared to plain DBA, and such PONs can support several use cases of 1
usual fronthaul and all combinations of non-ideal fronthaul (see O-RU and O-DU 2
combinations as per [16]). 3
2. Bandwidth per Xhauled node versus the number of nodes served per ODN (bandwidth 4
sharing) 5
3. phase synchronization accuracy versus topology of O-RU clusters in PONs and path 6
propagation difference up/down (max fiber distance and wavelength-dependent correction 7
factors). This will be described in [20].  8
 9
10.2.3 DOCSIS Networks  10
Today’s modern cable operators deploy and manage extensive hybrid fiber coaxial (HFC) networks. 11
The HFC plant reaches 93% of American households and connects to virtually every building 12
across north America. The data-over-cable service interface specification (DOCSIS®) is the 13
protocol designed to work on the HFC network. DOCSIS suite of specifications is standardized and 14
maintained by CableLabs® [169][170], and is deployed worldwide. DOCSIS technology has 15
evolved through six generations of progressive refinement. Originally developed for delivering high 16
speed broadband to residential customers, the cable industry has made many advances in the HFC 17
technology, making it a promising option for 5G transport. The HFC plants are already extensively 18
deployed in areas where 5G will be in the most demands, especially in dense urban and suburban 19
environments. Additionally, the HFC plant is active, providing power needed for small cell radios. 20
The cable operators typically have the right of way on the strands where radios can be hung, 21
eliminating the need to obtain permits from government. Leveraging the existing HFC deployments 22
significantly reduces time-to-market and cost of deploying 5G. 23
 24
10.2.3.1 DOCSIS technology overview 25
DOCSIS protocol is a Layer 2 transmission protocol that encapsulates the IEEE 802.3 Ethernet. The 26
technology supports both Layer 2 and Layer 3 services and is aware of both. DOCSIS network is a 27
point-to-multipoint access network, where the transmission of upstream and downstream is 28
controlled by the cable modem termination system (CMTS). Figure 10-14 shows a typical HFC 29
plant. The CMTS is typically located in either a headend or a hub, which is connected to the fiber 30
nodes through digital or analog fiber. From there, the communication path traverses through a coax 31
network of zero or more amplifiers to the neighborhoods. The hybrid (fiber and coax) network 32
topology is called “N+x”, where “N” is the fiber node, where the fiber ends and coax starts, and “x” 33
is the number of amplifiers traversed. The average north American plant is between N+2 and N+5. 34
The smaller the x, the closer the fiber node it is to the neighborhood, the larger the amount of 35
capacity would be available to each customer. 36
 37
The traditional physical CMTS controls 50k cable modems (CMs). It is expected to scale up as the 38
CMTS moves to the cloud native architecture. 39
 40
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        56
 1
Figure 10-14: Typical HFC plant 2
 3
Table 4 shows the capabilities of the most recent DOCSIS standards. While DOCSIS 4.0 will 4
provide over 10 Gbps of downstream capacity, even higher rates can be supported. A unique aspect 5
of the DOCSIS technology is its ability to progressively expand downstream and upstream 6
capacities when and where it is needed. Traditionally, the same plant is used to provide video and 7
broadband services. As the cable industry moves away from traditional video delivery, the RF 8
spectrum that is used for video can be reclaimed for broadband. Moreover, as cable operators 9
moves to reduce the “x” in the N+x topology, node segmentation takes place. For every node that is 10
segmented into two, the network capacity essentially doubles. 11
 12
Requirements D3.1 today
(2020)
D3.1 max
(2021-22)
D4.0
(2023-24)

Downstream spectrum
Upstream spectrum
Shared spectrum with
video

54 – 1002 MHz
5 – 42 MHz
Full spectrum through
video reclamation

258-1218 MHz
5 – 204 MHz
Extending to 1.8 GHz,
possibly 3 GHz

602 – 1794 MHz
5 – 492 MHz
DS capacity
US capacity
8.5 Gbps
0.1 Gbps
8.6 Gbps
1.4 Gbps
10.8 Gbps
3.7 Gbps
Upstream latency Best effort: 5 – 50 ms
With LLX / CTI: 1 – 2 ms (can be further reduced)
Synchronization Frequency sync only
No time sync Frequency + time sync through DTP
 13
Table 4 DOCSIS Capabilities; Today and the near future  14
 15
The DOCSIS specification provides a rich set of QoS mechanisms, including traffic classification, 16
queuing, multiple scheduling services, policing, traffic shaping. Each CM supports 16-32 service 17
flows in each direction. DOCSIS technology supports several native upstream scheduling 18
mechanisms: best effort, unsolicited grant service (UGS), real-time polling service (RTPS), and 19
proactive grant service (PGS). Latency on the upstream can range from minimum of 3-5 ms to 20
average of 12 ms under best effort, to 1-5 ms under UGS or PGS. For best effort scheduling service, 21
latency is dependent on the load, as well as frame configurations such as OFDM frame size and 22
interleaver depth. For UGS/PGS services, there are trade-offs between incurring capacity overhead 23
vs. lowering the latency. The recommendation is to transport mobile Xhaul traffic over the DOCSIS 24
network via best effort service. To address the latency concern, CableLabs standardized the Low 25
Headend / Hub
Cable
Modem
Mobile Core
CMTS
Fiber
Average reach ~30km
Coax (including
ampliﬁers, taps)
Cable
Modem
Cable
Modem
Fiber
node
Cable
Modem
Residential /
business
aggregation
Aggregation
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        57
Latency Xhaul (LLX) technology [167], which provides consistent 1-2 ms of latency on the 1
upstream. The latency could potentially be further reduced with some configuration optimizations. 2
The ORAN CTI [22] specification uses a similar scheduler pipelining mechanism as LLX. 3
Typically, the latency on the downstream is 1ms. 4
 5
Cable operators are in the midst of disaggregating the traditional CMTS. The “Distributed Access 6
Architecture” (DAA), shown in Figure 10-15, has been specified by CableLabs [171] and is being 7
deployed. Replacing the traditional “integrated” CMTS, PHY layer component of the CMTS is 8
being pushed to the fiber node and is called Remote PHY Device (RPD), while MAC and upper 9
layers are implemented as DAA Core, and is located in the headend or regional data center. The 10
DAA Core has been architected with the cloud native architecture. The links between the fiber node 11
and the hub are being upgraded to a digital fiber network, with DWDM as a starting point, but some 12
operators may choose to migrate to point-to-point coherent optics. A device called “CTD” is being 13
specified to terminate the fiber from the hub, and is essentially be a Layer 2 switch that provides 14
high-speed Ethernet to the neighborhood. A multitude of services can be provided through the CTD, 15
including 5G Fronthaul, PON, etc, thus achieving a form of transport convergence. 16
 17
 18
 19
Figure 10-15 Distributed Access Architecture (DAA) for transport convergence 20
10.2.3.2 Xhaul transport DOCSIS network 21
Functionally, the DOCSIS network can interconnect the small cells or radios as shown in Figure 22
10-16. However, the requirement on capacity and latency vary between Backhaul, Midhaul, and 23
Fronthaul. Backhaul and Midhaul are both based on an IP encapsulation of the original mobile 24
transport. Thus, the bandwidth requirement on the transport network roughly matches the mobile 25
traffic rate.  26
 27
The latency requirement for Backhaul is based on the application. Additional service-level 28
agreement (SLA) can be specified by the mobile operator. Midhaul latency is generally considered 29
to be less than 10 ms [172]. LLX can be implemented to ensure these requirements can be met, as 30
well as providing better latency performance, particularly for latency-sensitive flows. 31
 32

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        58
 1
Figure 10-16 Mobile Xhaul over DOCSIS Network  2
 3
Fronthaul is much more difficult to support over the coax portion of the HFC network. Studies have 4
shown that the eCPRI-based Fronthaul transport needs significantly more bandwidth, overhead, and 5
one-way latency in the neighborhood of 250 microseconds between the O-RU and O-DU and 6
between TNEs 100 microseconds. Even with LLX, it will be difficult to reduce the DOCSIS latency 7
to this level. Because of the stringent requirements, Fronthaul is better carried directly over the 8
digital fiber network and over CTD as shown in Figure 10-16. 9
 10
10.2.4 Microwave and mmave radio transport technologies 11
10.2.4.1 Overview of Microwave and mmwave technology 12
For over 20 years, microwave has been the primary solution for the rapid and cost-effective rollout 13
of mobile Backhaul infrastructure with over 50% of mobile sites worldwide today connected via 14
Microwave (MW) or Millimetre Wave (mmW) radio links and up to over 90% in some networks. 15
The evolution from 4G towards 5G presents significant challenges to all transport technologies and 16
wireless ones make no exception. 17
 18
Spectrum is a vital asset to support the mobile Backhaul requirements and certainly this topic 19
becomes increasingly relevant as future mobile access data rates and respective Backhaul capacity 20
requirements continue to rise. Various frequency bands are used today for mobile Backhaul 21
depending on the requested capacity, hop length, link availability, spectrum availability and 22
frequency re-use capability. The spectrum that is available for mobile Backhaul ranges from the 23
traditional microwave bands up to the millimetre wave spectrum as illustrated in the Figure 10-17. 24
These bands are allocated for fixed services by the WRC Radio regulations. Their channel plans are 25
detailed by the ITU-R F-series recommendations as well as some regional or national regulatory 26
regimes, such as CEPT. Fixed services spectrum uses authorisations awarded by national 27

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        59
administrations on the basis of link by link assignments as either coordinated or self-coordinated, or 1
as block assignments. 2
 3
 4
Figure 10-17 Microwave and mmwave spectrum 5
 6
The engineering of a MW or mmW link involves finding the optimal combination of link length, 7
capacity, frequency band and availability. 8
 9
The physics of radio waves propagation determine the relation among capacity, availability and link 10
length.  11
Since the available spectrum is proportional to the center frequency, the highest frequencies are also 12
those that carry the most capacity, but also cover the comparatively shortest link lengths.  13
As a rule of thumb, frequencies below 13 GHz can be considered mostly unaffected by the intensity 14
of rainfall and frequencies above are more and more influenced by the attenuation caused by rain, 15
so that as a general principle higher frequencies are used for shorter links, as illustrated in Figure 16
10-18. It should be noted that features could be inherited for links combining different bands as 17
indicated in the circles in this figure. 18
 19
 20
Figure 10-18 Interdependence among frequency, capacity and availability. Source ETS TS 21
mWT 22
 23
The availability of MW/mmW spectrum depends on both technological and regulatory factors. 24
Technology is available and under development to make full use of existing (6-86 GHz) and future 25
(90-300 GHz) spectrum: E-band (80 GHz) has been commercially deployed for several years, W-26
band (100 GHz) and D-band (150 GHz) are the most promising upcoming bands, with prototypes 27
already appearing in the market. 28
 29

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        60
Wider channels (112MHz, even 224MHz where possible) in traditional frequency bands and raw 1
availability of spectrum (10GHz in E-band, 18GHz in W-band and 30GHz in D-band) provide the 2
main resources to expand the capacity of MW and mmW radio systems. Overview of available 3
spectrum for wireless transport network with corresponding capability related to link capacity, 4
latency and hop length is illustrated in Figure 10-19. 5
 6
 7
 8
Figure 10-19 Bands characteristics and capabilities. Source ETSI TSG mWT 9
 10
10.2.4.2 Evolving technologies 11
There are a number of existing and evolving technologies which are applied in microwave and 12
mmwave radio to enhance its spectrum efficiency, achievable capacity and reducing latency. An 13
overview of these technologies is presented in[180]. As follow is a summary of these applicable 14
technologies in the microwave and mwave radio transport systems. 15
 16
Capacity and spectrum efficiency enhancement 17
Larger channels are no longer a technology limit. In MW bands recent regulatory limit shifted up to 18
Channel Spacing CS=224MHz, but not everywhere. Up to CS=2000MHz is standardised in E-Band 19
and above 100GHz. 20
Larger CS are needed where Carrier Aggregation, in same band or adjacent band is employed. 21
 22
Higher Modulation schemes reached the reasonable top at 4096QAM (and more). After 1024QAM 23
spectral efficiency gain is less than 10% for every step. Higher order result in reduction in system 24
gain (impacting availability), however, with adaptive modulation availability can be maintained 25
with lower modulation order (i.e. scarifying capacity).  26
 27
Frequency reuse with cross polar interference cancelation (XPIC) 28
This is a well-known technique for doubling the spectral efficiency by utilising the cross 29
polarisation of the channel bandwidth occupied. 30
 31
LoS-MIMO Line of Sight Multi-Input Multi-Output 32

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        61
Exploiting link geometry deployment, two different signals in the same channel can be transmitted. 1
4x4 LoS-MIMO is obtained with LoS-MIMO 2x2 plus XPIC 2
LoS MIMO needs optimal antennas separation. 3
Under optimal conditions, spectral efficiency close to four times capacity improvement can be 4
achieved, while lower performance in case of suboptimal conditions should be expected.  5
 6
 7
Figure 10-20 4x4 Multiplexing with LoS MIMO and XPIC  8
 9
Orbital Angular Momentum  10
OAM is a new transmission mechanism allowing multiplexing multiple streams simultaneously 11
over the same frequency channel without the limitation imposed by the optimum separation 12
distance of antennas in the LoS MIMO. Using different antennas, multiple OAM signals with 13
different spiral phase front (mode) can be transmitted. OAM modes are orthogonal of each other. 14
A pragmatic OAM system, uses uniform circular antenna array while the OAM signal is generated 15
at the base band as illustrated in the Figure 10-21. 16
 17
 18
Figure 10-21 Orbital Angular momentum transmission 19
 20
An OAM system with 8 Uniform Circular Array UCA antenna elements allows the transmission of 21
16 orthogonal streams resulting in capacity of around 105Gbps. [166] 22
 23
Bands & Carriers Aggregation (BCA) 24
BCA joins different channels that may be even in different bands, providing a single big capacity 25
pipe. Lower band will provide capacity pipe’s segment with high availability, while higher band the 26
best effort capacity pipe segment. Packets may be adaptively re-routed among different channels 27
according to their priority and channels condition. 28

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        62
    1
Figure 10-22 Bands and carrier aggregation concept and benefits 2
 3
One of the most valuable approach is 15/18/23 GHz with E-Band where dual band antennas are 4
available: 5
- Links up to 7-10Km are feasible. Capacity may even exceed 10Gbps 6
- High spectral efficiency obtained because E-Band can reach longer links than in traditional 7
approach.  8
BCA among two MW bands is another variant when distance becomes more challenging i.e.: rural 9
application. 10
   11
Geographical spectral efficiency: Dense reuse of channels 12
To better exploit the scarce resource (spectrum) it is advisable to increase not only the single 13
channel spectral efficiency but also the channel reusability in a given area, guaranteeing the 14
“interference free operation”. 15
 16
Nodal configuration is the key point to understand the concept of geographical spectral efficiency. 17
Better antenna class are introduced (e.g. ETSI Class 4), reducing significantly the minimum angle 18
between two links using the same/adjacent channels (angle discrimination) 19
Cross polar (XPIC) can also be used in reducing angle discrimination.  20
Co-Channel Interference Canceller (CCIC) further improve the re-use of channels with very narrow 21
angle discrimination. 22
 23
 24
Figure 10-23 Increase nodal capacity is now easy with no additional spectrum with XPIC 25
 26
When additional capacity is needed and then additional channels shall be used, CCIC permits an 27
optimal re-use of channels with very narrow angle discrimination 28
 29

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        63
 1
Figure 10-24 Further increase nodal capacity with increased spectral utilization by applying 2
CCIC technique 3
 4
Combination of the above technologies gives enhancements to microwave/mmwave capabilities as 5
indicated below [179]. 6
 7
 8
MW Backhaul Technology 56 MHz BW 112 MH BW 224 MHz BW +XPIC + Los 2x2
MIMO
+ BCA
(with higher
MW Band)
+BCA
(with mmW
Band)
6-15 GHz 0.5 Gbps 1 Gbps  2 Gbps  3-4Gbps
18-42 GHz 0.5 Gbps 1 Gbps 2 Gbps 2-4 Gbps 4-8 Gbps  4-10 Gbps
 9
mmW Backhaul
Technology 500 MHz BW 2 GHz BW 4 GHz BW +XPIC +LOS 2x2
MIMO/OAM
V-band (60GHz)  >4 Gbps
E-band (70/80GHz) 3.2 Gbps 12.8 Gbps  25.6 Gbps 51.2 Gbps
W-band (100GHz) 3.2 Gbps 12.8 Gbps 25.2 Gbps 51.2 Gbps 102.4 Gbps
D-band (150GHz) 3.2 Gbps 12.8 Gbps 25.6 Gbps 51.2 Gbps 102.4 Gbps
 10
Table 5 Microwave/mmwave enhancements  11
 12
10.2.4.3 Support for packet 13
MW or mmW links are characterised by their physical capabilities related to the combination of 14
link length, capacity, frequency band and availability, which is independent of their network layer 15
functionalities and use cases. These radio links are usually capable of multiplexing various types of 16
traffic over single radio channel and can be equipped with multiple network interfaces including 17
FE/GbE (RJ45, SFP) and CWDM filter support. Various packet functions are also supported as 18
optional features and can be configured by network operators, such as L2 pass through, feature rich 19
L2 switch with ERPS, H-QoS among others enabling carrier grade Ethernet services. MPLS-TP is 20
also supported for layer 2 services, and for timing and synchronisation support includes 1588v2 21
(TC, BC) and SyncE.  22
 23
These network features make MW and mmwave radio popular in Backhaul and Xhaul network 24
infrastructure by themselves. They are also used as a complementing technology to other physical 25
technologies where the MW and mmwave radio links can be used as part of network segments to 26
close links where underlying technology infrastructure is missing or absent or as a back-up in 27
mission critical part of the network. The section below illustrates some use case examples where 28
MW and mmwave radio links are utilised. 29
 30

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        64
10.2.4.4 Topology example use cases 1
Dense urban/urban (C-RAN plus D-RAN support) case 2
The figure below illustrates an example of transport network use in an end-to-end network with C-3
RAN infrastructure used to provide network coverage in a Dense Urban/Urban environment with 4
support for some D-RAN sites.  5
In this network, the core is connected to the O-CU via a Backhaul aggregate and pre-aggregate 6
transport part of the network. At the aggregation/pre-aggregation transport network, a bandwidth of 7
10G or more will be required, mainly 25G or more. MW and mmW will be used as alternative (ring 8
closer) or redundant circuit to fibre, and the application of wide-bandwidth products in the E/D/W 9
bands will be the main focus. 10
At the Midhaul transport network, MW and mmW radio equipment can be used in almost the same 11
band as conventional Backhaul. From the transport perspective, there is little different between 5G 12
and 4G transport networks, with additional synchronisation and delay requirements. 13
 14
 15
 16
 17
 18
 19
Figure 10-25 Network topology example for dense urban/urban environment with microware 20
and mmwave links 21
 22
In urban centers, high bandwidth services are required, hence Fronthaul LLS needs to be more than 23
25G (25-100G). In many cases, LLS is used when DU/RU are collocated in the same building. It 24
may be in short distances. In this case, MW/mmW radio equipment is of limited application.  25
Within the urban environment, some D-RAN sites may also be connected to edge site. In this case 26
IAB (integrated Access Backhaul) base station to base station communication, which cover about 27
100 metres with LoS, can be used.  28
 29
Rural (C-RAN) case 30
In this topology scenario, coverage in rural areas is migrated using existing fibres in a C-RAN 31
configuration for 4G. Generalisation of equipment is made possible by upgrading CPRI links to 32
Ethernet utilising eCPRI or IEEE1914.3 mechanism. The link capacity requirements in this case is 33
in the range 10 to 25G. The application of MW/mmW radio equipment is used to extend coverage 34
and other uses due to its ease of installation. 35
 36
 37

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        65
 1
Figure 10-26 Network topology example for rural environment with microware and mmwave 2
links. 3
10.3 Data Centers  4
Data center solutions, connectivity to the Xhaul transport infrastructure and orchestration are not in 5
scope of this version of document. However, it is important to note the critical role   6
Data centers play in 5G, with key components of the 5G architecture running on servers either 7
virtualized, containerized or as bare metal located in data centers. Consequently, operators need to 8
consider how data centers are connected to the Xhaul transport network and how end to end 9
transport services are built that span the DC and the WAN. At a high-level two basic approaches to 10
interconnecting DC to the Xhaul transport network are available. 11
 12
10.3.1 Complete separation between DC and WAN infrastructure  13
This refers to data centers that are discrete from the WAN infrastructure with each domain 14
potentially running different underlay and overlay transport network technologies. The inter-15
connection between the two domains is via “Data Center Interconnect” (DCIs) routers. These 16
routers reside in both the DC and WAN domains and have an awareness of the DC environment on 17
one side and an awareness of the WAN on the other. To provision an end-to-end network service, 18
the data center and the WAN are provisioned separately, using a cross-domain orchestrator to stitch 19
the two separate service environments together.  20
 21
10.3.2 DC integrated into WAN infrastructure. 22
This refers to data centers that run the same underlay and service infrastructures as the WAN. In 23
this case, although there maybe DCIs to create underlay separation between the WAN and the DC, 24
it is possible to treat the network service infrastructure as a single orchestration domain with WAN 25
transport services directly configured on the Top of Rack (ToRs) devices or on soft switches / 26
routers residing on the server themselves.  27
11 Packet-switched underlay network – MPLS based 28
This document presents two packet switched underlay technologies, the first is based on MPLS 29
contained in this section (section 11) and the second based on SRv6 contained in section 12. An 30
operator wishing to implement the transport architecture outlined in this document will need to 31
select one of the two and implement the requirements outlined in the associated section. It should be 32

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        66
noted that other transport architectures are potentially available but not covered in this revision of 1
the document.     2
 3
This section outlines a packet switched underlay model based on “Multi-Protocol Label Switching” 4
(MPLS).  MPLS data plane is based on label switching but has more than one control plane 5
technology. The first, classic approach is based on the control plane developed when MPLS was 6
first conceived in the mid 1990s and utilizes IGPs, BGP-LU, LDP and RSVP. An emerging 7
approach, very appropriate to mobile transport networks, is based on Segment Routing (SR) 8
extensions to IGP and BGP, which have been developed since around 2010 and aim to simplify and 9
minimize the number of protocols running in network, as well as reduce the state held on the TNEs. 10
This document describes scalable, multi-domain Xhaul transport architecture that can involve 11
domains with different MPLS control plane technologies (classic: LDP, RSVP, BGP-LU, as well as 12
emerging: SR, SR with Flex-Algo, or SR-TE), in order to support both brownfield (extending 13
existing mobile transport network using classic control plane with additional domains using new, 14
emerging SR control plane) and greenfield (building new Xhaul transport network) deployment use 15
cases. However, in order to avoid duplications with BBF technical reports documenting classic 16
MPLS control plane in mobile transport network designs, and to keep the size of this document 17
within reasonable limits, this document puts more focus on the new, emerging control plane based 18
on SR. 19
 20
Regardless which MPLS control plane – classic or SR – is used to distribute underlay transport 21
labels, the services are overlaid on-top. The service layer is independent from the underlaying 22
MPLS transport layer and supports native Ethernet services and layer 3 services. 23
 24
11.1 MPLS data plane  25
MPLS architecture relies on an MPLS data plane using MPLS encapsulated IP (for L3VPN) or 26
Ethernet (for L2VPN/EVPN) data packets. MPLS label stack, containing potentially multiple labels, 27
pushed on the data packet might encode various information, like for example: 28
• path through the transport network (transport labels) 29
• entropy for load balancing on transit routers (flow or entropy labels) 30
• function/service on egress router (L3VPN/EVPN labels) 31
 32
Figure 11-1 shows examples for Ethernet based eCPRI and IP based eCPRI encapsulated in MPLS. 33
 34
 35
Figure 11-1 MPLS encapsulation of Ethernet frame or IP packet 36
 37
To support packet switching transport with MPLS data plane, the transport device will need to 38
support: 39
 40

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        67
[R1]: MUST support MPLS architecture as defined by “Multiprotocol Label Switching 1
Architecture”, RFC 3031 [49]. 2
[R2]: MUST support MPLS label stack encoding defined by “MPLS Label Stack Encoding” RFC 3
3032 [50]. 4
[R3]: MUST support TTL processing defined by “Time To Live (TTL) Processing in Multi-5
Protocol Label Switching (MPLS) Networks”, RFC 3443 [57]. 6
[D1]: SHOULD support MPLS Explicit Null operation defined by “Removing a Restriction on the 7
use of MPLS Explicit NULL”, RFC 4182 [64]. 8
 9
11.2 MPLS control plane  10
The MPLS control plane is used to distribute information required to deliver the packets through the 11
transport network. It typically includes following information: 12
 13
• Topological information (nodes and links in transport network, including attributes, like 14
metrics, IP prefixes administrative groups, SRLGs, etc., associated with these topological 15
elements) 16
• IP prefixes 17
• MPLS transport label information 18
 19
Based on the collected information, control plane responsibility is to 20
 21
• calculate paths (shortest path to the destination, or path fulfilling certain administrative 22
constraints, which are not necessarily the shortest paths – for example low latency path) 23
• program data plane with information required to forward the MPLS packets through the 24
network (next-hops, MPLS labels stack manipulation rules – pop, push, swap) 25
• calculate and program in the data plane backup paths required for rapid protection 26
mechanisms (fast re-route – FRR) 27
 28
In a packet switched transport network consisting of a large number of routers, a mix of WAN and 29
data centers components, the need to support traffic engineering, the underlay control plane is a 30
typically a collection of independent routing domains that interact in a collaborate fashion. 31
 32
 33
Figure 11-2 MPLS packet switched underlay architecture 34
 35
Figure 11-2 illustrates how a large packet switched MPLS transport infrastructure might be 36
designed and how different components interact. It should be noted that not all these components 37
are required, and some serve the same purpose but in different ways.  38
ACCESS ACCESSAGGREGATIONAGGREGATION TRANSPORT
CORE
End-to-End Label Switched Path
Distributed PCE Layer
PCEP PCEPBGP-LS Telemetry
O-RU
O-RU
O-RU
O-RU
CSR CSRHSRHSR
PCE
PCE
PCE
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        68
1. Each routing domain has an IGP for internal connectivity. 1
2. Each routing domain has a protocol for distributing label information. It could be SR 2
extension to an IGP, or could be as well legacy protocol like RSVP or LDP. Architecture 3
supporting legacy MPLS transports (LDP and/or RSVP) is especially important in brown-4
field deployments, with legacy MPLS transport already in place. 5
3. Mechanisms to calculate Traffic Engineered paths within a routing domain. Depending on 6
scale, TE paths can be calculated on the transport devices themselves (distributed constrained 7
shortest path first – D-CSPF – calculation), calculation can be off-loaded from less powerful 8
transport devices to more powerful transport devices (on-box PCE), or even dedicated 9
servers could be deployed (off-box PCE). The diagram shows a distributed “Path 10
Computation Element” (PCE) layer with “Path Computation Element Protocol” PCEP 11
running between the edge nodes and the PCE 12
4. Mechanism to establish end-to-end MPLS LSPs between IGP domains. To achieve highly 13
scalable architecture, two mechanism could be considered: Seamless MPLS Architecture, or 14
a controller-based Architecture. Both architectures are described in more details in Section 15
11.5. 16
5. Mechanism to convey topology and network state information from the network to the PCE 17
and other management elements. BGP Link State (BGP-LS) and Telemetry feeds are 18
recommended tools to fulfil this requirement. 19
11.3 Classic MPLS control plane  20
The classic control plane is widely implemented, and designs and requirements are well 21
documented in: 22
 23
• Cisco Press, MPLS and VPN Architectures, Volume 1, 420 pages, by Ivan Pepelnjak, and 24
Jim Guichard, 2001 [173] 25
• Cisco Press, MPLS and VPN Architectures, Volume 2, 470 pages, by Ivan Pepelnjak, Jim 26
Guichard, and Jeff Apcar, 2003 [174] 27
• O’Reilly, MPLS in the SDN Era, 890 pages, by Antonio Sánchez-Monge, and Krzysztof 28
Grzegorz Szarkowicz, 2015 [175] 29
 30
The application of classic MPLS control plane protocols (IGP, LDP, RSVP, BGP) to the mobile 31
transport networks is defined by Broadband Forum (BBF) in following technical reports: 32
 33
• BBF TR-221: Technical Specification for MPLS in Mobile Backhaul Networks, 99 pages, 34
Oct 2011u [176] 35
• BBF TR-221, Amd.1: Technical Specifications for MPLS in Mobile Backhaul Networks, 24 36
pages, Nov 2013 [177] 37
• BBF TR-221, Amd.2: Technical Specifications for MPLS in Mobile Backhaul Networks, 22 38
pages, Sep 2017 [178] 39
11.3.1.1 LDP base requirements 40
If an operator wishes to use LDP MPLS control plane in some routing domain, then the routing 41
equipment in that routing domain will require: 42
 43
[D2]: SHOULD support constraint-based LSP setup using LDP defined by “Constraint-Based LSP 44
Setup using LDP”, RFC 3212 [53] 45
[R4]: MUST support LDP state machine defined by “LDP State Machine”, RFC 3215 [54] 46
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        69
[D3]: SHOULD support graceful restart mechanism for LDP defined by “Graceful Restart 1
Mechanism for Label Distribution Protocol”, RFC 3478 [59] 2
[R5]: MUST support LDP defined by “LDP Specification”, RFC 5036 [76] 3
[D4]: SHOULD support LDP extension for inter-area label switched paths defined by “LDP 4
Extension for Inter-Area Label Switched Paths (LSPs)”, RFC 5283 [78] 5
[D5]: SHOULD support LDP IGP synchronization defined by “LDP IGP Synchronization”, RFC 6
5443 [91] 7
[D6]: SHOULD support LDP capabilities defined by “LDP capabilities”, RFC 5561 [94] 8
11.3.1.2 RSVP base requirements 9
If an operator wishes to use RSVP MPLS control plane in some routing domain, then the routing 10
equipment in that routing domain will require: 11
 12
[R6]: MUST support “Resource ReSerVation Protocol (RSVP)”, RFC 2205 [36], RFC 2209 [37] , 13
RFC 2210 [38] 14
[R7]: MUST support “RSVP Diagnostic Messages”, RFC 2745 [47] 15
 16
[R8]: MUST support “RSVP Refresh Overhead Reduction Extensions”, RFC 2961[46] 17
 18
[D7]: SHOULD support “RSVP Cryptographic Authentication—Updated Message Type Value”, 19
RFC 3097 [51] 20
[R9]: MUST support “RSVP-TE: Extensions to RSVP for LSP Tunnels”, RFC 3209 [52] 21
 22
[D8]: SHOULD support “Signalling Unnumbered Links in Resource ReSerVation Protocol - 23
Traffic Engineering (RSVP-TE)”, RFC 3477 [58]  24
[R10]: MUST support “Traffic Engineering (TE) Extensions of OSPF Version 2”, RFC 3630 [60] 25
 26
[D9]: SHOULD support “Fast Reroute Extensions to RSVP-TE for LSP Tunnels”, RFC 4090 [62] 27
 28
[D10]: SHOULD support “OSPF Extensions in Support of Generalized Multi-Protocol Label 29
Switching (GMPLS)”, RFC 4203 [66] 30
[D11]: SHOULD support “Node-ID Based Resource Reservation Protocol (RSVP) Hello”, RFC 31
4558 [68] 32
 33
[R11]: MUST support “Record Route Object (RRO) Node-Id Sub-Object”, RFC 4561[70] 34
 35
[R12]: MUST support “IS-IS Extensions for Traffic Engineering”, RFC 5305 [84] 36
 37
[D12]: SHOULD support “Traffic Engineering Extensions to OSPF Version 3”, RFC 5329 [87] 38
 39
[R13]: MUST support Encoding of Attributes for MPLS LSP Establishment Using Resource 40
Reservation Protocol Traffic Engineering (RSVP-TE), RFC 5420 [89] 41
[R14]: MUST support “Label Switched Path (LSP) Attribute in the Explicit Route Object (ERO)”, 42
RFC 7570 [105] 43
[D13]: SHOULD support “Techniques to Improve the Scalability of RSVP-TE Deployments”, RFC 44
8370 [116] 45
[D14]: SHOULD support “Signaling RSVP-TE Tunnels on a Shared MPLS Forwarding Plane”, 46
RFC 8577 [122]  47
[D15]: SHOULD support “Refresh-interval Independent FRR Facility Protection”, draft-ietf-mpls-48
ri-rsvp-frr-07 [139] 49
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        70
11.4 SR/MPLS control plane 1
The following section outlines the control plane requirements for SR/MPLS environment. 2
11.4.1 Interior Gateway Protocol (IGP) for SR/MPLS  3
The distribution of labels, rapid convergence and distribution of traffic engineering information and 4
optional calculation of TE optimized forwarding planes in a single SR domain is done by a link-5
state IGP, in the form of either IS-IS or ISPF, with suitable SR enhancements. This document 6
separately outlines the requirements for IS-IS and OSPF with support for TiLFA for fast 7
convergence and optional support for flex-algorithm for the creation of multiple forwarding planes 8
optimised for different criteria and potentially using different topologies.   9
  10
SR/MPLS requires at least one IGP with SR awareness per routing domain or autonomous system.  11
There are two IGPs that support SR/MPLS; ISIS or OSPF. They are both link-state protocols and 12
have similar capabilities but there are operational differences which is beyond the scope of this 13
document.  14
 15
The IGP provides internal connectivity within a routing domain. The size of a routing domain is 16
determined by technical, operational and organizational considerations, such as protocol scalability 17
and spans of control. In large networks, such as a 5G infrastructure, extending from the access to 18
the transport core, it is very common to see multiple autonomous systems, IGPs and segmentation 19
within the IGPs. 20
 21
In an SR/MPLS environment the IGP is responsible for distributing node, prefix and label 22
information to all nodes within the routing domain and for calculating the “Routing Information 23
Base” (RIB) and the Label Forwarding Information Base (LFIB) on each TNE.  Optionally it can 24
provide fast convergence mechanisms and distribute traffic engineering attributes within the domain 25
and build multiple forwarding planes.  Both IS-IS and OSPF have facilities to create a hierarchical 26
routing structure within a routing domain using levels and areas respectively. They also have the 27
facility to bring external routing information into a routing domain and push its routing information 28
into other routing domains using route redistribution.  29
 30
Note: There are other techniques to achieve route re-distribution between routing domains, such as 31
BGP and centralised SDN controllers. 32
 33
Note: When looking at SR/MPLS IGP requirements there are sets of requirements for ISIS and 34
OSPF. Operators need to follow the requirements associated with IGPs they are using within their 35
network infrastructure.   36
11.4.1.1 Topology Independent Loop Free Alternative (TiLFA) 37
In the SR/MPLS underlay design outlined in this document, TiLFA can be used to achieve fast IGP 38
convergence in the event of a network failure.  Topology Independent Loop Free Alternative as 39
defined “Topology Independent Fast Reroute” using Segment Routing (draft ietf-rtgwg-segment-40
routing-ti-lfa [141]) is an IGP capability supported by Segment Routing that provides a local repair 41
mechanism that achieves 100% coverage within a routing domain against links, nodes and SRLGs 42
failures with convergence times, once failure detection has occurred, H/W dependent but typically 43
less than 50ms. 44
 45
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        71
For each destination in the network, TI-LFA pre-installs a backup forwarding entry for each 1
protected destination ready to be activated upon detection of the failure of a link used to reach the 2
destination.  TI-LFA provides protection in the event of any one of the following: single link 3
failure, single node failure, or single SRLG failure.  In link failure mode, the destination is protected 4
assuming the failure of the link.  In node protection mode, the destination is protected assuming that 5
the neighbor connected to the primary link has failed.  In SRLG protecting mode, the destination is 6
protected assuming that a configured set of links sharing fate with the primary link has failed (e.g. a 7
linecard or a set of links sharing a common transmission pipe). The mechanics of the TiLFA in an 8
SR/MPLS environment at failure is shown in Figure 12-2 and upon convergence in Figure 12-3. 9
One of the key advantages of TiLFA over some other fast convergence mechanisms, not well 10
illustrated in the example below, is that the TiLFA backup path is the same as the post convergence 11
path, thus minimising micro-loops that can occur as IGPs converge.    12
  13
 14
Figure 11-3 TiLFA at failure with SR/MPLS 15

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        72
  1
 2
 3
Figure 11-4 Post convergence traffic flow with SR/MPLS   4
11.4.1.2 IGP Flexible-Algorithm 5
In the SR/MPLS underlay design outlined in this document Flexible Algorithm can be used to 6
within an IGP domain to provide an IGP based traffic engineering facility.  Flexible-Algorithm is an 7
IGP based traffic engineering technique that permits multiple forwarding tables to be created in the 8
IGP based on operator programmed criteria. It is a simple, automatic way, in which a packet 9
switched transport network can be traffic engineered, using only the IGP, to create different 10
forwarding planes designed to meet specific operator defined criteria.  11
 12
 Standard ISIS and OSPF use the IGP metric on links and the “Shortest Path First” (SPF) algorithm 13
to calculate the “best” path between an ingress and egress point in the network. For many services 14
this approach is sufficient, however this approach cannot, for example, take into account real-time 15
link latency, utilization, packet loss and whether an ECMP path shares links using the same 16
underlying fibre ducts.  17
 18
 IGP Flex-Algo, defined in draft-ietf-lsr-flex-algo [141] is an IGP based Segment Routing traffic 19
engineering capability, that enables an operator to define their own custom algorithm, based on a 20
wide range of variables including: link latency, packet loss, bandwidth, affinity and shared risk link 21
groups (SRLG), to achieve a specific forwarding aim. This is illustrated in Figure 11-5. 22

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        73
 1
Figure 11-5 Flex-algo example with three topologies  2
 3
This enables an operator, using a single IGP instance, to build multiple forwarding tables within an 4
SR/MPLS underlay network based on different optimization criteria. The base forwarding instance 5
is still based on IGP metrics, but an operator can build additional IGP Traffic engineered 6
forwarding tables based, for example on lowest latency or avoiding certain links or a combination 7
of the two. 8
 9
The requirements to support flex-algorithm are all associated with the IGP requirements and 10
contained in sections 11.4.1.3 and 11.4.1.4 11
11.4.1.3 IS-IS SR/MPLS base requirements  12
If an operator wishes to use IS-IS to support SR/MPLS with TiLFA and optionally flexible 13
Algorithm (flex-algo) then a TNE providing the packet switching function will require: 14
 15
[R15]: MUST support Routing IPv4 with ISIS, as defined in ISO/IEC 10589, RFC 1195 [34], RFC 16
3719, and/or Routing IPv6 with ISIS, RFC5308 [85] 17
[R16]: MUST support “IS-IS Extensions for Segment Routing”, RFC 8667 [143] 18
[D16]: SHOULD support “Signaling Maximum SID Depth (MSD) Using IS-IS”, RFC 8491  19
 20
[D17]: SHOULD support “IGP Flexible Algorithm”, draft-ietf-lsr-flex-algo-15 [138] 21
 22
[R17]: MUST support IS-IS TE Extensions, RFC5305 [84] and RFC7810 [108]  23
 24
[D18]: SHOULD support “Topology Independent Fast Reroute using Segment Routing”, draft-ietf-25
rtgwg-segment-routing-ti-lfa-06 [141]  26
[D19]: SHOULD support ISIS cryptographic extensions as defined in RFC5304 [83] and RFC5310 27
[86] 28
[D20]: SHOULD support a Policy Control Mechanism in IS-IS Using Administrative Tags as 29
defined in RFC5130 [77] 30
[R18]: MUST support Domain-Wide Prefix Distribution with Two-Level IS-IS as defined in 31
RFC5302 [81]. 32

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        74
[R19]: MUST support Three-Way Handshake for IS-IS Point-to-Point Adjacencies as defined in 1
RFC5303 [82] 2
[D21]: SHOULD support purge originator identification TLV for IS-IS as defined in RFC6232 [96] 3
 4
11.4.1.4 OSPF SR/MPLS basic requirements  5
If an operator wishes to use OSPF (v2 or v3) to support SR/MPLS with TiLFA and optionally 6
flexible Algorithm (flex-algo) then a TNE providing the packet switching function will require: 7
 8
[R20]: MUST support OSPFv2 (support for IPv4 only) as defined in RFC2328 [39], or OSPFv3 9
(IPv4/IPv6 support) as defined in RFC5340 [88] and RFC5838 [95]. 10
[R21]: MUST support “OSPFv3 Extensions for Segment Routing”, RFC8666 [126] 11
[D22]: SHOULD support “Signalling Maximum SID Depth (MSD) Using OSPF,” RFC8476 [119]  12
 13
[D23]: SHOULD support “IGP Flexible Algorithm”, draft-ietf-lsr-flex-algo-15 [138] 14
 15
[R22]: MUST support “OSPF TE Extensions”, as defined in RFC3630 [60] and RFC7471    16
 17
[D24]: SHOULD support Topology Independent Fast Reroute using Segment Routing “draft-ietf-18
rtgwg-segment-routing-ti-lfa-06 [141]  19
[D25]: SHOULD support authentication/confidentiality for OSPFv3, as defined in RFC4552 [68] 20
 21
11.4.2 SR/MPLS Traffic Engineering  22
SR/MPLS supports two forms of SR policies or Traffic Engineering. Both solutions rely on the IGP 23
to gather and convey topological and resource information around the network and optionally to an 24
SR Path Computation Element (SR-PCE) via BGP-LS.  25
11.4.2.1 Segment Routing Traffic Engineering (SR-TE) 26
The SR policy consists of a list of SIDs the packet needs to traverse and is programmed into the 27
packet on the source node. The SID list can consist of a mix of prefix and adjacency SIDs. In SR 28
this form of TE allows a path to be:  29
1. Loosely source routed where some intermediate points, but not all, are specified between the 30
ingress and egress node. Paths between these intermediate points typically use the shortest 31
path, ECMP based routing determined by the IGP. 32
2.  Explicitly source routed where all intermediate points and even links are specified between 33
the ingress and egress nodes. 34
 35
In all cases, topology information and live network status within a routing domain is distributed 36
within a routing domain by the IGP with suitable extensions. In single routing domain environments 37
path computation can be performed either by individual head-end routers or via a centralised “SR 38
Path Computational Element” (SR-PCE). In multi-domain routing environments, then path 39
computation generally occurs on a “centralised SR-PCE” component that has topology information 40
for all domains being traversed. This function can be a standalone entity or be integrated into 41
strategically placed TNEs. This form of traffic engineering can support loose and explicitly routed 42
paths, low-latency paths, bandwidth-guaranteed paths, and disjoint paths. Precise TE capability 43
depends on the capabilities of the SR-PCE component. 44
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        75
11.4.2.2 IGP Flexible Algorithm 1
IGP Flexible-Algorithm is described in section 11.4.1.2. It is an IGP based traffic engineering 2
technique that permits multiple logical topologies to be created in an IGP domain based on operator 3
programmed criteria. When used in a single IGP routing domain the IGP calculates and maintains 4
the path. The ingress node simply needs to address the packet to the MPLS label associated with the 5
flexible algorithm. This removes the need for a head-end or centralised PCE path computation 6
element and also keeps SID list contained in the label stack to a minimum.  7
This works in a single IGP domain but depending on area border router and autonomous system 8
border router capabilities can be supported in multi-domain environments based on redistribution.     9
 10
For flex-algo requirements for SR/MPLS, see section 11.4.1.3  and 11.4.1.4.  11
11.5 Scaling the MPLS infrastructure  12
In order to provide communication between two MPLS TNEs, end-to-end MPLS LSPs must be 13
available between the TNEs. This implies, that unique MPLS label towards the transport network 14
element, which possibly resides in different routing domain, must be available. In highly scaled 15
environment the number of potential next-hops can be high, therefore care must be taken to ensure 16
that resources, especially forwarding plane resources, are utilized in an efficient way, while 17
providing end-to-end MPLS connectivity. 18
 19
While the requirement for having unique label per remote transport network element increases the 20
pressure on scaling, the benefit of unique label is faster detection of remote transport network 21
element failure, resulting from unique underlay prefix/label withdrawal. For architectures, which 22
can use prefix summarization for remote transport network element reachability, like for example 23
SRv6 architecture discussed in Section 12, such capability is missing, since remote transport 24
network element failure does not influence underlay summary prefix state. 25
11.5.1 Seamless MPLS architecture 26
Seamless MPLS (draft-ietf-mpls-seamless-mpls) is applicable to for both LDP and SR based control 27
planes. Seamless SR is based on following architectural aspects: 28
 29
• Transport network is divided into multiple smaller routing domains. Routing domains can be 30
represented by separate BGP autonomous systems, or separate IGP domains (areas). Or, 31
combination of BGP autonomous systems and IGP areas, where BGP autonomous systems 32
are further divided with IGP areas.  33
• The size of each routing domain might vary, but must be chosen in such a way, that even the 34
weakest transport device in given routing domain can participate in intra-domain routing and 35
intra-domain MPLS path (SR, SR-TE, Flex-Algo, RSVP, LDP) establishment without any 36
scaling concerns. 37
• Inter-domain routing and MPLS path establishment is achieved by BGP labelled unicast 38
(BGP LU), running on top of intra-domain protocols. BGP-LU is used for both prefix 39
distribution (PE loopbacks), as well as label distribution (labels allocated for PE loopbacks) 40
 41
This concept is outlined in Figure 11-6. 42
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        76
 1
Figure 11-6 Seamless MPLS architecture 2
 3
BGP-LU operation can be further optimized for enhanced scaling: 4
 5
• BGP-LU prefixes (loopbacks of remote TNE) are not installed in the forwarding table (FIB) 6
– they are just installed in the routing table (RIB), which consumes only control plane 7
resources. On typical transport device, control plane resources scale better than forwarding 8
plane resources 9
• Service NLRIs (L3VPN, EVPN) are automatically filtered on route reflectors (not shown in 10
Figure 11-6, for diagram simplicity), and distributed only to the TNEs requiring specific 11
service prefixes. This is achieved with constrained route distribution. 12
• Only these automatically filtered service NLRIs are installed in the forwarding plane, using 13
the BGP-LU transport label of the required BGP-LU prefix (remote TNE loopback). 14
Therefore, usage of forwarding plane resources is highly minimized. 15
• Further optimization of control plane resources is possible, where TNE (PE) requests from 16
route reflector only limited set of BGP-LU prefixes. This can be achieved with outbound 17
route filtering (ORF) of BGP-LU prefixes with ORF filter automatically generated to allow 18
only BGP-LU prefixes required to resolve protocol next-hops of accepted (filtered with 19
constrained route distribution mechanism) service NLRIs. 20
 21
With Seamless MPLS architecture, ingress PE uses recursive next-hop resolution, where service 22
NLRI received from remote PE is resolved over BGP protocol next-hop (BGP-LU loopback of 23
remote PE), which in turn is resolved over local intra-domain MPLS tunnel (SR/SR-TE or legacy 24
RSVP/LDP). 25
 26
If an operator wishes to use Seamless MPLS architecture for enhanced scaling of an MPLS 27
underlay, then the transport device will require: 28
 29
[R23]: MUST support “Seamless MPLS architecture”, draft-ietf-mpls-seamless-mpls (note: this is 30
an expired IETF draft but is a widely referenced document that describes the architecture)  31
[R24]: MUST support BGP Labelled Unicast (BGP-LU), as defined in RFC3107/RFC8277 32
[56][113] 33
[D26]: SHOULD support Constrained Route Distribution for BGP/MPLS virtual private networks, 34
RFC4684 [73] 35
AS X AS Y
ISIS + BFD ISIS + BFD ISIS + BFD ISIS + BFD
ISIS L1 ISIS L2
ISIS + BFD ISIS + BFD
ACCESS PRE-AGGREGATION AGGREGATION CORE EDGE
BGP-LU BGP-LU BGP-LU BGP-LU
NHSNHS NHSNHS NHSNHS NHSNHS
LDP
 SR/SR-TE
 RSVP
Inter-domain (end-to-
end) colored BGP-CT
tunnels tunneled inside
intra-domain tunnel in
each domain
Intra-domain tunnels
(LDP, RSVP, SR/SR-TE)
no leaking
BGP-LU distributes prefixes
(router’s loopbacks) together with
their labels, across all domains,
with following scaling
optimization:
§ BGP-LU not installed in the
forwarding plane (just in the
control plane) à typical
transport node control plane
scales few times (10x) better
than forwarding plane
§ BGP service prefixes (L3VPN,
EVPN) use automatic
constrained route distribution
(RFC 4684) à only required
BGP service prefixes delivered
§ Optionally: Outbound Route
Filtering (ORF, RFC 5292)
generated automatically from
the next-hops of received BGP
service prefixes, further
decreases the scaling pressure
66
77 77 99 71
V V V VV
SR-TE stack: 24, 26, 29LDP: 66 à Impl. Null RSVP: 71 à Impl. Null
3998877
24
26
29
88
26
29
88
29
88 88
V V V V
O-RU
O-RU
O-RU
O-RU
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        77
[R25]: MAY support Outbound Route Filter (ORF) for BGP, RFC5291/RFC5292 [79][80] 1
 2
In scenarios where seamless MPLS is operating in heterogeneous environments where some 3
domains are LDP based and some are SR based, then a Segment Routing Mapping Server can be 4
used to provide an interworking function between the two environments.  5
 6
[D27]: SHOULD support Segment Routing MPLS Interworking with LDP, RFC8661 [124] 7
 8
11.5.2 Controller based network scaling architectures  9
In addition to the “Seamless MPLS” approach which is an infrastructure-based scaling technique, 10
an external controller in the form of a distributed “Path Computational Element” (PCE) could be 11
used for scaling across multiple MPLS domains. The approach aims to simplify the underlying 12
infrastructure by reducing interactions between domains at the infrastructure level and delegating 13
cross domain path determination to a distributed PCE component.   14
 15
Figure 11-7 shows the general architecture with a distributed PCE layer that gathers IGP topology 16
information from their respective domain. PCE’s can exchange topology information with each 17
other using BGP-LS. In scenarios where the headend is not able to resolve the next-hop locally, it 18
may rely on PCE to provide an End-to-End Path when requested. The headend communicates with 19
the PCE using Path Computation Element Protocol (PCEP). This mechanism can helps save 20
resources on CSR’s contributing to overall Xhaul Network scale. 21
 22
 23
Figure 11-7 General concept of a controller-based architecture 24
 25
The figure above shows network simplification and scalability by:  26
• Creating a multi domain topology, each with its own PCE Controller 27
• Removing extensive Route leaking and filtering requirements between IGP domains.  28
• Removing the need for the headend to populate its routing table with topology and routing 29
information.  30
• Use Passive Stateful PCE Path Computation Request/Response as per RFC 8231 to create 31
an end-to-end path.  32
  33
Each domain may have its own path consideration and constraints that can independently be 34
implemented and communicated to the PCE/PCEs. PCEs can calculate end to end path based on 35
these constraints and communicated to the headend TNE via PCEP as shown in Figure 11-8.  36
 37

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        78
 1
 2
Figure 11-8 Path with Domain specific constraints using PCE based architecture 3
 4
The SID list provided by the PCE to the headend is in actuality a label stack. Depending on the 5
network size and the use-case, the SID list may exceed the platform’s label stack capacity. In those 6
cases, the SID list across domains could be further optimized by using techniques offered by one of 7
the following two architectures.  8
 9
• Using Forwarding Adjacency (FA) between domain boundaries, with associated Adjacency 10
SID, abstracting the path between domain boundaries 11
• Using SR-TE policies between domain boundaries, with associated Binding SID, abstracting 12
the path between domain boundaries 13
 14
Both these approaches are optional architectures that could be used to further optimize the size of 15
the label stack in controller-based solutions.  16
 17
While the PCE is shown as a separate entity in the figure above, an operator may choose to either 18
have PCE functionality integrated into the infrastructure router(s) or use a dedicated device or 19
devices for PCE. As long as the PCE’s in the distributed PCE layer can exchange topology 20
information and communicate with their respective PCEP clients, the positioning of PCE 21
functionality can be a choice an operator makes based on their respective network environment.  22
 23
Following are the requirements that should be met for implementing a controller-based architecture. 24
These additions are in addition to baseline Segment Routing and Segment Routing Traffic 25
Engineering requirements already mentioned:  26
 27
[R26]: MUST support BGP Link State (BGP-LS), RFC7752 [106] 28
 29
[R27]: MUST Support BGP-LS Extensions for SR as per draft-ietf-idr-bgp-ls-segment-routing-ext-30
18 [137] 31
 32
[D28]: SHOULD support "Signaling Maximum SID Depth using Border Gateway Protocol Link-33
State", RFC8814 [129] 34
 35
[R28]: MUST support SRTE Policy configuration as outlined in draft-ietf-spring-segment-routing-36
policy-08 [146] 37
[R29]: MUST support "Path Computation Element Protocol (PCEP)", RFC5440 [90]  38

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        79
 1
[R30]: MUST support PCEP for communication between CSR and PCE as per “Path Computation 2
Element Communication Protocol (PCEP) Extensions for Segment Routing”, RFC-8664  3
[125]  4
[D29]: SHOULD support PCEP extension to support Segment Routing Policy Candidate Paths as 5
per draft-ietf-pce-segment-routing-policy-cp-08 [145] 6
[D30]: SHOULD support “PCEP Extensions for Stateful PCE”, RFC8231 7
 8
[D31]: SHOULD support “BGP – Link State (BGP-LS) Advertisement of IGP Traffic Engineering 9
Performance Metric Extensions”, RFC8571 [121] 10
 11
11.5.2.1 Forwarding Adjacency architecture for cross domain scale 12
Forwarding Adjacency abstracting the path between domain boundary routers is one of the ways to 13
optimize and scale a controller based Xhaul architecture.  Forwarding Adjacency architecture is 14
based on following architectural concepts: 15
 16
• Transport network is divided into multiple smaller routing domains. Routing domains can 17
be represented by separate BGP autonomous systems, or separate IGP domains (areas). Or, 18
combination of BGP autonomous systems and IGP areas, where BGP autonomous 19
systems are further divided with IGP areas.  20
• The size of each routing domain might vary, but must be chosen in such a way, that even 21
the weakest transport device in given routing domain can participate in intra-domain 22
routing and intra-domain MPLS path (SR, SR-TE, Flex-Algo, RSVP, LDP) establishment 23
without any scaling concerns 24
• Head-end router (border router) of intra-domain LSP advertises the intra-domain LSP (of 25
any kind, i.e. SR, SR-TE, Flex-Algo, RSVP, LDP) as standard link in BGP Link State 26
(LS) topology database, with attributes similar to standard link, like for example SR 27
Adjacency-SID, administrative group (link color, link affinity), SRLG, etc. IGP 28
forwarding adjacency is established (RFC 4206) between domain border routers creating 29
abstracted topology of the routing domain. 30
•  BGP-LS (RFC 8571) is used to distribute abstracted topology. Given the fact that only 31
border routers, and only abstracted forwarding adjacency links between these border 32
routers are exported, the size of the topology database exported via BGP-LS is 33
considerably smaller. BGP-LS distribution can happen in two way, depending on the 34
overall network scale 35
o BGP-LS topology is exchanged between routers in different routing domains 36
o BGP-LS topology is exchanged between PCEs in distributed PCE layer 37
 38
The first approach is more suitable for small to medium scale deployments. In this approach, any 39
transport network element has the detailed topology visibility of the local routing domain, and 40
abstracted (with Forwarding Adjacencies advertised as links in BGP-LS) visibility of the remote 41
routing domains. This significantly reduces the topology database stored on the transport network 42
elements, therefore, in small/medium deployments PCE might not be necessary. 43
 44
The headend PE, based on the full topology database (detailed topology from local routing domain 45
+ topology abstracted with links representing forwarding adjacencies from remote routing 46
domains), performs calculation of end-to-end SR-TE paths (local constrained shortest path first – 47
CSPF – calculation). Each forwarding adjacency is represented in the resulting SR-TE label stack as 48
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        80
single Adjacency-SID associated with given forwarding adjacency, regardless of the underlying 1
MPLS transport used for given forwarding adjacency. At each border node, this Adjacency SID is 2
mapped to the actual transport label stack (LDP, RSVP, SR, SR-TE) used for the given forwarding 3
adjacency. 4
 5
This concept is outlined in Figure 11-9. 6
 7
Figure 11-9 Forwarding Adjacency architecture 8
 9
As further scaling optimization of this architecture, ingress PEs (typically cell site routers, with 10
limited control plane resources required for efficient end-to-end path computation) can be off-11
loaded from end-to-end path computation. Distributed PCEs (Path Computation Elements) can be 12
placed on more powerful transport devices (for example pre-aggregation or aggregation routers), or 13
containerized/virtualized PCE function can be placed in the distributed telco cloud environment to 14
execute end-to-end path computation duties over simplified (via forwarding adjacencies) full 15
network topology. In this scenario, ingress PE requests from PCE on demand path computation for 16
unresolved next-hops, as outlined in Figure 11-10. 17
 18
 19
Figure 11-10 Forwarding Adjacency architecture with distributed PCE layer 20

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        81
 1
If an operator wishes to use a forwarding adjacency based architecture for enhanced scaling of 2
MPLS underlay, then the transport device will require: 3
 4
[R31]: MUST support “forwarding adjacency links”, RFC4206 [114] 5
 6
11.5.2.2 Using Binding SID (BSID) for cross domain scale  7
As mentioned earlier, a controller-based architecture make use of the PCE to provides the SID list 8
to the headend for an end-to-end path across multiple domains. If no constraints are imposed on the 9
traffic, the SID list would consist of Area Border Routers Prefix SIDs as next hop. Traffic within 10
the domain will use IGP to get to the next hop while utilizing well established IGP routing 11
mechanism such as Equal Cost Multi Path (ECMP), metric based best path etc. At the ABR, the top 12
most SID is popped and the new next hop SID/Label is exposed, starting the routing within the next 13
IGP are or domain. The process is repeated until traffic reaches its destination. Figure 11-11 shows 14
such behaviour between IGP Domain 0,1 and 2.  15
 16
 17
 18
 19
 20
 21
 22
Figure 11-11 Basic SID list with no constraints  23
 24
There may be scenarios where more explicit path considerations and constraints could be required, 25
such as using low latency path or inclusion/exclusion of certain links or node. In those scenarios, 26
rather than provide a SID list that encompasses every link/node through the path, mechanisms such 27
as Flex Algo could be used to optimize the SID list on the head end. Figure 11-12 shows a scenario 28
with latency and link/node constraints across the path. In this case operator may use Flex-Algo 29
within a domain and use the PCE controller to provide SID list to the headend accordingly. Notice 30
while the SID depth is no different from the earlier scenario Figure 11-12) and while it still only 31
contain prefix-SIDs of ABR as next hop label, the SID’s now refer to the node’s Flex-Algo Prefix 32
SID, instead of default algo SID. Due to this, the traffic behavior is also different and the next Hop 33
SID automatically uses the desired constraints-based path.  34
 35

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        82
 1
 2
Figure 11-12 Constraint based SID list with Flex-Algorithm   3
 4
In scenarios where an operator is unable or unwilling to use Flex-Algo for SID list optimization, an 5
SRTE Binding SID (BSID) could be used. A Binding SID is a fundamental component of Segment 6
Routing network that may be used to provide SID and network scaling. A BSID is a SID that is 7
associated with an SRTE policy’s active path and is a representation of an End-to-End candidate 8
path. BSIDs could be used to provide additional scale by using nested SRTE policies across 9
domains. The PCE instead of providing a full path through multiple domains, may provide the 10
headend with a path within its own domain and a BSID corresponding to the next domain. When, 11
exposed at the domain boundaries, the BSID gets translated into another set of SID List, providing 12
extensibility and scalability throughout the topology. This process is shown in Figure 11-13. 13
 14
 15
 16
 17
Figure 11-13 SID scaling using binding SID  18
 19
Figure 11-13 compares the two scenarios where an End-to-End path is required with latency and 20
link/nodes constraints along the path. The first scenario is where the PCE provides a full SID list to 21
the headend for the entire path, with based path is required end to end. The second scenario uses a 22
Binding, exposed at domain boundaries and then use the appropriate constrains based SID within 23
the domain in question. 24
 25

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        83
If an operator wishes to implement controller based architectural scaling, in addition to underlying 1
segment-routing based transport with controllers, a device will need to support:  2
 3
[R32]: SHOULD support Binding SID as per draft-ietf-pce-binding-label-sid-08 [142] 4
11.6 MPLS Quality of Service 5
The MPLS data plane includes a 3-bit field in the MPLS header, the TC – Traffic Class (previously 6
EXP - Experimental) bits. These bits are assignable as a marker for QoS mechanisms in transport 7
nodes to use as a classification and marking tool. The per-hop behavior for MPLS forwarding 8
elements can be defined based on these markings. Because of the restricted size of the TC field, the 9
specific markings are not standardized, but are open for individual operators to define. For a full 10
discussion of a proposed marking scheme and QoS architecture for TNEs in Xhaul, see Chapter 14. 11
11.7 MPLS OAM 12
Basic OAM tools are Ping and Traceroute. Ping is required to test liveliness to a remote MPLS 13
underlay destination and traceroute is required to trace paths and perform hop-by-hop fault isolation 14
to remote MPLS underlay destination. The following capabilities are required on all TNEs. 15
 16
[R33]: MUST support “Detection of MPLS Data Plane Failures”, RFC8029 [109]  17
 18
[R34]: MUST support LSP Ping/Traceroute for SR IGP Prefix-SID and IGP Adjacency-SID with 19
MPLS Data Plane, as defined in RFC8287 [114] 20
 21
11.8 IP/MPLS service infrastructure 22
For IPv4, IPv6 and Ethernet services an overlay solution is used. Please refer to section 13 for a 23
description of overlay service recommendations for a 5G Xhaul infrastructure.   24
12 Packet-switched underlay network – SRv6 based 25
This document presents two packet switched underlay technologies, the first based on MPLS 26
contained in section 11 and the second based on SRv6 contained in this section (section 12). An 27
operator wishing to implement the transport architecture outlined in this document will need to 28
select one of the two and implement the requirements outlined in the associated section. It should be 29
noted that other transport architectures are potentially available but not covered in this revision of 30
the document. This section outlines a packet switched underlay model based on SRv6.   31
 32
SRv6 is based on the segment routing architecture as defined in RFC8402 [118].  For more 33
information on Segment Routing see annex A. In an SRv6 infrastructure, like an MPLS 34
environment, it is important to consider the underlay / fabric of the transport infrastructure 35
somewhat separately from the services that run on-top of the infrastructure. The emphasis with the 36
underlay / fabric is to provide an environment that will scale and support the services required by a 37
5G infrastructure. In contrast, the services infrastructure runs on-top of the underlay/fabric of the 38
transport network and supports the different components of the 5G infrastructure (Fronthaul, 39
Midhaul, Backhaul).   40
 This chapter is specific to an SRv6 underlay/fabric, its data plane, control plane and how to scale it. 41
Although there are some similarities with an SR-MPLS environment, in terms of architecture, there 42
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        84
are some key differences in requirements and particularly on how to scale the underlay 1
infrastructure, which is critical in a 5G environment. 2
 3
 The service infrastructure and how to support 5G service requirements for both SR-MPLS and 4
SRv6 are covered in a common services section (section 13) as the technologies used and the 5
designs have many similarities.   6
12.1 SRv6 data plane  7
SRv6 relies on an IPv6 data plane with segments defined using SIDs contained in the IPv6 header. 8
SRv6 SIDs are specified in “Segment Routing Header” (SRH) and standardised in RFC8754 [128]. 9
Packet switching TNEs supporting SRv6 will need to: 10
 11
[R35]: MUST support “IPv6 Segment Routing Header (SRH)” RFC8754 [128] 12
 13
[R36]: MUST support “SRv6 Network Programming”, RFC8986 [147] 14
 15
12.2 SRv6 control plane  16
The SRv6 control plane refers to routing and path control within the SRv6 underlay/fabric. This 17
infrastructure can extend to customer devices or stop at a Provider Edge (PE) function. Its role can 18
be summarised as learning the topology, calculation, and implementation of dynamic and explicit 19
routes across the SRv6 infrastructure and providing rapid protection and repair mechanisms. 20
In a packet switched transport network consisting of a large number of routers, a mix of WAN and 21
data centers components, the need to support traffic engineering, the underlay control plane is a 22
typically a collection of independent routing domains that interact in a collaborate fashion.  23
 24
 25
Figure 12-1 SRv6 underlay architecture 26
Figure 12-1 illustrates how a large packet-switched transport infrastructure might be designed and 27
how different components interact. It should be noted that not all these components are required, 28
and some serve the same purpose but in different ways.  29
1. Each routing domain has an IGP for internal connectivity. 30
2. Mechanisms to calculate SR policies or Traffic Engineer paths and convey them to the 31
source nodes. Figure 12-1 shows a distributed “Path Computation Element” (PCE), with 32

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        85
“Path Computation Element Protocol” (PCEP) running between the edge nodes and the PCE 1
but there are other network-based mechanisms to calculate SR policies such as Flex-algo 2
running in the IGP or head-end calculated paths.   3
3. Mechanisms to convey routing between IGP domains. This could be provided by 4
summarization and redistribution, E-BGP between AS boundaries or via a multi-domain 5
“Path Computation Element” (PCE). One of the key differences between an MPLS and IPv6 6
based underlay is that IPv6 enables address summarization within routing domains and also 7
between routing domains. This makes an IPv6 based underlay network more scalable and 8
less complex than a corresponding MPLS solution. 9
4. Mechanisms to convey topology and network state information from the network to the PCE 10
and other central management elements. BGP Link State (BGP-LS) and Telemetry feeds are 11
recommended tools to fulfil this requirement. 12
12.2.1 Interior Gateway Protocol (IGP) for SRv6 13
SRv6 requires at least one IPv6 IGP per routing domain or autonomous system.  There are two IGPs 14
that support SRv6; ISIS for IPv6 or OSPFv3. They are both link-state protocols and have similar 15
capabilities but there are operational differences which is beyond the scope of this document.  16
 17
The IGP provides internal connectivity within a routing domain. The size of a routing domain is 18
determined by technical, operational and organizational considerations, such as protocol scalability 19
and spans of control. In large networks, such as a 5G infrastructure, extending from the access to 20
the transport core, it is very common to see multiple autonomous systems, IGPs and segmentation 21
within the IGPs. 22
 23
In an SRv6 environment the IGP is responsible for distributing node, prefix and SID information to 24
all nodes within the routing domain and for calculating the forward tables between these nodes. 25
Optionally it can provide fast convergence mechanisms and distribute traffic engineering attributes 26
within the domain and build multiple forwarding planes.  Both IS-IS for IPv6 and OSPFv3 have 27
facilities to create a hierarchical routing structure within a routing domain using levels and areas 28
respectively. They also have the facility to bring external routing information into a routing domain 29
and push routing information into other routing domains using route redistribution.  30
 31
Note: There are other techniques to achieve route re-distribution between routing domains, such as 32
BGP and centralised SDN controllers. 33
Note: When looking at SRv6 IGP requirements, there are sets of requirements for ISIS for IPv6 and 34
OSPFv3. Operators need to follow the requirements associated with IGPs they are using within 35
their network infrastructure.   36
12.2.1.1 Topology Independent Loop Free Alternative (TiLFA) 37
In the SRv6 underlay design outlined in this document TiLFA can be used to achieve fast IGP 38
convergence in the event of a network failure.  Topology Independent Loop Free Alternative as 39
defined “Topology Independent Fast Reroute” using Segment Routing [141] is an IGP capability 40
supported by Segment Routing that provides a local repair mechanism that achieves 100% coverage 41
within a routing domain against links, nodes and SRLGs failures with convergence times, once 42
failure detection has occurred, H/W dependent but typically less than 50ms. 43
 44
For each destination in the network, TI-LFA pre-installs a backup forwarding entry for each 45
protected destination ready to be activated upon detection of the failure of a link used to reach the 46
destination.  TI-LFA provides protection in the event of any one of the following: single link 47
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        86
failure, single node failure, or single SRLG failure.  In link failure mode, the destination is protected 1
assuming the failure of the link.  In node protection mode, the destination is protected assuming that 2
the neighbor connected to the primary link has failed.  In SRLG protecting mode, the destination is 3
protected assuming that a configured set of links sharing fate with the primary link has failed (e.g. a 4
linecard or a set of links sharing a common transmission pipe). The mechanics of the TiLFA for 5
SRv6 at failure is shown in Figure 12-2 and upon convergence in Figure 12-3. One of the key 6
advantages of TiLFA over some other fast convergence mechanisms, not well illustrated in the 7
example below, is that the TiLFA backup path is the same as the post convergence path, thus 8
minimising micro-loops that can occur as IGPs converge.    9
 10
 11
 12
Figure 12-2 TiLFA at failure with SRv6 13

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        87
 1
 2
 3
Figure 12-3 Post convergence traffic flow with SRv6   4
12.2.1.2 IGP Flexible-Algorithm 5
IGP Flexible-Algorithm is an IGP based traffic engineering technique that permits multiple 6
forwarding tables to be created in the IGP based on operator programmed criteria. It is a simple, 7
automatic way, in which a packet switched transport network can be traffic engineered, using only 8
the IGP, to create different forwarding planes designed to meet specific operator defined criteria.  9
 Standard ISIS for IPv6 and OSPFv3 use the IGP metric on links and the “Shortest Path First” (SPF) 10
algorithm to calculate the “best” path between an ingress and egress point in the network. For many 11
services this approach is sufficient, however this approach cannot, for example, take into account 12
real-time link latency, utilization, packet loss and whether an ECMP path shares links using the 13
same underlying fibre ducts.  14
 15
 IGP Flex-Algo, defined in draft-ietf-lsr-flex-algo-15 [141] is an IGP based Segment Routing traffic 16
engineering capability, that enables an operator to define their own custom algorithm, based on a 17
wide range of variables including: link latency, packet loss, bandwidth, affinity and shared risk link 18
groups (SRLG), to achieve a specific forwarding aim. This is illustrated in Figure 12-4. 19

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        88
 1
Figure 12-4 Flex-algo example with three topologies  2
 3
This enables an operator, using a single IGP instance, to build multiple forwarding tables within an 4
SRv6 underlay network based on different optimization criteria. The default forwarding instance 5
uses shortest path forwarding based on IGP metrics, but an operator can build additional IGP 6
Traffic Engineered forwarding tables based, for example on lowest latency or avoiding certain links 7
or a combination of the two. 8
 9
The requirements to support flex-algorithm are all associated with the IGP requirements and 10
contained in sections 12.2.1.3 and 12.2.1.4. 11
 12
12.2.1.3 ISIS for IPv6 base requirements  13
If an operator wishes to use ISIS for IPv6 with TiLFA and optionally Flexible Algorithm (flex-algo) 14
with SRv6 then the TNE will require: 15
 16
[R37]: MUST support routing IPv6 with ISIS as defined in RFC5308 [85]. 17
 18
[R38]: MUST support “IS-IS Extensions to Support Routing over IPv6 Dataplane”, draft-ietf-lsr-19
isis-srv6-extensions-14 [143] 20
[R39]: MUST support “Topology Independent Fast Reroute using Segment Routing”, draft-ietf-21
rtgwg-segment-routing-ti-lfa-06” [141] 22
[R40]: MUST support IS-IS TE Metric Extensions, as defined in RFC5305 [84] and RFC7810 [108]. 23
 24
[D32]: SHOULD support “IGP Flexible Algorithm”, draft-ietf-lsr-flex-algo-15 [138]. 25
 26
12.2.1.4 OSPFv3 basic requirements  27
If an operator wishes to use OSPFv3 with TiLFA and optionally flexible Algorithm (flex-algo) with 28
SRv6 then the TNE will require: 29
 30
[R41]: MUST support OSPFv3, RFC5340 [88]. 31
 32

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        89
[R42]: MUST support “OSPFv3 Extensions for SRv6”, draft-ietf-lsr-ospfv3-srv6-extensions-02 1
[148] 2
[R43]: MUST support “Topology Independent Fast Reroute using Segment Routing”, draft-ietf-3
rtgwg-segment-routing-ti-lfa-04 [141]. 4
[R44]: MUST support OSPFv3 TE Metric Extensions, as defined in RFC5329 [87]. 5
 6
[D33]: SHOULD support “IGP Flexible Algorithm”, draft-ietf-lsr-flex-algo-15 [138]  7
  8
12.2.2 SRv6 Traffic Engineering 9
SRv6 supports two forms of SR policies or SR Traffic Engineering. Both solutions rely on the IGP 10
to gather and convey topological and resource information around the network and optionally to a 11
Path Computation Element (PCE) optimized for SR via BGP-LS.  12
12.2.2.1 Segment Routing Traffic Engineering (SR-TE) 13
The SR policy consists of a list of SIDs the packet needs to traverse and is programmed into the 14
packet on the source node. The SID list can consist of a mix of prefix and adjacency SIDs. In SR 15
this form of TE allows a path to be:  16
1. Loosely source routed where some intermediate points, but not all, are specified between the 17
ingress and egress node. Paths between these intermediate points typically use the shortest 18
path, ECMP based routing determined by the IGP. 19
2. Explicitly source routed where all intermediate points and even links are specified between 20
the ingress and egress nodes. 21
 22
In all cases, topology information and live network status within a routing domain is distributed 23
within a routing domain by the IGP with suitable extensions. In single routing domain environments 24
path computation can be performed either by individual head-end routers or via a centralised “SR 25
Path Computational Element” (SR-PCE). In multi-domain routing environments, then path 26
computation generally occurs on a “centralised SR-PCE” component that has topology information 27
for all domains being traversed. This function can be a standalone entity or be integrated into 28
strategically placed TNEs. This form of traffic engineering can support loose and explicitly routed 29
paths, low-latency paths, bandwidth-guaranteed paths, and disjoint paths. Precise TE capability 30
depends on the capabilities of the SR-PCE component. 31
 32
12.2.2.2 IGP Flexible Algorithm 33
IGP Flexible-Algorithm is described in section 12.2.1.2. It is an IGP based traffic engineering 34
technique that permits multiple logical topologies to be created in an IGP domain based on operator 35
programmed criteria. When used in a single IGP routing domain the IGP calculates and maintains 36
the path. The ingress node simply needs to address the packet to the “locator-block” associated with 37
the flexible algorithm. This removes the need for a head-end or centralised PCE path computation 38
element and also keeps SID list to a minimum.  39
 40
This works in a single IGP domain but depending on area border router and autonomous system 41
border router capabilities can be supported in multi-domain environments based on redistribution.     42
For flex-algo requirements in an SRv6 environment see section 12.2.1.3 and 12.2.1.4.  43
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        90
12.2.3 Inter-domain connectivity 1
In a large packet-switched transport network the underlay infrastructure will need to be sub-divided 2
into multiple routing domains. These routing domains can be within an autonomous system, at the 3
IGP level, or between autonomous systems.  One motivation is IGP scalability, but other reasons 4
include fault isolation between domains and organizational, for example different groups run the 5
DC and WAN infrastructures. However, even though the infrastructure is divided, connectivity 6
between routers in different routing domains is still required to build end to end services, therefore 7
the underlay infrastructure needs mechanisms to enable inter-domain connectivity and services to 8
be built inter-domain. In an SRv6 environment, potential mechanisms for interchanging routing 9
between domains include: 10
 11
1. IGP based hierarchy 12
2. Inter-domain IGP redistribution 13
3. Inter AS BGP   14
4. Controller based  15
 16
12.2.3.1 IGP based hierarchy 17
Both ISIS for IPv6 and OSPFv3 have the concept of routing hierarchy in their basic designs. In an 18
SRv6 environment, route summarization or injection of a default route between IS-IS levels or 19
OSPFv3 areas is sufficient to enable EVPN or L3VPNs services to be built over an SRv6 20
hierarchical IGP infrastructure.  21
 22
12.2.3.2 Inter-domain IGP redistribution  23
One or more routers are located on the boundary between two or more IGP routing domains. These 24
routers run multiple IGP instances, so have routing awareness of all the IGPs they participate in. 25
Routing information is mutually re-distributed between the IGP protocols allowing reachability 26
between the domains. This redistribution can be in the form of full routing, summarised routes or 27
even default routes. The choice is dependent on the level of awareness required between domains.   28
 29
 See Figure 12-5 for more detail. IGP route re-distribution in both IS-IS for IPv6 and OSPFv3 is a 30
common capability.   31
 32
12.2.3.3 Inter-AS BGP based routing  33
A router in each autonomous system exchanges routing information using BGP-4 supporting the 34
IPv6 address family. It should be noted that this scheme provides better separation than inter-35
domain IGP redistribution but typically requires separate routers in each autonomous system.  36
See Figure 12-6 for more detail.   37
 38
12.2.3.4 Controller based path calculation    39
In this scenario no routing information is exchanged between routing domains at the device level. 40
Instead, each routing domain provides its topology to an SR Path Computation Element (SR-PCE), 41
which is a central SDN control element responsible for determining and finding routes to a 42
destination node on behalf of a source node. Although the SR-PCE is logically a central component, 43
for scale and resiliency reasons it can consist of multiple individual entities distributed around and 44
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        91
serving different parts of the network. In this case the source TNE requires very limited visibility of 1
the overall network, as end to end cross-domain path computation is delegated to the SR-PCE.  2
 A SR-PCE function in an SRv6 underlay transport network has three main functions. 3
1. Gather data about the topology of the overall SRv6 network underlay. Several options exist: 4
a. The SR-PCE participates in the IGP domains and directly collects each domains IGP 5
link-state database. 6
b. Use BGP Link-State (BGP-LS) with appropriate extensions for SRv6 to collect the 7
IGP’s link-state database from each routing domain. This is done through one or 8
more BGP session between the SR-PCE and a TNE in each routing domain 9
participating in the domain’s IGP. The TNEs retrieve information from the IGP 10
LSDB and distribute it to the controller using the BGP link-state address family.  11
2. Communications between the PCE and PCC (path computation client) which is the headend 12
TNE. In this document it is assumed to PCEP (path computation Element Protocol) is used as 13
the communication mechanism between the source TNE and the SR-PCE. 14
3. Path computation. The PCE uses information gathered from the various domains to compute 15
an SR policy consisting of a SID list which packets using the SR policy will traverse. This 16
SID list can be a loosely sourced routed or explicitly source routed.    17
 18
12.3 Scaling an SRv6 underlay infrastructure 19
Scaling is one of the key challenges in building a packet switched infrastructure supporting 5G 20
services. Communications and support of L2 and L3 VPN services between two SRv6 TNEs relies 21
entirely on IP routing mechanisms on the underlay transport fabric. In SRv6 this is based on longest 22
IP prefix matching and forwarding, so TNEs can support routing and forwarding based on full IPv6 23
host routes, summarized IPv6 routes or default routing or a combination, derived from dynamic 24
routing protocols or statically defined. Further, different TNEs in the end to end path can use 25
different levels of route summarization appropriate to their position in the network and their 26
computation, memory and NPU/ASIC resources. 27
 28
12.3.1 Route summarization and redistribution 29
Summarization within ISIS for IPv6 and OSPFv3 is part of the base protocol capability and can 30
occur between levels in IS-IS for IPv6 and between areas in OSPFv3 using “Area Border Routers” 31
(ABRs). Summarization is also a common capability when redistributing routes between different 32
routing domains and is also supported between Autonomous Systems using BGP-4 between 33
“Autonomous System Border Routers” (ASBRs).  This is illustrated in Figure 12-5 and Figure 12-6 34
and shows how PE1-4’s routing tables can be controlled through route summarization.  35
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        92
 1
 2
Figure 12-5 Summarization / redistribution using IGPs 3
 4
 5
Figure 12-6 Summarization / redistribution using BGP-4 6
 7
This approach offers considerable flexibility, as operators can choose the level of summarization, 8
ranging from no summarization to default routing between domains depending on the size of the 9
network and level of awareness required between domains. It can also offer inter-domain traffic 10
engineering support based on flexible algorithm within an IGP domain. However, there are some 11
considerations and limitations:   12
1. Address planning is critical and needs to occur upfront.  13
2. Route summarization might result in sub-optimal routing in certain designs. 14
3. Some traffic engineering scenarios cannot be achieved with route summarization alone.  15
4. A source TNE cannot rely on a host route withdrawal to detect a remote TNE failure covered 16
by a summary route. In this situation over forms of device or service verification are required.    17
 18

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        93
[D34]: ABRs SHOULD have flex-algorithm aware summarization/redistribution function. This is a 1
local behavior. 2
 3
To use BGP-4 to transmit underlay SRv6 routing between Autonomous Systems requires:  4
 5
[R45]: BGP with IPv6 multi-protocol extensions (RFC1771, RFC2283, RFC2545) [35][40][43] 6
 7
12.3.2 Controller based scaling 8
Route summarization and redistribution is simple and allows massive scale and allows end to end 9
traffic engineering based on flex-algo. However, some traffic engineering scenarios cannot be 10
achieved, such as path diversity and bandwidth optimization, because summarization hides 11
topological details.  12
 13
To achieve more complex TE solutions, end-to-end visibility of the network is required, and a 14
controller-based solution based on an SR-PCE is used.  The SR-PCE receives detailed topology 15
information via BGP-LS protocol from each domain. The SR-PCE then has a full picture of end to 16
end reachability. When an ingress PE need to establish an end to end path it requests information 17
from SR-PCE via PCEP protocol. SR-PCE responds with stack of SIDs to establish path to egress 18
PE. 19
Figure 12-7 illustrates an SR-PCE based solution where PE1 needs to establish shortest path to PE3. 20
 21
 22
 23
Figure 12-7 Path computation based on SR-PCE 24
 25
1. SR-PCE gathers topology from each domain via BGP-LS. 26
2. PE1 sends a PCEP request to SR-PCE 27
3. PCE will respond with list of SIDs fcbb:bb00:300, fcbb:bb00:203:ffff::. 28
 29
For more complex path establishment SR-PCE will provide more complex SID list. 30
 31
[R46]: PCE and at least one TNE in each domain MUST support BGP Link State (BGP-LS), 32
RFC7752 [106] 33

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        94
[R47]: SR-PCE and at least one TNE in each domain MUST support “BGP Link State extensions 1
for IPv6 Segment Routing (SRv6)”, draft-ietf-idr-bgpls-srv6-ext-07 [149] 2
 3
[D35]: SR-PCE and at least one TNE in each domain SHOULD support extensions to advertise 4
Flexible Algorithm Definition as part of the topology, draft-ietf-idr-bgp-flex-algo-07 [138] 5
 6
[R48]: SR-PCE and TNEs providing PE functionality MUST support Path Computation Element 7
Protocol (PCEP), as described in RFC5440 [90] 8
 9
[R49]: SR-PCE and TNEs providing PE functionality MUST support PCEP protocol client 10
functionality with SRv6 extensions, draft-ietf-pce-segment-routing-IPv6-07 [144] 11
 12
[D36]: SR-PCE SHOULD support “IGP Flexible Algorithm be able to compute paths based on 13
different flexible algorithms 14
 15
12.3.3 SRv6 scaling conclusion 16
To build an underlay transport network that is highly scalable and supports traffic engineering the 17
simplest way is to uses a multi-domain IGP that supports flex-algo and use address summarization. 18
This design might not be sufficient for all application requirements. In these instances, a PCE based 19
solution could be deployed in tandem for TE use cases that require additional network visibility.   20
12.4 IPv6 Quality of Service 21
The IPv6 packet includes a 6-bit field in the IPv6 header, the DiffServ Code Point (DSCP). These 22
bits are assignable as a marker for QoS mechanisms in transport nodes to use as a classification and 23
marking tool. The per-hop behavior for IPv6 forwarding elements can be defined based on these 24
markings. For a full discussion of a proposed marking scheme and QoS architecture for TNEs in 25
Xhaul, section 14. 26
12.5 SRv6 OAM  27
SRv6 OAM functionality is required to understand the status of the network. SRv6 OAM is 28
described in “Operations, Administration, and Maintenance (OAM) in Segment Routing Networks 29
with IPv6 Data plane (SRv6)” “draft-ietf-6man-spring-srv6-oam” [150] 30
12.5.1 Ping / Traceroute to a remote IPv6 network address  31
Ping is required to test liveliness to a remote SRv6 underlay address and traceroute is required to 32
trace paths and perform hop-by-hop fault isolation to remote SRv6 underlay address. The following 33
capabilities are required on all TNEs. 34
 35
[R50]: TNEs MUST support ICMPv6 (RFC4443) [67] 36
 37
[R51]: TNEs MUST support IPv6 ping to query liveliness of a remote IPv6 address along the 38
shortest path for default SPF algorithm.  39
 40
[R52]: TNEs MUST support IPv6 ping to query liveliness of a remote IPv6 address along a path 41
calculated by a flex-algorithm.  42
 43
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        95
[R53]: TNEs MUST support IPv6 ping to query liveliness of a remote IPv6 address along a path 1
designated by a list of SIDs.    2
 3
[R54]: TNEs MUST support IPv6 traceroute to trace the path to a remote IPv6 address along the 4
shortest path for default SPF algorithm  5
 6
[R55]: TNEs MUST support IPv6 traceroute to trace the path to a remote IPv6 address along a path 7
calculated by a flexible algorithm. 8
 9
[R56]: TNEs MUST support IPv6 traceroute to trace the path to a remote IPv6 address along a path 10
designated by a list of SIDs. 11
 12
12.5.2 Ping / Traceroute to remote SID functions  13
Ping is required to test liveliness to a remote SID and traceroute is required to trace paths and 14
perform hop-by-hop fault isolation to a remote SRv6 SID. The following capabilities are required 15
on all Transport Nodes. 16
 17
[R57]: TNEs MUST support IPv6 ping to query liveliness of a remote EVPN and L3VPN services 18
along the shortest path for default SPF algorithm.  19
 20
[R58]: TNEs MUST support IPv6 ping to query liveliness of a remote EVPN and L3VPN services 21
along a path calculated by a flex-algorithm.  22
 23
[R59]: TNEs MUST support IPv6 ping to query liveliness of a remote EVPN and L3VPN service 24
along a path designated by a list of SIDs.  25
 26
[R60]: TNEs MUST support IPv6 traceroute to trace the path to a remote EVPN and L3VPN 27
service along the shortest path for default algorithm  28
 29
[R61]: TNEs MUST support IPv6 traceroute to trace the path to a remote EVPN and L3VPN 30
service along a path calculated by a flexible algorithm. 31
 32
[R62]: TNEs MUST support IPv6 traceroute to trace the path to a remote EVPN and L3VPN 33
service along a path designated by a list of SIDs. 34
 35
12.6 SRv6 on-going standardisation  36
SRv6 is a deployed network architecture based on RFCs and IETF adopted standards track 37
drafts.  Segment Routing is an active area of work at the IETF under the SPRING working group. 38
One area of study is the compression of SRv6 information. To this end, SPRING has formed a 39
design team to build requirements and analyse proposals. Future versions of this document may 40
describe compression techniques and other standards from the IETF as they are mature enough and 41
determined to be of use in the Xhaul based packet switched underlay fabric.  42
12.7 SRv6 Service infrastructure 43
An SRv6 transport underlay supports basic IPv6 services natively at the control and data plane. For 44
Ethernet, IPv4 and IPv6 VPN services an overlay control plane infrastructure is used to convey 45
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        96
VPN connectivity information. At the data plane the P devices can utilise a standard IPv6 data plane 1
with the PE devices requiring an IPv6 data plane with SRv6 VPN network programming awareness 2
[147].  Please refer to section 13 for a description of overlay service recommendations for a 5G 3
Xhaul infrastructure.   4
 5
13 Packet-switched Xhaul services Infrastructure 6
 To support an Xhaul environment requires the packet switched network to support L2 and L3 7
services. Both an MPLS and IPv6 packet switched underlays use EVPN to support L2 and MP-BGP 8
L3VPNs.  9
13.1 MP-BGP design 10
Both EVPN and MP-BGP L3VPNs use MP-BGP, with appropriate address-family support for 11
EVPN and L3VPN, to convey service connectivity information between Provider Edge (PE) 12
equipment. 13
Typically, the MP-BGP infrastructure for L2 and L3 services uses a Route Reflector design rather 14
than an I-BGP mesh between all PEs. In large networks, operators will normally implement a 15
hierarchical Route Reflector (RR) design for Multi-Protocol BGP (MP–BGP) peering. A pair of 16
Route Reflectors (RR) could be used at every domain to provide scalability and extensibility.  17
 18
 19
 20
Figure 13-1 Hierarchical Route Reflector Design for Multiple Protocol BGP 21
 22
Figure 13-1 shows the Hierarchical Route Reflector Design utilizing RR pairs in each network 23
domain, all peering with National Route Reflector core.   24
13.2 Ethernet services  25
Ethernet services will be provided by EVPN over either MPLS or IPv6 depending on the underlay 26
network employed by the operator. More details on EVPN are provided in annex B. 27
 28
EVPN VPWS can be used as transport service for Open Fronthaul (eCPRI), as well a transport 29
service for Radio over Ethernet (RoE) as outlined in following Figure 13-2 and Figure 13-3. 30

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        97
 1
 2
Figure 13-2 EVPN VPWS for eCPRI 3
 4
 5
Figure 13-3 EVPN VPWS for RoE 6
 7
To support EVPN VPWS for eCPRI or RoE, the transport device will need to support: 8
 9
[D37]: SHOULD support “Flow-Aware Transport of Pseudowires over an MPLS Packet Switched 10
Network”, RFC 6391 [98] 11
[R63]: MUST support “Virtual Private Wire Service Support in Ethernet VPN”, RFC 8214 [111] 12
 13
[D38]: SHOULD support “Extensions to BGP-Signaled Pseudowires to Support Flow-Aware 14
Transport Labels”, Internet Engineering Task Force, RFC 8395 [117] 15
[R64]: If using SRv6 as the underlay technology TNEs providing PE functionality MUST support 16
SRv6 BGP based Overlay services: draft-ietf-bess-srv6-services-07 [135]  17
 18
13.2.1 Ethernet services redundancy 19
EVPN VPWS can provide redundancy for O-RU to O-DU (Open Fronthaul) interface to react to the 20
transport network errors that might occur. 21
 22
Cell site
Hub site
CSR HSR
eCPRI flow
EVPN VPWS
O-RU O-DUeCPRI
Fronthaul
Cell site
Hub site
CSR HSR
CPRI flow
RoE flow
EVPN VPWS
RRH BBU
Fronthaul
RoE
Mapper
RoE
Mapper
RoE
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        98
13.2.1.1 Ethernet services redundancy – Option 1 1
 2
 3
Figure 13-4 EVPN VPWS redundancy option 1 4
 5
 This option assumes that O-DU supports Link Aggregation Group (LAG, also called Ethernet 6
bundling) and terminates the eCPRI stream on a MAC address associated with the Ethernet bundle 7
(e.g. MAC-B in the diagram). Therefore, eCPRI stream can arrive on any physical interface (via 8
HSR-1 or via HSR-2) to O-DU, and O-DU might sent eCPRI stream, again, on any interface (via 9
HSR-1 or via HSR-2) (see Figure 13-4). 10
 11
To facilitate fast failover in case of HSRO-DU link failure, following architectural design choices 12
are recommended: 13
 14
• O-DU uplinks towards HSR pair should be bundled on O-DU to create Link Aggregation 15
Group (LAG), often called an “Ethernet bundle”. 16
• EVPN VPWS service should be terminated on HSR pair as ‘multi-homed all-active’ service 17
 18
When O-RU generates eCPRI stream towards O-DU, this stream can be sent from CSR via EVPN 19
VPWS leg towards HSR-1, or via EVPN VPWS leg towards HSR-2. It is CSR implementation 20
choice, towards which HSR eCPRI stream will be sent. In case multiple O-RUs are connected to 21
single CSR, and multiple eCPRI streams are forwarded from CSR towards O-DU via HSR pair, 22
each eCPRI stream might take differen path (i.e. via HSR-1 or HSR-2). 23
 24
When for example HSR-1O-DU link failure happens, following EVPN failure detection 25
machinery, the EVPN VPWS leg between CSR and HSR-1 is disabled, and eCPRI stream flows 26
now over HSR-2. Since EVPN VPWS termination on HSR-2 was originally ‘active’ (multi-homed 27
all-active service) no special forwarding plane reprogramming is needed on HSR-2, allowing quick 28
delivery of rerouted eCPRI streams towards O-DU, as outlined in Figure 13-5. 29
 30
 31
Figure 13-5 EVPN VPWS Redundancy option 1 – failure event 32
 33
Cell site
Hub site
CSR
HSR-1
HSR-2
eCPRI flow
EVPN VPWS
MAC-A MAC-B
O-RU O-DUeCPRI
Fronthaul
Cell site
Hub site
CSR
HSR-1
HSR-2
eCPRI flow
EVPN VPWS
MAC-A MAC-B
O-RU O-DUeCPRI
Fronthaul

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        99
13.2.1.2 Ethernet services redundancy – Option 2 1
Option 2 is similar to option 1, with the difference that O-DU expects to receive eCPRI stream on a 2
particular interface (for example, on the interface from HSR-1), as outlined in Figure 13-6 3
 4
 5
 6
Figure 13-6 EVPN VPWS Redundancy Option 2 7
 8
For this option, following architectural design choices are recommended: 9
 10
• O-DU uplinks towards HSR pair should be bundled on O-DU to create Link Aggregation 11
Group (LAG), often called an “Ethernet bundle”. 12
• EVPN VPWS service should be terminated on HSR pair as ‘multi-homed single-active’ 13
service 14
• Deterministic Designated Forwarder (DF) election should be used on HSR pair, to ensure 15
that one of the HSR is deterministically elected as ‘active’ forwarder (DF), and second HSR 16
is deterministically elected as ‘standby’/’passive’ forwarder (non-DF) 17
• Per physical port (rather than default per-VLAN) DF election should be used on HSR pair 18
• HSR should signal the non-DF status to O-DU via OAM (for example LACP – Link 19
Aggregation Control Protocol – Out of Sync signalling) 20
 21
In addition to requirements already mentioned in the main section, to support this option following 22
requirements must be supported on HSR: 23
 24
[R65]: MUST support “Preference-based EVPN DF Election", draft-ietf-bess-evpn-pref-df-07 [135] 25
 26
[R66]: MUST support "EVPN multi-homing port-active load-balancing", draft-ietf-bess-evpn-mh-27
pa-01 [133] 28
 29
As outlined in Figure 13-7, when HSR-1O-DU link failure happens, following EVPN failure 30
detection machinery, the EVPN VPWS leg between CSR and HSR-1 is disabled, HSR-2 becomes 31
DF and EVPN VPWS leg between CSR and HSR-2 is enabled. Additionally, LACP Out-of-Sync 32
state from HSR-2 to O-DU is cleared, allowing forwarding over the HSR-2O-DU link. eCPRI 33
stream flows now over HSR-2. As opposed to Option 1, since EVPN VPWS termination on HSR-2 34
was originally ‘standby’ (multi-homed single-active service), DF election, LACP OOS clearance 35
and forwarding plane reprogramming is needed on HSR-2, thus failover is longer than in case of 36
Option 1. 37
 38
Cell site
Hub site
CSR
HSR-1
HSR-2
eCPRI flow
EVPN VPWS
MAC-A MAC-B
O-RU O-DUeCPRI
Fronthaul
DF
nDF
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        100
 1
 2
Figure 13-7 EVPN VPWS Redundancy Option 2 -failure event 3
13.2.1.3 Ethernet services redundancy – Option 3 4
Option 3 is further modification of option 2. It is suitable for O-DUs that do no support LAG 5
bundling, but have the capability to terminate eCPRI stream on some internal virtual MAC, 6
allowing reception of eCPRI stream over any uplink. 7
 8
 9
Figure 13-8 EVPN VPWS Redundancy Option 3 10
 11
For this option, following architectural design choices are recommended: 12
 13
• Two O-DU uplinks towards HSR pair are not bundled, but are standalone links placed inside 14
internal bridge on O-DU 15
• EVPN VPWS service should be terminated on HSR pair as ‘multi-homed single-active’ 16
service  17
 18
In this option, one of the HSR routers (for example HSR-1) is automatically elected by EVPN 19
control plane as ‘active’ (Designated Forwarder) router, and eCPRI stream is delivered over EVPN 20
VPWS service from CSR to active HSR only, where it is forwarded to O-DU in Ethernet frames. 21
Internal bridge at O-DU performs MAC learning (to learn O-RU MAC: MAC-A), therefore the 22
eCPRI stream generated from O-DU side and destined to O-RU (MAC-A) follows the same path 23
via HSR-1. 24
 25
As outlined in Figure 13-9, when HSR-1O-DU link failure happens, following EVPN failure 26
detection machinery, the EVPN VPWS leg between CSR and HSR-1 is disabled, HSR-2 becomes 27
DF and EVPN VPWS leg between CSR and HSR-2 is enabled. eCPRI stream flows now over HSR-28
2, and internal bridge in O-DU relearns the MAC-A via HSR-2. As opposed to Option 1, since 29
EVPN VPWS termination on HSR-2 was originally ‘standby’ (multi-homed single-active service), 30
DF election and forwarding plane reprogramming is needed on HSR-2, thus failover is longer than 31
in case of Option 1. 32
 33
Cell site
Hub site
CSR
HSR-1
HSR-2
eCPRI flow
EVPN VPWS
MAC-A MAC-B
O-RU O-DUeCPRI
Fronthaul
nDF
DF
Cell site
Hub site
CSR
HSR-1
HSR-2
eCPRI flow
EVPN VPWS
MAC-A MAC-B
O-RU O-DU
Fronthaul
eCPRI
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        101
 1
Figure 13-9 EVPN VPWS Redundancy Option 3 – failure event 2
13.2.1.4 Ethernet services redundancy – Option 4 3
Option 4 is suitable for O-DUs with only basic Ethernet support. i.e. O-DUs not supporting LAG, 4
internal bridge, or virtual MAC for eCPRI stream termination. Therefore, O-DU is represented by 5
two MAC addresses: MAC-B, on the link towards HSR-1, and MAC-C, on the link towards HSR-2. 6
 7
 8
Figure 13-10 EVPN VPWS Redundancy Option 4 9
 10
For this option, following architectural design choices are recommended: 11
 12
• Two O-DU uplinks towards HSR pair are not bundled, but are standalone links placed inside 13
internal bridge on O-DU 14
• EVPN VPWS service should be terminated on HSR pair as ‘multi-homed single-active’ 15
service  16
 17
In this option, eCPRI stream generated at O-RU uses of the O-DUs MAC addresses, for example 18
MAC-B, as the destination MAC in eCPRI Ethernet frames. Therefore, EVPN DF election must be 19
deterministic, similar to the DF election on Option 2. 20
 21
In addition to requirements already mentioned in the main section, to support this option following 22
requirements must be supported on HSR: 23
 24
[R67]: MUST support “Preference-based EVPN DF Election", draft-ietf-bess-evpn-pref-df-07 [135] 25
 26
As outlined in Figure 13-11, when HSR-1O-DU link failure happens, following EVPN failure 27
detection machinery, the EVPN VPWS leg between CSR and HSR-1 is disabled, HSR-2 becomes 28
DF and EVPN VPWS leg between CSR and HSR-2 is enabled. However, as opposed to all options 29
discussed previously, in this option, when failure happens, O-RU configuration must be changed by 30
the orchestrator, so that new MAC address (MAC-C) is used as the destination MAC in eCPRI 31
Ethernet frames. This contributes to the highest failover time among all options discussed so far. 32
 33
Cell site
Hub site
CSR
HSR-1
HSR-2
eCPRI flow
EVPN VPWS
MAC-A MAC-B
O-RU O-DU
Fronthaul
eCPRI
Cell site
Hub site
CSR
HSR-1
HSR-2
eCPRI flow
EVPN VPWS
MAC-A MAC-B
O-RU O-DUeCPRI
Fronthaul
DF
nDF
MAC-C
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        102
 1
Figure 13-11 EVPN VPWS Redundancy Option 4 – failure event 2
 3
  4
13.3 IP Services 5
Mobile IP services will be provided by MP-BGP based L3 VPNs (RFC4364) [72]  over MPLS or 6
SRv6 depending on the underlay network technology employed by the operator. More details on 7
L3VPN are provided in annex C.  8
 9
Operators that choose to implement IP services using the BGP Based L3VPN services may use the 10
following architectural design options:  11
 12
13.3.1 Building flexible L3VPN service topologies 13
L3VPN services can be used to establish L3 connectivity between various mobile components. BGP 14
L3 VPN support both IPv4 and IPv6 VPNs and offer flexible connectivity models including: 15
• IP N:N multipoint services 16
• IP 1:N multipoint services 17
• IP 1:1 connectivity services 18
 19
 20
Figure 13-12 Flexible L3 VPN service topologies 21
13.3.2 Constraints based Traffic Steering in L3VPNs 22
By default, BGP based L3VPN use shortest path routing across the transport underlay. However, 23
BGP based VPN traffic can also get automatically steered into SR policy, such as one created by a 24
flexible algorithm or a point-to-point TE tunnel. This is achieved by coloring VPN routes within 25
MP-BGP with extended community attributes. As illustrated in Figure 13-13 this can be done in 26
two ways  27
1. While defining a VRF, define a color for all Prefixes associated with that VRF, or 28
Cell site
Hub site
CSR
HSR-1
HSR-2
eCPRI flow
EVPN VPWS
MAC-A MAC-B
O-RU O-DUeCPRI
Fronthaul
DF
nDF
MAC-C

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        103
2. When sending or receiving BGP routes, using a policy to “color” the route.  1
 2
 3
 4
 5
Figure 13-13 Import / Export of route colors for SR policy selection 6
 7
In either case, the VPN route will now be tagged with a color and traffic for the destination will 8
automatically get steering into an SR policy based on the color associated with the VPN route.  9
 10
An operator using MP-BGP L3VPN services for Xhaul, TNEs need to support:  11
 12
[R68]: MUST support Multiprotocol Extensions for BGP-4, RFC4760 [74] 13
 14
[R69]: MUST Support BGP/MPLS IP Virtual Private Networks (VPNs), RFC4364 [72]  15
 16
[D39]: SHOULD support SRTE Policy configuration as outlined in draft-ietf-spring-segment-17
routing-policy-08 [146]  if SRTE policies are used with L3VPN services 18
[D40]: SHOULD support EVPN and L3VPN traffic steering based on “color extended community 19
BGP attributes” defined in RFC5512 [93]  20
[D41]: SHOULD support to color extended community attribute for VPNv4, VPNv6 and EVPN 21
BGP address families defined in RFC5512 [93]  22
[R70]: If using SRv6 as the underlay technology TNEs providing PE functionality MUST support 23
SRv6 BGP based Overlay services: draft-ietf-bess-srv6-services-07 [136] 24
 25
14 Quality of Service in packet-switched networks  26
This chapter discusses the Quality of Service (QoS) capabilities that must be deployed to support 27
the delivery of Xhaul traffic and non-Xhaul traffic in the RAN network.   28
14.1 Xhaul transport core interface QoS   29
For the purposes of this document, the transport network domain will be considered to be 30
constructed of two interface types. Core interfaces and edge interfaces. An interface is considered to 31
be transport network core if it is interconnecting two devices inside the transport domain. An 32
interface is considered transport network edge if it is connecting to an element outside the transport 33
domain.   34

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        104
14.1.1 Transport network core interface classification  1
Interfaces in the transport network core domain should perform classification on the outer transport 2
header only. The header in use will depend on the transport model selected by the operator. An 3
example classification scheme for MPLS TC (EXP) to queue is shown below in Figure 14-1 4
  5
[R71]: TNEs MUST support classification based on MPLS TC in a MPLS underlay transport 6
network   7
[R72]: TNEs MUST support classification based on IPv6 DSCP in an SRv6 underlay transport 8
network   9
 10
14.1.2 Core interface queue structure 11
The Xhaul domain must support a differentiated QoS architecture supporting priority queueing and 12
scheduling with weighted fair queue scheduler for non-priority packets.   13
 14
[R73]: MUST support EF forwarding described in IETF RFC 3246 [55]  mapped to a strict priority 15
scheduler with shaping and policing to prevent bandwidth starvation of other classes.  16
[R74]: MUST support AF forwarding model described in IETF RFC 2597 [44] mapped to a WFQ, 17
WRR or MDRR scheduler for non-priority classes.  18
[R75]: MUST support the ability to manipulate queue depths  19
  20
14.1.2.1 Flat queue model  21
In the flat queue model, the TNE should support a single level of queue on each physical interface. 22
This model is used for a single topology infrastructure, or a “soft slicing” where the QoS model for 23
each slice is common and bandwidth is dynamically shared between slices.  24
 25
Figure 14-1 Example Xhaul queue model 26
  27
The system should be capable of supporting a minimum of six queues within the core domain. An 28
example queuing model is provided in Figure 14-1which should be adapted to fit the available 29
queue and scheduling model available in the deployed hardware. 30
All deployments must support a strict priority queue dedicated to latency sensitive front haul traffic 31
(shown as queue marking 5). This queue must be serviced in strict priority over all other queues up 32

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        105
to its shaped or policed limit. If the node is providing transit for G.8275.2 PTPoIP as a PTP 1
unaware element, then PTP packets MUST also be scheduled in this queue. If the hardware 2
supports multiple levels of strict priority queueing, and is a PTP unaware node, then PTP MAY also 3
be scheduled in a dedicated highest priority queue to minimise packet delay variation (PDV).   4
Two further latency sensitive queues are defined, one for network control (shown as queue marking 5
7) and one for other latency sensitive transit traffic (e.g. latency sensitive Backhaul or Midhaul U-6
plane or latency sensitive business or consumer traffic) (shown as queue marking 2). These queues 7
should be configured to be serviced with a latency bound suitable for the traffic associated with 8
them. The scheduling model used will be dependent on the specific HW choice. For example, this 9
could be a guaranteed bandwidth queue with suitable scheduler weight and queue depth, or a high 10
priority queue with lower priority scheduling than that for the latency sensitive Fronthaul traffic. 11
Additional queues are defined for guaranteed bandwidth traffic (Management (shown as queue 12
marking 6), guaranteed bandwidth U-plane (shown as queue marking 3,4) and best effort traffic 13
(shown as queue marking 0,1)).  14
Additional queues may be supported if the carrier deems necessary.  15
The example queue model is show in Figure 14-1 provides example MPLS “Traffic Class” (TC) or 16
often called EXP, based classification for the queues. It should be noted that a transport based on 17
SRv6 allows the use of IP DSCP as the QoS marker, allowing for more flexibility in the queue 18
model, and that more queues may be used if the marking scheme and hardware are appropriate. 19
[R76]: MUST support a minimum of 6 HW queues per physical interface  20
 21
14.1.2.2 Hierarchical queue model   22
The hierarchical queue model provides a capability to support a segmented infrastructure “Hard 23
slicing” where the QoS model for each slice is different, or where each slice requires dedicated 24
bandwidth. (see Figure 14-2 for details)  25
Here the model the transport device must support a minimum of two levels of queue hierarchy on 26
each physical interface.  27
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        106
 1
 2
Figure 14-2 Sample hierarchical queue model  3
In this model the first, or child level defines the queues needed to support the capabilities associated 4
with the slice. These might be the same as in the flat queue model or may be different. In the second 5
or parent level, a scheduler is defined to allocate the maximum bandwidth that will be available to 6
that hard slice. It should be noted that the sum of the guaranteed bandwidth to each shaper should 7
not exceed the interface bandwidth.  As with the flat queue model, example is provided which 8
should be adapted for the specific HW in use. 9
 10
[R77]: MUST support a minimum of 6 HW queues per logical interface 11
  12
[R78]: MUST support a minimum of two tiers of configurable scheduler per physical interface  13
  14
14.1.3  Transport network core interface marking structure  15
Transport nodes should preserve the QoS marking of transported frames and packets across the 16
transport network. This should be accomplished via the use of the pipe or short pipe model 17
described in IETF RFC2983.   18
The specific marking structure used to define the transport QoS behaviours is left to the individual 19
providers and is dependent on the transport encapsulation implemented. An example marking 20
structure for MPLS TC values is shown in Figure 14-1.  21
  22
14.1.4 Core interface scheduling model  23
Latency sensitive and network control plane traffic should be forwarded using an expedited 24
forwarding model as described in IETF RFC 3246. In order to preserve minimum latency, these 25
queues should be scheduled with a maximum of one packet in each queue and be allocated a 26
priority forwarding schedule in the NPU. To prevent bandwidth starvation of non-priority scheduled 27
traffic, each priority queue should be shaped or policed at a combined capacity less than the 28
physical rate of each egress interface in the core, and with sufficient bandwidth capacity available 29

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        107
for the latency sensitive Fronthaul services they are supporting. Where needed a TSN based 1
scheduler may also be implemented if the interface bandwidth demands such optimisation. Details 2
on TSN may be found in section 10.1.1.2.1. 3
Guaranteed bandwidth queues (management and best effort) should be scheduled using the assured 4
forwarding model described in IETF RFC 2597. These queues should be scheduled using a 5
Weighted Round Robin (WRR), Minimum deficit round robin (MDRR) or weighted fair queue 6
(WFQ) scheduler, depending on the capability available in the NPU. Where network control traffic 7
is assigned to a guaranteed bandwidth queue, the scheduler for that queue must be configured to 8
support minimum latency to ensure time sensitive network control traffic (eg BFD) is not delayed.  9
The scheduler MUST have sufficient buffer capacity to avoid loss in these queues due to the effects 10
of micro-burst caused by the aggregation of traffic from different ingress interfaces, or speed 11
mismatch (higher to lower).    12
14.2 Xhaul transport network edge interface QoS  13
PE elements in the transport network domain will be responsible for providing logical separation 14
and encapsulation for transport. It can be assumed that there are two types of traffic that will be 15
presented for transport.  16
1. Ethernet frames. For example, RoE or eCPRI frames not using an IP header.  17
2. IP packets. For example, eCPRI packets or Backhaul traffic whose encapsulation includes an 18
IP header.   19
14.2.1 Transport network domain PE ingress classification of Ethernet frames.  20
For the purposes of this section, it is assumed that there will be a physical or logical interface 21
attaching the Ethernet frame device to the transport network edge PE. As an example, this could be 22
a Mobile Client, such as a Front Haul Gateway or O-DU supporting an RU using RoE. PE nodes in 23
the transport network will perform classification on traffic ingressing the domain using different 24
models.   25
  26
27
   28

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        108
Figure 14-3 Ethernet ingress classification models 1
 2
Figure 14-3 identifies three models by which transport domain PE devices will classify traffic 3
originating from a ROE mapper. All three models must be supported. 4
14.2.1.1 Untagged frames  5
As per Figure 14-3(1) the Mobile Client (MC) presents a flow of untagged frames to the PE. The 6
PE uses the local port as the context for the MC. This context must be preserved in the transport 7
network and presented at the port level (untagged) at the remote PE. Because the PE has no 8
knowledge of the MC encapsulation model, or priority needed for each frame, all frames must be 9
given the same appropriate treatment (and transport marking) to support the encapsulation mode. It 10
can be assumed that the operator has this knowledge at the port level and can apply the appropriate 11
local policy for both transport marking and PHB for all frames. These frames should be 12
encapsulated with the appropriate transport and marked with QoS marking for carried in the class 13
defined in section 14.1.2 for Latency sensitive Fronthaul traffic.  14
14.2.1.2 Tagged frames (VLAN), PCP unset or untrusted  15
As per Figure 14-3(2) The RoE mapper presents a flow of VLAN tagged frames to the PE. The PE 16
uses the VLAN and local port as the context for the RoE mapper. This context must be preserved in 17
the transport network and presented at the remote PE with the same VLAN marking. Because the 18
PE has no knowledge of the RoE mapper encapsulation model, or priority needed for each frame, 19
all frames must be given the same appropriate treatment and transport marking, to support the 20
encapsulation mode. It can be assumed that the operator has this knowledge at the either the VLAN 21
or port level and can apply the appropriate local policy for both transport marking and PHB for all 22
frames. These frames should be encapsulated with the appropriate transport and marked with QoS 23
marking for carried in the class defined in 14.1.2 for Latency sensitive Fronthaul traffic.  24
14.2.1.3 Tagged frames (VLAN), PCP set and trusted  25
As per Figure 14-3(3). The RoE mapper presents a flow of VLAN tagged frames to the PE. The PE 26
uses the VLAN and local port as the context for the RoE mapper. This context must be preserved in 27
the transport network and presented at the remote PE with the same VLAN marking. In this model 28
it is assumed that the RoE mapper supports a differential marking scheme for flows requiring 29
different treatment. The marking is placed in the PCP field of the VLAN header (described by IEEE 30
802.1p). The PE should be capable of classifying flows based on the PCP marking and 31
implementing an appropriate PHB. The PE should also apply an appropriate marking to the frames 32
as supported by the transport model in use. It can be assumed that the operator has this knowledge 33
of the PCP markings from the RoE mapper.  34
Each class presented by the RoE mapper must be mapped to one of the class markings defined in 35
section 14.1.2. It is assumed that the operator has appropriate knowledge to determine this 36
mapping.  37
 38
[R79]: MUST support classification of frames based on 802.1p PCP bit field  39
 40
[R80]: MUST support classification of frames based on incoming logical interface.  41
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        109
 1
[D42]: SHOULD support ingress policing using a single rate 2 color policer described in RFC2698 2
[45] or RFC4115 [63]  3
[D43]: SHOULD support ingress policing using a dual rate 3 color policer described in RFC2698 4
[45] or RFC4115 [63] 5
[R81]: MUST support ingress marking of frames based on result of soft policing using policers in 6
[D42]: and [D43]:. 7
  8
14.2.2  Transport domain PE ingress classification IP packets.  9
Unlike Ethernet frames, IP packets will always have a QoS marking field available to them. As a 10
result, we do not have to be concerned with IP packets arriving unmarked from a QoS perspective 11
and must simply determine if we trust the marking.   12
14.2.2.1 Marked packets DSCP untrusted  13
Similar to Figure 14-3(2). The client element presents a flow of DSCP marked packets to the PE. 14
The PE uses local port as the context for the flow. This context must be preserved in the transport 15
network and presented at the remote PE with the same DSCP marking. Because the PE has no 16
knowledge of the client element encapsulation model, or priority needed for each packet, all packets 17
must be given the same appropriate treatment and transport marking, to support the encapsulation 18
mode. It can be assumed that the operator has this knowledge at the port level and can apply the 19
appropriate local policy for both transport marking and PHB for all frames. These frames should be 20
encapsulated with the appropriate transport and marked with QoS marking for carried in the class 21
defined in section 14.1.2 for Latency sensitive Fronthaul traffic.  22
14.2.2.2 Marked packets DSCP trusted  23
Similar to Figure 14-3. The client element presents a flow of DSCP marked packets to the PE. The 24
PE uses the local port as the context for the flow. This context must be preserved in the transport 25
network and presented at the remote PE with the same DSCP marking. In this model it is assumed 26
that the client element or IP source supports a differential marking scheme for flows requiring 27
different treatment. The marking is placed in the DSCP field of the IP header. The PE should be 28
capable of classifying flows based on the DSCP marking and implementing an appropriate PHB. 29
The PE should also apply an appropriate marking to the frames as supported by the transport model 30
in use. It can be assumed that the operator has this knowledge of the DSCP markings from the client 31
element or IP source.  32
Each class presented by the client element or IP source must be mapped to one of the class 33
markings defined in section 14.1.2. It is assumed that the operator has appropriate knowledge to 34
determine this mapping.   35
[R82]: MUST support classification of frames based on IPv4 DSCP field  36
 37
[R83]: MUST support classification of frames based on IPv6 DSCP field  38
  39
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        110
14.2.3 Admission control  1
Admission control via policing should be applied to all sources of traffic at the PE. It is important to 2
recognise that suitable capacity MUST be made available for all traffic (especially where loss is 3
problematic) in each element of the infrastructure. Where statistical multiplexing is expected to be 4
used to support additional service capacity in the network, excess traffic, beyond that which is 5
committed, MUST be marked as excess and be eligible for drop to protect other committed traffic 6
under heavy usage periods.    7
14.2.4  PE Egress scheduling  8
Traffic arriving at a PE from the infrastructure, to be transmitted to a CE device should be done so 9
based on the model used for its classification on ingress.  10
14.2.4.1 Unclassified traffic  11
Where an interface is using unclassified traffic, that is;   12
all traffic is always marked as default by the CE or   13
traffic with no marking available e.g. ethernet without a VLAN header,   14
traffic should be forwarded on a FIFO basis. In this model it must be ensured that the flow of traffic 15
into the PE toward the CE MUST be at a lower rate than the interface attaching the PE to the CE to 16
avoid congestion resulting in latency or loss.  17
14.2.4.2 Classified traffic  18
Where an interface is using classified traffic, that is traffic has a marking scheme available, and 19
traffic is marked using this scheme, traffic should be forwarded to the CE with a queueing scheme 20
and scheduling model that is appropriate to the traffic for that CE. The specific marking and 21
queueing scheme are beyond the scope of this document but MUST be understood by the operator. 22
In this model the PE MUST support the classification and queueing of the CE destined traffic based 23
on the marking on the CE packet or frame. This is understood as “pipe model – after” as defined in 24
IETF RFC 2983, section 5 – 6.    25
  26
[R84]: MUST support classification of encapsulated frames or packets based on 802.1p PCP bit 27
field or IP DSCP field as described in IETF RFC 2983 section 5-6   28
  29
Latency sensitive and network control plane traffic should be forwarded using an expedited 30
forwarding model as described in IETF RFC 3246 [55]. In order to preserve minimum latency, 31
these queues should be scheduled with a maximum of one packet in each queue and be allocated a 32
priority forwarding schedule in the NPU. To prevent bandwidth starvation of non-priority scheduled 33
traffic, each priority queue should be shaped or policed at a combined capacity less than the 34
physical rate of each egress interface in the core, and with sufficient bandwidth capacity available 35
for the latency sensitive Fronthaul services they are supporting. Where needed a TSN based 36
scheduler may also be implemented if the interface bandwidth demands such optimisation. Details 37
on TSN may be found in section10.1.1.2.1.  38
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        111
Guaranteed bandwidth queues (management and best effort) should be scheduled using the assured 1
forwarding model described in IETF RFC 2597 [44]. These queues should be scheduled using a 2
Weighted Round Robin (WRR), Minimum deficit round robin (MDRR) or weighted fair queue 3
(WFQ) scheduler, depending on the capability available in the NPU. The scheduler MUST have 4
sufficient buffer capacity to avoid loss in these queues due to the effects of micro-burst caused by 5
the aggregation of traffic from different ingress interfaces, or speed mismatch (higher to lower).   6
15 Multicast 7
15.1 Multicast use cases 8
Multicast use cases in an Xhaul transport network come from two categories as described below. 9
 10
15.1.1 Multicast transport for fixed line services 11
As mentioned in the requirements section: 12
 13
“ITU-T GSTP-TN5G: Transport support of IMT-2020/5G” identifies the need for the transport to 14
be multi-service in nature. In addition to mobile services, the infrastructure needs to support fixed 15
line consumer and enterprise services. 16
 17
Fixed line consumer/enterprise services like IPTV and VPN all require multicast support in the 18
transport network. 19
15.1.2 MBMS/5MBS transport 20
3GPP TS 23.246 MBMS Architecture and Functional Specification (R15) and TR 23.757 Study on 21
architectural enhancements for 5G multicast-broadcast services (5MBS, R17) all support “shared 22
delivery method” with which multicast/broadcast traffic is transported via multicast to RAN nodes 23
(N3/N9), who will then transmit over the air. This means multicast transport is needed to CU-UP. 24
 25
Another “individual delivery method” may also be used – UPFs send individual copies of multicast 26
traffic over PDU sessions to UEs, transparent to RAN nodes. With this method, if UPFs are 27
distributed, then multicast should be used in DNN to the UPFs (N6). Note that, a UPF may be 28
connected to multiple DNNs as the anchored PDU sessions may belong to multiple DNNs. 29
 30
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        112
 1
 2
Figure 15-1 Mobile multicast use cases 3
15.2 Overlay and underlay multicast 4
The multi-service nature of 5G transport network is not only that the transport network is also used 5
for fixed line service transport. Even for 5G itself, the same transport network is used for multiple 6
purposes, e.g., N3/N9 transport, N6 transport and Xhaul transport. 7
 8
 9
 10
 11
Figure 15-2 VPN infrastructure for mobile 12
As described in the service capability section, different transports are rendered as different 13
IPVPN/EVPN overlay services over a common transport underlay. When it comes to multicast, it is 14
just an aspect of the IPVPN/EVPN, as further described in section 13. 15
 16

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        113
15.3 Recommendation/considerations for multicast solutions 1
With the background discussion provided in Annex E: Multicast Technologies background, the 2
following multicast solutions are recommended: 3
 4
1. BGP-MVPN and EVPN-BUM for overlay, with tunnel segmentation if different tunnel 5
types/instances are necessary/desired for different ASes/areas. 6
2. Currently deployed underlay tunnel solutions can still be used even with Segment Routing 7
(mLDP/RSVP protocol would only be used for multicast not unicast purposes). If end-to-8
end tunnels are used without tunnel segmentation, PIM RPF Vector or mLDP Recursive 9
FEC or controller signalling need to be used. 10
3. Controller-signaled multicast can be used if tree calculation and signalling by controllers are 11
desired to satisfy TE constraints, or to remove legacy LDP/RSVP protocols from the 12
network. This includes SR-P2MP an BGP-signalled mLDP tunnels, with the latter offering 13
the best flexibility – easy transition from existing mLDP deployment and flexible ways of 14
tunnel identification via mLDP FEC. 15
4. BIER can be deployed in (part of) the network when enough routers support BIER. 16
 17
Depending on the selected solution, some of the following standards may need to be supported. 18
 19
[O1]: BGP Encoding and Procedures for Multicast in MPLS/BGP IP VPNs, RFC6514 [101] 20
 21
[O2]: BGP MPLS-Based Ethernet VPN, RFC7432 [102] 22
 23
[O3]: Update on EVPN BUM Procedures, draft-ietf-bess-evpn-bum-procedure-updates-08 [131]  24
 25
[O4]: Protocol Independent Multicast - Sparse Mode (PIM-SM) Protocol Specification, RFC 7761 26
[107] 27
  28
[O5]: The Reverse Path Forwarding (RPF) Vector TLV, RFC 5496 [92] 29
 30
[O6]: Label Distribution Protocol Extensions for Point-to-Multipoint and Multipoint-to-Multipoint 31
Label Switched Paths, RFC6388 [97] 32
[O7]: Using Multipoint LDP When the Backbone Has No Route to the Root, RFC 6512 [99] 33
 34
[O8]: Extensions to Resource Reservation Protocol - Traffic Engineering (RSVP-TE) for Point-to-35
Multipoint TE Label Switched Paths (LSPs), RFC4875 [75]  36
[O9]: Controller Based BGP Multicast Signaling, draft-ietf-bess-bgp-multicast-controller-05 [130] 37
 38
[O10]: Segment Routing Point-to-Multipoint Policy, draft-ietf-pim-sr-p2mp-policy-00 [140] 39
16 Packet-switched orchestration and telemetry 40
Packet switched orchestration and telemetry covers how the packet switching transport network is 41
programmed at a device and service level and how telemetry data is retrieved from TNEs. This will 42
be covered either in a separate document or in a future revision of this document.   43
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        114
17 5G Slicing in a packet switched Xhaul network 1
Network Slicing is end-to-end partitioning of the network resources and network functions so that 2
selected applications/services/connections may run in isolation from each other for a specific 3
business purpose. The overall slicing architecture is shown in Figure 17-1 and covers the 4
orchestration infrastructure and at the physical layer can cover the radio access network, the mobile 5
core, including data centers and virtualization aspects, and the X-haul transport network.  6
 7
 8
Figure 17-1: Overall 5G slicing architecture  9
 10
Although slicing is a key capability of 5G, there is debate as to how it exactly translates into the 11
transport network, which mobile interfaces (Fronthaul, Midhaul, Backhaul, N6) need to be sliced, 12
what form they will take, and the number of slices required at the transport layer. For the purposes 13
of this document the Xhaul transport infrastructure needs the capability to support transport 14
requirements of the: 15
1. O-RAN and 3GPP control, management, and user plane interfaces.  16
2. Wireline consumer, enterprise, and wholesale services. 17
 18
This includes 5G transport slicing, which from a mobile perspective may include: 19
1. Transport separation between Fronthaul, Midhaul, Backhaul interfaces. 20
2. Transport separation between Control, management, and user plane interfaces of each domain. 21
3. Flexible mapping of Network Slice Instances (NSIs) to physical or logical transport network 22
instances.  23
 24
Figure 17-2 is an example of how three NSIs could be mapped to logical networks within an Xhaul 25
transport network. In this example there is a: 26
 27
1. Common logical transport infrastructure supporting the Fronthaul and Midhaul 28
management planes for all three NSIs 29
2. Common logical transport infrastructure supporting Fronthaul control and user planes for 30
all three NSIs. 31
3. Common logical transport network supporting Midhaul user plane for all three NSIs.  32

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        115
4. Common logical transport network supporting 3GPP control plane (Midhaul and Backhaul) 1
for all three NSIs. 2
5. Dedicated logical transport networks for backhaul and N6 networks for each NSI.  3
 4
Figure 17-2: Overall 5G slicing architecture  5
 6
The characteristics of a transport slice are defined in clause 5.2.3 of 3GPP Technical Report 22.891. 7
Although not referenceable, further useful information can be found in various expired personal 8
informational IETF drafts which provide thinking on the characteristics of a sliced transport 9
network. Some of the key points are: 10
 11
• Management and lifecycle of the network 12
o Definition 13
o Creation / deletion 14
o Modification 15
• Per slice OAM 16
• Resource Reservation  17
• Slice isolation  18
o Performance 19
o Operational 20
o Security  21
o Reliability 22
• Abstraction 23
o Virtualization of network functions (where appropriate) 24
o Use of shared compute resources  25
 26
In the transport space, the informal terms, hard and soft slicing has emerged. This refers to the level 27
of isolation between different slices. In both cases they need to support the functions outlined above 28
but the way the slice is built and managed differs considerably.  29
 30
• Hard slicing: Transport resources are dedicated to a specific “Network Slice Instance” (NSI). 31
In this case a resource is dedicated to a particular slice and not available to other slices.  32
 33

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        116
• Soft slicing: The transport slice has the characteristics outlined above but a resource is shared 1
and can be re-used by other slice instances. 2
  3
A packet switched infrastructure, as described in this document, has an extensive toolset, consisting 4
of underlay forwarding solutions, Quality of Service (QoS) and VPNs that allows an operator to 5
scalable partition the transport network to cater for both hard and soft slices use cases. Figure 17-3 6
outlines some of the packet switched features that can be flexibly combined to create transport slice 7
with different level of resource sharing.  8
 9
Figure 17-3: Packet switched toolset for transport level slicing  10
 11
When considering hard and soft slices it is important to bear in mind they should be thought of as a 12
spectrum of capability, rather than one or the other.  13
  14
17.1  Packet-switched underlay network 15
The packet-switched underlay network can use MPLS, SRv6 or a combination.  16
17.1.1 Underlay forwarding plane 17
The forwarding plane determines how traffic is sent over the packet-switched underlay 18
infrastructure. Three approaches are outlined for constructing the underlay forwarding plane/planes 19
for a 5G transport infrastructure. In each case the level of resource sharing reduces, hence the slice 20
solution becomes harder in nature but may introduces scalability and operational challenges.  21
17.1.2 Single forwarding plane for all slices 22
The underlay network relies solely on the routing protocols (IGPs and EGPs) to calculate a single 23
forwarding table based on shortest path routing. The routing protocol sees all links and there is a 24
single forwarding table based on IGP and BGP metrics. All traffic takes the shortest path between 25
two points within the network and utilises ECMP.  26
 In the context of slicing, all slices will see the same set of paths between end points within the 27
network. It can be considered the softest slicing solution in terms of the underlay forwarding plane. 28
However, although the infrastructure is shared between slices, it is highly scalable, widely 29
implemented in 4G and critical enterprise environments, and is capable of delivering high quality 30
SLAs.  31

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        117
17.1.3 Forwarding plane per 5G service 1
Within the underlay network multiple forwarding planes are built to meet the specific forwarding 2
behaviors associated with the different 5G services. One or more customers can then use these 3
forwarding planes. This is achieved using VPNs and traffic steering techniques. The forwarding 4
planes can utilise different topologies and be optimized based on different criteria. For example, a 5
URLLC service could be optimised to use only the most reliable links in the network and select the 6
best paths between endpoints based on delay. In contrast, the eMBB and mMTC services could be 7
designed to use cheap, high bandwidth links. To implement such an approach, a mix of shortest 8
path routing and traffic engineering solutions could be used. Traffic engineering options include 9
flex-algo, SR-TE or traditional MPLS TE. As with any traffic engineering approach, consideration 10
needs to be made of scale and state held within the network. Perhaps the simplest approach would 11
be to use the default Flex-algorithm for eMBB and the mMTC services and a delay optimized 12
flexible algorithm for the URLLC services. Backbone links and even nodes could be considered for 13
inclusion or exclusion into the delay optimized flex-algorithm forwarding planes based on whether 14
the algorithm is enabled on a TNE and TE affinities associated with the links.        15
17.1.4 Forwarding plane per slice customer 16
This is a variation on the previous scheme. In this case rather than a forwarding plane per 5G 17
service, a forwarding plane for individual customers is defined. The same techniques outlined above 18
would be used but very careful consideration needs to be given to scale and operational complexity 19
of such an approach. In this case, complexity is a function of the number of customers using the 20
network rather than the 5G services. 21
17.2 Quality of Service   22
Quality of Service is an important component in slicing a transport infrastructure. As with the 23
forwarding plane, different approaches could be taken depending on the level of isolation required 24
between slices. In considering QoS it is necessary to look at the edge QoS solution and the core 25
QoS solutions. For more info on QoS refer to section 14 and 22.  26
 27
17.2.1 Edge QoS  28
Edge interfaces to packet transport networks are generally the slowest and also where congestion 29
and packet drop most often occurs. Regardless of the QoS structures implemented in the transport 30
core, it is important with slicing to support edge conditioning of traffic on ingress and scheduling on 31
egress to the transport network. This ensures that each slice gets its contracted overall bandwidth 32
and class bandwidth as it enters and leaves the transport network. When slices are presented via 33
VLANs from the mobile clients, the PE router needs a hierarchical QoS capability that can account 34
for the overall contracted bandwidth at the VLAN level and also the overall contracted class 35
bandwidth within the VLAN.  36
 37
17.2.2 Core QoS  38
Within the core network different QoS strategies can be applied that determine the level of 39
bandwidth and queue sharing that occurs between different slices.  40
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        118
17.2.2.1 Shared queue architecture 1
This approach follows the classic Diffserv QoS architecture. Slice traffic would be conditioned, and 2
optionally marked or remarked as it enters the Diffserv domain, in this case the transport network. 3
These markings determine the behaviour or Per Hop Behaviour (PHB) the traffic receives in the 4
transport network. The number of PHBs in the core is determined by a number of factors; the 5
number of PHBs that can be marked, the number of queues supported on the TNEs and operational 6
complexity of the solution. Typically, in an MPLS network the traffic-class field is used to 7
designate the PHB in the core. This is 3 bits, giving a maximum of 8 markings. In an IPv6 network 8
the DSCP field is used to designate the PHB in the core. This is 6 bits giving a maximum of 64 9
markings. Even with more marking options, the number of PHBs in the core is generally kept small 10
(<8) to simplify configuration, management and is generally done as a one-time set-up when a link 11
is first installed. Core PHBs are enabled using a combination of queues, congestion management 12
and scheduling techniques and are normally set-up in a work preserving fashion.  That is, if one 13
PHB is not fully utilised, the unused capacity can be used by traffic with other PHBs.    14
Applying this to slicing. Each slice has an SLA based on overall and class bandwidth it receives. 15
This is enforced on ingress at the PEs. In this QoS model, core queues are shared between slices and 16
can be considered to be a soft form of slicing, but there is protection between slices through ingress 17
edge conditioning of traffic. This architecture it is well tried, and tested, is simple and scales and 18
able to support very tight SLAs.    19
 20
17.2.2.2 Dedicated queue architecture 21
This approach uses the same Diffserv QoS architecture, except one or more PHBs is exclusively 22
dedicated to a particular slice or slice type. An example could be a high priority PHB, which is 23
dedicated exclusively to the URLLC slice. At the same time other slices can share the remaining 24
PHBs. Given the small numbers of queues normally provisioned in the core (and available traffic 25
classes in an MPLS environment), this type of solution needs to be used with extreme care, however 26
it is a model used today successfully in production networks to support private line services which 27
run in a dedicated high priority queue, while simultaneously supporting other services using a 28
shared queue architecture.    29
 30
17.2.2.3 Dedicated links and queues per slice 31
This QoS approach achieves hard slicing by either dedicating core links or parts of a core link to a 32
specific slice. This can be achieved by one of the following approaches: 33
• Using dedicated links per slice in the transport network. This could be done using full 34
physical Ethernet interfaces or using Ethernet TDM technology, such as flexE, to create 35
dedicated Ethernet channels for different slices. Within these links, class-based queuing 36
would be used to support different traffic types associated with the slice. This approach 37
offers no ability to share unused bandwidth between slices as the links are hard partitioned. 38
• Using hierarchically scheduled bandwidth on core links. In this case core links would be split 39
using a shaper to govern the level of bandwidth available to each slice, with class-based 40
queuing within the shaped bandwidth. This approach does offer the ability to share unused 41
bandwidth between slices.    42
 43
In addition to dividing the core bandwidth up, mechanisms are required to guide slice traffic into 44
the links or shaped bandwidth associated with each slice. Slice traffic could be guided into the 45
appropriate links using a TE solution, such as flex-algo or SR-TE or alternatively the core routers 46
could be enhanced to schedule on a slice identifier within the packet. At this time there is discussion 47
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        119
on this subject but as yet nothing solid. Like with the previous example, this could support a small 1
number of slices but scaling up would be highly problematic with core provisioning, as well as edge 2
provisioning required when a slice is defined, changed or deleted. It is also a large departure in the 3
way QoS in core of packet networks has been managed in the past.     4
17.3 5G Services and slices 5
MP-BGP based L2 EVPN and L3VPNs are used to create slice instances. VPNs are very scalable in 6
terms of numbers and endpoints and allow different flexible connectivity models. This means slices 7
could be based on 5G service, customer or a combination of the two.  8
Each slice would have a VPN associated with it. By default, VPNs use the default IGP based 9
forwarding tables to forward traffic. However, VPN traffic can be directed, either by configuration 10
or using automated traffic steering techniques based on BGP “color extended community attribute” 11
to use traffic engineered paths. In this way, a URLLC slice could be directed to use a delay 12
optimised TE solution. Further control of the transport level connectivity within the VPN can be 13
achieved using Route Target filtering. For example, “Route Target” filtering could be used to stop 14
transport level connectivity between UPFs of different customers but allow O-CUs to communicate 15
with any UPF. 16
18 Supporting mobile scenarios on a packet switched Xhaul 17
network  18
What is clear in discussions with operators is that although section 8 outlines individual use cases, 19
operator’s expect multiple use cases to be present in their transport network based on the need to 20
support 4G and 5G, maturity and timelines of the O-RAN specifications, vendor product readiness, 21
use cases and latency requirements. Figure 18-1, an adapted operator’s view, illustrates this well 22
with multiple 4G/5G architectures present along with Enterprise and consumer services.  23
 24
 25
 26
Figure 18-1: An adapted O-RAN operator’s view of mobile component placement 27
 28

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        120
In order to make this document generic to operators, simplification is required. For this reason, the 1
following physical topology and underlay transport solution is used as a baseline. Operators can 2
then use the information and customize to their specific requirements.  3
18.1 Physical network  4
Figure 18-2 illustrates physical network layout for a metro network and its connectivity to the 5
transport core. It consists of: 6
1. A collection of metro networks surrounding a transport core. Each metro network consists of 7
a number of access networks that are consolidated by an aggregation network towards to the 8
transport core.  9
2. All sites which interface to customer or mobile components supports Provider Edge 10
functionality for L2 and L3 overlay services.  11
3. Data centers or location for positioning mobile components potentially exist at the hub sites, 12
edge sites and in centralised locations.  13
4. Connectivity between routers is provided by line-rate point-to-point Ethernet connections 14
derived from dark fibre or WDM.  15
5. Access networks could be based on either small rings, star, chained or hub and spoke 16
topologies. To simplify the following discussion, it is assumed each cell site has a CSR 17
connected via a single high-capacity dark fibre to the HSR in a star topology. Refer to 18
section 10 for more details on potential fiber access topologies.  19
6. The Aggregation network originates from the HSR and is based on either a ring or a hub and 20
spoke topology. For some scenarios the aggregation network may be split into a pre-21
aggregation and aggregation component. In this case, there is an intermediate layer of packet 22
switches and an additional layer of statistical multiplexing. (See greyed out router / switch in 23
Figure 18-2)  24
 25
    26
 27
 28
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        121
  1
 2
Figure 18-2: An example physical architecture 3
18.2 Logical underlay architecture 4
The logical underlay is illustrated in Figure 18-3 and consists of QoS enabled infrastructure that 5
either uses an IP/MPLS or an SRv6 underlay data and control plane. It provides any-to-any 6
connectivity between TNEs in the network, regardless of whether they reside in the same metro 7
network, a different metro network or the transport core and supports both shortest path routing and 8
traffic engineering. The underlay data planes, control plane and scaling mechanisms to enable this 9
for IP/MPLS and SRv6 are different and discussed in their respective chapters.  Both solutions have 10
the capabilities to support best effort and traffic engineered forwarding and the capability to create 11
discrete constrained based topologies using either traditional traffic engineering or flexible 12
algorithm which can be optimized on criteria such as shortest path, delay and bandwidth. 13

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        122
  1
 2
Figure 18-3: An example logical architecture 3
18.2.1 Underlay Quality of Service (QoS)  4
Refer to section 14 for fuller discussion on QoS. Underlay QoS refers to the scheduling used on the 5
core or backbone links. All backbone links are enabled with a combination of priority and class-6
based queueing. Assignment of traffic to different queues within the core is based on marking 7
within the packet header. In an MPLS backbone this is based on the EXP bits or traffic class field, a 8
three-bit field, giving a maximum number of core behaviours of eight. In an IPv6 backbone this is 9
based on the DSCP bits, a six-bit field, giving a maximum number of 64 markings and potential 10
behaviours.  11
  12
13
  14
Figure 18-4: Example Xhaul queue structure  15
 16

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        123
18.3  Service architecture  1
For mobile services it is proposed to use BGP based VPNs to overlay service functionality onto the 2
underlay transport network as described in section 13. 3
 4
NOTE: There is no reason this same transport infrastructure cannot support other VPN solutions 5
such as VXLAN or SD-WAN type solutions. 6
 7
All PEs and DCIs (which are also PEs) should be capable of supporting point to point Ethernet 8
services using EVPN VPWS services and L3 BGP VPN services described in section 13.  With 9
these two basic service overlays all the scenarios outlined in section 8 can be accommodated.  10
18.3.1 Automated VPN Traffic Steering 11
If required traffic from different VPNs or specific flows within an VPN can be steered into different 12
underlay transport forwarding planes based on coloring the MP-BGP VPN routes associated with 13
L2 and L3VPNs. This capability is described in Annex C: MP-BGP based L3VPNs . 14
18.4 Mobile services 15
There are many ways Fronthaul, Midhaul and Backhaul services could be provisioned onto the 16
transport network. Below are a set of assumptions and mechanisms used to illustrate how a packet 17
switched architecture can accommodate the scenarios outlined. These can easily be adapted based 18
on operator’s individual requirements. 19
18.4.1 Open Fronthaul 20
Figure 18-5 illustrates a physical and logical design that supports Open Fronthaul traffic. It is fairly 21
simple in nature, however, can easily be adapted based on operator designs and requirements. It 22
uses the underlay building blocks discussed earlier chapters.   23
 24
 25
Figure 18-5: Fronthaul physical and logical topology 26

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        124
18.4.1.1 Assumptions  1
1. 7.2x C/U planes connects to the transport network via a VLAN on a physical Ethernet 2
interfaces on O-RUs and O-DUs.  3
 4
2. 7.2x C/U planes traffic uses Ethernet / VLAN encapsulation option and not the optional 5
Ethernet / VLAN / IP encapsulation. 6
 7
3. 7.2x C/U planes uses an Ethernet service across the transport network to connect O-RUs to 8
O-DUs. This is via an Ethernet VPWS service using EVPN in the transport network.  9
 10
4. If the 7.2x C/U planes traffic used an Ethernet / VLAN / IP encapsulation, then the transport 11
network could use a L3 VPN service to transport the traffic between the O-RU and the O-12
DU, but this is not considered in the following sections. 13
 14
5. 7.2x M-plane connects to the transport network via a VLAN on a physical Ethernet. 15
 16
6. 7.2x M-plane uses an Ethernet / VLAN / IP encapsulation.  17
 18
7. 7.2x M-plane is operating in hybrid mode and uses an L3 VPN service across the transport 19
network to connect O-RUs, O-DUs and O-RAN NMS together. This is via an overlay MP-20
BGP based L3VPN.  21
 22
8. For the following discussion, the 7.2x C/U planes and M plane share a single physical 23
Ethernet interface on the O-RU towards the CSR with the C/U plane and M-Plane running in 24
different VLANs. 25
 26
9. Each O-RU has its own dedicated Ethernet port on the CSR. I.E There is no lower level 27
aggregation of statistical multiplexing component between an O-RU and the CSR. 28
 29
10. O-RU and O-DU set appropriate QoS markings for C/U and M-plane traffic that the 30
transport network can trust. In other words, the TNE does not need to classify different 31
components of Fronthaul for transport.  32
 33
18.4.1.2 Open Fronthaul physical design  34
1. 3 O-RUs per cell site.  35
 36
2. In the above example, each O-RU is connected via a dedicated Ethernet interface to a co-37
located CSR. This single Ethernet supports both the C/U planes and the M-plane presented 38
through two VLANs. The required capacity of the Ethernet link between the O-RU and the 39
CSR is primarily dependent on the O-RU’s radio capabilities. The requirements document 40
[18] provides some bandwidth examples for Open Fronthaul bandwidth. The expectation is 41
that the C/U plane traffic will be significantly larger than the M-Plane traffic, therefore these 42
Ethernet interfaces should be provisioned to deal with the peak 7.2x C/U-plane traffic of an 43
O-RU. It should be noted that unlike CPRI, the 7.2x C/U plane traffic loads are dependent on 44
user activity, so peak loading will only occur when the O-RU is running at full theoretical 45
capacity. 46
  47
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        125
3. In the above example, each CSR is connected using a single Ethernet link across dark fibre to 1
the HSR. Other underlay transport technologies and topologies are available, such as 2
redundant hub and spoke, chain or ring topologies in the access network. Regardless of the 3
WAN technology and topology employed, careful consideration needs to be given the 4
latency and jitter requirements of C/U plane traffic between the O-RU and the O-DU. 5
Transport considerations includes fibre distances between the O-RU and the O-DU, the 6
number of routers/switches traversed, link speeds and switching times of the intermediate 7
transport network equipment. (see section 10.1.1.2 for more details). As O-RAN 7.2x traffic 8
loads are dependent on user activity and in the example there are multiple O-RUs in the cell 9
site, the link capacity between the CSR and HSR does not need to be provisioned at the 10
theoretical sum of the O-RU’s peak rates because a statistical gain can be realised. The 11
bandwidth provisioning should be based on usage and the real achievable peak rates 12
associated with the O-RUs. The real achievable peak rates are dependent on factors such as 13
number and proximity of radios to each other and environmental factors. Precise 14
provisioning rules for 5G Fronthaul is subject to further investigation. 15
 16
18.4.1.3 QoS 17
1. The QoS scheme outlined in section 18.2.1, Figure 18-4 will be applied across the transport 18
network and is capable of supporting Fronthaul, Midhaul and Backhaul traffic running on 19
the same link. If an SRv6 underlay is to be used, then at a minimum the same queue 20
structure supported by appropriate operator selected DSCP marking should be used.  21
2. O-RU and O-DU set appropriate QoS markings for C/U and M-plane traffic that transport 22
network can trust.  23
3. In the above example, the C/U and M-plane traffic share the same physical interface, 24
separated using VLANs between the O-RU and CSR and between HSR and O-DU. In this 25
situation the mobile components (O-RUs and O-DUs) will need to support:  26
o An internal scheduling mechanism to prioritise C/U plane traffic over M-Plane 27
traffic onto the Ethernet interface  28
o Depending on O-RU→ CSR and HSR→ O-DU link speeds, the O-RU, O-DU, 29
CSR and HSR may need to support TSN frame pre-emption to control serialization 30
delay. (refer to section 10.1.1.2.1 for more details). Alternatively, if the O-RU or 31
CSR do not support TSN frame pre-emption or the O-DU or HSR do not support 32
TSN frame pre-emption, then separate physical interfaces could be used to separate 33
the C/U plane and M-plane traffic. 34
5. C/U and M-plane traffic from all O-RUs in the cell site share the same transport link 35
between the CSR and the HSR. In most scenarios where a CSR supports multiple radios 36
running fronthaul, this link will be 25Gbps or above and therefore does not really benefit 37
from TSN frame pre-emption. However, in some instances, the link between the CSR and 38
HSR may be lower speed and would benefit from TSN frame pre-emption. (see section 39
10.1.1.2.1 for more details) 40
6. C/U plane traffic must be prioritized over M-plane traffic on the WAN links. CSR and HSR 41
must be provisioned at both the link level and within the link level queuing infrastructure in 42
a manner such that C/U plane traffic is not dropped or experiences queuing delay in either 43
the CSR or HSR (or any additional Transport Network Equipment in the path between the 44
O-RU and the O-DU). M-plane traffic can experience a level of delay, so be configured to 45
use a lower priority queue or a queuing scheme such as Weighted Round Robin that reserves 46
a small amount of bandwidth for management traffic (see section 14 for an example) 47
 48
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        126
18.4.1.4 Services 1
1. C/U plane traffic uses an EVPN VPWS service to transport traffic between the O-RU and O-2
DU over the access network.  To simplify cell site VLAN provisioning, common VLANs 3
could be used between the O-RUs and the CSR and VLAN tag translation could occur on the 4
HSR to provide a unique interface / VLAN id per O-RU towards the O-DU. Depending on 5
the capabilities of the O-DU, EVPN has mechanisms that can provide service level resiliency 6
but this is dependent on the O-DU network implementation.    7
2. The M-plane traffic on the O-RU and O-DU connects, via a VLAN or physical interface to a 8
MP-BGP based L3 VPN service. This provides layer 3 connectivity between the 9
management components on the O-RU, O-DU and the O-RAN “Network Management 10
System” (NMS). “Route Target” filtering could be used to restrict transport level 11
connectivity between management entities. For example, the transport network could be 12
configured such that IP connectivity is only permitted between the NMS and the O-DUs and 13
O-RUs, while no IP connectivity is permitted between O-RUs.  14
 15
18.4.2 Non O-RAN Fronthaul 16
The assumptions, design approach and considerations discussed for the C/U planes in Open 17
Fronthaul are used for Non O-RAN Fronthaul. 18
18.4.3 Midhaul and Backhaul 19
Figure 18-6 illustrates a physical and logical design that could be employed to support Midhaul and 20
Backhaul traffic. It is a simple example but can easily be adapted based on operator designs and 21
requirements. It uses the underlay building blocks discussed earlier in the chapter.   22
 23
 24
Figure 18-6: Midhaul and Backhaul logical topology 25
 26

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        127
The following design assumptions have been made about the Midhaul / Backhaul components and 1
how traffic is presented and moved across the transport network. 2
18.4.3.1 Assumptions    3
1. Midhaul and Backhaul traffic share the same L3 VPNs. 4
2. There are dedicated and separate L3 VPNs for control plane traffic (N1, N2) and user plane 5
traffic (N3 and N9). 6
3. If legacy Backhaul is required, it shares the same L3 VPN transport services used by the 5G 7
EMBB Midhaul and Backhaul user and control plane infrastructure.  8
4. If there are multiple Midhaul and Backhaul transport slices, they could run in their own 9
independent L3 VPNs (see Annex F: Transport network slicing solution for WG1 Slicing 10
phase 1 (informational))  11
5. Midhaul and Backhaul mobile components will either use a discrete independent VLAN or a 12
dedicated physical Ethernet to separate control and user plane traffic. For the following 13
discussion it is assumed they are presented via VLANs on a single Ethernet interface. 14
6. Midhaul and Backhaul mobile components which require connectivity to different transport 15
slices will either use a discrete VLAN or a dedicated physical interfaces per slice. For the 16
following discussion it is assumed they will be presented via VLANs. 17
7. The transport and RAN teams have collaborated on Midhaul / Backhaul QoS marking and 18
packets emerging from O-DUs, O-CUs and UPFs have a DSCP marking appropriate to their  19
5G forwarding requirements, and the TNEs can trust these markings.  20
8. N6 networks associated with different customers, applications or use cases will use L3 21
VPNs.  22
  23
Note: Other service designs could be considered, for example a design where only Ethernet services 24
are used from the cell site and the hub sites and L3 VPN services are built at the hub sites. The 25
primary reason for the choice outlined above is to make the service orchestration as simple as 26
possible, so services only need to be configured where the mobile clients connect to the transport 27
network and no intermediate stitching points within the transport network are needed.   28
18.4.3.2 Midhaul and Backhaul physical topology  29
1. O-DUs connect, via Ethernet interface/interfaces, to a co-located HSR or CSR. The Ethernet 30
interface/interfaces support Midhaul control and user plane traffic via two discrete VLANs. 31
2. O-CUs connect, via Ethernet interfaces to the transport network. The O-CU Ethernet 32
interface/interfaces support Midhaul and Backhaul control and user plane traffic via two 33
discrete VLANs.  34
3. The UPFs connect, via Ethernet interfaces to the transport network. The UPF connects to the 35
common control and user plane Midhaul/Backhaul VPNs.  36
4. The location of the O-CUs and UPFs is service dependent and the choice of the operator. 37
Additionally, these components may be virtualized or containerized and reside in a data 38
center. In these scenarios, orchestration techniques maybe be required to connect the 39
virtualized O-DU, O-CU and UPF components in the data centers to the correct WAN 40
L3VPNs.   41
5. 5G Core components connect via Ethernet interfaces to the transport network in their 42
respective locations. These components may be virtualized or containerized and reside in a 43
data center. In these scenarios, orchestration techniques are required to connect the 5G 44
control plane entities in the data centers to the Midhaul and Backhaul control plane WAN 45
L3VPNs.   46
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        128
6. O-CU can reside in the same location as the O-DUs it controls, therefore Midhaul traffic 1
never leaves the site and is switched between the two entities by the either the CSR or HSR. 2
Alternatively, the O-CU can reside in a different location to the O-DUs it controls. In this 3
case the Midhaul traffic traverses the WAN.      4
7. HSRs provides the boundary between the access and aggregation networks. Connectivity to 5
the aggregation network will typically use high-capacity Ethernet interfaces or bundles of 6
high capacity Ethernet interface. The topology of the aggregation network is normally 7
resilient, but topologies vary based on operator choices and constraints, with hub and spoke 8
and rings very common.  9
8. Midhaul and Backhaul user plane traffic levels are dependent on UE usage, the 10
characteristics of the radio and the number the proximity of radios to each other. The O-RAN 11
Transport Requirements document [18] outlines some of the provisioning rules widely used 12
in planning 4G Backhaul networks. Although not directly applicable to the 5G, they provide 13
a good starting point in terms. Longer term network capacity planning should be done based 14
on monitoring and modelling the live network.  15
18.4.3.3 QoS 16
1. O-CU and UPFs mark the packets they produce based on the behaviour required for the 17
mobile data the carry.  18
2. Most Midhaul and Backhaul traffic is far more tolerant to delay, and jitter compared with 19
Fronthaul traffic. The exception could be some URLLC based use-cases. However, for most 20
5G use-cases fibre propagation delay, number of hops, transport equipment switching times 21
and serialization delays are not of a huge issue when designing Midhaul and Backhaul 22
networks.   23
3. In some scenarios, particularly when some forms of slicing are considered then edge QoS in 24
the form of ingress and egress H-QoS will be required.   25
18.4.3.4 Services 26
1. Two L3 VPNs are created; the first is Midhaul/Backhaul control plane traffic and the second 27
is Midhaul/Backhaul user plane traffic. O-DUs, O-CUs, UPFs connects to both L3VPNs. The 28
5GC connect only to the control plane L3VPN. If required “Route Target” filtering could be 29
used to restrict transport level connectivity between mobile entities. 30
18.5  Scenario 1 and 5  31
Both scenario 1 and 5 represent a full two split C-RAN architecture. The main difference is the 32
location and proximity of the O-DUs and the O-CUs to each other. In scenario 1 they are contained 33
in a single hub site location, in scenario 5 they are split across different locations.   34
 35
At the high level the basic service layout for scenario 1 and 5 is shown in Figure 18-7 and Figure 36
18-8 with a more detailed layout shown in Figure 18-9. 37
 38
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        129
 1
 2
Figure 18-7: Scenario 1 layout 3
 4
 5
 6
Figure 18-8: Scenario 5 layout 7

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        130
 1
 2
Figure 18-9: L2 / L3 service layout for scenario 1 and 5 3
 4
18.6 Scenario 2 5
Scenario 2 represent a full two split C-RAN architecture with the O-RU and O-DU at the cell site 6
and the O-CU at the hub site and is shown in Figure 18-10.  7
 8
  9
 10
Figure 18-10: L2 / L3 service layout for scenario 2 11
Notes on the design:  12
1. Fronthaul traffic is restricted to the cell site with the CSR providing a local L2 x-connect 13
function to get C/U plane traffic between the O-RUs and the O-DU. 14
2. The Midhaul/Backhaul control and user plane L3 VPNs need to be extended to the cell site 15
router. 16
3. In above diagram the O-DU has two physical interfaces with one supporting Fronthaul 17
traffic and the other supporting, via VLANs, Midhaul and Backhaul control plane, user 18

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        131
plane and Open Fronthaul M-plane traffic. In this instance TSN frame pre-emption is not 1
really a consideration. If the Midhaul, Backhaul and Fronthaul traffic shared the same 2
physical interface on the O-DU and the CSR, then depending on interface speeds, TSN 3
frame pre-emption may need to run between the O-DU and the CSR.  4
 5
18.7 Scenario 3 5G C-RAN with legacy D-RAN 6
18.7.1 Scenario 3a 7
Scenario 3a is a full two split C-RAN architecture with the O-RU at the cell site and O-DU at the 8
hub site sitting alongside an existing 4G D-RAN architecture and is shown in Figure 18-11. O-CU, 9
UPF and 5GC location are largely immaterial but they do need connectivity to the appropriate 10
L3VPNs.   11
 12
  13
 14
Figure 18-11: L2 / L3 service layout for scenario 3a 15
 16
Notes on the design:  17
1. Open Fronthaul traffic runs over the access network to an O-DU located in the Hub site 18
using a VPWS EVPN service.  19
2. The Midhaul/Backhaul control and user plane VPNs extend to the cell site to support the 20
existing 4G D-RAN solution.  21
3. The QoS design in the access network needs to be able to cater for the requirements of the 22
Open Fronthaul when D-RAN Backhaul traffic is sharing the same physical link.   23
 24
18.7.2 Scenario 3b 25
Scenario 3b is a full two split C-RAN architecture with the O-RU and O-DU at the cell site and the 26
O-CU located at the hub site sitting alongside an existing D-RAN architecture and is shown in 27
Figure 18-12 28

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        132
   1
 2
Figure 18-12: L2 / L3 service layout for scenario 3b 3
 4
Notes on the design:  5
1. The only difference between scenario 2 is that the legacy D-RAN equipment also connect to 6
the Midhaul/Backhaul control and user plane VPNs at the cell site. 7
18.8 Scenario 4 5G C-RAN with RoE mappers 8
Scenario 4 is a full two split C-RAN architecture with O-RAN 7.2x and non O-RAN Fronthaul 9
protocols running across the access network and is shown in Figure 18-13.    10
 11
Figure 18-13: L2 / L3 service layout for scenario 4 12
 13
Notes on the design:  14
1. O-RAN 7.2x C/U plane and non O-RAN Fronthaul packet traffic (between RoE mappers) 15
use VPWS EVPN services. 16

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        133
2. The expectation is both the O-RAN 7.2x C/U plane traffic and the non O-RAN Fronthaul 1
traffic will have similar delay and jitter characteristics so will run in the same traffic class 2
which is priority queued.  3
3. The vendor of the RoE mapper function will need to provide details of traffic rates and 4
whether they are fixed or vary depending on user data. The network planner will need to 5
take this into account when capacity planning the access links between the CSR and the 6
HSR.  7
 8
18.9 Scenario 6 5G C-RAN with distributed UPF 9
Scenario 6 includes the insertion of distributed UPFs in the architecture. The solution is illustrated 10
in Figure 18-14 and is easily to achieve by simply connecting the distributed UPF to the 11
Midhaul/Backhaul control and user plane L3VPNs. All necessary transport routing will occur 12
automatically through the L3VPN control plane. 13
   14
 15
Figure 18-14: L2 / L3 service layout for scenario 6 16
18.10 Scenario 7 Slicing  17
This section has been removed because WG-1 is now developing the overall slice architecture and 18
use cases [23] for the whole of the O-RAN organization. The WG-1 slicing task force is developing 19
the slicing solution in phases over multiple O-RAN releases.  The packet switched architecture has 20
an extensive set of capabilities to support transport level slicing which are outlined in section 17. As 21
the slice use cases are going to rapidly evolve based on WG-1 slice use cases, slicing scenario have 22
been moved to Annex F where solution examples are presented.  23
 24
19 Annex A: Overview of “Segment Routing” (SR)  25
This annex gives an overview of segment routing based on the Segment Routing Architecture 26
defined in RFC8402 [118] and related drafts. It has been included as an annex because the main 27
document describes two architecture/designs that utilise Segment Routing: SR-MPLS and SR for 28
IPv6 or SRv6. There are differences between the two implementations and capabilities of SR when 29

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        134
using an MPLS and IPv6 data plane but there are also many similarities in the basic architectural 1
concepts and the motivation for using the technology. Consequently, this annex describes the basic 2
SR architecture and the “logical architecture sections” in the main document goes into more detail 3
on the technologies and outlines implementation and design considerations when using the 4
respective technologies.  5
This annex utilises many different information sources and, in some instances, takes content very 6
literally from these sources. Information sources include IETF documents, research and vendor 7
documents.  8
 9
Background 10
The Internet and role of packet switching has changed over the last couple of decades and continues 11
to evolve rapidly. This is no better illustrated than by the 5G transport requirements which 12
anticipates billions of endpoints, massive numbers of antenna, huge bandwidths and the ability to 13
concurrently support different 5G services. This evolution is challenging existing Internet protocols, 14
equipment and increasing the operational complexity of the network. Segment Routing and related 15
technology evolutions aims to address these challenges through protocol simplification, removal of 16
state from the network and embracing technology advances such as Software Defined Networking.     17
Segment Routing  18
Segment Routing (SR) is based on the loose Source Routing concept. A node can include an 19
ordered list of instructions in the packet headers. These instructions steer the forwarding and the 20
processing of the packet along its path in the network. Single instructions are called segments, a 21
sequence of instructions is called a segment list or an SR Policy. Each segment can enforce a 22
topological requirement (e.g. pass through a node or an interface) or a service requirement (e.g. 23
execute an operation on the packet). The term segment refers to the fact that a network path towards 24
a destination can be split in segments by adding intermediate waypoints. The segment list can be 25
included by the original source of the packet or by an intermediate node. When the segment list is 26
inserted by an intermediate node, it can be removed by another node along the path of the packet, 27
supporting the concept of tunneling through an SR domain from an ingress node to an egress node.  28
The implementation of the Segment Routing Architecture requires a data plane which is able to 29
carry the segment lists in the packet headers and to properly process them. Control plane operations 30
complement the data plane functionality, allowing segment allocation (i.e. associate a segment 31
identifier to a specific instruction in a node) and distribution of segment identifiers within an SR 32
domain. 33
 34
 35

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        135
Figure 19-1: Segment Routing protocol stack and path capabilities  1
 2
Segment Routing architectural principle 3
There is a single SR architecture which is defined in RFC8402 [118]. This defines the general 4
concepts of SR and is independent from the specific data plane. Currently, there are two 5
instantiations of the SR architecture designed and implemented in production code, SR over MPLS 6
(SR-MPLS) and SR over IPv6 (SRv6). 7
 8
The architecture relies on segments identifiers (SIDs) and SID lists. The SID represents a single 9
instruction which can be related to how the packet is forwarded or a function that needs to be 10
performed on a packet. A SID lists or the SR policy is a collection of SIDs or instructions to be 11
performed on the packet as it traverses the SR domain. Typically, a SID list is imposed on the 12
packets as it enters the SR domain and is either empty or completed by the time it leaves an SR 13
domain.       14
There are two basic types of segment, global segments which correspond to instructions that are 15
globally valid in an SR domain and local segments which correspond to instructions that are only 16
valid within a single node. Some of the key segment types are: 17
1. IGP Prefix and IGP node segments: These are global SIDs and are instructions on how to 18
forward a packet towards a destination IP network or destination IP node. As the name 19
suggests these are conveyed by IGP and can be used by any node in the SR domain to send 20
traffic to a node or IP prefix.   21
2. Adjacency segments: These are local SIDs which identify the links available on that node.  22
3. Anycast segments: These are a special IGP-prefix segment that corresponds to an anycast 23
prefix. ie a prefix advertised by more than one router. Anycast and anycast segments are 24
commonly used to provide high availability or load balancing solutions.  25
4. Binding Segments: These are a single SID that is associated with a segment list or SR policy. 26
This means the node that processes the Binding SID replaces the single segment with a 27
segment list.  28
 29
Using Figure 19-1 as an example, the basic SR architecture can support:  30
1.  A dynamically locally computed forwarding path between an ingress and egress point 31
within the SR domain by the ingress node either imposing the node SID of the egress node 32
or the prefix SID of the destination prefix. Typically, a link-state IGP or an EGP routing 33
protocol, running locally on the routers calculate these types of paths and they exhibit the 34
behaviour of normal routing protocols such “Shortest Path Forwarding” and “Equal Cost 35
Multi Pathing (ECMP)  36
2. A fully deterministic path between the ingress and egress points. In this case the SR policy 37
or SID list includes all nodes on the path and if required adjacency SIDs to select links 38
between nodes.   39
3. A combination of the two.  The SR architecture can build a path between an ingress and 40
egress node that combines dynamically computed forwarding and explicitly routed 41
forwarding. 42
 43
Segment Routing data plane 44
Two data plane instantiations are designed and implement: SR over MPLS (SR-MPLS) and SR over 45
IPv6 (SRv6). 46
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        136
SR-MPLS 1
The MPLS data plane (SR-MPLS) is specified in RFC 8660 “Segment Routing with MPLS data 2
plane”, In the case of SR-MPLS, Segment Routing does not require any change to the MPLS 3
forwarding plane. An SR Policy is instantiated through the MPLS Label Stack and the Segment IDs 4
(SIDs) of a Segment List are inserted as MPLS Labels. The classical forwarding functions available 5
for MPLS networks allow implementing the SR operations.  6
IPv6 data plane (SRv6)  7
For the IPv6 data plane (SRv6), a new type of IPv6 Routing Extension Header, called Segment 8
Routing Header (SRH) has been defined in RFC 8754 IPv6 Segment Routing Header (SRH) [128]. 9
The SRH contains the Segment List (SR Policy) as an ordered list of IPv6 addresses: each address 10
in the list is a SID. A dedicated field, referred to as Segments Left, is used to maintain the pointer to 11
the active SID of the Segment List. This SRH format and basic operation of the SRH is illustrated 12
in Figure 19-2. 13
 14
Figure 19-2 SRH header and interaction between segments left and SID list  15
 16
To explain the SRv6 data plane, there are three categories of nodes: Source SR nodes, Transit nodes 17
and SR Segment Endpoint nodes. A Source SR node corresponds to the headend node, it can be a 18
host originating an IPv6 packet, or an SR domain ingress router encapsulating a received packet in 19
an outer IPv6 header.  20

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        137
 1
Figure 19-3  Packet and SRH construct for IPv4 payload for traffic following nodes 1,2,5 2
 3
In Figure 19-3, which considers the latter case the source SR node (S1) is an edge router that 4
encapsulates a packet (which can be IPv6, IPv4 or an Layer 2 frame) into an outer IPv6 packet and 5
inserts the SR Header (SRH) as a Routing Extension Header in the outer IPv6 header. The traffic is 6
destined for S5 and for policy reasons the operator wishes to send the traffic via S2 and S3 which is 7
not the shortest IGP path. To implement this policy, a segment list is composed of S2 and S5. S3 8
does not need to be specified because the shortest path from S2 to S5 is via S3. To implement this 9
SR policy and assuming the use of a reduced SRH (for details see RFC8754) the packet details are 10
shown in Figure 19-3. 11
  12
In addition to the basic operations, the SRv6 Network Programming model [147] describes a set of 13
functions that can be associated to segments and executed in a given SRv6 node. Examples of such 14
functions are: different types of packet encapsulation (e.g. IPv6 in IPv6, IPv4 in IPv6, Ethernet in 15
IPv6), corresponding decapsulation, lookup operation on a specific routing table (e.g. to support 16
VPNs). The list of functions described in [147] is not meant to be exhaustive, as any function can be 17
associated to a segment identifier in a node. Obviously, the definition of a standardized set of 18
segment routing functions facilitates the deployment of SR domains with interoperable equipment 19
from multiple vendors.  20
Segment Routing control plane 21
The control plane is responsible for calculating, building the connectivity graphs and implementing 22
the SR policies within the network. Segment Routing can support distributed, centralised or hybrid 23
control plane architectures. See Figure 19-4 for more details. 24
 25

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        138
 1
Figure 19-4 Distributed, centralised and hybrid control planes 2
 3
In a distributed control plane environment, the transport nodes independently interact with each 4
other to convey routing information and independently make decisions to set-up and enforce SR 5
policies. This is achieved through a combination of IGP and EGP protocols running between the 6
transport nodes. 7
In a centralised control plane environment, central SDN controllers have full knowledge of the 8
network topology and calculates SR policies centrally. These are then programmed onto the TNEs 9
in the network.  10
The hybrid approach splits the responsibilities, the TNEs in the network support some functions, 11
such as building the link state database, calculating the intra-domain routing tables, monitoring the 12
links to attached nodes, and rapidly recovering in the event of failure. At the same time the central 13
SDN controllers has a feed from the network and is used for functions that require a network wide 14
view such as transport optimization and inter-domain routing.  15
 16
MPLS-SR distributed control plane 17
The MPLS-SR distributed control plane utilises ISIS or OSPF routing protocols with extensions to 18
advertise the different types of IGP segments (prefix, node, adjacency, any cast) between TNEs. 19
 20
IPv6 distributed control plane  21
For the IPv6 data plane, the process of advertising the IGP-prefix, IGP-node and IGP-anycast 22
segments is simplified due to the use of IPv6 addresses as SIDs. In particular, there is no need to 23
extend the IGP routing protocols to distribute these segment types, because they are IPv6 prefixes 24
natively distributed by the routing protocols. This means that the Control Plane for SRv6 can use 25
the regular IPv6 link state IGP routing protocols (OSPFv3, ISIS) to support the basic operations, 26
while extensions are still needed ([126][127]) to distribute IGP-Adjacency segments and other SR 27
configuration information.  28
Centralised control plane 29
Although much emphasis has been made of fully centralised control plane models in academia for 30
path computation, they have not made their ways into production large scale WAN designs due to 31
the distributed nature of the transport nodes and the need for the transport nodes to communicate at 32

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        139
all times with the central controller. As a consequence, most WAN SDN designs today are hybrid in 1
nature, where intelligence is split between the TNEs and central SDN controllers. 2
   3
Note: This discussion above only considers path computation element of a central controller. There 4
are many aspects of a central SDN controller including orchestration, data analytics etc.  5
Hybrid control plane  6
The hybrid control plane is combination of distributed control plane and centralised SDN control 7
plane functionality. Like the central control plane model there is no universal architecture for a 8
hybrid control plane design, other than some functions are provided centrally and others occur 9
locally on the TNEs. One of the most common hybrid control plane designs is where the central 10
SDN controllers provides a “Path Computation Element” capability. In this model the PCE is 11
responsible for path computation and inter-domain routing with the distributed control planes 12
running on the TNEs, responsible for building the link state database, calculating the network graph 13
and building the shortest path forwarding tables. In this scenario there is generally a tight 14
relationship between the two components.  15
 16
A. The TNEs exchange topology information, creates the link-state database and computes the 17
shortest path forwarding tables.  18
B. The centralised control plane component needs to get visibility of the distributed protocol’s 19
link-state database to understand the current topology. Some of the common approaches are:    20
a. The SDN controller participates in the IGP and as a consequence builds its own link-21
state database and network graph. 22
b. The SDN controller extracts IGP databases from the network using proprietary 23
mechanisms. 24
c. BGP-LS (BGP link state) runs between one or more TNEs and centralised 25
controllers. In this way the centralised controller gets visibility of the network 26
through BGP-LS.  27
C. In many instances the centralised control plane needs to gather other information about the 28
network, for example link loading. At this time there is no single standard for collecting this 29
information. Common mechanisms include SNMP, telemetry feeds and operational data from 30
Yang models residing on the devices.  31
 32
D. The central controller and the TNEs also need to communicate when, either a TNE requests a 33
service from the central SDN controller, or the central SDN controller wants to push an explicit 34
policy to a TNE. An example would be if a TNE needs build an SR policy and wants to use the 35
services of a PCE. The transport node and the PCE need to communicate, some of the common 36
protocols used for this type of communications include:   37
a. Network Configuration Protocol  38
b. PCE (PCEP) 39
c. BGP 40
d. Openflow (SR-MPLS) 41
20 Annex B: IETF Ethernet Virtual Private Networks 42
IETF Ethernet Virtual Private Network (EVPN) is a service supporting transportation of Ethernet 43
frames across a transport network encapsulated in MPLS or IP. There are multiple flavors of IETF 44
EVPN service as outlined Table 6. 45
 46
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        140
Service Type Standardization status
Layer 2 VPN
E-LAN
VLAN-based RFC 7432 [102]: BGP MPLS-Based Ethernet
VPN VLAN-bundle
VLAN-aware bundle
E-Tree
VLAN-based RFC 8317 : Ethernet-Tree (E-Tree) Support in
Ethernet VPN (EVPN) and Provider Backbone
Bridging EVPN (PBB-EVPN)
VLAN-bundle
VLAN-aware bundle
E-Line
(VPWS)
VLAN-based

RFC 8214 [111]: Virtual Private Wire Service
Support in Ethernet VPN
draft-ietf-bess-evpn-vpws-fxc [132]: EVPN
VPWS Flexible Cross-Connect Service VLAN-bundle
Layer 3 VPN draft-ietf-bess-evpn-prefix-advertisement
[134]: IP Prefix Advertisement in EVPN
Table 6: EVPN service classification 1
 2
The two major building blocks of EVPN are: 3
• Control plane, based on Border Gateway Protocol (BGP), to distribute all necessary 4
information required for proper EVPN operation 5
• Data plane, with different underlay transport options, to suit various requirements: 6
o MPLS (LDP, RSVP, BGP-LU, MPLSoUDP, SR, SR-TE) 7
o IP (VxLAN, SRv6) 8
EVPN E-LAN service 9
EVPN E-LAN service is a Layer 2 multipoint-to-multipoint service. In essence, EVPN E-LAN 10
emulates Layer 2 switch behavior, with attachment circuits (ACs) of this single emulated Layer 2 11
switch present on multiple Provider Edge (PE) routers, which are placed in different network 12
locations, as outlined in Figure 20-1 13
 14
 15
Figure 20-1: EVPN E-LAN service  16
 17
Similar to ordinary Layer-2/Layer-3 switch, EVPN E-LAN service provides 18
• MAC learning capability (globally or per VLAN) 19
BGPBGP
CE1
BGP
PE1 PE3
PE4
PE2
BGP
CE3
CE4
CE2
CE1
 CE3
CE4
CE2

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        141
• capability to carry Ethernet frames without VLAN tag, as well as with single or double 1
VLAN tags, including VLAN translation 2
• single-homed, as well as multi-homed (both single-active and all-active) attachment circuits 3
capability 4
• BUM optimization mechanisms (like for example APR proxy/suppression, IGMP proxy, 5
etc.) 6
• handover to Layer-3 domain via integrated routing and bridging (IRB) operation and more. 7
EVPN E-Tree service 8
EVPN E-Tree service is an enhancement of EVPN E-LAN service, which allows to restrict certain 9
communication patterns, creating in essence a point-to-multipoint service model. As outlined in 10
Figure 20-2, each attachment circuit is designated as root (R) or leaf (L). As per EVPN E-Tree 11
definition, only rootroot and rootleaf Layer 2 communication is permitted, while leafleaf 12
Layer 2 communication is blocked. 13
 14
Figure 20-2 EVPN E-Tree services 15
 16
At a high level, EVPN E-LAN service can be considered as a special case of EVPN E-Tree service, 17
with all attachment circuits designated as root. In fact, many vendors use the same configuration 18
constructs for EVPN E-LAN and EVPN E-Tree, defaulting the attachment circuit to a root, with 19
some configuration knobs turning it to a leaf. 20
EVPN E-Line service 21
EVN E-Line, called as well EVPN Virtual Private Wire Service (VPWS), is point-to-point Layer 2 22
service, without MAC learning. Ethernet frame received from an end device connected via single-23
homed or multi-homed attachment circuit, is transported over this point-to-point service to the 24
remote end device, connected again via single-homed or multi-homed attachment circuit. Since it is 25
pure point-to-point communication, no MAC learning is required, and thus MAC learning is not 26
part of EVPN E-Line (VPWS) architecture. 27
 28
 29
BGPBGP
CE1
BGP
PE1 PE3
PE4
PE2
BGP
CE3
CE4
CE2
R
R
L L
R
CE1
 CE3
CE4
CE2
R
L
R
L
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        142
 1
Figure 20-3 EVPN E-Line (VPWS) services 2
 3
EVPN VLAN-based service 4
EVPN architecture uses three implementation constructs defined on the Provider Edge (PE) 5
devices: 6
• EVPN Instance (EVI), called as well MAC-VRF. Both EVI and MAC-VRF will be used in 7
this document interchangeably. 8
• Bridge (MAC learning) domain/table 9
• Broadcast (flooding) domain (i.e. VLAN) 10
 11
Depending how these building blocks are used, EVPN architecture defines 3 options for 12
constructing EVPN services: 13
• EVPN VLAN-based service (applicable to EVPN E-LAN, EVPN E-Tree and EVPN E-Line 14
services) 15
• EVPN VLAN-bundle service (applicable to EVPN E-LAN, EVPN E-Tree and EVPN E-16
Line services) 17
• EVPN VLAN-aware bundle (applicable to EVPN E-LAN and EVPN E-Tree services only) 18
 19
In EVPN VLAN-based service, each EVI contains single bridge (MAC learning) domain/table, 20
which in turn contains only single VLAN (single broadcast/flooding domain). This is the simplest 21
EVPN service with 1:1 mapping between VLAN and EVI as outlined in 0 22
 23
CE1
 PE1 PE3
PE4
PE2
CE3
CE4
CE2
CE1
 CE3
CE4
CE2

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        143
 1
Figure 20-4 EVPN VLAN-based services (E-LAN, E-Tree, E-Line) 2
 3
EVPN VLAN-based service allows for VLAN tag translation, where incoming VLAN tag (for 4
example VLAN 2) is translated to VLAN 12. Additionally, EVPN VLAN-based service provides 5
the option to carry the Ethernet frame with the VLAN tag across EVPN network, or to strip the 6
VLAN tag before carrying the Ethernet frame across EVPN network.  7
EVPN VLAN-bundle service 8
The biggest challenge with basic EVPN VLAN-based service is scaling. In EVPN VLAN-based 9
service each VLAN consumes dedicated EVI, thus the maximum numbers of VLANs that can be 10
served is limited by the supported EVIs on given hardware platform. EVPN VLAN-bundle service 11
addresses this scaling limitation by bundling multiple VLANs into single bridge (MAC learning) 12
domain/table and single EVI (MAC-VRF), as outlined in Figure 20-5 13
 14
 15
Figure 20-5 EVPN VLAN-bundle service (E-LAN, E-Tree, E-Line) 16
 17

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        144
VLAN bundling allows dramatic reduction of MAC-VRFs that otherwise would be needed on the 1
router. Therefore, EVPN VLAN-bundle service is typically used in high-scale environment, with 2
large number of VLANs or VLAN combination (QinQ) that must be transported with EVPN 3
service. 4
 5
This scaling improvements brings improvements, as well some restrictions for service deployment, 6
namely: 7
• the original VLAN tag must be carried with frame across EVPN network and VLAN 8
translation is not supported 9
• MACs cannot be re-used across different VLANs 10
 11
The first restriction is related to the fact, that underlying transport identification (i.e. MPLS label 12
with MPLS underlay transport, Function:Argument with SRv6 underlay transport) identifies the 13
bridge domain. Therefore, the underlying transport identification cannot be used to distinguish 14
Ethernet frames - received over EVPN network - as belonging to different VLANs, if VLAN tag is 15
stripped before sending Ethernet frames across EVPN. And this differentiation is required to 16
maintain per-VLAN broadcast/flooding restrictions. 17
 18
Second restriction is the straight result of bundling multiple VLANs in single bridge (MAC 19
learning) domain/table. All these bundled VLANs use the same MAC table, therefore all MACs 20
must be unique across all bundle VLANs. 21
EVPN VLAN-aware bundle Service 22
Restrictions of EVPN VLAN-bundle services are removed by EVPN VLAN-aware bundle service. 23
EVPN VLAN-aware bundle service is a trade-off between EVPN VLAN-based service (poor 24
scaling, but simple service without restrictions related to VLAN translation or MAC learning) and 25
EVPN VLAN-bundle service (good scaling, but restricting VLAN translation support and requiring 26
MAC uniqueness). 27
EVPN VLAN-aware bundle brings back separate bridge (MAC learning) domain/table per VLAN, 28
still keeping the option to bundle multiple VLANs with single EVI (MAC-VRF), as outlined in 29
Figure 20-6. 30
 31
 32
 33
Figure 20-6 EVPN VLAN-aware bundle service (E-LAN, E-Tree) 34

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        145
 1
Scaling wise EVPN VLAN-aware bundle service sits between EVPN VLAN-based service and 2
EVPN VLAN-bundle service, optimizing number of required EVIs (MAC-VRFs), while keeping 3
the same number of bridge (MAC learning) domain as EVPN VLAN-based service. 4
 5
Table 7 summarizes different EVPN service models. 6
 7
Characteristic VLAN-
based
VLAN-
bundle
VLAN-
aware
bundle
Broadcast domains (VLANs) per EVI =1 1 1
Bridge (MAC learning) domains per EVI =1 =1 1
MACs must be unique across VLANs no yes no
VLAN translation allowed yes no yes
VLAN tag carried over yes/no yes yes
Ethernet Tag (BD) ID on BD scoped routes 0 0 ≠0
Port based service support no yes yes
Table 7: EVPN VLAN service types comparison 8
 9
MEF/EVPN service mapping 10
MEF 6.3 (Subscriber Ethernet Service Definitions) defines Ethernet services commonly used in 11
Ethernet metro aggregation architectures. Table 8 provides the mapping between MEF service 12
definitions and IETF EVPN service implementations. 13
 14
MEF 6.3 Ethernet Service Definitions IETF EVPN Service Definition
EPL: Ethernet Private Line EVPN E-Line VLAN-based
EVPL: Virtual Ethernet Private Line EVPN E-Line VLAN-bundle (port-based)
EP-LAN: Ethernet Private LAN EVPN E-LAN VLAN-based
EVP-LAN: Ethernet Virtual Private LAN EVPN E-LAN VLAN-bundle (port-based)
EVPN E-LAN VLAN-aware bundle (port based)
EP-Tree: Ethernet Private Tree EVPN E-Tree VLAN-based
EVP-Tree: Ethernet Virtual Private Tree EVPN E-Tree VLAN-bundle (port-based)
EVPN E-Tree VLAN-aware bundle (port based)
Table 8: MEF Ethernet Services to IETF EVPN Services Mapping 15
 16
21 Annex C: MP-BGP based L3VPNs 17
BGP based layer 3 VPNs are a widely deployed connectivity application in Service Provider 18
networks and work over an MPLS and SRv6 underlay infrastructure. BGP based layer 3 VPNs are 19
based on RFC4364 [72] and use a peer-to-peer model that uses Border Gateway Protocol (BGP) to 20
distribute VPN-related information. It is a highly scalable, peer-to-peer model that through route 21
filtering allows flexible connectivity models within a VPN. VPN topologies include any-to-any L3 22
multi-point solutions as well as L3 tree solutions where connectivity constraints are built into the 23
VPN infrastructure through controlled exportations and importation of routing information. This 24
makes it an effective and efficient technology for L3 services associated with Xhaul transport. 25
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        146
Building blocks of a L3 VPN service 1
A typical L3 VPN services consists of the following:  2
 3
• A Provider Edge (PE) Routers 4
• Virtual Routing Forwarding  5
• Route Distinguisher (RD) 6
• Route Target (RT) 7
• Multi-Protocol Border Gateway Protocol (MP-BGP) 8
 9
Provider Edge (PE) Router 10
A Provider Edge router is a device which originate or terminate a L3VPN service. In other words, a 11
PE router is a VPN Endpoint, providing connectivity to the device(s) connected to it.  12
 13
 14
Figure 21-1 PE routers in an Xhaul infrastructure 15
 16
In the context of an Xhaul packet switched transport network, the first L3 router enabling an 17
L3VPN services to provide connectivity between the O-RAN/3GPP components (O-RU, O-DU, O-18
CU, UPF, 5GC) is considered a Provider Edge (PE) router. This could be a CSR or HSR in 19
Fronthaul, Midhaul or Backhaul network. 20
 21
VPN Routing and Forwarding Tables (VRF) and Route Distinguishers (RD) 22
The VPN routing and forwarding table (VRF) is a key element in the BGP based L3 VPN 23
technology. A VRF essentially define a VPN instance on PE and associates a routing table instance 24
with the VRF.  A VRF exist on PEs and more than one VRF can exist on a single PE. The VRF 25
contains routes that should be available to a particular set of sites participating in the VPN.  26
 27

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        147
 1
Figure 21-2: VRF routing tables  2
 3
A route distinguisher (RD) is a unique value assigned to a VRF on the PE Router. A RD allows to 4
associate and identify routes in the routing table with a particular VRF and allows for overlapping 5
IP address space across VRFs.   6
An RD is a local value on the PE in the Packet Switched RAN network.  7
 8
Route Targets (RT) 9
Route Targets (RT) are the mechanism by routing information distribution can be controlled within 10
an BGP based VPN throughout the packet switched network. Route Target are propagated through 11
the BGP based VPN network using route-target extended MP-BGP communities. Every PE within 12
the MPLS VPN network define a set of RT values for import or export of VPN route information.  13
BGP based VPN uses route-target communities as follows: 14
• When a VPN route is injected into MP-BGP, the route is associated with a list of VPN 15
route-target communities.  16
• An import list of route-target communities is associated with each VRF. This list defines the 17
values that should be matched against to decide whether a route is eligible to be imported 18
into this VRF. 19
 20
 21
 22
Figure 21-3: Use of route targets to control IP connectivity within a VRF 23

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        148
 1
Figure 21-3 above shows a simple mechanism of import and export Route Communities within an 2
BGP based VPN network. Where required CSR route-targets may be configured to be different than 3
the Hub Site Router.  For scalability and efficient resource usage on the CSR, the operator may 4
choose to not import all the routes, rather importing the routes from Hub Site Route and only certain 5
other CSR’s.  6
 7
When used properly, Route Targets can be used to limit the routing table information within a BGP 8
based VPN Network and create different IP topologies at the transport layer. These include: 9
• Any to any topologies. 10
• Hub and spoke topologies. 11
 12
Multi-Protocol Border Gateway Protocol (MP-BGP) 13
MPLS VPN information is propagated through the network using Multi-Protocol BGP (MP-BGP) 14
[74]. All PE routers within the VPN must configure MP-BGP peering and enable VPN address-15
family and extended communities to exchange VPN related information including routing table 16
information and Route Targets.  17
 18
 Traffic Steering into an BGP VPN 19
SR Policy operations is defined in [145]. It is a mechanism that can be to create an end-to-end Label 20
Switch Path (LSP) using required metrics and constraints. While using Segment Routing Traffic 21
Engineering (SRTE) for traffic path programmability, it is possible to automatically steer BGP 22
based VPN traffic into an SRTE policy.  23
Typically, an SRTE Policy is uniquely identified using the following 3-tuple consisting of:  24
• Head End: Starting point of the LSP created by SRTE Policy 25
• End Point: Destination for the traffic using the SRTE LSP 26
• Color: A numeric value associated with the SRTE policy.  27
 28
The SRTE policy color is a numeric value that is configurable in the SRTE policy. Once a VPN 29
endpoint route is “tagged” with this color, traffic can be automatically steered into the SRTE policy.  30
 31
 32
 33
Figure 21-4: Traffic Steering with BGP based VPNs 34

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        149
 1
As shown above, the VPN routes can be colored in 2 possible ways:  2
 3
1. While defining a VRF, define a color for all Prefixes associated with that VRF, or 4
2. When sending or receiving BGP routes, using a policy to “color” the route.  5
 6
In either case, the VPN route will now be tagged with a color and traffic for the destination or a 7
flow will automatically be steered into the SRTE policy that may provide traffic path 8
programmability.  9
 10
22 Annex D: Quality of Service  11
This annex gives an overview of packet base Quality of Service (QoS).   12
What is Quality of Service? 13
Quality of Service (QoS) is an umbrella term that broadly covers the concept of the management of 14
traffic flowing in an infrastructure. In general, QoS can provide two capabilities.  15
 16
1. A mechanism to actively manage the transmission of data on a network link out of a node 17
when the link is approaching or at saturation. This active management consists of 18
determining which data to forward next, which data to store for forwarding later, and which 19
data to drop. 20
2. The enforcement of service level agreements through the selective acceptance of traffic into a 21
node (or network) based on pre-determined criteria. Again, this acceptance is based on 22
determining which data to accept now, which data to store for acceptance later and which 23
data to deny (drop).  24
 25
Fundamentally, QoS only operates when traffic is flowing at the limits of capacity (either physical 26
or logical). When flowing traffic rates are well below the limits, QoS is essentially benign allowing 27
all traffic to be forwarded as it is received.  28
 29
Why do we need QoS?  30
Applications using the network transport will often require certain capabilities from the network in 31
order for the application to function correctly.  32
 33
For example, real-time voice services (telephone calls) can only sustain a maximum latency 34
between the end points before interactive speech becomes difficult. This maximum latency between 35
endpoints translates to two demands from the network; 36
1. Minimising hop by hop latency as a voice packet flows across the network infrastructure.  37
2. Minimising jitter in the packets arriving in order to minimise the de-jitter buffer needed at the 38
receiver (which adds latency).  39
It is useful then to be able to identify voice traffic in the network and prioritise if for forwarding 40
such that it is not held up behind other less time sensitive traffic.  41
 42
A second example might be an Electronic Point of Sale (EPoS) application. This type of application 43
does not need to be particularly latency bound (a few 10s or 100s of milliseconds in packet arrival 44
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        150
does not impact the service), but it does need to have some guaranteed capacity in order to function. 1
I.e. we do not want the EPoS application struggling for bandwidth because a large data transfer is 2
taking place. This guaranteed bandwidth requirement also translates into network demands. 3
1. Minimising packet loss for the traffic, if necessary, storing it locally until the local egress 4
interface has capacity to send it on. 5
2. Reserving a minimum amount of transmission time (or bandwidth) on the interface for 6
packets of the EPoS application. 7
 8
QoS then is the mechanism by which operators can identify applications in an infrastructure and 9
apply policies to the infrastructure to maintain the applications transport requirements, even when 10
capacity is limited. 11
 12
 QoS functional elements 13
As a suite of capabilities QoS can be broken out into three distinct sets of functions; 14
 15
1. Traffic Classification and marking 16
2. Congestion avoidance 17
3. Congestion management 18
 19
These three function sets work together to identify (classify) different traffic types, determine how 20
much of each traffic type should be allowed into and through the network (congestion avoidance) 21
and how traffic should be scheduled when interfaces become congested (congestion management). 22
 23
The specific implementation of these capabilities is vendor specific, but in general, the behaviours 24
required by the different functions are defined as policies, either individually or in groups. These 25
policies are applied to interfaces on the switching elements in a network defining the local 26
behaviours within the device.  27
 28
 Network level behaviour 29
QoS is implicitly a mechanism that is node based; the management of traffic in one node has no 30
direct impact on the management of traffic in surrounding nodes. However, it is important to design 31
the QoS behaviour of a network as a whole. Just as the operator has the ability to define the routing 32
behaviour of a network as a system of single hop by hop elements, so she has the ability to define 33
the QoS behaviour of the network as a system.  34
 35
The interaction between elements is defined by the policies applied to each device and the use of 36
“class of service” markings applied to the packets or frames flowing between the devices. 1 37
The use of markings allows an individual network element to pass some information about a given 38
packet to a downstream element. How this downstream element interprets the information and then 39
acts on it, is out of the upstream elements control, but a network of devices under single 40
administrative control may choose to use such information in network wide policy. 41

1 The term “class of service” is used here to generically to describe the specific markings in a packet or frame. The
author recognises Class of Service (COS) identifies such markings in an ethernet header as defined by IEEE 802.1p.
Here it is generically used to refer to Ethernet COS, IP TOS, IP PREC, IP DSCP. MPLS EXP etc.
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        151
 1
 Node level behaviour  2
 3
 4
Figure 22-1: Example network QoS structure 5
 6
 7
The per-hop-behaviours (PHBs) needed in the network can generally be broken down into two sets; 8
Those needed at the edge of the network (Edge QoS), and those needed in the core of the network 9
(Core QoS). We can see these behaviours in Figure 22-1. 10
 11
1. Edge QoS. Here we are classifying traffic entering into the network on ingress (1) and 12
scheduling traffic leaving the network on egress (6). Here the ingress classification (from the 13
client) and egress classification (5) and scheduling (6) (towards the client) will (often) be 14
determined by the markings of the client traffic being transported across the network. In 15
addition, ingress traffic (CE → PE), once classified, will be marked (2) to conform to the 16
transport marking scheme.  17
 18
2. Core QoS. Here traffic will be classified by the transport marking scheme (4) (mentioned 19
above), scheduled based on those markings (3). This PHB is designed to manage traffic at an 20
aggregate level, treating services that have common requirements (defined by marking) with 21
a common behaviour.  22
 23
These two sets of PHBs present different requirements to the underlying hardware and care should 24
be taken when selecting a platform to ensure the operators desired PHBs can be met.  25
 26
Traffic classification and marking 27
As previously mentioned, traffic classification is a term used to describe the identification and 28
categorisation of traffic arriving at or inside network element. Classification broadly falls into two 29
distinct categories.  30
 Context based classification 31
In this model, traffic arriving at a node is classified, (allocated to some internal category) based on 32
the context associated with its arrival. For example, it might be assumed that all traffic arriving 33

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        152
from a specific logical or physical interface will be categorized as “best effort”. This specific 1
example actually represents the default behaviour for almost all network elements, and is useful to 2
recognise, as it forms the basis for all other classifications. I.e. that without further modification, 3
traffic arriving at a node will be treated as best effort (without any specific explicit treatment).  4
 5
 6
Figure 22-2: Context based classification  7
 8
In a second example, all traffic arriving on a specific interface might be classified (categorised) as 9
“real time” or “high priority” (see Figure 22-2).  10
 11
Whilst use of context-based classification is important, it is a somewhat blunt tool, and assumes that 12
all traffic arriving on an interface is of a single type and requires common treatment. 13
Packet based classification 14
Packet based classification gives us a more selective tool with which to categorize traffic arriving at 15
a node. In this mode the node examines one or more fields in the arriving packets (or frames) to 16
make a categorisation decision.  17
 18
One approach is to classify traffic into categories based on some flow or application information. 19
E.g. looking at source or destination, ether type, IP protocol, UDP or TCP port number etc. In this 20
way the node can infer the required QoS capabilities (e.g. low latency, minimum bandwidth, best 21
effort etc) from the type of traffic embedded in the packet.  22
 23
However, often the most pragmatic approach is to require that packets are pre-marked with an 24
explicit class of service marking. The specific marking model varies dependent on the transport 25
being used, but the primary marking schemes to support are Ethernet Priority Code Point (PCP) and 26
Drop Eligible Indicator (DEI) fields in Ethernet IEEE 802.1p (part of the IEEE 802.1q VLAN 27
header specification), IP TOS, PREC or DSCP ECN fields (defined in IETF RFC 2474 [42] and 28
RFC 3168) and MPLS TC (EXP) field (defined in IEFT RFC 3032 [50] and RFC 5462). 29
 30
The use of a class of service (COS) marking allows a packet source or network element to encode 31
the required QOS capabilities for a packet directly into the packet header. By predetermining the 32
marking scheme to be used for the COS field, classification using the COS field alone is often 33
sufficient to determine the PHB needed in the network node. 34
  35
Traffic marking 36
As described above, a key attribute of a QoS architecture is the ability to pass information about the 37
QoS status of a packet between nodes, and indeed within a node (e.g. from ingress to egress). This 38

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        153
is achieved through the use of QoS markings, setting bit fields within a packet header that convey 1
some information about the QoS related context in which this packet should be considered.  2
 3
Again, as described above there are numerous traffic marking schemes that are in use, each 4
associated with a particular header type in the packet (or frame) world.  There are also bit fields 5
defined in meta-data associated with internal forwarding in many router and switch silicon 6
architectures. Two examples of this are “Discard Class or Loss Priority” often associated with the 7
output of a policing function, but also explicitly settable, and “qos-group” or “forwarding-class”, an 8
opaque field provided in many router implementations  9
 10
The use and meaning associated with the values represented by these bits is entirely arbitrary, as 11
defined and used by users and operators. However, the value in their use lies in having agreed 12
meaning that can be passed between network elements. Some fields, for example IP DSCP, have 13
recommend meanings that can be used between providers. Others, such as MPLS TC (EXP) are not 14
formally defined so as to remain flexible.  15
 16
 17
 18
 19
 20
Figure 22-3: QoS marking domains 21
 22
A useful concept is to consider the use of a given marking scheme within a QoS marking domain. 23
Nodes within a domain use a common scheme, allowing identification of traffic with common 24
properties (a traffic class). Policies are then implemented in each node to maintain the appropriate 25
network behaviour for each identified class, when under congested conditions. 26
 27
At the border between domains, the border element should be capable of classifying traffic in the 28
first domain and remarking it for transport in the second. Usually the policy for forwarding will be 29
implemented to honour the class structure for the domain it is in. In Figure 22-3, three domains are 30
identified, 1) the customer domain, 2) the provider domain 3) an intra-node domain. The PE 31
elements on the left and right of the provider domain border the provider and customer domains and 32
will be responsible for remarking traffic as appropriate. 33
 34

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        154
 Congestion management 1
Thus far we have discussed the options available for classifying traffic. But once classified, we 2
must consider what behaviour to apply. As mentioned in the introduction to this section, a QoS 3
node fundamentally only has three options available for managing traffic; forward, store for 4
forwarding later or drop, but these options need only be applied when a bandwidth resource is 5
congested.  6
 7
A key function of QoS is that of managing traffic during congestion of an interface. Congestion 8
occurs when the quantity of traffic to be forwarded on an interface exceeds its capacity. In its 9
simplest form, QoS alleviates this issue by buffering the excess traffic, i.e. storing it in a local 10
memory until such time that the interface is available to send the stored traffic.  11
 12
FIFO Queueing 13
By storing packets that need to be sent on to this interface in the order they arrive, and then sending 14
them in that order when the interface is free, we form a “first in - first out” (FIFO) queue of packets. 15
 16
This mechanism, queueing, is a useful tool for managing temporary congestion, smoothing out the 17
bursts of traffic flowing into an interface, and forms the basis for more complex congestion 18
management tools.  19
There are four aspects of this capability that cause challenges; 20
1. By storing traffic for later sending, we intrinsically delay that traffic that is being stored, 21
adding latency to the transmission path for that traffic. Further, because the delay may be 22
variable, (different amounts of traffic may be enqueued prior to this packet arriving) we 23
introduce jitter. 24
2. As the speeds of the interfaces we are managing increases, we need more memory capacity to 25
store enough packets to substantively impact the traffic we are buffering.   26
3. The process is of queuing traffic is indiscriminate, in that it impacts all traffic flowing 27
through an interface. This presents the same latency implication to all applications using this 28
interface, some of which will be impacted by this delay, and some will not. 29
4. As the amount of memory available to for the queue is finite, so the queue is finite. As a 30
result, if the rate of arrival of traffic to be forwarded is significantly higher than the interface 31
rate or the burst is long enough, the available buffer will be filled, and no further traffic can 32
be enqueued. At this point, the node has no option but to drop traffic. When a queue is full, 33
and a node can no longer add packets to the end (tail) of the queue, traffic otherwise destined 34
for that queue is indiscriminately dropped from the tail (tail drop), until such time that the 35
queue depth reduces and space is made available again for new traffic.   36
It is worth remembering at this point that the application of QoS to an infrastructure is only a 37
solution for the management of how traffic is dropped under congestion conditions. It is not a 38
replacement for capacity planning and management for traffic that MUST be delivered. I.e. if a 39
network has less capacity available than the amount of traffic that MUST be delivered, applying 40
QoS is not an alternative to augmenting the capacity.  41
However, it is possible to significantly improve on this single FIFO queue model by combining the 42
use of classification with a programable scheduler to determine how we enqueue traffic, how much 43
we allow to pass, and what to do with the excess.  44
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        155
Class based queueing 1
By combining the concept of classifying traffic with the concept of a queue it is possible to allow a 2
node to manage the traffic flowing on an interface with a behaviour that is appropriate to the 3
applications flowing in each class. 4
 5
 6
 7
Figure 22-4: Class based queue 8
 9
In this model, shown in Figure 22-4, the system identifies separate logical queues for traffic 10
destined to flow out of a given interface. Traffic can be placed into a queue based on the 11
classification information gained on arrival (or if the system is capable, after the packet has been 12
forwarded – determined that it should flow out of this specific interface).  13
 14
For example, traffic identified as needing low latency, and low jitter (real time – our voice calls 15
from earlier) may be placed into queue 1. Traffic requiring some minimum bandwidth (business 16
critical, our EPOS traffic) may be placed into queue 3. Traffic that has no specific requirements 17
(best effort) may be placed into queue 5.  18
 19
The scheduler can now be programmed to dequeue traffic and place it onto the interface following a 20
pre-determined policy – for example;  21
1. Serve all high-priority traffic first (E.g. Queue 1 and 2), up to a given capacity of the 22
interface for each queue (for example 50% and 5%). Any traffic arriving at this interface for 23
those queues that cannot be served at that rate should be placed into the appropriate queue. 24
2. Serve traffic from low priority queues (queue 3, 4 and 5) with equal priority (a packet from 25
each) until the minimum guaranteed amount of bandwidth allocated to that queue is met. 26
3. Continue to serve traffic from low priority queues until either the queue is empty, or the 27
interface bandwidth is consumed. 28
 29
This type of policy allows maximum use of the available bandwidth whilst guaranteeing each 30
application class will have access to at least the capacity it needs to support the user base.  31
 32
Hierarchical class-based queueing  33
A further extension of this model uses a tiered set of schedulers to manage sets of queues that are 34
“grouped” to manage traffic associated with some logical entity on the physical interface.   35
 36

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        156
 1
Figure 22-5: Hierarchical class-based queue model 2
 3
In Figure 22-5, we have a physical interface supporting two VLANs (orange and green). Each 4
VLAN has its own set of queues and its own scheduling policy for those queues. In this example the 5
orange VLAN has two priority queues and three guaranteed bandwidth queues. The top scheduler 6
(2) is responsible for managing the flow of traffic from the queues associated with that VLAN.   7
The bottom scheduler (3) manages this flow of traffic from the queue associated with the green 8
VLAN.   9
Schedulers (2) and (3) are regarded as child schedulers, and their packet flow is in turn managed by 10
scheduler (1), the parent scheduler for the interface. The parent scheduler will be programmed with 11
a policy to manage the traffic flowing from the child schedulers toward the interface.   12
  13
This model can allow for a “hard” distribution of the bandwidth on an interface amongst the logic 14
entities sharing that capacity, protecting the use of the bandwidth for a given VLAN (orange) from 15
the others on the same interface (green).    16
  17
 Congestion avoidance 18
Thus far we have discussed the options available for classifying traffic, and managing traffic when 19
interfaces are under congestion. We have examined how, when an interface is congested, we can 20
consider which packets to forward and when. However, one important aspect of QoS is the 21
prevention of congestion in the first place.  22
 23
Congestion avoidance predominantly takes two forms;  24
1. Admission control 25
2. Selective queue management through random early discard (RED) or weighted random early 26
discard (WRED) 27
 28

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        157
Admission control 1
This is the process of managing the amount of traffic that we accept into the network. This can be 2
thought of as the enforcement of the contract between the applications requirements’, and the 3
networks transport capability. Returning to the example of the voice call, let us imagine that the 4
transport network has enough capacity to sustain 100 concurrent voice calls (Figure 22-6). If more 5
than 100 calls are introduced there is insufficient bandwidth to sustain all the calls and some traffic 6
will inevitably be dropped. Because the dropping of traffic will be random, all calls will be 7
impacted by the packet loss equally, resulting in reduced voice quality or even call failure.  8
 9
 10
 11
Figure 22-6: Admission control policing 12
 13
By pre-agreeing that the network can only sustain a certain number of calls, we can police the 14
traffic arriving from the application, to the agreed level. In the case of the packet network, this 15
means agreeing a sustainable bandwidth level and honouring that level at the point that we accept 16
traffic from the application. 17
 18
Once we have classified traffic to determine that it is indeed voice traffic, we can apply a policer to 19
that traffic class to limit the amount of traffic that is accepted. If the application violates the agreed 20
rate, the policer will drop some packets, forwarding only the rate that was previously agreed. It 21
should be noted that for this particular application, again voice quality will suffer, however, the 22
onus is now on the application to stay within the terms of the agreed contract for the benefit of its 23
own service.  24
 25
So, if by dropping traffic at admission to the network we will see the application service degrade, 26
just as if we drop traffic in the middle of the network, why bother? Should we not simply let the 27
traffic have access and hope there is sufficient capacity to support the application demand? 28
 29

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        158
 1
 2
Figure 22-7: Service protection through admission control 3
 4
This is where the network level administration of QoS becomes relevant. Consider the same core 5
network resource now being shared by two customers (Figure 22-7). If we recognise that the 6
network can only sustain 100 calls, we can ask each customer to restrict their usage, customer 1 to 7
60 calls and customer 2 to 40 calls. By applying a policy that is specific to each customer context 8
we can ensure that the available network resources are not exhausted, resulting in poor performance 9
for all.  10
If customer 1 exceeds their allocated capacity, some traffic will be dropped at point 1, resulting in 11
poor quality for customer 1. However, if customer 2 continues to maintain their agreement not to 12
exceed 40 calls, there will always be sufficient capacity to support their service without quality 13
impairment.  14
 15
Weighted Random Early Discard (WRED) 16
When looking at congestion management, the idea of a queue was introduced in order to store 17
traffic that is awaiting transmission on an outgoing interface. In this previous discussion we saw 18
that in a simple queue, when the queue is full, any additional traffic is “tail dropped” 19
indiscriminately.  20
 21
For applications that are using a TCP connection over IP, the loss of one (or more) packets from the 22
flow will cause the TCP algorithm to time out and request re-delivery of these missing packets. This 23
also triggers the shortening of the TCP “ack window” resulting in lower throughput for that TCP 24
session. As many TCP session may be impacted at once in a tail drop scenario, this can cause a 25
number of sessions to “back off” reducing the total load on the interface, and the queue to be 26
drained.  27
 28
Over time, the TCP sessions will re-open their ACK window, increasing bandwidth until the 29
interface is once again full and the queue overwhelmed. The result is a sawtooth type bandwidth 30
profile that oscillates around the maximum interface speed. This oscillation reduces the efficiency 31
of the interface usage as there are periods where the interface is not full.  32
 33
Weighted Random early discard (WRED) is a modification to the queue drop behaviour that can 34
help to eliminate this effect. The premise is to manage the acceptance of traffic into the queue in a 35
more intelligent manner.  36
 37

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        159
 1
Figure 22-8: RED queue threshold 2
 3
Consider the queue in Figure 22-8, with three depths at 0% (1), 50% (2) and 100 % (3).  4
As traffic arrives for the interface and the scheduler has no capacity available to forward them, the 5
waiting packets are placed into the queue. Between queue depth 1 (0% full) and depth 2 (50% full), 6
packets are added as normal to the queue.  7
 8
When the queue depth reaches the lower RED threshold (2) (50% in this example), rather than 9
adding all new packets to the queue, some will be randomly dropped. The probability of dropping 10
any given packet changes dependent on the current (and possibly historical) queue depth. As the 11
depth of the queue increases, so does the probability of drop until at depth 3 (100% full) the 12
probability of drop is 1 (always drop).  13
 14
The impact of this weighted behaviour is to slowly encourage individual TCP sessions flowing in 15
the queue to find an appropriate TCP window (and hence bandwidth) such that the queue never 16
saturates, and the link can continue flowing at 100% capacity, eliminating the sawtooth behaviour.  17
 18
Class based Weighted Random Early Discard  19
We have seen it is possible to add traffic to a specific queue based on a classification, in order to 20
differentiate between traffic types flowing in an interface. This allows us the flexibility to manage 21
bandwidth to support different applications in an infrastructure. We have also seen that under some 22
circumstances it might be possible to “borrow” bandwidth from one class to use in another class, in 23
the event that the bandwidth in an interface is not fully used. 24
 25
It is possible to extend the idea differentiated behaviour for traffic from between queues, to within a 26
single queue using the Weighted RED scheme.  27
 28
Figure 22-9: Class based WRED function and behaviour 29
 30
Consider the PE node on the left of Figure 22-9, two CEs are sending EPOS traffic. The PE has 31
capacity at (3) for 100 concurrent sessions. The number of sessions sent by each CE varies. CE 1 32

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        160
has a minimum sustained rate of 60 sessions, and CE 2 has a minimum sustained rate of 40. Each 1
has a possible peak rate of 100.  2
 3
As traffic arrives from CE 1 at point (1) the PE node classifies traffic as part of the EPOS 4
application. It also measures the amount arriving and allows up to 100 sessions worth into the 5
network. For traffic that exceeds the rate of 60 sessions, the node marks this traffic as having 6
exceeded its contracted rate or “out of contract”. The same classification, policing and marking 7
takes place at point (2) for CE 2 with a rate of 40 sessions. This extra marking is termed the discard 8
class or DC.  9
 10
When the incoming packets arrive at the queue depicted on the right, again a decision is made prior 11
to enqueuing the traffic. This time the decision is based on two criteria, the current queue depth 12
AND the discard class of the packet as determined in the ingress policing and marking stages.  13
 14
A sample policy may behave as follows. For queue depth between 0% (4) and 33% (5) all traffic is 15
enqueued.  16
 17
For traffic that is marked with the out-of-contract discard class marking, between queue depth 33% 18
(5) and 66% (6) the drop probability varies from 0 to 1; that is at depth 66% (6) all traffic that is 19
marked out of contract will be dropped. Traffic that is marked “in contract” will continue to be 20
queued for transmission.  21
  22
For queue depths between 66% (6) and 100% (7) no out of contract traffic will be enqueued, and 23
the drop probability for in contract traffic will increase from 0 to 1.  24
 25
It should be noted that the PE is not policing “sessions”, but simply the traffic associated with those 26
sessions. The PE has no knowledge of the nature of the traffic sent by the CE (for example the 27
membership of a specific packet to a specific session), other than the markings being examined.  28
It should also be noted that traffic dropped at point (3) will contain packets associated with many or 29
all of the sessions from the CE whose traffic is being marked “out-of-contract”. The impact of this 30
is that all TCP sessions that lose traffic will again contract their TCP windows, with the effect of 31
lowering the BW used by each session and so reducing the load on the network. 32
 33
This behaviour has the following benefits;  34
1. both CE users 1 and 2 potentially have access to all the bandwidth at point 3 in the diagram 35
when the other user is not consuming it. The network resources at 3 can be optimally used 36
when demand from one user is high and the other is low. Compare this to the example in 37
Service protection through admission control where the network resource at point 3 may be 38
under used.  39
2. If a user constrains their traffic to within the contracted limit, they are guaranteed delivery, 40
even under congestion caused by out-of-contract traffic being generated by other users.  41
 42
23 Annex E: Multicast Technologies background 43
Overlay multicast 44
Overlay multicast refers to CE to CE multicast over the transport provider/underlay network. In the 45
following example, CE1 is a First Hop Router (FHR) connecting to the source and CE2/3 are Last 46
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        161
Hop Routers (LHRs) connecting to receivers. Receivers send IGMP/MLD joins towards the LHRs, 1
who then send PIM joins towards their upstream routers PE2/3. If the PEs were connected by a 2
LAN, PE2/3 would simply send PIM joins onto the LAN towards PE1. Since PEs are actually 3
Provider Edge routers that are not directly connected and typically serve many VPNs, Overlay 4
Multicast signalling is used to signal the overlay multicast state over, not through, the 5
provider/underlay network. 6
 7
 8
 9
 10
 11
 12
 13
Figure 23-1: Multicast procedures 14
 15
PIM-based overlay signalling for IPVPN 16
The most straightforward way is to emulate a per-VPN pseudo LAN over the underlay network. 17
The pseudo LAN is instantiated by a multicast GRE tunnel for the VPN. Every PE for the VPN 18
joins that tunnel and can send both control plane (PIM messages) and data plane traffic over the 19
tunnel. For example, multicast group 225.1.1.1 used for VPN1, 225.1.1.2 for VPN2, etc. 20
 21
This is referred to as PIM-MVPN and also commonly as Rosen-MVPN. It is documented in the 22
historic RFC6037 and further in RFC6513 [100] (which documents both PIM- and BGP-based 23
overlay signaling). 24
 25
Essentially, a PE runs PIM sessions over each pseudo LAN for each VPN. This is different from 26
unicast, with which no per-VPN session is used. 27
 28
When there are a large number of VPNs, many PIM sessions and signalling (including message 29
refreshes) are incurred so it does not scale well. Most implementations and deployments of 30
PIM/Rosen-MVPN only use IP multicast tunnel in the underlay to emulate the pseudo LANs. 31
BGP-based overlay signalling for IPVPN and EVPN 32
For BGP-based signalling, the overlay signalling is done by BGP just like unicast case. It is referred 33
to as C-multicast signalling (C for Customer) in RFC6513 [100]. BGP-based signaling use unified 34
methods for both unicast and multicast, and inherits all BGP scaling mechanisms and properties. 35
 36
Multicast (IP) VPN with BGP based overlay signalling is referred to BGP-MVPN. While RFC6513 37
covers both PIM-MVPN and BGP-MVPN concepts, BGP-MVPN specific encoding and procedures 38
are specified in RFC6514 [101] . 39
 40
BGP-MVPN also defines mechanisms to signal what type and instance of provider/underlay tunnels 41
are used to transport overlay multicast traffic over the underlay, via Inclusive or Selective PMSI 42
Auto-Discovery routes (often referred to as I/S-PMSI or x-PMSI routes). IP multicast, RSVP-TE 43
P2MP, mLDP P2MP/MP2MP, Ingress Replication, BIER and future additions could all be used as 44
underlay tunnels. 45
 46
src CE1
CE2
CE3
PE1
PE2
PE3
rcvr
rcvr
IGMP PIM
PIM
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        162
EVPN BUM (Broadcast, Unknown unicast and Multicast) only uses BGP overlay signalling and is 1
modelled after BGP-MVPN. It is specified in RFC7432 [102] and further in draft-ietf-bess-evpn-2
bum-procedures-update [131]. 3
 4
BGP-MVPN and EVPN BUM have been widely deployed and are often referred to as Next 5
Generation MVPN (NG-MVPN). 6
Underlay multicast 7
As mentioned above, overlay multicast traffic is transported via underlay multicast tunnels. Many 8
tunnel types have been mature and widely deployed, but with Segment Routing and central control 9
being widely adopted, it is necessary to review the pros and cons of various multicast technologies 10
and see what is the best solution in the SR era. 11
 12
Traditionally, multicast requires per-tree state on all routers of a tree for efficient replication. PIM, 13
RSVP-TE P2MP, mLDP P2MP/MP2MP are all signaling protocols to instantiate tree state on those 14
routers, either from the tree leaves towards the tree root or vice versa. The new SR-P2MP (aka tree-15
sid) tunnels also requires per-tree state on all root/leaf and replication nodes, except that the 16
signalling is directly from a central controller who also calculates the tree. 17
 18
Obviously, this does not go well with one principal of Segment Routing – no per-flow state inside 19
the network. An alternative is Ingress Replication – the root tunnels individual copies directly to 20
leaves so no per-tree state is needed inside the network. Since replication is done at the root, it is not 21
efficient and does not work well for high fan-out hight data rate cases. 22
  23
BIER is a new technology that achieves efficient replication w/o incurring per-tree state, so it is the 24
ideal multicast solution for an SR network. However, it uses a new encapsulation and forwarding 25
algorithm, so it requires new hardware. While this makes it difficult to deploy in non-greenfield 26
networks, there are very well-designed brownfield deployment methods, and all major vendors have 27
BIER implementation & hardware available/upcoming. 28
 29
For an SR network, an operator may prefer to remove all legacy multicast signaling – 30
PIM/RSVP/mLDP and switch to controller-based tree calculation and signaling via either PCEP or 31
BGP (i.e., SR-P2MP [draft-ietf-pim-sr-p2mp-policy] [draft-ietf-spring-sr-replication-segment] or 32
BGP-signaled IP/MPLS multicast [draft-ietf-bess-bgp-multicast-controller]). Notice that this still 33
incurs per-tree state inside the network and the only real benefit is the central calculation of the tree 34
based on many TE constraints/considerations. 35
 36
In summary, depending on many factors, multicast technologies can be considered in the following 37
preference order: 38
 39
1. PIM or mLDP/RSVP-TE P2MP that are already deployed in existing network 40
2. Controller calculated/signalled IP multicast or mLDP/RSVP/SR-P2MP tunnel 41
3. BIER if enough routers in the network support it 42
 43
MVPN/EVPN and Seamless MPLS/SR 44
Given the vast scale of the transport network, a seamless architecture is used as described 45
previously. In this architecture, two aspects have impacts to multicast: 46
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        163
 1
a) Only border routes have routes to edges nodes (that connect to 5G NFs like RAN/UPF), 2
while internal routers only have routes to the border routes.  3
b) Different ASes/areas may support/prefer different types of underlay tunnels 4
 5
For a), to establish end-to-end underlay tunnels across those different ASes/areas, PIM RPF vector 6
[RFC5496] or mLDP Recursive FEC [RFC6512] can be used – a leaf router looks up the route to 7
the tree root, with the BGP protocol nexthop being a border router in the local area/AS. It then 8
encodes that border router’s address in PIM/mLDP signaling so that internal routers will signal 9
towards that border router instead of the original root. When that border router gets the signaling, it 10
encodes the next border router’s address so that the internal routers in the next AS/area will signal 11
towards the next border router, and so on so forth. 12
 13
An alternative solution for a) is use controller signaled multicast, since the routers become dumb 14
forwarding devices programed with forwarding state for the tunnels by the controllers. 15
 16
For b), MVPN/EVPN-BUM tunnel segmentation [RFC7524] [draft-ietf-bess-evpn-bum-procedures-17
update] can be used – MVPN/EVPN-BUM procedures will run on the border routers and the 18
signaled tunnel type/identification are changed as MVPN/EVPN routes are re-advertised by the 19
border routers, so that different tunnel types/instances are used in different ASes/areas. 20
 21
 22
 23
Figure 23-2: Multicast diagram  24

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        164
24 Annex F: Transport network slicing solution for WG1 1
Slicing phase 1 (informational)  2
WG-1 is developing a slicing architecture contained in O-RAN.WG1.Slicing-Architecture-v05.00 3
[23]. It is a multi-phased effort based on the roadmaps of the different O-RAN working groups 4
combined with use cases specified by O-RAN operators.     5
 This annex addresses the transport network solution needed to support the use cases in phase 1 of 6
WG-1’s slicing architecture [23]. There are potentially many ways a packet switched network can 7
address the requirements outlined by WG-1, so this section has been presented as an example 8
solution that uses capabilities defined in the main body of this document.   9
WG-1 Phase 1 requirements and scope  10
The role of the transport network in a 5G slicing scenario is to provide the necessary connectivity, 11
isolation, and Quality of Service (QoS) so management, control and user plane traffic can flow 12
between the mobile components, making up a slice, in an appropriate fashion.  13
 14
Figure 24-1 shows the use cases for slicing phase 1 in O-RAN.WG1.Slicing-Architecture-v05.00 15
[23].  16
 17
 18
 19
Figure 24-1: WG-1 Phase 1 use cases  20
 21
In slicing phase 1 the scope is: 22
• Three public geographically dispersed slices covering mobile broadband, mMTC and NB-23
IoT. 24
• Each cell site supports two radio deployments: 1) Distributed: The O-RU is in the cell site 25
and the O-DU is in a data center, located across the transport access network. In this 26
instance, the O-RAN 7.2x C/U protocols and procedures run across the transport access 27
infrastructure. 2) Collocated: The O-RU and O-DU reside in the cell site. In this instance, O-28
RAN 7.2x protocols and procedures run locally in the cell site and Midhaul interfaces run 29
across the transport access infrastructure.      30

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        165
• All three slices are present in all cell sites and all slices use both radio deployment options. 1
 2
Not illustrated in Figure 24-1, the following has been assumed or are limitations in slicing phase 1. 3
• The transport network is multi-service and in addition to 5G needs to support: 4
o 3G and 4G mobile services 5
o Wireline services (consumer, enterprise, and wholesale)  6
• Mobile components connect directly to the transport network or reside in a data centre that 7
connects to the transport network.  8
• O-RAN and 3GPP components present slices to the transport network or Data Center 9
infrastructure via a discrete physical or logical Ethernet interface (VLANs). 10
• Data Centers, connected to the transport network, present slices to the transport network via 11
logical Ethernet interfaces (VLANs). 12
• There is no slicing in phase 1 in Fronthaul and Midhaul due to lack of slicing support in 13
these O-RAN mobile components.  14
• For the transport network to support QoS appropriate to the QoS requirements of the mobile 15
infrastructure, it is necessary for the O-RAN and 3GPP mobile components, in the Midhaul 16
and Backhaul, to set the DSCP bits in the IP header of the GTP-U packets based on the 17
importance of the user’s traffic in the radio domain. In slicing phase 1 this is possible for 18
Uplink and Downlink traffic in Backhaul but only for Downlink traffic in Midhaul.  19
• No multicast capability in required in slicing phase 1. 20
 21
NOTE: This annex only covers 5G slice requirements, however transport network architects also 22
need to consider the other network services outlined above when designing the transport network.  23
 24
 25
 26
 27
Figure 24-2: 3GPP and O-RAN connectivity requirements for phase 1 slicing architecture  28
 29
Figure 24-2 developed jointly between WG-1 and WG-9 shows the capabilities and transport 30
connectivity requirements in slicing phase 1. Components and interfaces in the grey shaded box at 31
the top of the figure are shared entities and interfaces, while entities and interfaces shaded green, 32

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        166
orange and pink at the bottom of the figure are dedicated to a slice. At the transport level the design 1
in slicing phase 1 provides: 2
1. A single O-RAN / 3GPP control plane for all slices. 3
2. Fronthaul service overlay is common for all slices.  4
3. Midhaul service overlay is common for all slices. 5
4. Sliced Backhaul user planes are presented to the transport network by mobile 6
components via a discrete VLAN per slice.  User plane traffic from each slice is carried 7
in a discrete L3 VPN in the transport network.    8
Overall Packet Switched Transport Architecture 9
In reading this annex the reader is expected to be fully familiar with the main body and other 10
annexes in this document.  The solution is based on a packet switched transport architecture 11
illustrated in Figure 24-3.  12
  13
  14
Figure 24-3 Packet switched transport for mobile Xhaul 15
 16
It is a converged end to end packet switched infrastructure, beginning at the cell site, located in the 17
edge of the access layer, and stretching to the core of the transport layer. The packet switching 18
TNEs are IP/MPLS or SRv6, QoS enabled, high capacity, low latency devices interconnected by 19
point-to-point Ethernet interfaces running at the full capacity of the Ethernet interface, typically 20
using either point to point fibres or via a WDM infrastructure.  It incorporates data centers suitably 21
placed across the transport network infrastructure to support virtual and physical NFs associated 22
with mobile and fixed services but also potentially the placement of “Application Functions” 23
associated with value-add services and customer specific application. 24
 25
The logical architecture is based on a common underlay packet switching infrastructure based on 26
either MPLS or SRv6 overlaid with an L2 / L3 service infrastructure (VPNs) that uses the 27
capabilities of the underlay packet switched network to support the mobiles interfaces.  28
The underlay packet switching infrastructure provides basic network services such as any-to-any 29
connectivity between TNEs, scaling, fast convergence, shortest path and traffic engineered 30
forwarding, packet-based Quality of Service (QoS) and timing.  31
 32
The service layer supports native Ethernet services, using EVPN technology and IP VPN services, 33
using MP-BGP based L3VPNs. These services can utilise the facilities offered by the underlay 34
packet switched infrastructure to support the different mobile interfaces in an appropriate fashion. 35

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        167
Where possible transport services are built on an end-to-end basis without intermediate stitching / 1
switching points within the transport infrastructure. This approach has been taken to minimise the 2
transport service orchestration overhead. 3
NOTE: The term MP-BGP L3VPNs is used throughout this annex. It refers to L3VPNs built using 4
MP-BGP based on either RFC4364 [72] or SRv6 BGP based Overlay Services [136] depending on 5
whether the underlay technology is MPLS or SRv6. More details on the technology can be found in 6
section 13 and Annex C: MP-BGP based L3VPNs of this document.   7
Underlay network for WG-1 slicing phase 1 8
A packet switched network has extensive capabilities to support slicing. These are outlined in 9
section 17. Slicing phase 1 is basic in nature with limited slicing capabilities in the O-RAN mobile 10
components and the actual slices (mobile BB, MMTc, NB-IOT) are low priority and delay tolerant 11
slices. With this in mind, and more onerous slicing use cases in later phases, it is proposed to 12
support slicing phase 1 in the transport network using: 13
• Default IGP instances running in each underlay routing domain calculating shortest path 14
routing based on IGP metrics. 15
• IGP supporting fast and loop free convergence. 16
• Inter-domain connectivity performed using either BGP, IGP redistributing or a PCE 17
infrastructure.    18
• Management, control and user planes in phase 1 use shortest path routing between TNEs. 19
I.E the Service VPNs associated with each slice are not mapped to TE tunnels or Flex-algos 20
in phase 1.  21
• QoS based on Diffserv model defined in RFC2475 [41] and outlined in section 14 and 22
Annex D: Quality of Service. 23
 24
As the WG-1 slicing use cases and slicing capabilities within O-RAN equipment matures so more 25
sophisticated forwarding techniques, such as SR-TE and Flex-algo may need to be utilised. Please 26
see section “17 5G Slicing in a packet switched Xhaul network” for discussion on how they can be 27
applied and used to support slicing. 28
 29
NOTE: When designing and implementing the transport network Service Providers may want to 30
anticipate more onerous slicing use cases and implement facilities such as Traffic Engineering, 31
Flex-Algo from the outset. 32
Service Models for WG-1 slicing phase 1 33
Management and control plane networks are needed to support the management, orchestration and 34
monitoring of the transport network and the data center infrastructure. In addition to these 35
“infrastructure components”, various management and control plane networks are needed for the 36
management and control of the 5G 3GPP and O-RAN mobile environments. In the transport slicing 37
architecture outlined, some management networks are discrete VPN entities, while others combine 38
multiple management functions together in a single VPN entity. The reasoning behind these choices 39
is outlined, but individual Service Provider’s need to assess their organizational needs and adjust 40
accordingly.   41
Transport and DC management networks    42
Both the transport network and the data center infrastructure are common capabilities used by a 43
range of different services and in many organizations managed by different organizational groups. 44
In this example the management networks for these two entities are separate, as shown in Figure 45
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        168
24-4. Both these management networks need to be set-up in advance on the transport network and 1
are pre-requisites to building a slice across the transport network or instantiating VMs or containers 2
within data centers. 3
  4
 5
Figure 24-4: Transport network and Data centre management networks  6
   7
Transport network management network  8
A network wide in-band L3VPN is used for day-to-day management of the TNEs. Every TNE has a 9
local network management interface with an associated network management IP address configured 10
at installation. This is typically a loopback interface. Management protocols associated with TNE’s 11
configuration, monitoring and service orchestration are configured to use this interface as their 12
source interface. The TNE’s management interface is placed into a transport management network 13
built using a MP-BGP L3VPN mapped to the underlay’s default routing algorithm. The transport 14
network management network can be designed as an any-to-any network, meaning that every entity 15
within the L3VPN can connect to every other entity in the transport network management network 16
but are more commonly built so that the central network management functions can communicate 17
with the TNEs and vice versa but the TNEs cannot communicate with each other.  18
In addition to the in-band management network Service Providers may also choose to have an out-19
of-band network for scenarios when in-band network connectivity has been lost. Various solutions 20
exist, often based on access to the console port, through mobile, xDSL or PON technology.     21
 22
Data Centre (DC) management network  23
In the packet switched transport architecture data centers are distributed around the transport 24
network and although distributed they need to be managed centrally. The DC infrastructure supports 25
“Network Functions” and “Applications Functions”. Some functions are associated with the 26
provision of 5G and O-RAN capabilities and services, but DCs should be considered common 27
infrastructure facilities that support services beyond just 5G. The DC management network needs to 28
support the O2 interface defined by O-RAN but also other data center management, monitoring and 29
orchestration facilities. In the slicing phase 1 the handoff between the DC and the transport network 30
is via a logical Ethernet attachment circuit. The design of the management infrastructure within the 31
DC is handled by WG-6, with connectivity between DCs and the central DC management 32

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        169
infrastructure provided by the transport network. Each DC needs to hand-off the DC management 1
network to a TNE via a logical Ethernet interface. On the TNE the logical Ethernet attachment 2
circuit is placed into a MP-BGP L3VPN mapped to the underlay’s default routing algorithm. This 3
VPN connects all the DCs to the central DC management, monitoring and orchestration functions. 4
The DC management network can be designed as an any-to-any network, meaning that every entity 5
within the L3VPN can connect to any other entity in the DC management network but are more 6
commonly built so that the central network management functions can communicate with the DC 7
components (servers and DC switches) and vice versa, but the DC components cannot communicate 8
with each other.  9
O-RAN control and management networks 10
The O-RAN control and management networks are illustrated in Figure 24-5.  11
 12
 13
 14
Figure 24-5: Fronthaul Management and O-RAN management and control networks   15
 16
O-RAN Fronthaul Management network (M-Plane) 17
Based on guidance from WG-1, the O-RAN Fronthaul M-Plane interface is supported across the 18
transport network as a dedicated VPN. This is discrete from the O-RAN Management and Control 19
VPN that supports the O-RAN A1, E2 and O1 interfaces. The reasoning is Fronthaul management 20
and O-RAN control and management may fall under different organizations within the Service 21
Provider. 22
Full details of Fronthaul M-plane can be found in section 7.1. The design outlined assumes the 23
Fronthaul M-plane is operating in hybrid mode, where a central Fronthaul NMS needs IP 24
connectivity to the O-DUs and the O-RUs. The O-RAN Fronthaul Management network in the 25
transport network uses a MP-BGP L3VPN mapped to the underlay’s default routing algorithm. The 26
VPN needs to be present on all TNEs, through VLAN attachment circuits, that support O-RAN 27
mobile components managed using the Fronthaul M-Plane. O-RAN mobile components that use the 28
M-Plane need to connect to the O-RAN Fronthaul Management VPN, via a logical Ethernet 29
interface, when they are installed or instantiated.   30
The O-RAN Fronthaul Management VPN can be designed as an any-to-any network, meaning that 31
every O-RAN component in the L3VPN can connect to any other entity in the O-RAN Fronthaul 32

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        170
Management VPN, or more likely built so the M-Plane management components can communicate 1
with the O-RUs and O-DUs but the O-RUs and O-DUs are restricted in their connectivity.  2
 3
O-RAN Control and Management network (A1, E2, O1 interfaces) 4
Based on guidance from WG-1, the O-RAN A1, E2, O1 interfaces are supported across the 5
transport network as single VPN, discrete from the Fronthaul management VPN. The A1 and E2 6
interfaces are associated with the RAN Intelligent Controllers (RICs). The O-RAN architecture has 7
two RIC entities: 8
• near Real-Time RIC (nRT-RIC). nRT-RICs use the E2 interface to communicate with the O-9
CUs, and O-DUs they control. The interface controls the O-CUs and O-DUs with execution 10
actions coming from the nRT-RIC and feedback in the opposite direction.  11
 12
• Non-Real-Time RIC (NRT-RIC). NRT-RICs use the A1 interface to communicate with the 13
nRT-RICs they control. The interface provides the nRT RIC with policies, enrichment info, 14
and ML model updates. In the other direction the nRT RIC provides policy feedback to the 15
NRT-RIC. 16
 17
The O1 interface is associated with “Service and Management Orchestration”. This interface is an 18
FCAPs interface with configuration, registration, security, performance, and monitoring 19
responsibilities.   20
 21
The O-RAN Control and Management network in the transport network uses MP-BGP L3VPN 22
technology, mapped to the underlay’s default routing algorithm. It needs to be present on all TNEs 23
through VLAN attachment circuits, that support O-RAN components that use either the A1, E2 or 24
O1 interfaces. All O-RAN components using these interfaces need to connect, via the TNEs, to the 25
O-RAN Management and Control VPN when they are installed or instantiated.   26
 The O-RAN control and management VPN can be designed as an any-to-any network, meaning 27
that every O-RAN component in the L3VPN can connect to any other entity in the O-RAN control 28
and management VPN, or more likely built as a tiered VPN using a combination of Virtual Routing 29
and Forwarding (VRFs) instances and Route Targets (RTs) so that: 30
• SMO can communicate nRT-RICs, O-CU-CPs, O-CU-UPs and O-DUs. 31
• NRT-RIC can communicate with the nRT-RICs.  32
• nRT-RICs can communicate with the O-CUs, O-DUs and O-RUs.  33
3GPP Control Plane network 34
In slicing phase 1 there is a single 3GPP core infrastructure running in a 3GPP control plane VPN 35
across the transport network. See Figure 24-6 for details. It carries all the 3GPP control plane traffic 36
associated with Midhaul and Backhaul including F1-c, E1, Xn-c, N1, N2, N4 for all slices. In the 37
longer term it is expected that some 5G slices may have a dedicated 3GPP control plane, in these 38
instances they could share the same transport VPN or utilise separate transport VPNs for each 39
slice’s mobile core.  40
 41
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        171
 1
Figure 24-6: Midhaul and Backhaul control plane network   2
 3
In slicing phase 1 connectivity for the 3GPP control plane network across the transport network 4
uses a MP-BGP L3VPN, mapped to the underlay’s default routing algorithms. The 3GPP control 5
plane VPN needs to be present on all TNEs via VLAN attachment circuits, where mobile 6
components using the 3GPP control plane reside. All 3GPP mobile components using these control 7
plane interfaces need to connect, via VLANs to the TNEs, to the 3GPP control plane VPN when 8
they are installed or instantiated.    9
 The 3GPP control and management VPN can be designed as an any-to-any network, meaning that 10
every 3GPP component in the L3VPN can connect to any other entity in the 3GPP control plane 11
VPN, or more likely built with limited connectivity based on the mobile core connectivity 12
requirements.  13
O-RAN and 3GPP user planes networks 14
Slicing phase 1 connectivity architecture for the O-RAN and 3GPP user planes is shown in Figure 15
24-7. 16
 17
 18
Figure 24-7: O-RAN and 3GPP user plane networks  19
 20

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        172
Fronthaul C/U plane network 1
In slicing phase 1, slicing is not supported on O-RAN fronthaul components. In practical terms this 2
means O-RUs and O-DUs are common across all slices and there is a single 7.2X stream between 3
the O-RU and O-DU which carries all the control and user plane traffic associated with all slices. 4
The 7.2X C/U interface must support Ethernet encapsulation or optionally support Ethernet / IP 5
encapsulation.  Slicing phase 1 use cases assume the Ethernet variant of 7.2X protocol, meaning 6
Ethernet connectivity needs to exist between O-RUs and the parent O-DU. Slicing phase 1 use cases 7
also assumes two radio deployments options:  8
Co-located O-RU and O-DU at cell site.   9
Ethernet connectivity needs to exist between the O-DU and the O-RUs and they both reside in the 10
cell site. There are two options: 11
• Direct point to point connectivity between the O-RU and the O-DU. In this case the 12
transport network plays no part in the connectivity or the provision of an Ethernet service 13
between the O-RU and the O-DU.  14
• Connectivity between the O-DU and O-RUs is via the “Cell Site Router” (CSR). In this case 15
the Cell Site Router provides layer 2 bridging, or an L2 Ethernet cross connect function 16
between the O-DUs port and the O-RUs port. See section 18.6 for more details. 17
O-RU and O-DU separated by access network 18
For distributed solutions, the O-RUs and the O-DUs are separated across the access portion of the 19
transport network. In this instance, an Ethernet service needs to be created across the MPLS or 20
SRv6 transport underlay. This Ethernet service can use either an E-Line or E-Tree service 21
configured using EVPN over MPLS or SRv6. See sections 13.2 for more details. 22
 23
Midhaul user plane network (F1-u and Xn-u) 24
In slicing phase 1 slicing is not supported on Midhaul O-RAN mobile components. O-DUs and O-25
CU-UPs currently have no concept of slicing with the F1-u traffic from all slices originating and 26
terminating on the same physical/logical interface and IP address on the O-DU.  In scenarios, where 27
the O-CU-UP is shared between two or more slices there is also no concept of slicing on the O-CU-28
UP. In situations, as shown in Figure 24-2, where the O-CU-UP is dedicated to a slice, then the O-29
CU-UP function will only receive and send traffic associated with that slice, however the O-CU-UP 30
slice assignment and selection process is completely transparent to the transport network and under 31
the control of the O-RAN / 3GPP control planes. For this reason, in slicing phase 1 a single MP-32
BGP L3VPN mapped to the underlay’s default routing algorithm is used across the transport 33
network to support all Midhaul user plane traffic regardless of slice it belongs too. The L3VPN 34
needs to be present on all TNEs and DC-TNEs that either connect directly or indirectly (through the 35
DC infrastructure) to mobile components that support Midhaul user plane interfaces. Access to the 36
Midhaul user plane VPN, on the TNEs, is via VLAN interfaces.  Longer term as slicing technology 37
matures in the Midhaul domain, it may make sense to combine Midhaul and Backhaul user plane 38
traffic associated with a slice, into a single L3VPN across the transport network.   39
 A further challenge with Midhaul user plane interfaces in slicing phase 1 is the lack of a QoS 40
mapping function on the O-DUs to map radio orientated QoS parameters, controlled by the 3GPP / 41
ORAN control planes, to transport orientated DSCP QoS markings. The means the transport 42
network QoS mechanisms must treat all uplink traffic the same, regardless of the importance of the 43
data contained within Midhaul GTP-U packets.   44
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        173
Backhaul user plane network (N3 and N9) 1
WG-1 slicing phase 1 includes user plane slicing in the backhaul network. In phase 1 it is assumed 2
traffic associated with different slices originates from individual logical Ethernet interfaces on the 3
O-CU-UPs backhaul interfaces and UPFs. These same slice mappings can be maintained across the 4
transport network by mapping VLAN attachment circuits to different L3VPNs on the TNEs. Within 5
the transport network MP-BGP L3VPNs can be built per slice or shared between slices depending 6
how the mobile component present the slice traffic to the transport network. For slicing phase 1 7
these L3VPNs are mapped to the underlay’s default routing algorithm but as the slice use cases 8
becomes more sophisticated, then flex-algo or Traffic Engineering can be used to map L3VPN to 9
different underlay forwarding planes.     10
 Backhaul L3VPNs associated with a slice needs to be present on all TNEs and DC-TNEs that 11
either connect directly or indirectly (through the DC infrastructure) to mobile components 12
supporting that backhaul slice. Access to the backhaul slice L3VPN, on the TNEs, is via an VLAN 13
attachment circuit.   14
Backhaul Transport Slicing 15
A full description of MP-BGP based L3VPNs is contained in sections 13.3 and Annex C: MP-BGP 16
based L3VPNs. MP-BGP L3VPN’s have two basic mechanisms for controlling connectivity within 17
the L3 VPN. 18
• Virtual Routing and Forwarding (VRF) instances: These are local routing table associated 19
with L3VPNs on Provider Edge (PE) TNEs. The VRF holds local and remote VPN routing 20
information. Local routes are those learnt by the PE and normally consist of attachment 21
circuits, routes learnt via routing protocols running over attachment circuits or via local 22
static routing. Remote VPN routes are those learnt from remote PEs. They are conveyed 23
between PEs using MP-BGP and held in the PE’s BGP table. Installation of remote routes 24
into the VRF routing table is determined by local policies. One of the key policy attributes 25
used to determine what routes are installed into a VRF are Route Targets (RTs). 26
• Route Targets (RTs): RTs are a BGP attribute attached to VPN routes when they are 27
distributed by MP-BGP from the PE where they were learnt. At the destination PE, filtering 28
of routes based on RTs is one of the key mechanisms for determining what remote VPN 29
routes are imported into a VRF.  30
 31
Slicing phase 1 supports three general purpose 5G service slices which are network wide. For these 32
simple slice use cases the design shown in Figure 24-8 could be used.  33
 34
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        174
 1
 2
Figure 24-8: Backhaul phase 1 slice design   3
 4
• Three discrete VRFs associated with the phase 1 slices are created on PEs supporting slices. 5
Alphabetic VRF names have been used in the illustration. In reality, VRF names are 6
numeric values. 7
o VRF1: MBB 8
o VRF2: MMTC 9
o VRF3: NB-IOT 10
• Mobile components associated with each slice connects to the PE via a VLAN attachment 11
circuit which is placed into the appropriate slice VRF. 12
• Each VRF on the PE learns local IP subnets containing the mobile slice components, either 13
as connected routes, via a routing protocol or via static routes.   14
• A single RT is associated with each slice type. Alphabetic RT names have been used in the 15
illustration. In reality, RT values are numeric values.  16
o VRF1: RT = MBB 17
o VRF2: RT = MMTC 18
o VRF3: RT = NB-IOT 19
• Routes from each slice’s VRF are exported where they are learnt with their associated RT.  20
• On remote PEs each VRF imports the RT associated with their slice.    21
• This creates an any-to-any transport network allowing connectivity between all mobile 22
backhaul components in the slice.  23
 24
NOTE: In many cases, Service Providers already use L3VPNs based on RFC4364 [72] and have 25
their own methodologies for creating L3VPNs. In these instances, Service Providers could use their 26
own VPN design methodology to create an any-to-any VPN for Backhaul slices.   27
NOTE: This is a very simple transport slice architecture and suitable for general-purpose slices. 28
More complex slice use cases may need different L3VPN topologies, the combination of VRFs, 29
RTs and BGP communities and filtering provides a powerful toolset to build these connectivity 30
models.       31

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        175
Data Network (N6) 1
The data network or N6 is not part of the RAN and is not in scope of the WG-1 slicing work item. 2
However, slicing is likely to extend to the N6 data network and although not discussed in this 3
annex, the techniques outlined for the backhaul user plane slices are largely applicable when 4
building the data networks across a common transport infrastructure. 5
Transport Network Quality of Service architecture  6
Quality of Service (QoS) in the transport network in phase 1 is based on the Diffserv architecture 7
defined in RFC2475 [41] and further outlined in section 14 and Annex D: Quality of Service of this 8
document. The QoS architecture is widely implemented by Service Providers but there are 9
significant differences between Service Providers in their QoS implementations. These include 10
DSCP bit usage, edge policies, marking strategies, numbers of queues on the edge and within the 11
core and how different Per Hop Behaviours (PHBs) are implemented. As such, this section aims to 12
outline some of the key considerations associated with 5G traffic and provide example QoS 13
implementations. Service Providers can use this information to adjust their existing QoS 14
architectures to support 5G.    15
Transport QoS considerations in a 5G environment  16
Section 7 outlines the 5G interfaces the transport network needs to support. In many areas the 17
transport QoS requirements for 5G are very similar to existing 4G and general multi-service 18
environments. However, there are some notable considerations and exceptions.   19
• In the context of packet-based switching technologies for O-RAN and 3GPP, the interplay 20
of QoS and transport slicing has two dimensions: 1) The QoS requirements of O-RAN 21
fronthaul traffic 2) The QoS requirements of the mobile services, which depends on the kind 22
of application being delivered to the end users. 23
The transport slicing and QoS architectures needs to consider both dimensions to properly 24
allocate QoS resources capable of satisfying the expected requirements for each kind of 25
traffic in terms of bandwidth, latency, etc. 26
• O-RAN 7.2X Fronthaul Traffic: The user and control traffic stream is eCPRI based and runs 27
between the O-RU and the O-DU. It is a new traffic type associated with 5G. The transport 28
requirements of the Fronthaul user and control plane interfaces are contained in the O-29
RAN.WG9.CTRP-REQ-V01.00 [18] but can be summarised as high bandwidth and more 30
importantly from a QoS perspective, extremely low delay (micro-seconds) traffic. Due to the 31
low delay requirements this traffic type will only ever be seen in the access network and 32
requires packet switching equipment to treat this traffic with the highest priority through the 33
device and when scheduling onto attached interfaces. Today, many Service Providers use 34
the highest priority queue for voice traffic, consequently, changes to the existing QoS 35
schemes may be required. 36
• 5G introduces Midhaul: Midhaul interfaces run between the O-DUs and the O-CUs and 37
between O-CUs. The F1 interfaces are a new set of interfaces associated with 5G, but use 38
GTP-U and GTP-C which is the same packet types used in Backhaul. It is expected that 39
Midhaul and Backhaul interfaces will have similar QoS requirements.      40
• 5G introduces new service types, each with their own QoS requirements.  GTPv1-U [5] is 41
used in the Midhaul and Backhaul for F1-u, Xn-u, N3 and N9 interfaces. The importance of 42
user data packet within the mobile environment is indicated in the QoS Field Identifier 43
(QFI)[13] in the GTP-U extension header[5]. High performance TNEs do not have the 44
capability to examine the QFI field and instead use the DSCP field in the IP header or the 45
MPLS Traffic Class to understand the QoS requirements of packets in the transport network. 46
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        176
For the transport network to be able to treat GTP-U packets in an appropriate fashion for the 1
mobile service, the Midhaul and Backhaul mobile components (O-DUs, O-CU-UP and 2
UPFs) must have mechanisms to set a DSCP value in the GTP IP header appropriate to the 3
QoS requirements of the mobile service carried in the GTP-U packet. This could be 4
achieved using a mapping mechanism on the Midhaul and Backhaul mobile components that 5
map QFI values to DSCP values.    6
 7
NOTE: QFI to DSCP mapping facilities is a capability seen on Backhaul mobile component’s user 8
plane interfaces.  The status of this capability and associated management and control on Midhaul 9
mobile components and user plane interfaces is less clear.   10
 11
3GPP QoS flows and Transport QoS  12
The concept of QoS in 4G LTE is based on bearers, while the concept of QoS in 5G mobile systems 13
is based on the QoS Flow concept [1] which provides finer granularity. Flows in 5G and 4G bearers 14
can be divided into guaranteed bit rate flows (GBR QoS flows) and non-guaranteed bit rate flows 15
(non-GBR QoS flows). 5G also has a new delay critical GBR flow category which is not present in 16
LTE. QoS flows also have a service expectation which is expressed in terms of packet delay budget 17
and packet error rate. The mobile QoS requirements are designated through the QoS Class Identifier 18
(QCI) in LTE [2] and the 5G QoS Identifier (5QI) in 5G [2]. These identifiers can be operator 19
defined but there is also a standardized list contained in [1]. Within the mobile domain traffic (of 20
flows in 5G or bearers in LTE) with a given QCI or 5QI should receive the same forwarding 21
treatment (e.g, in terms of scheduling, admission threshold, etc) 22
 23
NOTE: The majority of the QCI and 5QI are the same, with very few exceptions of identifiers 24
being defined in one case but not in the other (e.g., QCI 75 or 5QI 10). 25
 26
TNEs in the transport network use DSCP or MPLS traffic class markings in the IP/MPLS header to 27
determine QoS packet handling. To keep consistency of QoS treatment between the overlay mobile 28
service and the underlay packet-based switching networks it is interesting to look at the different 29
standard QCI and 5QI in qualitative terms to understand how distinct kind of flows (or bearers) at 30
the mobile overlay level could be grouped together and treated on the underlay packet transport 31
network. This equates to how different QCIs and QFIs could be mapped to DSCP markings on the 32
5G mobile components (RUs, DUs, CUs) and how these DSCPs are mapped to Diffserv Per Hop 33
Behaviour (PHB) in the transport network.     34
Figure 24-9 plots the standardized QCIs and 5QIs in relation to the defined packet loss and delay 35
characteristics (Note, the figure is not to scale). Each box in the plot firstly contains the 5QI or QCI 36
(top part) complemented with the values for delay / packet loss (at the bottom).  To distinguish 37
profiles, that are GBR, non-GBR or delay-critical GBR, a color code is used which is indicated in 38
the legend of the figure (note that delay-critical GBR 5QI in 5G are simply GBR QCI in LTE).    39
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        177
 1
Figure 24-9: 5QI and QCI plot in relation to packet loss and delay  2
 3
The plot identifies 5QI or QCI values with common QoS flow characteristics. It is now possible to 4
group 5QI and QCI which require similar forwarding treatment in the transport network. Figure 5
24-10 presents a set of exemplary groupings, in which the 5QI/QCI are categorized into four 6
distinct groups: group 1 consists QoS flows requiring low delay, e.g., < 30 ms; group 2 consists 7
QoS flows which are in the delay range between 50 and 75 ms; group 3 consists of the rest of GBR 8
5QI/QCI; while finally group 4, as default group, aggregates the rest 5QI/QCI of non-GBR profile. 9
 10
 11
Figure 24-10: Exemplary 5QI and QCI grouping 12
 13
Using the above example, a transport QoS architecture is illustrated in the next section that supports 14
a set of transport core PHBs capable of supporting existing services, new 5G services, 5G fronthaul 15
and protecting the infrastructure needed to deliver timing, OAM and the transport routing protocols. 16

O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        178
 This annex makes no recommendations on QFI to DSCP mapping that needs to occur on the 1
Midhaul and Backhaul mobile components, as there are huge variations between operators on their 2
usage of DSCP markings. Viable approaches include: 3
• A dedicated DSCP marking for some 5QI / QFI value. 4
• Group 5QI and QFI values, for example as shown above, and allocate all 5QIs and QFIs in 5
the group the same DSCP value.  6
• A combination of the two.  7
 8
Regardless of the edge marking scheme chosen, the most important action occurs on the ingress 9
TNE which needs to map the edge DSCP marking to the appropriate core marking (MPLS TC for 10
MPLS underlays and DSCP for SRv6) to ensure the appropriate PHB in the core of the transport 11
network.     12
Transport QoS Architecture for O-RAN slicing phase 1 13
To ensure that Service Level Objectives (SLOs) requested for a particular slice are maintained, 14
appropriate QoS scheme must be deployed in the transport network. 15
 16
The details of QoS functionality are provided in Annex D: Quality of Service. This section provides 17
guidelines about particular use of the QoS concepts discussed in that annex to deploy the QoS 18
architecture suitable for providing SLO guarantees for transport network slices. 19
 20
As outlined in Figure 24-11, the QoS model for transport network slicing has following major 21
building blocks: 22
 23
• Ingress (input) policing (rate limiting) at the edge of the transport domain 24
• Mapping function at the edge of the transport domain to map potentially large number of 25
edge traffic classes, as described above, into finite (typically 4 to 8) traffic classes used in 26
the transport core 27
• Flat egress (output) scheduling (queuing) on the transport network transit links in the 28
transport core with typically 4 to 8 traffic classes 29
• Egress (output) scheduling (queuing) at the edge – optionally hierarchical 30
 31
NOTE: Figure 24-11 is an example figure and for simplicity shows the traffic in one direction only 32
(from UPF on the left-hand side to O-CU-UP on the right-hand side). However, traffic is typically 33
bidirectional (for example UPF → O-CU-UP and O-CU-UP → UPF), therefore the building blocks 34
mentioned earlier are deployed in opposite direction, too. Thus, all transport transit interfaces 35
(transport core) should have flat egress (output) scheduling (queuing) deployed, whereas all 36
transport edge interfaces would have input policing and potentially hierarchical output scheduling 37
(queuing) deployed. Also, the figure shows only one type of interface (backhaul). Similar QoS 38
concepts apply to over sliced interfaces. 39
 40
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        179
 1
Figure 24-11: High-level QoS model for transport slices 2
 3
Subsequent subsections provide detailed discussion for each element in the QoS architecture for 4
transport network slicing. 5
Edge QoS 6
When the slice is created, mechanisms required to implement the slice in the transport network are 7
deployed on the transport network elements (TNEs). An important building block is edge QoS to 8
condition slice traffic at the transport edge and ensure that slice traffic does not exceed the 9
previously requested and agreed bandwidth values.  10
Ingress policing 11
Ingress policing (rate limiting) is used to police (rate limit) the traffic of a particular slice when the 12
traffic enters the transport network domain. This slice admission control function is required to 13
ensure, that only traffic contracted by given slice at the time of slice creation (or at the time of last 14
slice modification) is accepted with SLO guarantees. 15
 16
NOTE: In the deployments where there is no expectation from the transport network to provide 17
SLO guarantees (delay, jitter, bandwidth, no traffic interference between different slices, etc.) 18
ingress policing might be omitted. 19
 20
Depending on the actual slice QoS profile requested at the slice creation/modification, out-of-21
contract traffic is either completely dropped at the transport network edge (if requested slice 22
guaranteed rate and maximum rate are equal, or maximum guaranteed rate is missing in the 23
request), or, if slice guaranteed rate is not equal to slice maximum rate, slice traffic is handled at the 24
edge in following way: 25
 26
• Slice traffic under guaranteed rate: unconditionally accepted at the transport edge 27
 28
UPF
UPF
UPF
UPF
UPF
UPF
DC-X DC-Y
VRF-3
VRF-4
VRF-6
Transport Network Domain
Slice 1 (MBB Backhaul: N3/M9)
Slice 2 (mMTC Backhaul: N3/N9)
Slice 3 (NB-IOT Backhaul: N3/N9)
Slice 4 (another slice)
Slice 5 (another slice)
Slice 6 (another slice)
VRF-1
VRF-2
VRF-3
VRF-4
VRF-5
VRF-6
transport edge
slice-aware
ingress policing
VRF-1
VRF-2
VRF-5
Slice 1
(VLAN 1)
O-CU-UP
O-CU-UP
O-CU-UP
O-CU-UP
O-CU-UP
O-CU-UP
Slice 1 (VLAN 1)
Slice 2 (VLAN 2)
Slice 3 (VLAN 3)
Slice 4 (VLAN 4)
Slice 5 (VLAN 5)
Slice 6 (VLAN 6)
Edge TNE Edge TNE
transport core
non slice-aware
(flat) egress scheduling
transport edge
slice-aware
(hierarchical) egress scheduling
edge to core
class mapping
traffic flow direction
Slice 2
(VLAN 2)
Slice 3
(VLAN 3)
Slice 4
(VLAN 4)
Slice 5
(VLAN 5)
Slice 6
(VLAN 6)
recommended
optional
Transport Core Traffic Class 1
Transport Core Traffic Class 2
Transport Core Traffic Class 3
Transport Core Traffic Class 4
Transport Core Traffic Class 5
Transport Core Traffic Class 6
Transport Core Traffic Class 7
Transport Core Traffic Class 8
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        180
• Slice traffic above guaranteed rate and below maximum rate: conditionally accepted at the 1
transport edge. Such traffic is marked at the transport edge with QoS marking (DSCP or 2
MPLS Traffic Class bits) representing high drop probability. Using Class based Weighted 3
Random Early Discard (CBWRED) functionality, described in Annex D: Quality of Service , 4
and implemented on transit transport network elements, when the transit links are nearing 5
the congestion (transmit queues on transit links are building up) CBWRED kicks in, and 6
starts to drop packets marked with QoS marking representing high drop probability. Thus, 7
when no congestion is observed in the transport network, slice traffic can go up to the 8
contracted value (no drop at the transport edge for traffic volumes below slice maximum 9
rate), but when congestion is close, high drop probability packets (above guaranteed rate but 10
below maximum rate) are being dropped at transit. This ingress marking behaviour at the 11
transport edge can be implemented with 2-rate 3-color policer. 12
 13
• Slice traffic above maximum rate (or above guaranteed rate, if maximum rate is not 14
specified during slice creation/modification): unconditionally dropped at the transport edge. 15
This behaviour can be implemented with 1-rate policer (when slice guaranteed rate equals to 16
maximum rate), or 2-rate 3-color policer (when different guaranteed and maximum rates are 17
specified for the slice) 18
 19
There could be many reasons for misbehaving (send traffic above the contracted rates) slices, like 20
for example: 21
 22
• Underestimate for slice traffic volume during slice creation (or slice modification) events 23
• Some failure within the slice, causing extra traffic generation 24
• Denial of Service (DoS) attack started from within the slice 25
 26
With slice admission control, implemented via ingress policing (rate limiting), it can be ensured that 27
only legitimate in-contract slice traffic is allowed at the transport edge, thus slices that are sending 28
traffic above the agreed rate do not negatively impact other slices. This provides traffic isolation 29
guarantees between the different slices. 30
 31
NOTE: In phase 1 of network slicing, only per slice SLO parameters might be specified. There 32
might be no granular per class (per 5QI) SLO parametrization within each slice in phase 1. 33
Therefore, in phase 1 of network slicing single policer (1-rate or 2-rate-3-color) per slice might be 34
sufficient for slice admission control implementation. 35
 36
In subsequent phases, with more granular requirements introduced within the slice, more individual 37
policers, or even hierarchical policers, might be introduced to implement slice admission control. 38
 39
Ingress Edge to core traffic class mapping 40
As described above multiple 5QIs, resulting in multiple DSCP markings, are potentially being used 41
at the edge of the transport domain in the deployments. 42
 43
Transport core uses flat QoS model with up to 8 traffic classes. Therefore, mapping function to map 44
multiple transport edge classes (represented by multiple DSCP values) to limited set of transport 45
core classes is implemented on the edge transport network element as outlined in Figure 24-12. 46
 47
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        181
 1
Figure 24-12: Transport edge to transport core QoS mapping 2
 3
In case of MPLS encapsulation in transport core, the result of this transport edge to transport core 4
class mapping is setting the Traffic Class (TC) field in outer MPLS label. If SRv6 is used as the 5
encapsulation mechanism, DSCP field in the outer IPv6 header carries the transport core class 6
marking, while DSCP field in the inner IPv4/IPv6 header, similarly to MPLS encapsulation, carried 7
transport edge class marking. 8
 9
Transport network elements (TNEs) associate transport core class markings, carried either in outer 10
MPLS header TC field, or in outer IPv6 header DSCP field, with QoS output queues and execute 11
appropriate scheduling mechanism to implement desired traffic prioritization during link congestion 12
events.  13
 14
Egress (hierarchical) scheduling 15
On the egress side of transport edge, egress scheduling is implemented to provide desired traffic 16
prioritization among the slices, as well as among traffic classes within each slice. Depending on the 17
hardware capabilities of the edge transport network element, as well as the intended granularity 18
(more “soft’ or more “hard”) of the enforcement, one of the following option can be used: 19
 20
• flat, non-hierarchical QoS model with up to 8 queues (similar to transport core QoS model) 21
• hierarchical, slice-aware QoS model with separate set of queues per each slice 22
 23
As described in the Annex D: Quality of Service , the hierarchical QoS scheduler model allows for a 24
“hard” distribution of the bandwidth on an interface amongst the slices sharing that interface 25
capacity, protecting the use of the bandwidth for a given slice from the other slices on the same 26
interface. 27
 28
transport edge
ingress policing
& classification
Slice 1 (VLAN 1)
Slice 2 (VLAN 2)
Slice 3 (VLAN 3)
Slice 4 (VLAN 4)
Slice 5 (VLAN 5)
Slice 6 (VLAN 6)
Transport Core Traffic Class 1
Transport Core Traffic Class 2
Transport Core Traffic Class 3
Transport Core Traffic Class 4
Transport Core Traffic Class 5
Transport Core Traffic Class 6
Transport Core Traffic Class 7
Transport Core Traffic Class 8
transport core
(flat) egress scheduling
ETH header
IP header
Payload
DSCP
PCP
DSCP
TCVLAN
IP header
Payload
ETH header
MPLS header
header
removed
new headers
added
label
TC
MPLS header
label
retained
slice identification at
the transport edge
transport
edge QoS
marking
transport
core QoS
marking
Edge àCore mapping
For illustration purposes only
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        182
Core QoS 1
Transport core QoS is based on a flat model with 4 to 8 queues. Transport core QoS settings are 2
rather static in the nature, provisioned at the time the transport network infrastructure (interface 3
settings, IGP settings, BGP settings, etc.) is provisioned. Especially, there is no direct correlation 4
between transport core QoS and 5G slices being created/modified/deleted. Transport core QoS 5
doesn’t maintain any state related to individual 5G slice, neither adjust its QoS parameters based on 6
5G slices creation/modification/deletion events. 7
 8
The goal of transport core QoS is to ensure right prioritization of transport core traffic classes 9
during congestion events caused by network failures and subsequent traffic rerouting, so that during 10
such congestion events traffic classes requiring low latency are using priority forwarding, and 11
traffic classes requiring low drop are not dropped. The goal of transport core QoS is not end-to-end 12
bandwidth management or providing granular guarantees per each 5G slice. 13
 14
Table 9 contains a list of typical flows that can be observed in the multiclass transport network used 15
to transport 5G flows as well , taking into consideration s exemplary 5QI /QCI grouping describe d 16
earlier in t his annex . When recommending appropriate transport core QoS model, overall QoS 17
policies for all flows must be taken into consideration. 18
 19
Traffic type Packet size
(order of magnitude)
Per-hop latency
(order of magnitude)1)
Per-hop PDV
(order of magnitude)1)
PTP (unaware mode)2) ~100 bytes constant average ~0.5 µs3)
CPRI (RoE) ~1500 bytes ~1-20 µs ~1-20 µs
eCPRI CU-plane ~1500 bytes ~1-20 µs ~1-20 µs
OAM with aggressive timers ~100 bytes ~1 ms ~1 ms
5QI/QCI Group 1 (low latency U-plane) variable ~1 ms ~1 ms
Low latency business traffic variable ~1 ms ~1 ms
Network Control: OAM with relaxed timers,
IGP, BGP, LDP, RSVP, PTP aware mode
(T-TC/T-BC)4)
variable ~5 ms ~1-3 ms
O-RAN/3GPP C-plane and M-plane5) variable ~5 ms ~1-3 ms
5QI/QCI Group 2 (medium latency U-plane) variable ~5 ms ~1-3 ms
5QI/QCI Group 3 (remaining GBR U-plane) variable ~10 ms ~5 ms
Guaranteed business traffic variable ~10 ms ~5 ms
5QI/QCI Group 4 (remaining non-GBR U-
plane)
Other traffic types (best effort)
variable ~10-50 ms ~5-25 ms
 20
Table 9: Different flows per-hop latency/PDV (order of magnitude) 21
 22
Note 1 : Per-hop latency includes all latency contributors, like f rame transmission delay , s elf-23
queueing delay, queuing delay, store-and-forward delay , as described in section 10.1.1.2.1. Exact 24
per-hop requirements depend on the overall network budget, number of hops, budget allocated to 25
fibers, etc. The table intends to emphasize only relative order of magnitude for per -hop 26
latency/PDV to illustrate the process of assigning traffic to QoS queues. 27
 28
Note 2: PTP unaware mode (i.e., router transiting G.8275.2 PTP stream, without T -TC function on 29
the router that could add timestamps to transit PTP packets) → strict-priority queue is required to 30
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        183
minimize jitter (actual latency value is not relevant, but its average should be constant). Minimizing 1
the latency via strict-priority queue minimizes jitter as well. 2
 3
Note 3: Max|TE| accumulated across the network must be ≤1.1 µs. 4
 5
Note 4: T-BC/T-TC (both G.8275.1 and G.8275.2) with physical layer time stamping → guaranteed 6
bandwidth queue is OK, strict -priority queue is not required, since jitter/PDV will be accounted by 7
physical layer timestamps in PTP packet. Also, latency value is not relevant, but average latency 8
should be constant. QoS should ensure that PTP packets are not dropped during congestion, and 9
guaranteed bandwidth queue is sufficient for that. 10
 11
Note 5: O-RAN/3GPP C-plane and M-plane kept in separate queue to ensure minimum bandwidth 12
guarantees during congestion events preventing failures of  these planes, as well as to increase 13
security and separation of these planes. 14
 15
There are variety of QoS models, depending on the hardware support available on transport network 16
element platform . It is out of scope for this document to discuss all the various QoS models 17
supported by different hardware platforms of transport network elements. However, for illustration 18
purposes, two major, most common QoS models are worth to mention : with single pr iority queue, 19
and with multiple priority queues. 20
 21
 22
Figure 24-13: Example of QoS model with single priority queue 23
 24
Figure 24-13 outlines an example QoS model with a single priority queue. In this hardware model, 25
all flows with ultra-low latency/PDV sensitivity (PTP unaware mode, CPRI/RoE, eCPRI CU-plane) 26
must be placed in this priority queue, while other flows should be distributed among remaining 27
bandwidth queues. Bandwidth queue used for flows with low (but not ultra-low) latency/PDV 28
sensitivity (OAM with aggressive timers, latency sensitive U-plane and business traffic) should be 29
parametrized with relatively high weight used in WFQ/WRR/WDRR/MDRR (Weighted Fair 30
Port
CPRI (RoE), eCPRI CU-P, PTP unaware mode PQ
BQ
BQ
BQ
BQ
BQ
BQ
BQ
Weight
Weight
Weight
Weight
Weight
PIR
PIR
PIR
PIR
PIR
PIR
PIR
PIR
PIR mandatory à to avoid
starving of remaining queues
OAM with aggressive timers, 5QI/QCI Group 1
(low latency U-plane), low latency business traffic
Network Control: OAM with relaxed timers,
IGP, BGP, LDP, RSVP, eCPRI S-P, PTP (T-TC/T-BC)
PIR optionalO-RAN/3GPP C-plane and M-plane
(e.g. eCPRI M-plane, other management)
5QI/QCI Group 2 (medium latency U-plane)
Scheduler parameters
5QI/QCI Group 3 (remaining GBR U-plane),
guaranteed business traffic
spare
5QI/QCI Group 4 (remaining non-GBR U-plane)
Other best effort (may be guaranteed)
WFQ/WRR/WDRR/MDRR Scheduling
Weight
Weight
Very high weight (BW over-dimensioning) to ensure frequent enqueuing in
order to avoid queue congestion, and thus to keep queue latency to minimum
Queue buffer size aligned to maximum latency requirements
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        184
Queueing, Weighted Round Robin, Weighted Deficit Round Robin. Modified Deficit Round Robin) 1
scheduling algorithms, so that this queue is serviced very frequently, to avoid queue congestion and 2
to minimize latency/PDV. With high weight parameterization this queue behaves almost as priority 3
queue. 4
 5
To avoid starvation of the remaining queues during congestion events, priority queue, as well as 6
highly over dimensioned bandwidth queue must have a policer or shaper to limit the rate (PIR – 7
Peak Information Rate) of the queue. Queue buffer sizes in both cases must be aligned to maximum 8
latency requirements of the traffic assigned to the queue. 9
 10
 11
Figure 24-14: Example of QoS model with multiple priority queues 12
 13
Figure 24-14 outlines an example of queue assignment on hardware platforms supporting multiple 14
priority queues, dequeued in strict priority order. With multiple priority queues available it is 15
recommended to place PTP packets in unaware mode in the highest priority queue, to minimize the 16
PDV of these packets to the highest possible degree. Putting these packets above CPRI(RoE) or 17
eCPRI has only minimal influence on CPRI/eCPRI packets PDV, since PTP packets are very small 18
(~100 bytes). For example, serialization delay of such small packet on 10 GE interface is only 80 19
ns, so PDV factor contributing to CPRI/eCPRI PDV is very small as well and can be easily handled 20
by the CPRI/eCPRI reassembly functions. 21
 22
 23
 24
 25
 26
 27
 28
 29
 30
Port
Priority  Scheduling
Weight
Weight
Weight
Weight
Weight
PIR
PIR
PIR
PIR
PIR
PIR
PIR
PIR
PIR mandatoryPIR optional
Scheduler parameters
Queue buffer size aligned to maximum latency requirements
WFQ/WRR/WDRR/MDRR Scheduling
PTP unaware mode
CPRI (RoE), eCPRI CU-P
OAM with aggressive timers, 5QI/QCI Group 1
(low latency U-plane), low latency business traffic
Network Control: OAM with relaxed timers,
IGP, BGP, LDP, RSVP, eCPRI S-P, PTP (T-TC/T-BC)
O-RAN/3GPP C-plane and M-plane
(e.g. eCPRI M-plane, other management)
5QI/QCI Group 2 (medium latency U-plane)
5QI/QCI Group 3 (remaining GBR U-plane),
guaranteed business traffic
5QI/QCI Group 4 (remaining non-GBR U-plane)
other best effort (may be guaranteed)
PQ
BQ
BQ
BQ
BQ
PQ
BQ
PQ
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        185
 1
Annex ZZZ : O-RAN Adopter License Agreement 2
BY DOWNLOADING, USING OR OTHERWISE ACCESSING ANY O-RAN SPECIFICATION, 3
ADOPTER AGREES TO THE TERMS OF THIS AGREEMENT. 4
This O-RAN Adopter License Agreement (the “Agreement”) is made by and between the O-RAN 5
Alliance and the entity that downloads, uses or otherwise accesses any O-RAN Specification, 6
including its Affiliates (the “Adopter”). 7
This is a license agreement for entities who wish to adopt any O-RAN Specification. 8
Section 1: DEFINITIONS 9
 10
1.1 “Affiliate” means an entity that directly or indirectly controls, is controlled by, or is under 11
common control with another entity, so long as such control exists. For the purpose of this Section, 12
“Control” means beneficial ownership of fifty (50%) percent or more of the voting stock or equity 13
in an entity. 14
1.2 “Compliant Implementation” means any system, device, method or operation (whether 15
implemented in hardware, software or combinations thereof) that fully conforms to a Final 16
Specification. 17
1.3 “Adopter(s)” means all entities, who are not Members, Contributors or Academic Contributors, 18
including their Affiliates, who wish to download, use or otherwise access O-RAN Specifications. 19
1.4 “Minor Update” means an update or revision to an O-RAN Specification published by O-RAN 20
Alliance that does not add any significant new features or functionality and remains interoperable 21
with the prior version of an O-RAN Specification. The term “O-RAN Specifications” includes 22
Minor Updates. 23
1.5 “Necessary Claims” means those claims of all present and future patents and patent 24
applications, other than design patents and design registrations, throughout the world, which (i) are 25
owned or otherwise licensable by a Member, Contributor or Academic Contributor during the term 26
of its Member, Contributor or Academic Contributorship; (ii) such Member, Contributor or 27
Academic Contributor has the right to grant a license without the payment of consideration to a 28
third party; and (iii) are necessarily infringed by a Compliant Implementation (without considering 29
any Contributions not included in the Final Specification). A claim is necessarily infringed only 30
when it is not possible on technical (but not commercial) grounds, taking into account normal 31
technical practice and the state of the art generally available at the date any Final Specification was 32
published by the O-RAN Alliance or the date the patent claim first came into existence, whichever 33
last occurred, to make, sell, lease, otherwise dispose of, repair, use or operate a Compliant 34
Implementation without infringing that claim. For the avoidance of doubt in exceptional cases 35
where a Final Specification can only be implemented by technical solutions, all of which infringe 36
patent claims, all such patent claims shall be considered Necessary Claims. 37
1.6 “Defensive Suspension” means for the purposes of any license grant pursuant to Section 3, 38
Member, Contributor, Academic Contributor, Adopter, or any of their Affiliates, may have the 39
discretion to include in their license a term allowing the licensor to suspend the license against a 40
licensee who brings a patent infringement suit against the licensing Member, Contributor, 41
Academic Contributor, Adopter, or any of their Affiliates. 42
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        186
Section 2: COPYRIGHT LICENSE 1
2.1 Subject to the terms and conditions of this Agreement, O-RAN Alliance hereby grants to 2
Adopter a nonexclusive, nontransferable, irrevocable, non-sublicensable, worldwide copyright 3
license to obtain, use and modify O-RAN Specifications, but not to further distribute such O-RAN 4
Specification in any modified or unmodified way, solely in furtherance of implementations of an 5
ORAN 6
Specification. 7
2.2 Adopter shall not use O-RAN Specifications except as expressly set forth in this Agreement or 8
in a separate written agreement with O-RAN Alliance. 9
Section 3: FRAND LICENSE 10
3.1 Members, Contributors and Academic Contributors and their Affiliates are prepared to grant 11
based on a separate Patent License Agreement to each Adopter under Fair Reasonable And Non- 12
Discriminatory (FRAND) terms and conditions with or without compensation (royalties) a 13
nonexclusive, non-transferable, irrevocable (but subject to Defensive Suspension), non-14
sublicensable, worldwide patent license under their Necessary Claims to make, have made, use, 15
import, offer to sell, lease, sell and otherwise distribute Compliant Implementations; provided, 16
however, that such license shall not extend: (a) to any part or function of a product in which a 17
Compliant Implementation is incorporated that is not itself part of the Compliant Implementation; 18
or (b) to any Adopter if that Adopter is not making a reciprocal grant to Members, Contributors and 19
Academic Contributors, as set forth in Section 3.3. For the avoidance of doubt, the foregoing 20
licensing commitment includes the distribution by the Adopter’s distributors and the use by the 21
Adopter’s customers of such licensed Compliant Implementations. 22
3.2 Notwithstanding the above, if any Member, Contributor or Academic Contributor, Adopter or 23
their Affiliates has reserved the right to charge a FRAND royalty or other fee for its license of 24
Necessary Claims to Adopter, then Adopter is entitled to charge a FRAND royalty or other fee to 25
such Member, Contributor or Academic Contributor, Adopter and its Affiliates for its license of 26
Necessary Claims to its licensees. 27
3.3 Adopter, on behalf of itself and its Affiliates, shall be prepared to grant based on a separate 28
Patent License Agreement to each Members, Contributors, Academic Contributors, Adopters and 29
their Affiliates under Fair Reasonable And Non-Discriminatory (FRAND) terms and conditions 30
with or without compensation (royalties) a nonexclusive, non-transferable, irrevocable (but subject 31
to Defensive Suspension), non-sublicensable, worldwide patent license under their Necessary 32
Claims to make, have made, use, import, offer to sell, lease, sell and otherwise distribute Compliant 33
Implementations; provided, however, that such license will not extend: (a) to any part or function of 34
a product in which a Compliant Implementation is incorporated that is not itself part of the 35
Compliant Implementation; or (b) to any Members, Contributors, Academic Contributors, Adopters 36
and their Affiliates that is not making a reciprocal grant to Adopter, as set forth in Section 3.1. For 37
the avoidance of doubt, the foregoing licensing commitment includes the distribution by the 38
Members’, Contributors’, Academic Contributors’, Adopters’ and their Affiliates’ distributors and 39
the use by the Members’, Contributors’, Academic Contributors’, Adopters’ and their Affiliates’ 40
customers of such licensed Compliant Implementations. 41
Section 4: TERM AND TERMINATION 42
4.1 This Agreement shall remain in force, unless early terminated according to this Section 4. 43
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        187
4.2 O-RAN Alliance on behalf of its Members, Contributors and Academic Contributors may 1
terminate this Agreement if Adopter materially breaches this Agreement and does not cure or is not 2
capable of curing such breach within thirty (30) days after being given notice specifying the breach. 3
4.3 Sections 1, 3, 5 - 11 of this Agreement shall survive any termination of this Agreement. Under 4
surviving Section 3, after termination of this Agreement, Adopter will continue to grant licenses (a) 5
to entities who become Adopters after the date of termination; and (b) for future versions of ORAN 6
Specifications that are backwards compatible with the version that was current as of the date of 7
termination. 8
Section 5: CONFIDENTIALITY 9
Adopter will use the same care and discretion to avoid disclosure, publication, and dissemination of 10
O-RAN Specifications to third parties, as Adopter employs with its own confidential information, 11
but no less than reasonable care. Any disclosure by Adopter to its Affiliates, contractors and 12
consultants should be subject to an obligation of confidentiality at least as restrictive as those 13
contained in this Section. The foregoing obligation shall not apply to any information which is: (1) 14
rightfully known by Adopter without any limitation on use or disclosure prior to disclosure; (2) 15
publicly available through no fault of Adopter; (3) rightfully received without a duty of 16
confidentiality; (4) disclosed by O-RAN Alliance or a Member, Contributor or Academic 17
Contributor to a third party without a duty of confidentiality on such third party; (5) independently 18
developed by Adopter; (6) disclosed pursuant to the order of a court or other authorized 19
governmental body, or as required by law, provided that Adopter provides reasonable prior written 20
notice to O-RAN Alliance, and cooperates with O-RAN Alliance and/or the applicable Member, 21
Contributor or Academic Contributor to have the opportunity to oppose any such order; or (7) 22
disclosed by Adopter with O-RAN Alliance’s prior written approval. 23
Section 6: INDEMNIFICATION 24
Adopter shall indemnify, defend, and hold harmless the O-RAN Alliance, its Members, 25
Contributors or Academic Contributors, and their employees, and agents and their respective 26
successors, heirs and assigns (the “Indemnitees”), against any liability, damage, loss, or expense 27
(including reasonable attorneys’ fees and expenses) incurred by or imposed upon any of the 28
Indemnitees in connection with any claims, suits, investigations, actions, demands or judgments 29
arising out of Adopter’s use of the licensed O-RAN Specifications or Adopter’s commercialization 30
of products that comply with O-RAN Specifications. 31
Section 7: LIMITATIONS ON LIABILITY; NO WARRANTY 32
EXCEPT FOR BREACH OF CONFIDENTIALITY, ADOPTER’S BREACH OF SECTION 3, 33
AND ADOPTER’S INDEMNIFICATION OBLIGATIONS, IN NO EVENT SHALL ANY 34
PARTY BE LIABLE TO ANY OTHER PARTY OR THIRD PARTY FOR ANY INDIRECT, 35
SPECIAL, INCIDENTAL, PUNITIVE OR CONSEQUENTIAL DAMAGES RESULTING FROM 36
ITS PERFORMANCE OR NON-PERFORMANCE UNDER THIS AGREEMENT, IN EACH 37
CASE WHETHER UNDER CONTRACT, TORT, WARRANTY, OR OTHERWISE, AND 38
WHETHER OR NOT SUCH PARTY HAD ADVANCE NOTICE OF THE POSSIBILITY OF 39
SUCH DAMAGES. O-RAN SPECIFICATIONS ARE PROVIDED “AS IS” WITH NO 40
WARRANTIES OR CONDITIONS WHATSOEVER, WHETHER EXPRESS, IMPLIED, 41
STATUTORY, OR OTHERWISE. THE O-RAN ALLIANCE AND THE MEMBERS, 42
CONTRIBUTORS OR ACADEMIC CONTRIBUTORS EXPRESSLY DISCLAIM ANY 43
WARRANTY OR CONDITION OF MERCHANTABILITY, SECURITY, SATISFACTORY 44
QUALITY, NONINFRINGEMENT, FITNESS FOR ANY PARTICULAR PURPOSE, ERROR-45
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        188
FREE OPERATION, OR ANY WARRANTY OR CONDITION FOR O-RAN 1
SPECIFICATIONS. 2
Section 8: ASSIGNMENT 3
Adopter may not assign the Agreement or any of its rights or obligations under this Agreement or 4
make any grants or other sublicenses to this Agreement, except as expressly authorized hereunder, 5
without having first received the prior, written consent of the O-RAN Alliance, which consent may 6
be withheld in O-RAN Alliance’s sole discretion. O-RAN Alliance may freely assign this 7
Agreement. 8
Section 9: THIRD-PARTY BENEFICIARY RIGHTS 9
Adopter acknowledges and agrees that Members, Contributors and Academic Contributors 10
(including future Members, Contributors and Academic Contributors) are entitled to rights as a 11
third-party beneficiary under this Agreement, including as licensees under Section 3. 12
Section 10: BINDING ON AFFILIATES 13
Execution of this Agreement by Adopter in its capacity as a legal entity or association constitutes 14
that legal entity’s or association’s agreement that its Affiliates are likewise bound to the obligations 15
that are applicable to Adopter hereunder and are also entitled to the benefits of the rights of Adopter 16
hereunder. 17
Section 11: GENERAL 18
This Agreement is governed by the laws of Germany without regard to its conflict or choice of law 19
provisions.  20
This Agreement constitutes the entire agreement between the parties as to its express subject matter 21
and expressly supersedes and replaces any prior or contemporaneous agreements between the 22
parties, whether written or oral, relating to the subject matter of this Agreement.  23
Adopter, on behalf of itself and its Affiliates, agrees to comply at all times with all applicable laws, 24
rules and regulations with respect to its and its Affiliates’ performance under this Agreement, 25
including without limitation, export control and antitrust laws. Without limiting the generality of the 26
foregoing, Adopter acknowledges that this Agreement prohibits any communication that would 27
violate the antitrust laws. 28
By execution hereof, no form of any partnership, joint venture or other special relationship is 29
created between Adopter, or O-RAN Alliance or its Members, Contributors or Academic 30
Contributors. Except as expressly set forth in this Agreement, no party is authorized to make any 31
commitment on behalf of Adopter, or O-RAN Alliance or its Members, Contributors or Academic 32
Contributors. 33
In the event that any provision of this Agreement conflicts with governing law or if any provision is 34
held to be null, void or otherwise ineffective or invalid by a court of competent jurisdiction, (i) such 35
provisions will be deemed stricken from the contract, and (ii) the remaining terms, provisions, 36
covenants and restrictions of this Agreement will remain in full force and effect. 37
Any failure by a party or third party beneficiary to insist upon or enforce performance by another 38
party of any of the provisions of this Agreement or to exercise any rights or remedies under this 39
Agreement or otherwise by law shall not be construed as a waiver or relinquishment to any extent 40
of the other parties’ or third party beneficiary’s right to assert or rely upon any such provision, right 41
or remedy in that or any other instance; rather the same shall be and remain in full force and effect. 42
 43
O-RAN.WG9.XPSAAS-v02.00

________________________________________________________________________________________________
Copyright © 2021 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ        189
 1
 2