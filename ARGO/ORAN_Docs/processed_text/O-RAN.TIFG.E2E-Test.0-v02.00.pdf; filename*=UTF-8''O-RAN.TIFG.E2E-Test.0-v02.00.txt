O-RAN.TIFG.E2E-Test.0-v02.00
Technical Specification
O-RAN ALLIANCE Test and Integration Focus Group
End-to-end Test Specification
This is a re-published version of the attached final specification.
For this re-published version, the prior versions of the IPR Policy will apply, except that the previous
requirement for Adopters (as defined in the earlier IPR Policy) to agree to an O-RAN Adopter License
Agreement to access and use Final Specifications shall no longer apply or be required for these Final
Specifications after 1st July 2022.
The copying or incorporation into any other work of part or all of the material available in this
specification in any form without the prior written permission of O-RAN ALLIANCE e.V.  is prohibited,
save that you may print or download extracts of the material on this site for your personal use, or copy
the material on this site for the purpose of sending to individual third parties for their information
provided that you acknowledge O-RAN ALLIANCE as the source of the material and that you inform
the third party that these conditions apply to them and that they must comply with them.
______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 1

1
  O-RAN.TIFG.E2E-Test.0-v02.00
Technical Specification

O-RAN ALLIANCE Test and Integration Focus Group

End-to-end Test Specification

Copyright © 2021 by O-RAN ALLIANCE e.V.

By using, accessing or downloading any part of this O-RAN specification document, including by copying, saving, distributing,
displaying or preparing derivatives of, you agree to be and are bound to the terms of the O -RAN Adopter License Agreement
contained in the Annex ZZZ of this specification. All other rights reserved.

O-RAN ALLIANCE e.V.
Buschkauler Weg 27, 53347 Alfter, Germany
Register of Associations, Bonn VR 11238
VAT ID DE321720189
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ.

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 2

O-RAN.TIFG.E2E-Test.0-v02.00
Revision History 1
Date Revision Description
2020.03.06 01.00 Initial version
2020.07.09 02.00 Additional tests for Ch. 4-6, new Ch. 8 for load/stress tests incl. Ch. 3 updates

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 3

O-RAN.TIFG.E2E-Test.0-v02.00
Contents 1
Revision History ................................................................................................................................................. 2 2
 Introductory Material .............................................................................................................. 11 3
1.1 Scope of the document ..................................................................................................................................... 11 4
1.2 References........................................................................................................................................................ 11 5
1.3 Definitions and Abbreviations ......................................................................................................................... 12 6
1.3.1 Definitions .................................................................................................................................................. 12 7
1.3.2 Abbreviations ............................................................................................................................................. 13 8
1.4 Revision Guideline .......................................................................................................................................... 14 9
 Overview ................................................................................................................................. 15 10
 Testing methodology and configuration ................................................................................. 16 11
3.1 System under test ............................................................................................................................................. 17 12
3.1.1 Load and Stress Interfaces to SUT ............................................................................................................. 18 13
3.2 Test and measurement equipment and tools .................................................................................................... 18 14
3.3 Test report ........................................................................................................................................................ 20 15
3.4 Data traffic ....................................................................................................................................................... 21 16
3.5 Mobility Classes .............................................................................................................................................. 22 17
3.6 Radio conditions .............................................................................................................................................. 22 18
3.7 Inter-cell interference ....................................................................................................................................... 24 19
3.8 Spectral efficiency ........................................................................................................................................... 25 20
 Functional tests ....................................................................................................................... 26 21
4.1 LTE/5G NSA attach and detach of single UE ................................................................................................. 26 22
4.1.1 Test description and applicability ............................................................................................................... 26 23
4.1.2 Test setup and configuration ...................................................................................................................... 27 24
4.1.3 Test Procedure ............................................................................................................................................ 27 25
4.1.4 Test requirements (expected results) .......................................................................................................... 28 26
4.2 LTE/5G NSA attach and detach of multiple UEs ............................................................................................ 29 27
4.2.1 Test description and applicability ............................................................................................................... 29 28
4.2.2 Test setup and configuration ...................................................................................................................... 29 29
4.2.3 Test Procedure ............................................................................................................................................ 30 30
4.2.4 Test requirements (expected results) .......................................................................................................... 30 31
4.3 5G SA registration and deregistration of single UE ......................................................................................... 31 32
4.3.1 Test description and applicability ............................................................................................................... 31 33
4.3.2 Test setup and configuration ...................................................................................................................... 32 34
4.3.3 Test Procedure ............................................................................................................................................ 32 35
4.3.4 Test requirements (expected results) .......................................................................................................... 33 36
4.4 Intra-O-DU mobility ........................................................................................................................................ 33 37
4.4.1 Test description and applicability ............................................................................................................... 33 38
4.4.2 Test setup and configuration ...................................................................................................................... 34 39
4.4.3 Test Procedure ............................................................................................................................................ 35 40
4.4.4 Test requirements (expected results) .......................................................................................................... 35 41
4.5 Inter-O-DU mobility ........................................................................................................................................ 36 42
4.5.1 Test description and applicability ............................................................................................................... 36 43
4.5.2 Test setup and configuration ...................................................................................................................... 36 44
4.5.3 Test Procedure ............................................................................................................................................ 38 45
4.5.4 Test requirements (expected results) .......................................................................................................... 38 46
4.6 Inter-O-CU mobility ........................................................................................................................................ 39 47
4.6.1 Test description and applicability ............................................................................................................... 39 48

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 4

O-RAN.TIFG.E2E-Test.0-v02.00
4.6.2 Test setup and configuration ...................................................................................................................... 39 1
4.6.3 Test Procedure ............................................................................................................................................ 40 2
4.6.4 Test requirements (expected results) .......................................................................................................... 41 3
4.7 Registration and deregistration to a single network slice ................................................................................. 42 4
4.7.1 Test description and applicability ............................................................................................................... 42 5
4.7.2 Test setup and configuration ...................................................................................................................... 42 6
4.7.3 Test Procedure ............................................................................................................................................ 42 7
4.7.4 Test requirements (expected results) .......................................................................................................... 43 8
4.8 Registration and deregistration to multiple network slices .............................................................................. 45 9
4.8.1 Test description and applicability ............................................................................................................... 45 10
4.8.2 Test setup and configuration ...................................................................................................................... 45 11
4.8.3 Test Procedure ............................................................................................................................................ 45 12
4.8.4 Test requirements (expected results) .......................................................................................................... 46 13
4.9 Idle Mode Intra-O-DU mobility....................................................................................................................... 48 14
4.9.1 Test description and applicability ............................................................................................................... 48 15
4.9.2 Test setup and configuration ...................................................................................................................... 48 16
4.9.3 Test Procedure ............................................................................................................................................ 49 17
4.9.4 Test requirements (expected results) .......................................................................................................... 49 18
4.10 Idle mode Inter-O-DU mobility ....................................................................................................................... 50 19
4.10.1 Test description and applicability ............................................................................................................... 50 20
4.10.2 Test setup and configuration ...................................................................................................................... 50 21
4.10.3 Test Procedure ............................................................................................................................................ 51 22
4.10.4 Test requirements (expected results) .......................................................................................................... 52 23
4.11 Idle mode Inter-O-CU mobility ....................................................................................................................... 53 24
4.11.1 Test description and applicability ............................................................................................................... 53 25
4.11.2 Test setup and configuration ...................................................................................................................... 53 26
4.11.3 Test Procedure ............................................................................................................................................ 54 27
4.11.4 Test requirements (expected results) .......................................................................................................... 55 28
4.12 5G/4G Inter-RAT Mobility - 5G to LTE handover ......................................................................................... 56 29
4.12.1 Test description and applicability ............................................................................................................... 56 30
4.12.2 Test setup and configuration ...................................................................................................................... 56 31
4.12.3 Test Procedure ............................................................................................................................................ 56 32
4.12.4 Test requirements (expected results) .......................................................................................................... 57 33
4.13 5G/4G Inter-RAT Mobility - LTE to 5G handover ......................................................................................... 57 34
4.13.1 Test description and applicability ............................................................................................................... 57 35
4.13.2 Test setup and configuration ...................................................................................................................... 58 36
4.13.3 Test Procedure ............................................................................................................................................ 58 37
4.13.4 Test requirements (expected results) .......................................................................................................... 59 38
 Performance tests .................................................................................................................... 60 39
5.1 Expected throughput calculation ...................................................................................................................... 61 40
5.1.1 4G LTE ...................................................................................................................................................... 61 41
5.1.2 5G NR ........................................................................................................................................................ 62 42
5.2 Downlink peak throughput .............................................................................................................................. 64 43
5.2.1 Test description and applicability ............................................................................................................... 64 44
5.2.2 Test setup and configuration ...................................................................................................................... 64 45
5.2.3 Test Procedure ............................................................................................................................................ 65 46
5.2.4 Test requirements (expected results) .......................................................................................................... 65 47
5.3 Uplink peak throughput ................................................................................................................................... 67 48
5.3.1 Test description and applicability ............................................................................................................... 67 49
5.3.2 Test setup and configuration ...................................................................................................................... 67 50
5.3.3 Test Procedure ............................................................................................................................................ 67 51

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 5

O-RAN.TIFG.E2E-Test.0-v02.00
5.3.4 Test requirements (expected results) .......................................................................................................... 68 1
5.4 Downlink throughput in different radio conditions ......................................................................................... 69 2
5.4.1 Test description and applicability ............................................................................................................... 69 3
5.4.2 Test setup and configuration ...................................................................................................................... 69 4
5.4.3 Test Procedure ............................................................................................................................................ 70 5
5.4.4 Test requirements (expected results) .......................................................................................................... 70 6
5.5 Uplink throughput in different radio conditions .............................................................................................. 71 7
5.5.1 Test description and applicability ............................................................................................................... 71 8
5.5.2 Test setup and configuration ...................................................................................................................... 71 9
5.5.3 Test Procedure ............................................................................................................................................ 72 10
5.5.4 Test requirements (expected results) .......................................................................................................... 73 11
5.6 Bidirectional throughput in different radio conditions ..................................................................................... 74 12
5.6.1 Test description and applicability ............................................................................................................... 74 13
5.6.2 Test setup and configuration ...................................................................................................................... 74 14
5.6.3 Test Procedure ............................................................................................................................................ 74 15
5.6.4 Test requirements (expected results) .......................................................................................................... 75 16
5.7 Downlink coverage throughput (link budget) .................................................................................................. 77 17
5.7.1 Test description and applicability ............................................................................................................... 77 18
5.7.2 Test setup and configuration ...................................................................................................................... 77 19
5.7.3 Test Procedure ............................................................................................................................................ 78 20
5.7.4 Test requirements (expected results) .......................................................................................................... 78 21
5.8 Uplink coverage throughput (link budget) ....................................................................................................... 79 22
5.8.1 Test description and applicability ............................................................................................................... 79 23
5.8.2 Test setup and configuration ...................................................................................................................... 80 24
5.8.3 Test Procedure ............................................................................................................................................ 80 25
5.8.4 Test requirements (expected results) .......................................................................................................... 81 26
5.9 Downlink aggregated cell throughput (cell capacity) ...................................................................................... 82 27
5.9.1 Test description and applicability ............................................................................................................... 82 28
5.9.2 Test setup and configuration ...................................................................................................................... 82 29
5.9.3 Test Procedure ............................................................................................................................................ 84 30
5.9.4 Test requirements (expected results) .......................................................................................................... 85 31
5.10 Uplink aggregated cell throughput (cell capacity) ........................................................................................... 86 32
5.10.1 Test description and applicability ............................................................................................................... 86 33
5.10.2 Test setup and configuration ...................................................................................................................... 86 34
5.10.3 Test Procedure ............................................................................................................................................ 87 35
5.10.4 Test requirements (expected results) .......................................................................................................... 88 36
5.11 Impact of fronthaul latency on downlink peak throughout .............................................................................. 89 37
5.11.1 Test description and applicability ............................................................................................................... 89 38
5.11.2 Test setup and configuration ...................................................................................................................... 89 39
5.11.3 Test Procedure ............................................................................................................................................ 90 40
5.11.4 Test requirements (expected results) .......................................................................................................... 91 41
5.12 Impact of fronthaul latency on uplink peak throughout ................................................................................... 92 42
5.12.1 Test description and applicability ............................................................................................................... 92 43
5.12.2 Test setup and configuration ...................................................................................................................... 92 44
5.12.3 Test Procedure ............................................................................................................................................ 93 45
5.12.4 Test requirements (expected results) .......................................................................................................... 94 46
5.13 Impact of midhaul latency on downlink peak throughout................................................................................ 95 47
5.13.1 Test description and applicability ............................................................................................................... 95 48
5.13.2 Test setup and configuration ...................................................................................................................... 95 49
5.13.3 Test Procedure ............................................................................................................................................ 96 50
5.13.4 Test requirements (expected results) .......................................................................................................... 97 51
5.14 Impact of midhaul latency on uplink peak throughout .................................................................................... 98 52

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 6

O-RAN.TIFG.E2E-Test.0-v02.00
5.14.1 Test description and applicability ............................................................................................................... 98 1
5.14.2 Test setup and configuration ...................................................................................................................... 98 2
5.14.3 Test Procedure ............................................................................................................................................ 98 3
5.14.4 Test requirements (expected results) .......................................................................................................... 99 4
 Services ................................................................................................................................. 101 5
6.1 Data Services ................................................................................................................................................. 101 6
6.1.1 Web Browsing .......................................................................................................................................... 102 7
6.1.1.1 Test Description .................................................................................................................................. 102 8
6.1.1.2 Test Setup ........................................................................................................................................... 102 9
6.1.1.3 Test Methodology/Procedure.............................................................................................................. 103 10
6.1.1.4 Test Expectation (expected results) .................................................................................................... 103 11
6.1.2 File upload/download ............................................................................................................................... 105 12
6.1.2.1 Test Description .................................................................................................................................. 105 13
6.1.2.2 Test Setup ........................................................................................................................................... 105 14
6.1.2.3 Test Methodology/Procedure.............................................................................................................. 106 15
6.1.2.4 Test Expectation (expected results) .................................................................................................... 106 16
6.2 Video Streaming ............................................................................................................................................ 108 17
6.2.1 Video Streaming – Stationary Test .......................................................................................................... 109 18
6.2.1.1 Test Description .................................................................................................................................. 109 19
6.2.1.2 Test Setup ........................................................................................................................................... 109 20
6.2.1.3 Test Methodology/Procedure.............................................................................................................. 110 21
6.2.1.4 Test Expectation (expected results) .................................................................................................... 110 22
6.2.2 Video Streaming – Handover between same Master eNB but different O-RUs – Intra O-DU ................ 112 23
6.2.2.1 Test Description .................................................................................................................................. 112 24
6.2.2.2 Test Setup ........................................................................................................................................... 112 25
6.2.2.3 Test Methodology/Procedure.............................................................................................................. 113 26
6.2.2.4 Test Expectation (expected results) .................................................................................................... 113 27
6.2.3 Video Streaming – Handover between same MeNB but different O-DUs – Inter O-DU Intra O-CU ..... 115 28
6.2.3.1 Test Description .................................................................................................................................. 115 29
6.2.3.2 Test Setup ........................................................................................................................................... 115 30
6.2.3.3 Test Methodology/Procedure.............................................................................................................. 116 31
6.2.3.4 Test Expectation (expected results) .................................................................................................... 116 32
6.2.4 Video Streaming – Handover between same MeNB but different O-CUs – Inter O-CU ......................... 117 33
6.2.4.1 Test Description .................................................................................................................................. 118 34
6.2.4.2 Test Setup ........................................................................................................................................... 118 35
6.2.4.3 Test Methodology/Procedure.............................................................................................................. 118 36
6.2.4.4 Test Expectation (expected results) .................................................................................................... 119 37
6.2.5 Video Streaming – Handover between different MeNB while staying connected to same SgNB ........... 120 38
6.2.5.1 Test Description .................................................................................................................................. 120 39
6.2.5.2 Test Setup ........................................................................................................................................... 121 40
6.2.5.3 Test Methodology/Procedure.............................................................................................................. 121 41
6.2.5.4 Test Expectation (expected results) .................................................................................................... 122 42
6.3 Voice Services – Voice over LTE (VoLTE) .................................................................................................. 123 43
6.3.1 VoLTE Stationary Test ............................................................................................................................ 124 44
6.3.1.1 Test Description .................................................................................................................................. 124 45
6.3.1.2 Test Setup ........................................................................................................................................... 124 46
6.3.1.3 Test Methodology/Procedure.............................................................................................................. 125 47
6.3.1.4 Test Expectation (expected results) .................................................................................................... 125 48
6.3.2 VoLTE Handover Test ............................................................................................................................. 127 49
6.3.3 Voice Service - LTE and NR handover tests ............................................................................................ 127 50
6.3.3.1 Test Description .................................................................................................................................. 127 51

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 7

O-RAN.TIFG.E2E-Test.0-v02.00
6.3.3.2 Test Setup ........................................................................................................................................... 127 1
6.3.3.3 Test Methodology/Procedure.............................................................................................................. 128 2
6.3.3.4 Test Expectation (expected results) .................................................................................................... 129 3
6.4 Voice Service – EPS Fallback ....................................................................................................................... 130 4
6.4.1 EPS Fallback Test .................................................................................................................................... 131 5
6.4.1.1 Test Description .................................................................................................................................. 132 6
6.4.1.2 Test Setup ........................................................................................................................................... 132 7
6.4.1.3 Test Methodology/Procedure.............................................................................................................. 133 8
6.4.1.4 Test Expectation (expected results) .................................................................................................... 133 9
6.5 Voice Service – Voice over NR (VoNR) ....................................................................................................... 135 10
6.5.1 Voice over NR Test .................................................................................................................................. 136 11
6.5.1.1 Test Description .................................................................................................................................. 136 12
6.5.1.2 Test Setup ........................................................................................................................................... 136 13
6.5.1.3 Test Methodology/Procedure.............................................................................................................. 137 14
6.5.1.4 Test Expectation (expected results) .................................................................................................... 137 15
6.5.2 VoNR – Intra-Distributed Unit (O-DU) handover ................................................................................... 139 16
6.5.2.1 Test Description .................................................................................................................................. 139 17
6.5.2.2 Test Setup ........................................................................................................................................... 139 18
6.5.2.3 Test Methodology/Procedure.............................................................................................................. 140 19
6.5.2.4 Test Expectation (expected results) .................................................................................................... 140 20
6.5.3 VoNR – Intra-Central Unit (O-CU) Inter-Distributed Unit (O-DU) handover ........................................ 142 21
6.5.3.1 Test Description .................................................................................................................................. 142 22
6.5.3.2 Test Setup ........................................................................................................................................... 142 23
6.5.3.3 Test Methodology/Procedure.............................................................................................................. 143 24
6.5.3.4 Test Expectation (expected results) .................................................................................................... 143 25
6.5.4 VoNR – Inter-Central Unit (O-CU) handover .......................................................................................... 144 26
6.5.4.1 Test Description .................................................................................................................................. 145 27
6.5.4.2 Test Setup ........................................................................................................................................... 145 28
6.5.4.3 Test Methodology/Procedure.............................................................................................................. 145 29
6.5.4.4 Test Expectation (expected results) .................................................................................................... 146 30
6.6 Video Service – Video over LTE (ViLTE) .................................................................................................... 147 31
6.6.1 ViLTE Stationary Test ............................................................................................................................. 148 32
6.6.1.1 Test Description .................................................................................................................................. 149 33
6.6.1.2 Test Setup ........................................................................................................................................... 149 34
6.6.1.3 Test Methodology/Procedure.............................................................................................................. 149 35
6.6.1.4 Test Expectation (expected results) .................................................................................................... 150 36
6.6.2 ViLTE Handover Test .............................................................................................................................. 151 37
6.6.3 ViLTE - LTE to NR handover test ........................................................................................................... 152 38
6.6.3.1 Test Description .................................................................................................................................. 152 39
6.6.3.2 Test Setup ........................................................................................................................................... 152 40
6.6.3.3 Test Methodology/Procedure.............................................................................................................. 153 41
6.6.3.4 Test Expectation (expected results) .................................................................................................... 153 42
6.7 Video Service – EPS Fallback ....................................................................................................................... 155 43
6.7.1 Video Service – EPS Fallback testing ...................................................................................................... 156 44
6.7.1.1 Test Description .................................................................................................................................. 156 45
6.7.1.2 Test Setup ........................................................................................................................................... 157 46
6.7.1.3 Test Methodology/Procedure.............................................................................................................. 157 47
6.7.1.4 Test Expectation (expected results) .................................................................................................... 158 48
6.8 Video Service – Video over NR .................................................................................................................... 160 49
6.8.1 Video over NR – Stationary Testing ........................................................................................................ 161 50
6.8.1.1 Test Description .................................................................................................................................. 161 51
6.8.1.2 Test Setup ........................................................................................................................................... 161 52

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 8

O-RAN.TIFG.E2E-Test.0-v02.00
6.8.1.3 Test Methodology/Procedure.............................................................................................................. 161 1
6.8.1.4 Test Expectation (expected results) .................................................................................................... 162 2
6.8.2 Video over NR – Intra-Distributed Unit (DU) handover.......................................................................... 163 3
6.8.2.1 Test Description .................................................................................................................................. 163 4
6.8.2.2 Test Setup ........................................................................................................................................... 164 5
6.8.2.3 Test Methodology/Procedure.............................................................................................................. 164 6
6.8.2.4 Test Expectation (expected results) .................................................................................................... 165 7
6.8.3 Video over NR – Intra-Central Unit (CU) Inter-Distributed Unit (DU) handover ................................... 166 8
6.8.3.1 Test Description .................................................................................................................................. 166 9
6.8.3.2 Test Setup ........................................................................................................................................... 167 10
6.8.3.3 Test Methodology/Procedure.............................................................................................................. 167 11
6.8.3.4 Test Expectation (expected results) .................................................................................................... 168 12
6.8.4 Video over NR – Intra-Central Unit (CU) handover ................................................................................ 169 13
6.8.4.1 Test Description .................................................................................................................................. 169 14
6.8.4.2 Test Setup ........................................................................................................................................... 169 15
6.8.4.3 Test Methodology/Procedure.............................................................................................................. 170 16
6.8.4.4 Test Expectation (expected results) .................................................................................................... 171 17
6.9.1 Augmented Reality ................................................................................................................................... 173 18
6.10.1 Sensors ..................................................................................................................................................... 177 19
 Security ................................................................................................................................. 180 20
7.1 gNB Security Assurance Specification (SCAS) required by 3GPP SA3  ....................................................... 180 21
7.2 Optional: DoS, fuzzing and blind exploitation types of security test ............................................................. 181 22
7.2.1 S-Plane PTP DoS Attack (Network layer) ............................................................................................... 182 23
7.2.1.1 Test description & applicability.......................................................................................................... 182 24
7.2.1.2 Test setup and configuration ............................................................................................................... 182 25
7.2.1.3 Test procedure .................................................................................................................................... 182 26
7.2.1.4 Test requirements (expected results) .................................................................................................. 183 27
7.2.2 C-Plane eCPRI DoS Attack (Network layer) ........................................................................................... 183 28
7.2.2.1 Test description & applicability.......................................................................................................... 183 29
7.2.2.2 Test setup and configuration ............................................................................................................... 183 30
7.2.2.3 Test procedure .................................................................................................................................... 183 31
7.2.2.4 Test requirements (expected results) .................................................................................................. 184 32
7.2.3 Near-RT RIC A1 Interface DoS Attack (Network layer) ......................................................................... 184 33
7.2.3.1 Test description & applicability.......................................................................................................... 184 34
7.2.3.2 Test setup and configuration ............................................................................................................... 184 35
7.2.3.3 Test procedure .................................................................................................................................... 184 36
7.2.3.4 Test requirements (expected results) .................................................................................................. 185 37
7.2.4 S-Plane PTP Unexpected Input (Network layer) ...................................................................................... 185 38
7.2.4.1 Test description & applicability.......................................................................................................... 185 39
7.2.4.2 Test setup and configuration ............................................................................................................... 185 40
7.2.4.3 Test procedure .................................................................................................................................... 185 41
7.2.4.4 Test requirements (expected results) .................................................................................................. 186 42
7.2.5 C-Plane eCPRI Unexpected Input (Network layer) ................................................................................. 186 43
7.2.5.1 Test description & applicability.......................................................................................................... 186 44
7.2.5.2 Test setup and configuration ............................................................................................................... 186 45
7.2.5.3 Test procedure .................................................................................................................................... 186 46
7.2.5.4 Test requirements (expected results) .................................................................................................. 187 47
7.2.6 Near-RT RIC A1 Interface Unexpected Input (Network layer) ............................................................... 187 48
7.2.6.1 Test description & applicability.......................................................................................................... 187 49
7.2.6.2 Test setup and configuration ............................................................................................................... 187 50
7.2.6.3 Test procedure .................................................................................................................................... 187 51

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 9

O-RAN.TIFG.E2E-Test.0-v02.00
7.2.6.4 Test requirements (expected results) .................................................................................................. 188 1
7.2.7 Blind exploitation of well-known vulnerabilities over Near-RT RIC A1 interface (Network layer) ....... 188 2
7.2.7.1 Test description & applicability.......................................................................................................... 188 3
7.2.7.2 Test setup and configuration ............................................................................................................... 188 4
7.2.7.3 Test procedure .................................................................................................................................... 188 5
7.2.7.4 Test requirements (expected results) .................................................................................................. 189 6
7.3 O-Cloud resource exhaustion type of security test (Virtualization layer) ...................................................... 189 7
7.3.1 O-Cloud side-channel DoS attack ............................................................................................................ 189 8
7.3.1.1 Test description & applicability.......................................................................................................... 189 9
7.3.1.2 Test setup and configuration ............................................................................................................... 189 10
7.3.1.3 Test procedure .................................................................................................................................... 190 11
7.3.1.4 Test requirements (expected results) .................................................................................................. 190 12
 Load and Stress Tests ........................................................................................................... 191 13
8.1 Simultaneous RRC_CONNECTED UEs ....................................................................................................... 191 14
8.1.1 Test description and applicability ............................................................................................................. 191 15
8.1.2 Test setup and configuration .................................................................................................................... 192 16
8.1.3 Test Procedure .......................................................................................................................................... 192 17
8.1.4 Test requirements (expected results) ........................................................................................................ 193 18
8.2 Benchmark of UE State Transition ................................................................................................................ 193 19
8.2.1 Test description and applicability ............................................................................................................. 193 20
8.2.2 Test setup and configuration .................................................................................................................... 194 21
8.2.3 Test Procedure .......................................................................................................................................... 194 22
8.2.4 Test requirements (expected results) ........................................................................................................ 195 23
8.3 Traffic Load Testing ...................................................................................................................................... 195 24
8.3.1 Test description and applicability ............................................................................................................. 195 25
8.3.2 Test setup and configuration .................................................................................................................... 196 26
8.3.3 Test Procedure .......................................................................................................................................... 196 27
8.3.4 Test requirements (expected results) ........................................................................................................ 197 28
8.4 Traffic Model Testing .................................................................................................................................... 197 29
8.4.1 Test description and applicability ............................................................................................................. 197 30
8.4.1.1 Traffic model ...................................................................................................................................... 198 31
8.4.2 Test setup and configuration .................................................................................................................... 200 32
8.4.3 Test Procedure .......................................................................................................................................... 200 33
8.4.4 Test requirements (expected results) ........................................................................................................ 201 34
8.5 Long hours stability Testing .......................................................................................................................... 202 35
8.5.1 Test description and applicability ............................................................................................................. 202 36
8.5.2 Test setup and configuration .................................................................................................................... 202 37
8.5.3 Test Procedure .......................................................................................................................................... 202 38
8.5.4 Test requirements (expected results) ........................................................................................................ 203 39
8.6 Multi-cell Testing .......................................................................................................................................... 203 40
8.6.1 Test description and applicability ............................................................................................................. 203 41
8.6.2 Test setup and configuration .................................................................................................................... 204 42
8.6.3 Test Procedure .......................................................................................................................................... 204 43
8.6.4 Test requirements (expected results) ........................................................................................................ 204 44
8.7 Emergency call .............................................................................................................................................. 205 45
8.7.1 Test description and applicability ............................................................................................................. 205 46
8.7.2 Test setup and configuration .................................................................................................................... 205 47
8.7.3 Test Procedure .......................................................................................................................................... 206 48
8.7.4 Test requirements (expected results) ........................................................................................................ 206 49
8.8 ETWS (Earthquake and Tsunami Warning System) ..................................................................................... 207 50
8.8.1 Test description and applicability ............................................................................................................. 207 51

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 10

O-RAN.TIFG.E2E-Test.0-v02.00
8.8.2 Test setup and configuration .................................................................................................................... 207 1
8.8.3 Test Procedure .......................................................................................................................................... 207 2
8.8.4 Test requirements (expected results) ........................................................................................................ 208 3
Annex A: Template of test report ................................................................................................................... 209 4
Annex ZZZ: O-RAN Adopter License Agreement ........................................................................................ 212 5
Section 1: DEFINITIONS .............................................................................................................................................. 212 6
Section 2: COPYRIGHT LICENSE ............................................................................................................................... 212 7
Section 3: FRAND LICENSE ........................................................................................................................................ 213 8
Section 4: TERM AND TERMINATION ...................................................................................................................... 213 9
Section 5: CONFIDENTIALITY ................................................................................................................................... 213 10
Section 6: INDEMNIFICATION ................................................................................................................................... 214 11
Section 7: LIMITATIONS ON LIABILITY; NO WARRANTY .................................................................................. 214 12
Section 8: ASSIGNMENT ............................................................................................................................................. 214 13
Section 9: THIRD-PARTY BENEFICIARY RIGHTS .................................................................................................. 214 14
Section 10: BINDING ON AFFILIATES ...................................................................................................................... 214 15
Section 11: GENERAL ................................................................................................................................................... 214 16
 17

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 11

O-RAN.TIFG.E2E-Test.0-v02.00
 Introductory Material 1
1.1 Scope of the document 2
The present document has been produced by the O-RAN ALLIANCE (O-RAN). 3
The present document provides description of the End-to-end (E2E) System Tests. 4
1.2 References 5
The following documents contain provisions which, through reference in this text, constitute provisions of the present 6
document. 7
- References are either specific (identified by date of publication, edition numbe r, version number, etc.) or 8
non-specific. 9
- For a specific reference, subsequent revisions do not apply. 10
- For a non-specific reference, the latest version applies. In the case of a reference to a 3GPP document (including 11
a GSM document), a non-specific reference implicitly refers to the latest version of that document in Release 15. 12
[1] 3GPP TR 21.905: “Vocabulary for 3GPP Specifications” 13
[2] 3GPP TR 36.104, “Evolved Universal Terrestrial Radio Access (E-UTRA); Base Station (BS) radio 14
transmission and reception” 15
[3] NGMN Alliance, “Definition of the testing framework for the NGMN 5G pre-commercial networks trials”,  16
White paper July 2019, version 3.0. Available: http://www.ngmn.org 17
[4] NGM Alliance, “NGMN 5G pre-commercial networks trials - major conclusions”, White paper, December 18
2019, version 1.0. Available: http://www.ngmn.org 19
[5] 3GPP TR 38.913: “Study on new radio access technology Radio interface protocol aspects”, March 2017. 20
[6] IETF RFC7323, “TCP Extensions for High Performance”, September 2014 21
[7] 3GPP TS 38.215, “Physical layer measurements”, September 2020  22
[8] 3GPP TS 36.133, “Requirements for support of radio resource management” 23
[9] 3GPP TS 38.133, “Requirements for support of radio resource management” 24
[10] O-RAN ALLIANCE, “O-RAN Architecture Description v2.0”, July 2020  25
[11] O-RAN ALLIANCE, “O-RAN End-to-End System Testing Framework Specification 1.0”, July 2020 26
[12] O-RAN ALLIANCE, “O-RAN Fronthaul Interoperability Test Specification (IOT) 2.0”, April 2020  27
[13] ITU-R M.2410-0, “Minimum requirements related to technical performance for IMT-2020 radio 28
interface(s)”, November 2017.  29
[14] 3GPP TR 36.814, “Further advancements for E-UTRA physical layer aspects”, March 2017  30
[15] 3GPP TS 38.306, "User Equipment (UE) radio access capabilities", December 2020 31
[16] O-RAN ALLIANCE, “O-RAN Fronthaul Control, User and Synchronization Plane Specification v5.0”, 32
November 2020 33
[17] 3GPP TS 38.300, “NR and NG-RAN Overall Description; Stage 2”, December 2020 34
[18] 3GPP TS 38.101-1, “User Equipment (UE) radio transmission and reception; Part 1: Range 1 Standalone”, 35
December 2020 36

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 12

O-RAN.TIFG.E2E-Test.0-v02.00
[19] 3GPP TS 38.101-2, “User Equipment (UE) radio transmission and reception; Part 2: Range 2 Standalone”, 1
December 2020 2
[20] 3GPP TS 38.211, “Physical channels and modulation”, December 2020 3
[21] 3GPP TS 38.213, “Physical layer procedures for control”, December 2020 4
[22] 3GPP TS 38.214, “Physical layer procedures for data”, December 2020 5
[23] 3GPP TR 38.308, “Physical Layer Aspects”, September 2017 6
[24] 3GPP TSG RAN WG1 Meeting #92 R1-1801352, “Discussion on NR UE peak data rate”, March 2018 7
[25] 3GPP TS 36.211, “Physical channels and modulation”, September 2020 8
[26] 3GPP TR 38.801, “Radio access architecture and interfaces”, March 2017 9
[27] 3GPP TS 33.511: “Security Assurance Specification (SCAS) for the next generation Node B (gNodeB) 10
network product class” (Release 16), September 2020 11
[28] 3GPP TS 23.502, “Procedures for the 5G System; Stage-2”, December 2020 12
[29] 3GPP TS 23.401 “General Packet Radio Service (GPRS)enhancements for Evolved Universal Terrestrial 13
Radio Access Network (E-UTRAN) access” 14
[30] 3GPP TS 37.340 “Overall description; Stage-2” 15
[31] 3GPP TS 38.401 “5G; NG-RAN; Architecture description 16
[32] 3GPP TS 38.473 “5G; NG-RAN; F1 Application Protocol (F1AP)” 17
[33] 3GPP TS 37.470 “W1 interface General aspects and principles”, July 2020 18
[34] NGM Alliance, “Vertical URLLC Use Cases and Requirements”, February 2020, version 2.5.4. Available: 19
http://www.ngmn.org 20
 21
1.3 Definitions and Abbreviations 22
1.3.1 Definitions 23
For the purposes of the present document, the terms and definitions given in 3GPP TR 21.905 [1] and the following 24
apply. A term defined in the present document takes precedence over the definition of the same term, if any, in 3GPP 25
TR 21.905 [1]. 26
C-Plane Control Plane: refers specifically to real-time control between O-DU and O-RU, and should not be 27
confused with the UE’s control plane 28
NSA Non-Stand-Alone network mode that supports operation of SgNB attached to MeNB 29
O-CU O-RAN Central Unit – a logical node hosting PDCP, RRC, SDAP and other control function s.  30
This can be considered short-hand for the O-CU-CP and O-CU-UP in an O-RAN system 31
O-DU O-RAN Distributed Unit: a logical node hosting RLC/MAC/High-PHY layers based on a lower 32
layer functional split. O-DU in addition hosts an M-Plane instance. 33
O-RU O-RAN Radio Unit: a logical node hosting Low-PHY layer and RF processing based on a lower 34
layer functional split. This is similar to 3GPP’s “TRP” or “RRH” but more specific in including 35
the Low-PHY layer (FFT/iFFT, PRACH extraction). O-RU in addition hosts M-Plane instance. 36
PTP Precision Time Protocol (PTP) is a protocol for distributing precise time and frequency over 37
packet networks. PTP is defined in the IEEE Standard 1588. 38

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 13

O-RAN.TIFG.E2E-Test.0-v02.00
PDCCH Physical Downlink Control Channel applies for LTE and NR air interface 1
PBCH Physical Broadcast Channel applies for LTE and NR air interface 2
SA Stand-Alone network mode that supports operation of gNB attached to a 5G Core Network 3
SCS OFDM Sub Carrier Spacing 4
SSB Synchronization Signal Block, in 5G PBCH and synchronization signal are packaged as a single 5
block 6
S-Plane Synchronization Plane: Data flow for synchronization and timing information between nodes 7
U-Plane User Plane: refers to IQ sample data transferred between O-DU and O-RU 8
 9
1.3.2 Abbreviations 10
For the purposes of the present document, the abbreviations given in 3GPP TR 21.905 [1] and the following apply. An 11
abbreviation defined in the present document takes precedence over the definition of the same abbreviation, if any, in 12
3GPP TR 21.905 [1].  13
CFI Control format indicator 14
DL Downlink: data flows from the core network towards the UE 15
DoS Denial of service 16
DUT Device under Test 17
E2E End-to-End  18
eNB Evolved Node B (LTE base station) 19
FFS For further study 20
gNB Next-generation Node B (5G NR base station) 21
IOT Interoperability Testing 22
IUT Interface under Test 23
KPI Key performance indicator 24
MCS Modulation and coding scheme  25
OTA Over the air 26
PDSCH Physical downlink shared channel 27
PUSCH Physical uplink shared channel 28
PRB Physical resource block (12 x Resource Elements per PRB) 29
QUIC  Quick UDP Internet Connections 30
SUT System under test 31
TCP Transmission Control Protocol: connection-oriented IP protocol 32
TIFG O-RAN Test and Integration Focus Group 33
UDP User Datagram Protocol: connectionless IP protocol 34
UE User Equipment: terminology for a mobile device/terminal in LTE and NR 35
UL Uplink: data flows from the UE towards the core network 36

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 14

O-RAN.TIFG.E2E-Test.0-v02.00
VoLTE Voice over LTE 1
VoNR Voice over NR 2
ViLTE Video over LTE 3
1.4 Revision Guideline 4
The contents of the present document are subject to continuing work within O -RAN and may change following formal 5
O-RAN approval. Should the O-RAN modify the contents of the present document, it will be re-released by O-RAN with 6
an identifying change of release date and an increase in version number as follows: 7
Release x.y.z 8
Where: 9
x the first digit is incremented for all changes of substance, i.e. technical enhancements, corrections, updates, 10
etc. (the initial approved document will have x=01). 11
y the second digit is incremented when editorial only changes have been incorporated in the document. 12
z the third digit included only in working versions of the document indicating incremental changes during the 13
editing process. 14
 15
 16
 17

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 15

O-RAN.TIFG.E2E-Test.0-v02.00
  Overview 1
This test specification is focused on validating the end-to-end system functionality, performance, and key features of the 2
O-RAN system as a black box.  It is based on the principles outlined in the end -to-end system test framework document 3
[11].   4
Chapter 3 specifies the test methodology, tools, and baseline configurations, which apply to the entire specification, 5
unless otherwise indicated.   6
Chapter 4 specifies the baseline functional testing of the radio access network from an end-to-end perspective.   7
Chapter 5 specifies the performance testing of the radio access network from an end-to-end perspective, with a focus 8
on end-user experienced key performance indicators (KPIs) 9
Chapter 6 specifies the test cases and KPIs around the application level services of the radio access network, including 10
data, voice and video as experienced by an end-user  11
Chapter 7 specifies the test cases around end-to-end radio access network security 12
Chapter 8 specifies load and stress test cases which assess the stability and robustness of the gNB/eNB under loaded 13
scenarios 14
Future versions of this specification may add additional chapters for test cases on timing and synchronization, 15
operations, administration, and maintenance (OAM), and other cases of relevance from an end -to-end system 16
perspective. 17
 18

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 16

O-RAN.TIFG.E2E-Test.0-v02.00
 Testing methodology and configuration 1
This chapter describes the common testing methods and configurations which will be used in the subsequent chapters. To 2
ensure fair and comparable test results among various test campaigns, consistent test setups should be utilized. The test 3
conditions should reflect the realistic operational environment as much as possible to ensure me aningful and as close to 4
real-world results as possible . This end-to-end test  specification harmonizes the test conditions, methodologies, and 5
procedures, but the test configuration (parameters) of DUT/SUT is not specified in this document. However, it is required 6
to record the complete test configuration used in the test report to enable the test to be reproduced  if needed, and for the 7
test results to potentially be used for other purposes, e.g. benchmarking or comparison.   8
There are sever al design areas in RAN, where the vendors can differentiate  such as RF performance (e.g. receiver 9
sensitivity, PA design) , radio link adaptation algorithms (e.g. radio channel estimation, MCS selection, MIMO mode 10
selection, transmission mode selection, UL power control), scheduling and overhead management (e.g. number of control 11
channels). These different approaches result in competitive advantages leading to differences in end-to-end performance 12
which can be assessed with the tests defined in this document. Hence, it is not possible to set pass/fail criteria for all the 13
tests, but the pass/fail criteria are set whenever possible, e.g. in the functional tests in Chapter 4. The expected performance 14
values are also indicated for reference network configurations.  15
Unless otherwise stated in this document , the tests are suitable for both laboratory as well as field environments. All 16
laboratory tests should be conducted over a cable, or in case of OTA tests, inside a shielded box/room , to ensure 17
repeatability. In the laboratory, radio signal strength (i.e. attenuation) on the 5G  NR path and/or 4G  LTE path can be 18
modified by using variable attenuators.  The end-user device, if used,  should be placed inside a shielded box to avoid 19
interference from external signals. The laboratory environment should allow for stable and repeatable testing conditions, 20
and it is more suitable for benchmarking. On the other hand, the field environment allows for the evaluation of complex 21
scenarios with realistic radio channel variations and behavior of the network (e.g. inter-cell interference and handovers). 22
It is assumed that the  field tests will be performed over the air (OTA). In the field, radio signal strength is modified by 23
placing the UE in different positions inside the cell.  24
Unless otherwise stated in this document , the same  operating system  (e.g. Windows 10)  with default settings and 25
configuration should be  utilized for both ends (i.e. the host applications at end -user device (test UE) and application 26
(traffic) server) in order to ensure a consistent test environment.  27
Unless otherwise stated in this document, the tests are suitable for both TDD and FDD. 28
Unless otherwise stated in this document , the following network architectures  [26] depicted in Figure 3-1 below are 29
addressed and supported: 30
• 4G LTE – Option 1 31
• 5G NR standalone (5G SA) - Option 2 32
• 5G NR non-standalone (5G NSA)  - Option 3 / Option 3a / Option 3x 33

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 17

O-RAN.TIFG.E2E-Test.0-v02.00
Figure 3-1 : The network architectures supported in this document - red dashed lines indicate control plane 1
while blue solid lines indicate user plane. 2
3.1 System under test 3
The whole O-RAN system is the System under Test (SUT) and can be viewed as a n integrated black box in the context 4
of E2E testing [11], i.e. the internal functionality and architecture of SUT is out of scope . It is expected that all involved 5
O-RAN functions and interfaces can properly interoperate together, and an end-to-end communication link can be 6
established between the end-user device and the application server  or another end -user device . The testing of 7
interoperability and conformance of the internal functions of the SUT is out of scope for this document. The SUT is 8
expected to be in service mode and run in their normal operation state. The E2E KPIs are defined across the whole end-9
to-end communication link between the end-user device and the application (traffic) server or another end-user device – 10
see Figure 3-2.  11
 12
Figure 3-2: The O-RAN system as System under Test (SUT) and E2E KPIs 13
O-RAN SystemUE Uu
System under Test (SUT)
3GPP CoreS1, NG 3GPP
Services
Other
Services
End to end (E2E) KPIs

4G LTE – Option 1

5G SA – Option 2

5G NSA – Option 3

5G NSA – Option 3a

5G NSA – Option 3x
EPC
4G LTE
5GC
5G NR
EPC
4G LTE 5G NR
EPC
4G LTE 5G NR
EPC
4G LTE 5G NR

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 18

O-RAN.TIFG.E2E-Test.0-v02.00
The end-to-end communication link includes O -RAN as well as non -O-RAN (e.g. core, end -user device) components  1
which could negatively affect the end-to-end performance, e.g. limited capability of the test UEs and/or the application 2
servers to generate/receive enough data traffic, or a bottleneck in the transport network. All these unwanted contributions 3
should be avoided or at least minimized in order to measure unbiased KPIs. In addition, there can be a performance 4
difference between different vendors and chipsets depending on the ir level of maturity. Commercial (production grade) 5
devices (e.g. test UE) are preferred  whenever possible , ensuring the tests are sufficiently documented, stable and 6
repeatable.  7
All O-RAN components [10] (such as O -CU-CP and O -CU-UP, O -DU, O -RU) and interfaces [10] (such as Open 8
Fronthaul, X2 ) included in the System under Test  are recommended to have been tested  against their respective 9
conformance and interoperability O-RAN specifications.  10
3.1.1 Load and Stress Interfaces to SUT 11
A key priority of load and stress testing is to exercise the performance of the SUT nea r or exceeding full capacity. The 12
capacity of the O -DU and O -CU within the SUT may be such that applying all traffic necessary to reach full capacity 13
may require many O-RUs which may not always be feasible. Figure 3-3 defines additional traffic insertion points where 14
traffic may be applied directly at the Open Fronthaul or F1 Interfaces to the SUT in order to provide additional background 15
UE stack traffic along with appl ication of test traffic at the Uu interface to O -RU(s) in the SUT. This should allow for 16
adequate stress on the complete SUT for execution of the E2E test scenarios.   17
 18
 19
 20
 21
 22
 23
 24
 25
 26
 27
Figure 3-3: Interfaces for applying incremental traffic to System Under Test for Load and Stress scenarios  28
3.2 Test and measurement equipment and tools 29
All the tests shall be performed in a non-intrusive manner; that is, in a manner in which the SUT is not required to support 30
any functionality or mode of operation beyond that required for normal operation in a production network. The SUT is 31
not expected to be used as test tools when deployed in a production network, and therefore it should not be used as tes t 32
tools during end-to-end testing. 33
All the measurement equipment and tools used in the tests must be properly calibrated and configured in advance in order 34
to minimize the influence of the test equipment on the measurements results. The parameters (e.g. attenuation) of cables, 35
attenuators, splitters, combiners, etc. must be also measured in advance  and compensated for in the final measurement 36
results.   37
Table 3-1 Test and measurement equipment and tools 38
O-RAN System (SUT)
UE
 Uu
 3GPP Core
S1, NG
 3GPP
Services
Other
Services
End to end (E2E) KPIs
UE+O-RU
(Optional)
OFH
UE+O-DU
(Optional)
F1
O-RU #1
RF
 OFH
O-RU #n
RF
 OFH
O-DU #1
F1
OFH
OFH
O-DU #m
F1
OFH
OFH
O-CU
S1/
NG
F1
F1
F1

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 19

O-RAN.TIFG.E2E-Test.0-v02.00
Test tool Description
Real UE and/or
UE emulator
The UE ( Real UE or UE emulator ) is used to establish stateful end -to-end connection and to
generate or receive data traffic.
The real UE used in this context as a test tool is typically a UE which is designed for commercial
or testing applications with certain test and diagnostic functions enabled for test and measurement
purposes. Such test and diagnostic functions should not affect the performance.
The real UE requires  a SIM card (real or emulated) which is pre-provisioned with subscriber
profiles. A UE emulator or multiple real UEs can be used in multi -UE test scenarios requiring
multiple UEs sessions. The UE is connected with the SUT either via RF cables or via an over the
air (OTA)  connection. In a lab environment, t he UE should be placed inside an RF shielded
box/room in order to avoid interference from external signals.
The logging tool connected to the UE is used to capture measurements and KPI logs for test
validation and reporting.
UE + O-RU
emulator
The UE + O-RU emulator is used to establish stateful connections and to generate or receive data
traffic directly at the Open fronthaul connection to an O-DU within the O-RAN SUT.
The UE + O -RU emulator is typically designed for testing applications with certain test and
diagnostic functions enabled for test and measurement purposes
The UE + O -RU emulator is used to create background traffic driven by multiple stateful UE
stacks for purposes of increasing the load on the O -DU in addition to the E2E traffic applied at
the Uu interface to the SUT
UE + O-DU
emulator
The UE + O-DU emulator is used to establish stateful connections with, and to generate or receive
data and signaling traffic directly at the F1 connection to the O-CU within the O-RAN SUT.
The UE + O -DU emulator is typically designed for testing a pplications with certain test and
diagnostic functions enabled for test and measurement purposes.
The UE + O-DU emulator is used to create background traffic driven by multiple UE connections
for purposes of increasing the load on the O -CU in addition to t he E2E traffic applied at the Uu
interface to the SUT
4G/5G Core or
Core emulator
The 4G/5G core or core emulator is used to terminate 4G/5G NAS sessions, and to support core
network procedures required for RAN (SUT) testing. 4G/5G core or core emulator must support
end-to-end connection and data transfer between Application server and Real UE/UE emulator.
IMS Core or IMS
Core emulator
The IMS Core or IMS core emulator is used to support voice and video calling services like
VoLTE, ViLTE, VoNR, Video over NR and EPS Fallback using protocols like SIP and RTP.
IMS core or IMS core emulator s hould interface with the 4G/5G core to setup dedicated
bearers/QoS Flows to support voice and video calling services.
Application
(traffic) server
The application (traffic) server is used as an endpoint for generation and/or termination of various
data traffic streams to/from Real UE(s)/UE emulator. The application server should be capable to
generate data traffic for the services under test.
The application server should be placed as close as possible to the core /core emulator,  and
connected to the core/core emulator via a transport link with sufficient capacity.
Protocol analyzer The protocol analyzer is used for test results verification, and for troubleshooting and root cause
analysis of failed tests. Note that if IPsec encryption is applied at the network interface , then it
would not be possible to use the protocol analyzer without decryption of IPsec.
Network
impairment
emulator
The network impairment emulator is used for tests which require insertion of impairment (packet
delay and/or jitter) at the network interface (e.g. Open fronthaul).

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 20

O-RAN.TIFG.E2E-Test.0-v02.00
RF attenuators
and/or Fading
generator
RF attenuators are used for tests which require radio signal attenuation. Fading generators can be
used to simulate specific radio channel conditions (e.g. Urban, Rural, High Speed Train).
RF shielded
box/room
The RF shielded box/room is used for over the air (OTA) connectivity between the UE and SUT
in the l ab environment . The RF shielded box/room should support reliable MIMO testing, if
MIMO is required.
Packet generation
tool / DoS
emulator
The packet generation tool / Denial of Service (DoS) emulator is used for DoS traffic generation
of security tests. The tool must support crafting network traffic on network layers from 2 to 7,
which conform to network protocols such as: Ethernet, IP, UDP, TCP, PTP, eCPRI, TLS,
HTTP/HTTPS. The tool is intended to be deployable in various network segments
(communication planes) according to the testing needs.
Packet capture
tool
The packet capture tool is used to capture samples of data traffic for validation, analysis, and
troubleshooting. In the case of security test cases it can be used to capture samples of le gitimate
traffic, which then can be used as templates for fuzzing attacks. The tool must support capturing
network traffic on network layers from 2 to 7, which conform to network protocols such as:
Ethernet, IP, UDP, TCP, PTP, eCPRI, TLS, QUIC, HTTP/HTTPS.  The tool is intended to be
deployable in various network segments (communication planes) according to the testing needs .
Network tap A network tap is a hardware or software device which provides access and visibility to the data
flowing across a computer network.
Fuzzing tool The protocol fuzzing tool is used for unexpected protocol input generation of security tests. The
tool must support mutating and replaying of captured network traffic on network layers from 2 to
7, which conform to network p rotocols such as: Ethernet, IP, UDP, TCP, PTP, eCPRI, TLS,
HTTP/HTTPS. The tool is intended to be deployable in various network segments
(communication planes) according to the testing needs.
Vulnerability
scanning tool
The vulnerability scanning tool is used for blind exploitation of well-known vulnerabilities during
security tests. The tool should rely on cyclically updated database of known vulnerabilities based
on Common Vulnerabilities and Exposures (CVE) and should support scanning network services
running on TCP/IP stack of protocols. The tool is intended to be deployable in various network
segments (communication planes) according to the testing needs.
NFV
benchmarking
and resource
exhaustion tool
The Network Function Virtualization (NFV) tool is used for O -Cloud system performance
measurement and resource exhaustion type of DoS attack generation. This tool should be able to
be deployed on any types of O-Cloud environment (public or private) with testing VNF(s) and/or
CNF(s) support.
3.3 Test report 1
Tests should be described in the test report with sufficient detail to allow the tests to be reproducible by different parties  2
and to enable benchmarking and comparison.  The unified reporting of test results is important for benchmarking and 3
comparison of results. The following common minimum set of configuration parameters and information about the 4
test environment needs to be reported in each test report [3]:  5
• Carrier frequency 6
• Total transmission (effective) bandwidth and number of total RBs 7
• Duplex mode (e.g. FDD, TDD) 8
• Sub-carrier spacing 9
• Carrier prefix length 10
• Slot length 11

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 21

O-RAN.TIFG.E2E-Test.0-v02.00
• Number of supported MIMO layers at both eNB/gNB and UE sides 1
• Antenna configuration (number of Tx/Rx antenna elements, e.g. 4T4R) at both eNB/gNB and UE sides 2
• Transmit power at antenna connectors at both eNB/gNB and UE sides 3
• Antenna gains at both eNB/gNB and UE sides 4
• DL/UL ratio (configuration) in case of TDD duplex mode 5
• Test UE position with respect to O-RU antenna (e.g. LOS/NLOS/nLOS) in case of field test 6
• Deployment scenario (e.g. indoor hotspot, macro/micro, dense urban, urban, rural) in case of field test 7
• List of utilized test and measurement equipment and tools (incl. logging tools, test UE(s)) including the type 8
and version. The Operating Systems (incl. the version) used at end-user device and application server should 9
be noted as well. If TCP performance has been measured, the setting of TCP configuration parameters should 10
be also noted.  11
• Information about the SUTs (e.g. O-RU, O-DU, O-CU-CP, O-CU-UP) including the type, parameters, 12
configuration, SW and HW versions, Interface profiles (e.g. Open fronthaul IOT profile [12]).  13
The template for a complete test report can be found in Annex A. Photos should also be taken as part of the test report in 14
order to illustrate the test environment. Additional parameters and counters are specified in the description of each test in 15
the subsequent chapters. 16
3.4 Data traffic  17
Full buffer and finite buffer traffic models are utilized in the tests. 18
The full buffer traffic model is characterized by a constant number of users in the cell during the test, wherein the buffers 19
of the users' data flows always have unlimited amount of data to transmit. The model is preferred due to its simplicity. 20
In the Finite buffer traffic model , a user is assigned a finite payload to transmit or receive when it arrives. The user 21
arrival process of this model captures the fact that the users in the network are not simultaneously active at the same time, 22
but they rather become active when they start a data session that require the download/upload of data. Examples of models 23
are FTP traffic model 1/2/3 [14] largely used in 3GPP simulations.  24
Data throughput can be measured at different protocol layers. Each network protocol layer adds extra overhead (header 25
information), thereby reducing the data throughput available to the layer above.  The highest throughput is provided at 26
the physical layer including user data and the overhead from higher protocol layers . The data throughput at the RLC 27
(Radio Link Control) layer is independent from radio -specific overhead and therefore well -suited for comparison with 28
other access technologies. The application layer throughput is the net throughput seen by user applications operating on 29
top of either UDP, TCP or Q UIC – for example, the typical FTP overhead is around 3 -5%, typical HTTP overhead is 30
around 30% compared to RLC throughput. Unless otherwise stated in this document, the reported throughput (user data 31
rate) shall consider all the overhead (control channels, reference signals).  32
UDP (User Datagram Protocol), TCP (Transmission Control Protocol) and QUIC (Quick UDP Internet Connections) are 33
typical transport layer protocols utilized in the tests.  34
UDP is a simple, connection-less transport layer protocol which does not guarantee error -checking and recovery. UDP 35
throughput is more suitable for benchmarking as it is not affected by the system configuration parameters. UDP is also 36
faster, lighter (less overhead) and more efficient than TCP.   37
TCP is reliable, connection-oriented transport layer protocol which includes error-checking and recovery, and guarantees 38
data delivery with preserved order of data packets . The performance of a TCP connection can be impacted by various 39

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 22

O-RAN.TIFG.E2E-Test.0-v02.00
factors such as end-to-end latency, number of retransmissions, packet loss, and TCP configuration parameters such as 1
window size, window scale, timestamps, etc. [6]. The default values of TCP configuration parameters can also vary in 2
different Operation Systems (incl. different versions of the same OS) which are used at the end-user device (test UE) and 3
application (traffic) server. It is recommended to use the same OS (incl. the version) with default setting at the end-user 4
device (test UE) and application server. Since the settings and behavior of TCP connections cannot be easily unified and 5
normalized, the measurement of TCP performance is recommended only as an illustrative indicator , and UDP 6
performance should be used for the benchmarking and assessment of system performance.  7
QUIC (Quick UDP Internet Connections) is a general-purpose transport layer protocol built on top of UDP to support 8
the next generation of application layer protocols. QUIC provides features like connection establishment, congestion 9
control, stream multiplexing and forwar d error correction to provide a secure and reliable connection -oriented protocol 10
over UDP. QUIC is being used as the standard transport mechanism for HTTP/3.  11
In addition, the following application layer protocols are utilized in the tests.  12
Hypertext Transfer Protocol (HTTP) is the application layer protocol used in the internet. HTTP is a stateless protocol 13
which follows the request-response model between client and the server. The client places a request for a resource to the 14
server, and the server responds back to the client with requested resource and/or the appropriate response code. HTTP’s 15
support for headers between the client-server makes this protocol simple, extensible, and powerful. 16
 Session Initiation Protocol (SIP) is an application layer signalling protocol for real-time sessions like IP telephony. This 17
is a text-based protocol which allows negotiation between two end points to initiate a session, maintain the session and 18
terminate the session. SIP is the default signalling protocol used in the telecom network for VoLTE, ViLTE, VoNR and 19
Video over NR. 20
Real-time Transport Protocol (RTP) is an application layer protocol used to transmit real -time data such as audio and 21
video over IP network. RTP is the default data plane protocol used in the teleco m network for services like VoLTE, 22
ViLTE, VoNR and Video over NR. RTP does not guarantee Quality of Service but works in conjunction with Real Time 23
Control Protocol (RTCP) to detect and convey packet loss and jitter information.  24
File Transfer Protocol  (FTP) is an application layer protocol to transfer files on a computer network. FTP follows a 25
client-server model where the client can upload the file to the server, or download he file from the server. FTP protocol 26
uses two separate connections between the client and the server – one for control and the other one for data or transfer of 27
file. FTP along with multiple variants of the protocol have become the de -facto standards to transfer file on the internet. 28
3.5 Mobility Classes 29
The following classes of mobility are defined [13]: 30
• Stationary: 0 kph 31
• Pedestrian: 0 kph to 10 kph (typical value 4 kph) 32
• Vehicular: 10 kph to 120 kph (typical value 50 kph) 33
• High speed vehicular: 120 kph to 500 kph (typical value 150 kph) 34
High speed vehicular speeds close to 500 kph are mainly used for high speed trains. 35
3.6 Radio conditions 36
The radio signal quality is described by the radio parameters such as RSRP and SINR. These radio parameters are defined 37
differently in LTE and 5G NR. 38

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 23

O-RAN.TIFG.E2E-Test.0-v02.00
• LTE RSRP (Reference Signal Received Power) [7]  is defined as the linear average over the power contributions 1
(in [W]) of the resource elements that carry cell -specific reference signals  (CRS) within the considered 2
measurement frequency bandwidth. The RSRP is reported from UE back to eNB. The reporting range of RSRP 3
is defined from -140dBm to -44dBm [8].   4
• LTE SINR (Signal to Noise and Interference Ratio) has not been formerly  defined in the 3GPP specification. 5
The UE does not send the results back to eNB. The SINR is measured and used only in UE. Specific 6
implementations may vary, and it is up to the manufacturer to decide, how to implement this measurement. This 7
is making difficult to compare results of different devices. In [7], SINR is defined as the linear average over the 8
power contribution (in [W]) of the resource elements carrying cell-specific reference signals divided by the linear 9
average of the noise and interference power contribution (in [W]) over the resource elements carrying cell -10
specific reference signals within the same frequency bandwidth. 11
• 5G SS-RSRP (Synchronization Signal based Reference Signal Received Power)  [7] is defined as the linear 12
average over the power contributions (in [W]) of the resource  elements that carry secondary synchronization  13
(SS) signals. SS-RSRP is the equivalent of the RSRP parameter used in LTE systems . The reporting range of 14
SS-RSRP is defined from -140dBm to -40dBm [9].   15
• 5G SS-SINR [7] is defined as the linear average over the power contribution (in [W]) of the resource elements 16
carrying secondary synchronisation signals divided by the linear average of the noise and interfe rence power 17
contribution (in [W]) over the resource elements carrying secondary synchronisation signals within the same 18
frequency bandwidth . The SS -SINR is reported from UE back to gNB. The reporting range of SS -SINR is 19
defined from -23dB to 40dB.  20
It is worth to note that in 5G , the Channel State Information Reference Signal (CSI -RS) can also be used for RSRP and 21
SINR measurements. Due to different transmit powers of CSI-RS and SS, CSI-RS-based SINR and RSRP measurement 22
values are usually greater than SS-based SINR and RSRP.   23
The minimum coupling loss (MCL) [2] between O-RU (antenna) and UE must be ensured: 24
• Macro cell deployment scenario (wide area BS): MCL = 70dB  corresponding to minimal O -RU (antenna) to 25
UE distance along the ground equal to around 35 m 26
• Small cell (micro cell) deployment scenario (medium range BS): MCL = 53dB corresponding to minimal O-RU 27
(antenna) to UE distance along the ground equal to around 5 m 28
• Pico cell deployment scenario (local area BS): MCL = 45dB  corresponding to minimal O -RU (antenna) to UE 29
distance along the ground equal to around 2 m 30
The radio parameters (RSRP, SINR) should be measured across the entire range  covering scenarios from  cell centre to 31
cell edge. Based on the test results, the RSRP and SINR distribution stati stics can be calculated and described as a 32
cumulative distribution function (CDF) curve. According to the CDF curve, the four types of radio conditions can be 33
defined as: excellent (95%-100%), good (80%-90%), fair (40%-60%) and poor (5%-15%) [3]. Table 3-2 shows the RSRP 34
and SINR thresholds for various radio conditions.Table 3-2 RSRP and SINR thresholds for various radio conditions 35
Note that the RSRP values should primarily be used for UL assessments, and the SINR values for DL assessment. 36
 37
Table 3-2 RSRP and SINR thresholds for various radio conditions 38
Radio
conditions
RSRP (dBm)
SS-RSRP (dBm)
DL SINR (dB)
DL SS-SINR (dB)

Excellent
(cell centre)
> -75

> 25

• Utilization of the highest possible MCS,
transport block size and MIMO rank
• Peak performance measurements
• Negligible interference from neighbor cells

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 24

O-RAN.TIFG.E2E-Test.0-v02.00
Good -75 to -90
(typical value = -85)
15 to 20
(typical value = 17)

Fair -90 to -105
(typical value = -95)
5 to 10
(typical value = 7)

Poor
(cell edge)
< -105
(typical value = -110)
< 5
(typical value = 3)
• Minimum performance measurements
• Strong interference from neighbor cells
There are many different factors that influence signal strength and quality during the field testing; these factors include, 1
but are not limited, to the following: 2
• Proximity to the cellular tower (antenna) 3
• Load in neighbor cells 4
• Surrounding physical barriers (mountains, buildings, etc.) 5
• Weather conditions  6
3.7 Inter-cell interference 7
The tests are conducted either in a single cell scenario without any inter-cell interference or in a multi-cell scenario where 8
the serving cell is surrounded by neighboring cells generating traffic load (interference on the serving cell) in the downlink 9
or uplink directions.  10
Generating a re alistic traffic load is important for meaningful results. As the number of real UE s are always a limiting 11
factor, artificial (dummy) traffic load and interference generation could be used.  12
In the single cell scenario, the serving cell is isolated, and all the surrounding neighbor cells are turned off (neither control 13
nor data channels are used).  14
In the multi-cell scenario, all the neighbor cells are turned on. The following load setups are defined: 15
• Load 0% - all the surrounding neighbor cells are turned on without any data traffic and end-user device attached. 16
Inter-cell interference is generated only on control channels (broadcasting, synchronization channels) without 17
any inter-cell interference on data channels.  18
• Load 30% - all the surrounding neighbor cells are turned on with data traffic. Inter-cell interference is generated 19
both on control and data channels. The level of interference on data channel is controlled by the amount of data 20
traffic. The interference level of 30% in downlink means that 30% of downlink PRBs are randomly occupied 21
with a dummy traffic. In uplink, t his interfer ence level corresponds to 3dB  rise of  IoT (Interference over 22
Thermal) noise at the receiver side (i.e. eNB/gNB antenna(s)). The received interference noise from the UEs of 23
neighbor cells uplink transmission should lead to 3dB rise of receiver’s noise power  [3]. 24
• Load 50% - all the surrounding neighbor cells are turned on with data traffic. Inter-cell interference is generated 25
both on control and data channels. The level of interference on data channel is controlled by the amount of data 26
traffic. The interference level of 50% in downlink means that 50% of downlink PRBs are randomly occupied 27
with a dummy traffic. In uplink, this interference level correspo nds to 5dB rise of IoT (Interference over 28
Thermal) noise at the receiver side (i.e. eNB/gNB antenna(s)). 29
• Load 70% - all the surrounding neighbor cells are turned on with data traffic. Inter-cell interference is generated 30
both on control and data channels. The level of interference on data channel is controlled by the amount of data 31
traffic. The interference level of 70% in downlink means that 70% of downlink PRBs are randomly occupied 32
with a dummy traffic. In uplink, this interference level corresponds to 7 dB rise of IoT (Interference over 33
Thermal) noise at the receiver side (i.e. eNB/gNB antenna(s)). 34
• Load 100% - fully loaded multi -cell scenario  generating the highest possible inter -cell interference . All the 35
surrounding neighbor cells are turned on with data traffic. Inter-cell interference is generated both on control and 36

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 25

O-RAN.TIFG.E2E-Test.0-v02.00
data channels. The level of interference on data channel is controlled by the amount of data traffic. The 1
interference level of 100% in downlink means that 100% of downlink PRBs are occupied with a dummy traffic. 2
In uplink, this interference level corresponds to 9dB rise of IoT (Interference over Thermal) noise at the receiver 3
side (i.e. eNB/gNB antenna(s)). 4
3.8 Spectral efficiency 5
The spectral (or spectrum) efficiency  (SE) is an important crit erion for fair performance assessment and benchmarking  6
of different systems when various transmission bandwidths, duplex modes (FDD/TDD) and TDD DL/UL configurations 7
are normalized. 8
The spectral efficiency is calculated by dividing the data throughput by the aggregated channel bandwi dth (incl. guard 9
bands) in DL or UL assuming single user and FDD duplex mode. The corresponding link frame structure is fully (100%) 10
utilized in frequency and time domains.   11
𝑆𝐸𝐹𝐷𝐷_𝐷𝐿 [𝑏𝑝𝑠 𝐻𝑧⁄ ] = 𝐷𝐿 𝑑𝑎𝑡𝑎 𝑡ℎ𝑟𝑜𝑢𝑔ℎ𝑝𝑢𝑡 [𝑏𝑝𝑠]
𝐷𝐿 𝑐ℎ𝑎𝑛𝑛𝑒𝑙 𝑏𝑎𝑛𝑑𝑤𝑖𝑑𝑡ℎ [𝐻𝑧] 12
𝑆𝐸𝐹𝐷𝐷_𝑈𝐿 [𝑏𝑝𝑠 𝐻𝑧⁄ ] = 𝑈𝐿 𝑑𝑎𝑡𝑎 𝑡ℎ𝑟𝑜𝑢𝑔ℎ𝑝𝑢𝑡 [𝑏𝑝𝑠]
𝑈𝐿 𝑐ℎ𝑎𝑛𝑛𝑒𝑙 𝑏𝑎𝑛𝑑𝑤𝑖𝑑𝑡ℎ [𝐻𝑧] 13
In case of TDD where the same spectrum is used at different times for the uplink and downlink, the spectral efficiency is 14
in addition multiplied  by the fraction of resources ( slots and symbols, not including switching gap) allocated to the 15
particular link direction within 10ms radio frame. 16
𝑆𝐸𝑇𝐷𝐷_𝐷𝐿 [𝑏𝑝𝑠 𝐻𝑧⁄ ] = 𝐷𝐿 𝑑𝑎𝑡𝑎 𝑡ℎ𝑟𝑜𝑢𝑔ℎ𝑝𝑢𝑡 [𝑏𝑝𝑠]
𝑐ℎ𝑎𝑛𝑛𝑒𝑙 𝑏𝑎𝑛𝑑𝑤𝑖𝑑𝑡ℎ [𝐻𝑧] ∗ 𝑛𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑡𝑜𝑡𝑎𝑙 𝑠𝑦𝑚𝑏𝑜𝑙𝑠 𝑝𝑒𝑟 𝑓𝑟𝑎𝑚𝑒
𝑛𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝐷𝐿 𝑠𝑦𝑚𝑏𝑜𝑙𝑠 𝑝𝑒𝑟 𝑓𝑟𝑎𝑚𝑒  17
 18
𝑆𝐸𝑇𝐷𝐷_𝑈𝐿 [𝑏𝑝𝑠 𝐻𝑧⁄ ] = 𝑈𝐿 𝑑𝑎𝑡𝑎 𝑡ℎ𝑟𝑜𝑢𝑔ℎ𝑝𝑢𝑡 [𝑏𝑝𝑠]
𝑐ℎ𝑎𝑛𝑛𝑒𝑙 𝑏𝑎𝑛𝑑𝑤𝑖𝑑𝑡ℎ [𝐻𝑧] ∗ 𝑡𝑜𝑡𝑎𝑙 𝑛𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑠𝑦𝑚𝑏𝑜𝑙𝑠 𝑝𝑒𝑟 𝑓𝑟𝑎𝑚𝑒
𝑛𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑈𝐿 𝑠𝑦𝑚𝑏𝑜𝑙𝑠 𝑝𝑒𝑟 𝑓𝑟𝑎𝑚𝑒  19
 20
 21
 22

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 26

O-RAN.TIFG.E2E-Test.0-v02.00
 Functional tests 1
This chapter describes the tests evaluating and assessing the functionality of the radio access network from a network 2
end-to-end perspective . The focus of the testing is on the end -user functionality based on 3GPP and O -RAN 3
specifications.  Pass-fail criteria are defined for the tests wherever possible.  4
The general test methodologies and configurations are mentioned in Chapter 3.  5
Unless otherwise stated in the chapter, the tests are suitable and can be conducted in both laboratory as well as field 6
environments, with pros and cons of both environments as described in Chapter 3.  7
The following end-to-end functional tests are defined in this chapter as an extension of the NGMN testing framework [3]:  8
• LTE/5G NSA attach and detach of a single UE  9
• LTE/5G NSA attach and detach of multiple UEs  10
• 5G SA registration and deregistration of a single UE  11
• Intra- O-DU mobility  12
• Inter- O-DU mobility  13
• Inter- O-CU mobility  14
• 5G SA registration and deregistration to Single network slices  15
• 5G SA registration and deregistration to multiple network slices  16
• Idle Mode Intra- O-DU mobility  17
• Idle Mode Inter- O-DU mobility  18
• Idle Mode Inter- O-CU mobility 19
• 5G/4G Inter-RAT mobility - 5G to LTE handover 20
• 5G/4G Inter-RAT mobility - LTE to 5G handover 21
The test description, setup and procedures are detailed in the following sections for each test case. 22
4.1 LTE/5G NSA attach and detach of single UE 23
4.1.1 Test description and applicability 24
The purpose of this test is to validate E2E O-RAN C-plane functionality with a single UE. These tests are valid for 25
either LTE or 5G NSA. In this test scenario, the successful attach and detach procedure shall be validated by the “Power 26
ON” and “Power OFF” of a single UE, as described in the following specifications: 27
1. LTE Attach as per 3GPP TS 23.401 [29], Section 5.3.2.1 E-UTRAN Initial Attach  28
2. LTE Detach as per 3GPP TS 23.401 [29], Section  5.3.8.2.1 UE-initiated Detach procedure for E-UTRAN 29
3. 5G NSA Attach as per 3GPP TS 23.401 [29], Section 5.3.2.1 E-UTRAN Initial Attach and 3GPP TS 37.340 30
[30] Section 10.2.1 EN-DC (Secondary Node Addition)  31

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 27

O-RAN.TIFG.E2E-Test.0-v02.00
4. 5G NSA Detach as per 3GPP 23.401, Section  5.3.8.2.1 UE-initiated Detach procedure for E-UTRAN and 1
3GPP TS 37.340 [30] Section 10.4.1 EN-DC (Secondary Node Release) [29] 2
The test procedure shall be performed in excellent radio conditions for 10 iterations. Attach success rate, detach success 3
rate,  and attach latency shall be measured and captured.  4
4.1.2 Test setup and configuration 5
The test setup is a single cell scenario (i.e. isolated cell without any inter-cell interference – see Section 3.7) with a 6
stationary UE (real or emulated) placed under excellent radio conditions as defined in Section 3.6, using LTE RSRP 7
(for LTE) or 5G SS-RSRP (for 5G NSA) as the metric.  Within the cell there should be only one active UE. The test is 8
suitable for lab as well as field environments.   9
Test configuration: The test configuration is not specified. The utilized test configuration (parameters) should be 10
recorded in the test report.   11
Laboratory setup : The radio conditions experienced by the UE can be modified using a variable attenuator/fading 12
generator inserted between the antenna connectors (if available) of the O-RU and the UE, or appropriately emulated using 13
a UE emulator. The test environment should be setup to achieve excellent radio conditions ( LTE RSRP (for LTE) or 5G 14
SS-RSRP (for 5G NSA)  as defined in Section 3.6) for the UE, but t he minimum coupling loss (see  Section 3.6) should 15
not be exceeded.  The UE should be placed inside an RF shielded box or RF shielded room if the UE is not connected via 16
cable. 17
Field setup: The UE is placed in the centre of the cell close to the radiated eNB/gNB antenna(s), where excellent radio 18
conditions (LTE RSRP (for LTE) or 5G SS -RSRP (for 5G NSA)   as defined in Section 3.6) should be observed. The 19
minimum coupling loss (see Section 3.6) should not be exceeded. 20
Please refer to Figure 5-1 for the E2E test setups for LTE and 5G NSA. 21
4.1.3 Test Procedure 22
The test steps below are applicable for either LTE or 5G NSA: 23
1. The test setup is configured according to the test configuration. The test configuration should be recorded in the test 24
report. The serving cell under test is activated and unloaded. All other cells are powered off. 25
2. The UE (real or emulated UE) is placed under excellent radio conditions (Cell centre close to radiated eNB/gNB 26
Antenna) as defined by LTE RSRP (for LTE) or 5G SS-RSRP (for 5G NSA) in Section  3.6.  27
3. The End-to-end setup shall be operational for LTE or 5G NSA  as applicable for the test scenario, and there should 28
not be any connectivity issues.  29
4. Start the logs to capture the call flow and signalling messages  30
5. “Power ON” the UE to attach to the LTE or 5G NSA cell. Wait for a successful attach.  31
6. “Power OFF” the connected UE to detach from the network. Wait for a successful detach. 32
7. Stop and save the test logs. The logs should be captured and kept for test result reference and measurements  33
8. Repeat steps 4 to 7, for a total of 10 times and record the KPIs mentioned in Section 4.1.4.  34

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 28

O-RAN.TIFG.E2E-Test.0-v02.00
4.1.4 Test requirements (expected results) 1
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 2
should be captured and reported in the test report for performance assessment 3
• Radio parameters such as RSRP, RSRQ 4
• KPIs  mentioned in Table 4-1 and Table 4-2 5
Validate the successful procedures from the collected logs. Expected success rate for Attach/Detach and Secondary Node 6
Addition/Release KPI is 100%. The attach-detach procedure should pass 10 consecutive time s to mark the test case  as 7
passing. The gap analysis should be provided for the measured and the expected target KPIs. 8
• LTE Attach-Detach test case validation and KPI measurements  9
• Validate successful attach 10 times with LTE cell (Refer to 3GPP TS 23.401 [29], Section 5.3.2.1 E-10
UTRAN Initial Attach ). In the UE logs or applications installed on UE, check that UE is attached to 11
correct cell (example PCI, Global eNB ID, ARFCN as per test configuration)  12
• Measure the attach success rate by validating attach request  and attach complete for each iteration. 13
Record the attach success rate in Table 4-1 14
• Measure the attach latency by calculating the time between attach request to attach complete. Capture 15
the latency for each iteration and s ort the latency value observed for each iteration in ascending order. 16
Record the Minimum, Average (Sum of all latency value/ Total Iterations, Total Iterations are 10  in this 17
case) and Maximum latency value observed in Table 4-2 18
• Validate successful detach attach with LTE cell (Refer to 3GPP TS 23.401 [29], Section 5.3.8.2.1 UE-19
initiated Detach procedure for E -UTRAN). Signalling connection release should be validated from 20
message flow (UE context release and RRC connection release messages). Measure the detach success 21
rate by validating detach request and detach accept for each ite ration. Record the detach success rate in 22
Table 4-1 23
• 5G NSA Attach-Detach test case validations and KPI measurements 24
• Validate successful multiple attach es with 5G NSA cell (3GPP TS 23.401 [29], Section 5.3.2.1 E -25
UTRAN Initial Attach and 3GPP TS 37.340 [30] Section 10.2.1 EN-DC for Secondary Node Addition)). 26
In the UE logs or applications installed on UE, check that UE is attached to correct cell (example PCI, 27
Global eNB ID/Global gNB, ARFCN/NR-ARFCN as per test configuration for LTE /5G cells ) 28
• Measure the attach success rate by validating attach request and attach complete for each iteration. Also 29
measure the secondary node addition success rate by validating the SgNB addition request and SgNB 30
reconfiguration complete as per flow 3GPP TS 37.340 [30] Section 10.2.1 EN-DC for each iteration.  31
Record the attach success rate  and secondary node addition success rate  in Table 4-1 32
• Validate successful detach attach (LTE Detach and 5G Secondary Node release) with 5G NSA cell 33
(Refer to 3GPP TS 23.401 [29], Section 5.3.8.2.1 UE-initiated Detach procedure for E -UTRAN and 34
3GPP TS 37.340 [30] Section 10.4.1 EN -DC for Secondary Node Release) ). Signalling connection 35
release should be validated  the from message flow (UE context release and RRC connection release 36
messages). 5G secondary node should also get release d successfully.  Measure the detach succe ss rate 37
by validating detach request and detach accept for each iteration. Record the detach success rate in Table 38
4-1 39
Table 4-1 KPI to be captured for single UE attach-detach test case 40

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 29

O-RAN.TIFG.E2E-Test.0-v02.00
LTE KPI 5G NSA KPI
Attach Success
Rate
Detach Success
Rate
Attach
Success Rate
Detach
Success Rate
SgNB addition
Success rate

 1
Table 4-2 Latency KPI for attach 2
LTE Attach Time (millisecond)
Minimum Maximum Average

4.2 LTE/5G NSA attach and detach of multiple UEs 3
4.2.1 Test description and applicability 4
The purpose of this test is to validate E2E O-RAN C-plane functionality with multiple UEs. These tests are valid for 5
either LTE or 5G NSA. In this test scenario, the successful attach and detach procedure shall be validated by the “Power 6
ON” and “Power OFF” of multiple (at least 2) UEs, as described in the following specifications: 7
1. LTE Attach as per 3GPP TS 23.401 [29], Section 5.3.2.1 E-UTRAN Initial Attach  8
2. LTE Detach as per 3GPP TS 23.401 [29], Section  5.3.8.2.1 UE-initiated Detach procedure for E-UTRAN 9
3. 5G NSA Attach as per 3GPP TS 23.401 [29], Section 5.3.2.1 E-UTRAN Initial Attach and 3GPP TS 37.340 10
[30] Section 10.2.1 EN-DC (Secondary Node Addition)  11
4. 5G NSA Detach as per 3GPP 23.401, Section  5.3.8.2.1 UE-initiated Detach procedure for E-UTRAN and 12
3GPP TS 37.340 [30] Section 10.4.1 EN-DC (Secondary Node Release) [29] 13
The test procedure shall be performed in excellent radio conditions for 10 iterations. Attach success rate, detach success 14
rate,  and attach latency shall be measured and captured. 15
4.2.2 Test setup and configuration 16
The network setup is a single cell scenario (i.e. isolated cell without any inter-cell interference – see Section 3.7) with 17
multiple stationary UEs (real or emulated) placed under excellent radio conditions as defined in Section 3.6, using LTE 18
RSRP (for LTE) or 5G SS-RSRP (for 5G NSA) as the metric.  The test is suitable for lab as well as field environments.   19
Test configuration: The test configuration is not specified. The utilized test configuration (parameters) should be 20
recorded in the test report.   21
Laboratory setup : The radio conditions experienced by the UEs can be modified using variab le attenuators/fading 22
generator inserted between the antenna connectors (if available) of the O-RU and the UE, or appropriately emulated using 23
a UE emulator. The test environment should be setup to achieve excellent radio conditions (LTE RSRP (for LTE) or 5G 24
SS-RSRP (for 5G NSA)  as defined in Section 3.6) for the UEs, but the minimum coupling loss (see Section 3.6) should 25
not be exceeded.  The UEs should be placed inside an RF shielded box or RF shielded room if the UEs are not connected 26
via cable. 27

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 30

O-RAN.TIFG.E2E-Test.0-v02.00
Field setup : The multiple UEs are placed in the centre of the cell close to the radiated eNB/gNB antenna(s), where 1
excellent radio conditions (LTE RSRP and 5G SS -RSRP  as defined in Section 3.6) should be observed. The minimum 2
coupling loss (see 3.6) should not be exceeded. 3
Please refer to Figure 5-1 for the E2E test setups for LTE and 5G NSA. 4
4.2.3 Test Procedure 5
The test steps below are applicable for either LTE or 5G NSA: 6
1. The test setup is configured according to the test configuration. The test configuration should be recorded in 7
the test report. The serving cell under test is activated and unloaded. A ll other cells are powered off. 8
2. The multiple UEs (real or emulated) are placed under excellent radio conditions (Cell centre close to radiated 9
eNB/gNB Antenna) as defined by LTE RSRP (for LTE) or 5G SS-RSRP (for 5G NSA) in Section  3.6.  10
3. The End-to-end setup shall be operational for LTE or 5G NSA as applicable for the test scenario, and there 11
should not be any connectivity issues.  12
4. Start the logs to capture the call flow and signalling messages  13
5. “Power ON” the multiple connected UEs to attach to the LTE or 5G NSA cell.  Wait for the successful attach 14
of all UEs.  15
6. “Power OFF” the multiple UEs to detach from the network. Wait for the successful detach of all UEs 16
7. Stop and save the test logs. The logs should be captured and kept for test result reference and measurements  17
8. Repeat steps 4 to 7, for a total of 10 times and record the KPIs mentioned in Section 4.2.4 18
4.2.4  Test requirements (expected results) 19
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 20
should be captured and reported in the test report for performance assessment 21
• Radio parameters such as RSRP, RSRQ 22
• KPIs  mentioned in Table 4-3 and Table 4-4 23
Validate the successful procedures for each UE from the collected logs. Expected success rate for Attach/Detach and 24
Secondary Node Addition/Release KPI is 100%. The attach-detach procedure should pass 10 consecutive time s for all 25
UEs to mark the test case as passing. The gap analysis should be provided for the measured and the expected target KPIs.   26
• LTE Attach-Detach test case validations and KPI measurements  27
• Validate successful attach of each UE 10 times with LTE cell (Refer to 3GPP TS 23.401 [29], Section 28
5.3.2.1 E -UTRAN Initial Attach ). In the UE logs or applications installed on UE, check that UE is 29
attached to correct cell (example PCI, Global eNB ID, ARFCN as per test configuration) 30
• Measure the attach success rate by validating attach request and attach complete for each iteration. 31
Record the attach success rate in Table 4-3 for each UE. 32
• Measure the attach latency by calculating the time between Attach Request to attach complete. Capture 33
the latency for each iteration and s ort the latency value observed for each iteration in ascending order. 34
Record the Minimum, Average (Sum of all latency value/ Total Iterations, Total Iterations are 10  in this 35
case) and Maximum latency value observed in Table 4-4 for each UE 36

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 31

O-RAN.TIFG.E2E-Test.0-v02.00
• Validate successful detach attach with LTE cell (Refer to 3GPP TS 23.401 [29], Section 5.3.8.2.1 UE-1
initiated Detach procedure for E -UTRAN). Signalling connection release should be validated from 2
message flow (UE context release and RRC connection release messages). Measure the detach success 3
rate by validating detach request and detach accept for ea ch iteration. Record the detach success rate in 4
Table 4-3 5
• 5G NSA Attach-Detach test cases validations and KPI measurements 6
• Validate successful multiple UE attach with 5G NSA cell (3GPP TS 23.401 [29], Section 5.3.2.1 E-7
UTRAN Initial Attach and 3GPP TS 37.340 [30] Section 10.2.1 EN-DC for Secondary Node Addition)). 8
In the UE logs or applications installed on UE, check that UE is attached to correct cell (example PCI, 9
Global eNB ID/Global gNB, ARFCN/NR-ARFCN as per test configuration for LTE /5G cells ) 10
• Measure the attach success rate by validating attach request and attach complete for each iteration. Also 11
measure the secondary node addition success rate by validating the SgNB addition request and SgNB 12
reconfiguration complete as per flow 3GPP 37.340 Section 10.2.1 EN-DC for each iteration.  Record the 13
attach success rate  and secondary node addition success rate in Table 4-3 for each UE 14
• Validate successful detach attach (LTE Detach and 5G Secondary Node release) with 5G NSA cell 15
(Refer to 3GPP TS 23.401 [29] Section, 5.3.8.2.1 UE-initiated Detach procedure for E -UTRAN and 16
3GPP TS 37.340 [30] Section 10.4.1 EN -DC for Secondary Node Release) ). Signalling connection 17
release shoul d be validated from the message flow (UE context release and RRC connection release 18
messages). 5G secondary node should also get release d successfully.  Measure the detach success rate 19
by validating detach request and detach accept for each iteration. Record the detach success rate in Table 20
4-3 for each UE 21
Table 4-3 KPI to be captured for multi-UE attach-detach test case 22
LTE KPI 5G NSA KPI
Attach
Success Rate
Detach
Success Rate
Attach
Success
Rate
Detach
Success
Rate
SgNB
addition
Success rate

 23
Table 4-4 Latency KPI for multi-UE attach 24
LTE Attach Time (millisecond)
Minimum Maximum Average

4.3 5G SA registration and deregistration of single UE 25
4.3.1 Test description and applicability 26
The purpose of the test is to verify the full registration and de-registration procedure with a single UE. The test also 27
verifies the PDU session establishment and release procedures. 28
The test focuses on the procedure of ‘Initial registration’ as defined in 3GPP TS 23.502 [28] Section 4.2.2.2.2. 29
The test focuses on the procedure of ‘UE-initiated de-registration’ as defined in 3GPP TS 23.502 [28] Section 4.2.2.3.2. 30
This test also validates PDU session establishment and release procedures .  31

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 32

O-RAN.TIFG.E2E-Test.0-v02.00
The test validates the 3GPP standard registration/de-registration procedure and the latency of the procedure. Bi-1
directional data transmission shall be observed before the de-registration procedure to verify the stability of the network 2
slice. 3
4.3.2 Test setup and configuration 4
The test setup is a single cell scenario (i.e. isolated cell without any inter-cell interference – see Section 3.7) with a 5
stationary UE (real or emulated) placed under excellent radio conditions as defined in Section 3.6, using SS-RSRP as 6
the metric. Within the cell there should be only one active UE. The application server should be placed as close as 7
possible to the core/core emulator and connected to the core/core emulator via a transport link with enough capacity so 8
as not to limit the expected data throughput. The UE, RAN, and 5G Core shall support the network slicing, at least for 9
one Single Network Slice Selection Assistance Information (S-NSSAI). The test is suitable for lab as well as field 10
environments.   11
Test configuration: The test configuration is not specified. The utilized test configuration (parameters) should be 12
recorded in the test report.   13
Laboratory setup : The radio conditions experienced by  the UE can be modified using a variable attenuator/fading 14
generator inserted between the antenna connectors (if available) of the O-RU and the UE, or appropriately emulated using 15
a UE emulator. The test environment should be setup to achieve excellent radio conditions (using SS-RSRP as defined in 16
Section 3.6) for the UE, but t he minimum coupling loss (see  Section 3.6) should not be exceeded.  The UE should be 17
placed inside an RF shielded box or RF shielded room if the UE is not connected via cable.  18
Field setup: The UE is placed in the centre of cell close to the radia ted eNB/gNB antenna(s), where excellent radio 19
conditions (SS-RSRP as defined in Section 3.6) should be observed. The minimum coupling loss (see 3.6) should not be 20
exceeded. 21
Please refer Figure 5-1 for E2E test setup for 5G SA. 22
4.3.3 Test Procedure 23
Below are the test procedure steps 24
1. The test setup is configured according to the test configuration. The test configuration should be recorded in 25
the test report. The serving cell under test is activated and unloaded. All other cells are powered off. 26
2. Power on the UE and the UE shall send REGISTRATION REQUEST message.  UE shall successfully register 27
to the 5G SA network.  28
3. Full-buffer UDP bi-directional data transmission (see Section 3.4) between the application server and UE is 29
initiated. 30
4. The registration procedure messages shall be captured, and the latency of the registration procedure shall be 31
measured and recorded in Table 4-5. The duration of the test should be at least 3 minutes when the throughput 32
is stable. The PDU session establishment procedure messages shall also be captured and verified. 33
5. Power off the UE and UE shall send DEREGISTRATION REQUEST message. UE shall successfully de-34
register from the 5G SA network. 35
6. The de-registration procedure messages shall be captured, and the latency of de-registration procedure shall be 36
measured and recorded in Table 4-5. The PDU session release procedure messages shall also be captured and 37
verified. 38
7. Repeat steps 2 to 6, for a total of 10 times and record the KPIs mentioned in Table 4-5. 39

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 33

O-RAN.TIFG.E2E-Test.0-v02.00
4.3.4 Test requirements (expected results) 1
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 2
should be captured and reported in the test report for the performance assessment.  3
• Radio parameters such as RSRP, RSRQ, CQI, PDSCH SINR (average sample per second)  4
• Latency KPI mentioned in Table 4-5 5
Validate from collected logs registration (as per 3GPP TS 23.502 [28] Section 4.2.2.2.2) and deregistration (as per 6
3GPP TS 23.502 [28] Section 4.2.2.3.2) procedures and also validate ‘UE Requested PDU Session Establishment for 7
Non-roaming and Roaming with Local Breakout case’ as defined in 3GPP TS 23.502 [28] Section 4.3.2.2.1, and the 8
procedure of ‘PDU Session Release for UE or network requested PDU Session Release for Non -Roaming and Roaming 9
with Local Breakout case’ as defined in 3GPP TS 23.502 [28] Section 4.3.4.2. The procedure should pass 10 10
consecutive times to mark the test case as passing. The gap analysis should be provided for the measured and the 11
expected target KPIs. 12
The Registration Time latency is measured by calculating the time between Registration Request to Registration 13
Complete; The De-registration Time latency is measured by calculating the time between De-registration Request to 14
Signaling Connection Release. Capture the latency for each iteration and sort the latency value observed for each 15
iteration in ascending order. Record the Minimum, Average (Sum of all latency value/ Total Iterations, Total Iterations 16
are 10 in this case) and Maximum latency value observed in Table 4-5.1.  17
Table 4-5 5G SA registration/de-registration latency KPI record table of single UE 18
KPI Repeat Times Calculation
1 2 3 4 5 6 7 8 9 10 Minimum Maximum Average
Registration
Time (single
slice)
(millisecond)

De-
registration
Time (single
slice)
(millisecond)

4.4 Intra-O-DU mobility 19
4.4.1 Test description and applicability 20
The purpose of the test is to verify intra O-CU, intra O-DU handover of a UE.  The test validates the O-CU, O-DU, and 21
O-RU functionality in handling inter-cell mobility when two O-RUs are connected to an O-DU. The test measures the 22
DL / UL throughput variations, handover latency, handover interruption and packet loss during the mobility.  Test 23
scenarios are classified into two groups as Standalone (SA) and Non-Standalone (NSA). 24
Intra O-DU mobility with SA shall follow 3GPP TS 38.401 [29], Section 8.2.1 and 3GPP TS 38.473 [32], Section 8.3.4 25
for the call flow.  Intra O-DU mobility with NSA shall follow 3GPP TS 38.401 [29], Section 8.2.1, 3GPP TS 38.473 26
[32], Section 8.3.4 and 3GPP TS 37.340 [30] for the call flow.  27

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 34

O-RAN.TIFG.E2E-Test.0-v02.00
4.4.2 Test setup and configuration 1
In NSA mode, the test setup consists of one 4G cell (MeNB) and two 5G cells (SgNB). Each 5G cell is associated with 2
a single O-DU, connected to a single O-CU, refer to Figure 4-1 for the test setup topology.  The test environment shall 3
have a single UE with active data traffic. The application server should be placed as close as possible to the core and 4
connected to the core via a transport link with enough capacity. 5
 6
Figure 4-1 Intra O-DU mobility test bed for NSA mode of operation. 7
In SA, the test setup consists of two 5G cells, each one associated with the same O-DU and O-CU connected to a 5G 8
core network (see Figure 4-2 for the test setup topology).  The test environment shall have a single UE with active data 9
traffic. The application server should be placed as close as possible to the core and connected to the core via a transport 10
link with enough capacity. 11
 12
Figure 4-2 Intra O-DU mobility test bed for SA mode of operation. 13
Test configuration : The test configuration is not specified. The utilized test configuration (parameters) should be 14
recorded in the test report.  15
Laboratory setup: The radio conditions of UE can be modified by a variable attenuator/fading generator inserted 16
between the antenna connectors (if available) of the O-RU and the UE, or appropriately emulated using a UE emulator. 17

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 35

O-RAN.TIFG.E2E-Test.0-v02.00
The radio conditions of the UE are initially set to excellent using RSRP as the metric. The minimum coupling loss (see 1
Section 3.6) should not be exceeded. The UE should be placed inside and RF shielded box or RF shielded room if the 2
UE is not connected via cable. The UE handover between the cells can be achieved by changing the radio signal 3
strength of the source and target cells using variable attenuators.  4
 5
Field setup: The drive route with source and target cells should be defined. The UE is placed in the centre of cell close 6
to the radiated eNB/gNB antenna(s), where excellent radio conditions ( using RSRP as the metric as defined in Section 7
3.6) should be observed. The minimum coupling loss (see 3.5) should not be exceeded. The change in radio conditions is 8
achieved by moving the UE along the drive route from source cell to target cell.  9
4.4.3 Test Procedure 10
Below are the NSA mode steps  11
1. The 4G and 5G cell setups are configured following Section 3.2. 12
2. All the three cells are configured according to the test configuration. The cells are activated and unloaded.  13
3. Both 5G cells are configured as neighbors to each other, so that the UE can trigger measurement events for 14
handover.   15
4. The source (cell 1) and target  (cell 2 ) 5G cells for intra O-DU mobility shall be depicted as in Figure 4-1. 16
5. The test UE is under source 5G cell coverage. 17
6. Power on the UE and UE shall successfully complete the LTE attach followed by successful SgNB addition to 18
source 5G cell. 19
7. The full-buffer UDP bi-directional data transmission (see 3.4) from the application server is initiated. 20
8. The UE shall move from the source 5G cell to the target 5G cell to trigger a handover. 21
Below are the SA Mode steps  22
1. The 5G cell setup is configured following Section 3.2. 23
2. Configure two 5G cells within an O-DU according to the test configuration. The cells are activated and 24
unloaded.  25
3. Both 5G cells are configured as neighbors to each other, so that  the UE can trigger measurement events for 26
handover.   27
4. The source and target 5G cells for intra O-DU mobility shall be depicted as in Figure 4-2. 28
5. The test UE is under source cell coverage. 29
6. Power on the UE and UE shall successfully register to source 5G cell. 30
7. The full-buffer UDP bi-directional data transmission (see 3.4) from the application server is initiated. 31
8. The UE shall move from source cell to target cell to perform handover.  32
4.4.4 Test requirements (expected results) 33
The intra O-DU handover call flow shall be verified for both NSA and SA use cases. Following functionalities shall 34
also be validated: 35

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 36

O-RAN.TIFG.E2E-Test.0-v02.00
• PDU Session is established when full-buffer bi-directional data transmission is initiated. (Only in SA Mode) 1
• Handover is successful.  2
In addition to the common minimum set of configuration parameters defined (see Section 3.3), the following metrics 3
and counters shall be recorded and reported for the performance assessment. 4
eNB/gNB/Application server side: 5
• Transmit downlink throughput measured at application server in time (average per second)  6
• Received uplink throughput measured at application server in time (average per second)  7
• Uplink packet loss percentage during handover. 8
• Uplink BLER, MCS, MIMO rank (RI) on PUSCH in time (average per second)   9
UE side: 10
• Radio parameters such as RSRP, RSRQ, SINR on PDSCH in time (average per second)  11
• Downlink BLER, MCS, MIMO rank (RI) on PDSCH in time (average per second). 12
• Received Downlink throughput (L1 and L3 PDCP layers) in time (average per second).  13
• Downlink packet loss percentage during handover 14
• Uplink throughput (L1 and L3 PDCP layers) in time (average per second) 15
• Channel utilization, i.e. Number allocated/occupied downlink and uplink RBs in time (per TTI/average per 16
second) and Number of allocated/occupied slots in time. 17
• KPIs related to Handover failure, Call drop, Handover latency, Handover interruption time.  18
4.5 Inter-O-DU mobility 19
4.5.1 Test description and applicability 20
The purpose of the test is to verify intra O-CU, inter O-DU handover of a UE.  The test validates the O-CU, O-DU 21
functionality in handling Inter O-DU handover. The test measures the DL / UL throughput variations, handover latency, 22
handover interruption and packet loss during the handover procedure.  Test scenarios are classified into two groups as 23
SA (Standalone) and NSA (Non-Standalone).  24
Inter O-DU mobility with SA shall follow 3GPP TS 38.401 [31], Section 8.2.1 and Inter O-DU mobility with NSA shall 25
follow 3GPP TS 38.401[31], Section 8.2.2 for the call flow.  26
3GPP 38.401 v15.7.0 has introduced a new CR 0104, which has modified the initial part of call flow for Section 8.2.2. 27
The ORAN system supporting 3GPP specification later than v15.7.0 shall follow Section 8.2.2 of 3GPP TS 38.401 28
v15.7.0 or later to verify the inter O-DU handover. 29
4.5.2 Test setup and configuration 30
In Non-Standalone Mode, the test setup consists of one 4G Cell (MeNB) and two 5G cells (SgNB). Each 5G Cell is 31
associated with different O-DUs, connected to the same O-CU.  The test environment shall have a single UE with active 32
data traffic. The application server should be placed as close as possible to the core and connected to the core via a 33
transport link with enough capacity. 34
 35

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 37

O-RAN.TIFG.E2E-Test.0-v02.00
 1
Figure 4-3 Inter O-DU mobility test bed for NSA mode of operation. 2
In standalone Mode, the test setup consists of two 5G cells, each one associated with a different O-DU, connected to the 3
same O-CU.  The test environment shall have a single UE with active data traffic. The application server should be 4
placed as close as possible to the core and connected to the core via a transport link with enough capacity. 5
 6
Figure 4-4 Inter O-DU mobility test bed for SA mode of operation 7
 8
Test configuration : The test configuration is not specified. The utilized test configuration (parameters) should be 9
recorded in the test report.  10
Laboratory setup: The radio conditions of UE can be modified by a variable attenuator/fading generator inserted 11
between the antenna connectors (if available) of the O-RU and the UE, or appropriately emulated using a UE emulator. 12
The radio conditions of UE are initially set to excellent. The minimum coupling  loss (see Section 3.6) should not be 13
exceeded. The UE should be placed inside RF shielded box or RF shielded room if the UE is not connected via cable. 14
The UE handover between the cells can be achieved by changing radio signal strength of source and targe t cells using 15
variable attenuators.  16
 17

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 38

O-RAN.TIFG.E2E-Test.0-v02.00
Field setup: The drive route with source and target cells should be defined. The UE is placed in the centre of cell close 1
to the radiated eNB/gNB antenna(s), where excellent radio conditions (RSRP as defined in Section  3.6) should be 2
observed. The minimum coupling loss (see 3.5) should not be exceeded. The change in radio conditions is achieved by 3
moving the UE along the drive route from source cell to target cell. 4
4.5.3 Test Procedure 5
In Non-Standalone Mode 6
1. The 4G and 5G cell setups are configured following Section 3.2. 7
2. All the three cells are configured according to the test configuration. The cells are activated and un loaded.  8
3. Both 5G cells are configured neighbors to each other, so that UE can trigger measurement events for handover.   9
4. The test UE is under source O-DU cell coverage. 10
5. Power on the UE and the UE shall successfully complete the LTE attach followed by successful SgNB 11
addition to source O-DU. 12
6. The full-buffer UDP data transmission (see 3.4) from the application server is initiated. 13
7. The UE shall move from source O-DU to target O-DU to perform handover. 14
In Standalone Mode 15
1. The 5G cell setup is configured following Section 3.2. 16
2. All the 5G cells are configured according to the test configuration. The cells are activated and unloaded.  17
3. Both 5G cells are configured neighbors to each other, so that UE can trigger measurement events for handover.   18
4. The test UE is under source O-DU cell coverage. 19
5. Power on the UE and the UE shall successfully register to source O-DU cell. 20
6. The full-buffer UDP data transmission (see 3.4) from the application server is initiated. 21
7. The UE shall move from source O-DU to target O-DU to perform handover. 22
4.5.4 Test requirements (expected results) 23
The inter O-DU handover call flow shall be verified for both NSA and SA use cases. Following functionalities shall 24
also be validated: 25
• PDU Session is established when full-buffer bi-directional data transmission is initiated. (Only in SA Mode) 26
• Handover is successful.  27
In addition to the common minimum set of configuration parameters defined ( see Section 3.3), the following metrics 28
and counters shall be recorded and reported for the performance assessment. 29
eNB/gNB/Application server side: 30
• Transmit downlink throughput measured at application server in time (average per second)  31
• Received uplink throughput measured at application server in time (average per second)  32
• Uplink packet loss percentage during handover. 33
• Uplink BLER, MCS, MIMO rank (RI) on PUSCH in time (average per second)   34
UE side: 35
• Radio parameters such as RSRP, RSRQ, SINR on PDSCH in time (average per second)  36
• Downlink BLER, MCS, MIMO rank (RI) on PDSCH in time (average per second). 37

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 39

O-RAN.TIFG.E2E-Test.0-v02.00
• Received Downlink throughput (L1 and L3 PDCP layers) in time (average per second).  1
• Downlink packet loss percentage during handover 2
• Uplink throughput (L1 and L3 PDCP layers) in time (average per second) 3
• Channel utilization, i.e. Number allocated/occupied downlink and uplink RBs in time (per TTI/average per 4
second) and Number of allocated/occupied slots in time. 5
• KPIs related to Handover failure, Call drop, Handover latency, Handover interruption time.  6
4.6 Inter-O-CU mobility 7
4.6.1 Test description and applicability 8
The purpose of the test is to verify inter O-CU handover of the UE.  The test validates the O-CU, O-DU functionality in 9
handling inter O-CU mobility connected to same 5G Core Network (in SA) or Master eNB (in NSA) . The test 10
measures the DL / UL throughput variations, handover latency, handover interruption and packet loss during the 11
mobility.  Test scenarios are classified into two groups as Standalone (SA) and Non -Standalone (NSA). 12
Inter O-CU mobility with SA- Xn based Handover call flow shall follow 3GPP TS 38.401 [31], Section 8.9.4 and 13
Section 8.9.5. Inter O-CU mobility with NSA shall follow 3GPP TS 37.340 [30], Section 10.5.1 for the call flow. 14
4.6.2 Test setup and configuration 15
In non-standalone mode, the test setup consists of a 4G cell (MeNB) and two 5G cells (SgNB), each 5G cell is 16
associated with a different O-DU and O-CU connected to the same 4G core network, refer to Figure 4-5 for the test 17
setup topology.  The test environment shall have single UE with active data traffic.  The application server should be 18
placed as close as possible to the core and connected to the core via a transport link with enough capacity. 19
 20
Figure 4-5 Inter O-CU mobility test bed for NSA mode of operation 21
In standalone mode, The test setup consists of two 5G cells, each one associated with a different O-DU and O-CU 22
connected to same 5G Core network (see Figure 4-6 for the test setup topology).  The test environment shall have a 23
single UE with active data traffic. The application server should be placed as close as possible to the core and connected 24
to the core via a transport link with enough capacity. 25

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 40

O-RAN.TIFG.E2E-Test.0-v02.00
 1
Figure 4-6  Inter O-CU mobility test bed for SA mode of operation. 2
 3
Test configuration : The test configuration is not specified. The utilized test configur ation (parameters) should be 4
recorded in the test report.  5
Laboratory setup: The radio conditions of UE can be modified by a variable attenuator/fading generator inserted 6
between the antenna connectors (if available) of the O-RU and the UE, or appropriately emulated using a UE emulator. 7
The radio conditions of UE are initially set to excellent. The minimum coupling loss (see Section 3.6) should not be 8
exceeded. The UE should be placed inside RF shielded box or RF shielded room if the UE is not c onnected via cable. 9
The UE handover between the cells can be achieved by changing radio signal strength of source and target cells using 10
variable attenuators. 11
 12
Field setup: The drive route with source and target cells should be defined. The UE is placed in  the centre of cell close 13
to the radiated eNB/gNB antenna(s), where excellent radio conditions (RSRP as defined in Section 3.6) should be 14
observed. The minimum coupling loss (see 3.5) should not be exceeded. The change in radio conditions is achieved by 15
moving the UE along the drive route from source cell to target cell. 16
 17
4.6.3 Test Procedure 18
In Non-Standalone Mode 19
1. The 4G and 5G cell setups are configured following Section 3.2. 20
2. Configure two 5G cells connected to different O-DU and O-CU according to the test configuration. The cells 21
are activated and unloaded.  22
3. 5G cells are configured as neighbors to 4G cell, so that UE can trigger measurement events for mobility.   23
4. The source cell (source O-DU and source O-CU) is the cell where UE is initially placed as depicted in Figure 24
4-5. 25
5. Power on the UE and the UE shall successfully complete the LTE attach followed by successful SgNB 26
addition to source 5G  cell. 27

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 41

O-RAN.TIFG.E2E-Test.0-v02.00
6. The full-buffer UDP bi-directional data transmission (see 3.4) from the application server is initiated. 1
7. The UE shall move from source 5G cell to target 5G cell. 2
In Standalone Mode 3
1. The 5G cell setup is configured following Section 3.2. 4
2. Configure two 5G cells connected to different O-DU and O-CU according to the test configuration. The cells 5
are activated and unloaded.  6
3. Both 5G cells are configured neighbors to each other, so that UE can trigger measurement events for handover.   7
4. The source cell (source O-DU and source O-CU) is the cell where UE is initially placed as depicted in Figure 8
4-6. 9
5. Power on the UE and UE shall successfully register to source 5G cell. 10
6. The full-buffer UDP bi-directional data transmission (see 3.4) from the application server is initiated. 11
7. The UE shall move from source cell to target cell to perform handover.  12
4.6.4 Test requirements (expected results) 13
The inter O-CU handover call flow shall be verified for NSA use case. Following functionalities shall also be validated: 14
• PDU Session is established when full-buffer bi-directional data transmission is initiated. (Only in SA Mode) 15
• Handover is successful.  16
In addition to the common minimum set of configuration parameters defined (see Section 3.3), the following metrics 17
and counters should be recorded and reported for the performance assessment 18
UE side: 19
• Radio parameters such as RSRP, RSRQ, SINR on PDSCH in time (average per second)  20
• Downlink BLER, MCS, MIMO rank (RI) on PDSCH in time (average per second).  21
• Received Downlink throughput (L1 and L3 PDCP layers) in time (average per second). 22
• Downlink packet loss percentage during handover 23
• Uplink throughput (L1 and L3 PDCP layers) in time (average per second) 24
• Channel utilization, i.e. Number allocated/occupied downlink and uplink RBs in time (per TTI/average per 25
second) and Number of allocated/occupied slots in time. 26
• KPIs related to Handover failure, Call drop, Handover latency, Handover interruption time.  27
eNB/gNB/Application server side: 28
• Transmit downlink throughput measured at application server in time (average per second)  29
• Received uplink throughput measured at application server in time (average per second)  30
• Uplink packet loss percentage during handover. 31
• Uplink BLER, MCS, MIMO rank (RI) on PUSCH in time (average per second)   32
     33

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 42

O-RAN.TIFG.E2E-Test.0-v02.00
4.7 Registration and deregistration to a single network slice 1
4.7.1 Test description and applicability 2
The purpose of the test is to verify the full procedure of the registration and de-registration to the single eMBB network 3
slice in the 5G SA network. 4
The test focuses on the procedure of ‘Initial registration’ as defined in 3GPP 23.502 Section 4.2.2.2.2, with a single 5
eMBB network slice. The network slice information (i.e., Network Slice Selection Assistance Information, NSSAI) 6
defined in Table 4-6 shall be verified within the 3GPP standard registration procedure. 7
The test focuses on the procedure of ‘UE-initiated de-registration’ as defined in 3GPP 23.502 Section 4.2.2.3.2, with a 8
single eMBB network slice. Even though the network slice information is not included in the deregistration procedure, 9
the mandatory IEs defined in Table 4-6 shall be verified within the 3GPP standard deregistration procedure. 10
The granularity of slice awareness is in PDU session level, so this test also verifies the PDU session establishment and 11
release procedures.  12
The test measures not only the 3GPP standard registration/de-registration procedure but also the latency of the 13
procedure. The bi-directional data transmission shall be involved before the de-registration procedure to verify the 14
stability of the network slice. 15
4.7.2 Test setup and configuration 16
The network setup is single cell scenario (i.e. isolated cell without any inter -cell interference – see Section 3.8) with 17
stationary UE (real or emulated UE) placed in the excellent radio conditions as defined  in Section 3.7 – RSRQ should 18
be considered in case of downlink. Within the cell there should be only one active UE.  The application server should be 19
placed as close as possible to the core/core emulator and connected to the core/core emulator via transport link with 20
enough capacity not limiting the expected data throughput. The UE, RAN, and 5G Core shall support the network 21
slicing, at least for one eMBB Single Network Slice Selection Assistance Information (S-NSSAI). The test is suitable 22
for lab as well as field environment.   23
Test configuration: The test configuration is not specified. The utilized test configuration (parameters) should be 24
recorded in the test report.   25
Laboratory setup: The radio conditions of UE can be modified by a variable attenuator/fading generator inserted between 26
the antenna connectors of O-RU and UE. The minimum attenuation of radio signal should be set to achieve the excellent 27
radio conditions (RSRQ as defined in Section 3.7), but the minimum coupling loss (see Section 3.7) shoul d not be 28
exceeded.  The UE should be placed inside RF shielded box or RF shielded room if the UE is not connected via cable.  29
Field setup: The UE is placed in the centre of cell close to the radiated gNB antenna(s), where excellent radio conditions 30
(RSRQ as defined in Section 3.7) should be observed. The minimum coupling loss (see 3.6) should not be exceeded.  31
Please refer Figure 5.2-1 for E2E test setup for 5G SA. 32
4.7.3 Test Procedure 33
Below are the test procedure steps 34
1. The NR cells shall be configured in the initial conditions defined in Session 3.2. 35
2. Only one eMBB S-NSSAI (i.e., Slice/Service Type (SST) = 0x01) under the NSSAI shall be configured in UE, 36
RAN, and 5G Core. 37

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 43

O-RAN.TIFG.E2E-Test.0-v02.00
3. Power on the UE and UE shall send REGISTRATION REQUEST message. The only one requested S-NSSAI 1
shall be included in the message with the configured value in step 2. And UE shall successfully register to the 2
5G SA network. 3
4. The full-buffer UDP bi-directional data transmission (see Session 3.3) between UE and the application server 4
is initiated. 5
5. The registration procedure messages shall be captured to verify the mandatory IEs defined in Table 4-6, and 6
the latency of registration procedure shall be measured and recorded in Table 4-8. The duration of the test 7
should be at least 3 minutes when the throughput is stable. The PDU session establishment procedure messages 8
shall also be captured and verified. 9
6. Power off the UE and UE shall send DEREGISTRATION REQUEST message. And UE shall successfully de-10
register from the 5G SA network. 11
7. The de-registration procedure messages shall be captured to verify the mandatory IEs defined in Table 4-7, and 12
the latency of de-registration procedure shall be measured and recorded in Table 4-8. The PDU session release 13
procedure messages shall also be captured and verified. 14
8. Repeat steps 3 to 7, 10 times and record the KPI mentioned in Table 4-8. 15
4.7.4 Test requirements (expected results) 16
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 17
should be captured and reported in the test report for the performance assessment. 18
• Radio parameters such as RSRP, RSRQ, CQI, PDSCH SINR (average sample per second)  19
• Mandatory IEs mentioned in Table 4-6, and Table 4-7 20
• Latency KPI mentioned in Table 4-8 21
Validate from collected logs registration (as per 3GPP 23.502 Section 4.2.2.2.2) and deregistration (as per 3GPP 23.502 22
Section 4.2.2.3.2) procedure and also validate ‘UE Requested PDU Session Establishment for Non-roaming and 23
Roaming with Local Breakout case’ as defined in 3GPP 23.502 Section 4.3.2.2.1, and the procedure of ‘PDU Session 24
Release for UE or network requested PDU Session Release for Non -Roaming and Roaming with Local Breakout case’ 25
as defined in 3GPP 23.502 Section 4.3.4.2. 26
Table 4-6 defines the verification steps along with validation of mandatory IEs for 5G SA registration procedure in 27
single eMBB network slice. 28
Table 4-6 5G SA Registration verification with mandatory IEs in single eMBB network slice  29
St. Procedure Msg
Flow Expected Output
1
RRCSetupComplete

UE →
gNB
Verify that UE sends only one S-NSSAI in IE s-nssai-List
and SST = ‘0x01’ in the S-NSSAI to gNB.
2 Registration Request
UE →
AMF

Verify that UE sends only one S-NSSAI in IE Requested
NSSAI to 5G AMF, and IE 5GS registration type = ‘initial
registration’, SST = ‘0x01’ in the S-NSSAI.

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 44

O-RAN.TIFG.E2E-Test.0-v02.00
3
Initial Context Setup
Request

AMF
→ gNB
Verify that 5G AMF sends only one S-NSSAI in IE
Allowed NSSAI and S-NSSAI (under parent IE PDU Session
Resource Setup Request Item) to gNB, and SST = ‘0x01’ in
the S-NSSAI.
4 Registration Accept AMF
→ UE
Verify that 5G AMF sends only one S-NSSAI in IE
Allowed NSSAI and Configured NSSAI to UE, SST = ‘0x01’
in the S-NSSAI and IE Rejected NSSAI shall not exist.
 1
Table 4-7 defines the verification steps along with validation of mandatory IEs for 5G SA der egistration procedure in 2
single eMBB network slice. 3
Table 4-7 5G SA Deregistration verification with mandatory IEs in single eMBB network slice 4
St. Procedure Msg
Flow Expected Output
1 De-registration
Request
AMF →
UE
Verify that 5G AMF sends IE De-registration type = ‘Switch
Off’ to UE.
2 De-registration
Accept
UE →
AMF

Verify that UE sends IE De-registration accept message identity
= ‘Deregistration accept (UE originating)’ to 5G AMF.
 5
Table 4-8 defines the KPI record table of registration/ registration testing in single eMBB network slice. 6
Table 4-8 Single eMBB Network Slice KPI record table 7
KPI Repeat Times Calculation
1 2 3 4 5 6 7 8 9 10 Minimum Maximum Average
Registration
Time (single
slice)
(millisecond)

De-
registration
Time (single
slice)
(millisecond)

 8
 9

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 45

O-RAN.TIFG.E2E-Test.0-v02.00
4.8 Registration and deregistration to multiple network slices 1
4.8.1 Test description and applicability 2
The purpose of the test is to verify the full procedure of the registration and de-registration to multiple network slices 3
(i.e., eMBB, URRLC, MIoT, V2X) in the 5G SA network. 4
The test focuses on the procedure of ‘Initial registration’ as defined in 3GPP 23.502 Section 4.2.2.2.2, with multiple 5
network slices. The network slice information (i.e., Network Slice Selection Assistance Information, NSSAI) defined in 6
Table 4-9 shall be verified within the 3GPP standard registration procedure. 7
The test focuses on the procedure of ‘UE-initiated de-registration’ as defined in 3GPP 23.502 Section 4.2.2.3.2, with 8
multiple network slices. Even though the network slice information is not included in the deregistration procedure, and 9
also the mandatory IEs defined in Table 4-10 shall be verified within the 3GPP standard deregistration procedure. 10
The granularity of slice awareness is in PDU session level, so this test also verifies the PDU session establishment and 11
release procedures.  12
The test measures not only the 3GPP standard registration/de-registration procedure but also the latency of the 13
procedure. The bi-directional data transmission shall be involved before the de-registration procedure to verify the 14
stability of the network slice. 15
4.8.2 Test setup and configuration 16
The network setup is single cell scenario (i.e. isolated cell without any inter -cell interference – see Section 3.8) with 17
stationary UE (real or emulated UE) placed in the excellent radio conditions as defined in Section 3.7 – RSRQ should 18
be considered in case of downlink. Within the cell there should be only one active UE.  The application server(s) should 19
be placed as close as possible to the core/core emulator and connected to the core/core emulator via transport link with 20
enough capacity not limiting the expected data throughput. The test is suitable for lab as well as field environment. The 21
UE, RAN, and 5G Core shall support the multiple Network Slice Selection Assistance Information (S-NSSAI). At least 22
two network slices of eMBB, URRLC, MIoT, V2X shall be covered for this test. The test is suitable for lab as well as 23
field environment.   24
Test configuration: The test configuration is not specified. The utilized test configuration (parameters) should be 25
recorded in the test report.   26
Laboratory setup: The radio conditions of UE can be modified by a variable attenuator/fading generator inserted between 27
the antenna connectors of O-RU and UE. The minimum attenuation of radio signal should be set to achieve the excellent 28
radio conditions (RSRQ as defined in Sec tion 3.7), but the minimum coupling loss (see Section 3.7) should not be 29
exceeded.  The UE should be placed inside RF shielded box or RF shielded room if the UE is not connected via cable.  30
Field setup: The UE is placed in the centre of cell close to the ra diated eNB/gNB antenna(s), where excellent radio 31
conditions (RSRQ as defined in Section 3.7) should be observed. The minimum coupling loss (see 3.6) should not be 32
exceeded. 33
Please refer Figure 5.2-1 for E2E test setup for 5G SA. 34
4.8.3 Test Procedure 35
Below are the test procedure steps 36
1. The NR cells shall be configured in the initial conditions defined in Session 3.2.  37

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 46

O-RAN.TIFG.E2E-Test.0-v02.00
2. Multiple S-NSSAI (i.e., Slice/Service Type (SST) = 0x01/0x02/0x03/0x04) under the NSSAI shall be 1
configured in UE, RAN, and 5G Core. 2
3. Power on the UE and UE shall send REGISTRATION REQUEST message. The multiple requested S-NSSAI 3
shall be included in the message with the configured value in step 2. And UE shall successfully register to the 4
5G SA network. 5
4. The full-buffer UDP bi-directional data transmission (see Session 3.3) between UE and the application server 6
is initiated in eMBB network slice if eMBB network slice is covered. 7
5. The full-buffer UDP bi-directional data transmission (see Session 3.3) between UE and the application server 8
is initiated in URRLC network slice if URRLC network slice is covered. 9
6. The full-buffer UDP bi-directional data transmission (see Session 3.3) between UE and the application server 10
is initiated in mIoT network slice if MIoT network slice is covered. 11
7. The full-buffer UDP bi-directional data transmission (see Session 3.3) between UE and the application server 12
is initiated in V2X network slice if V2X network slice is covered. 13
8. The registration procedure messages shall be captured to verify the mandatory IEs defined in Table 8.1, a nd 14
the latency of registration procedure shall be measured and recorded in Table 4-11. The duration of the test 15
should be at least 3 minutes when the throughput is stable. The PDU session establishment procedure messages 16
shall also be captured and verified for the network slices of eMBB/URRLC/MIoT/ V2X.  17
9. Power off the UE and UE shall send DEREGISTRATION REQUEST message. And UE shall successfully de-18
register from the 5G SA network. 19
10. The de-registration procedure messages shall be captured to verify the mandatory IEs defined in Table 8.2, and 20
the latency of de-registration procedure shall be measured and recorded in Table 4-11. The PDU session 21
release procedure messages shall also be captured and verified for the network slices of eMBB/URRLC/MIoT/ 22
V2X. 23
11. Repeat steps 3 to 10, 10 times and record the KPI mentioned in Table 8.3. 24
4.8.4 Test requirements (expected results) 25
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 26
should be captured and reported in the test report for the performance assessment. 27
• Radio parameters such as RSRP, RSRQ, CQI, PDSCH SINR (average sample per second)  28
• Mandatory IEs mentioned in Table 4-9, and Table 4-10 29
• Latency KPI mentioned in Table 4-11 30
Validate from collected logs registration (as per 3GPP 23.502 Section 4.2.2.2.2) and deregistration (as per 3GPP 23.502 31
Section 4.2.2.3.2) procedure and also validate ‘UE Requested PDU Session Establishment for Non-roaming and 32
Roaming with Local Breakout case’ as defined in 3GPP 23.502 Section 4.3.2.2.1  for multiple network slices of 33
eMBB/URRLC/MIoT/ V2X, and the procedure of ‘PDU Session Release for UE or network requested PDU Session 34
Release for Non-Roaming and Roaming with Local Breakout case’ as defined in 3GPP 23.502 Section 4.3.4.2  for 35
multiple network slices of eMBB/URRLC/MIoT/ V2X. 36
Table 4-9 defines the verification steps along with validation of mandatory IEs for 5G SA registration procedure in 37
multiple network slices. 38
Table 4-9 5G SA Registration verification with mandatory IEs in multiple network slices 39

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 47

O-RAN.TIFG.E2E-Test.0-v02.00
St. Procedure Msg
Flow Expected Output
1
RRCSetupComplete

UE →
gNB
Verify that UE sends multiple S-NSSAI in IE s-nssai-List
and SST = ‘0x01/0x02/0x03/0x04’ in the S-NSSAI to gNB.
2 Registration Request
UE →
AMF

Verify that UE sends multiple S-NSSAI in IE Requested
NSSAI to 5G AMF, and IE 5GS registration type = ‘initial
registration’, SST = ‘‘0x01/0x02/0x03/0x04’ in the S-NSSAI.
3
Initial Context Setup
Request

AMF
→
gNB
Verify that 5G AMF sends multiple S-NSSAI in IE Allowed
NSSAI and S-NSSAI (under parent IE PDU Session Resource
Setup Request Item) to gNB, and SST =
‘0x01/0x02/0x03/0x04’ in the S-NSSAI.
4 Registration Accept AMF
→ UE
Verify that 5G AMF sends multiple S-NSSAI in IE Allowed
NSSAI and Configured NSSAI to UE, SST =
‘0x01/0x02/0x03/0x04’ in the S-NSSAI and IE Rejected
NSSAI shall not exist.
 1
Table 4-10 defines the verification steps along with validation of mandatory IEs for 5G SA der egistration procedure in 2
multiple network slices. 3
Table 4-10 5G SA Deregistration verification with mandatory IEs in multiple network slices 4
St. Procedure Msg
Flow Expected Output
1 De-registration
Request
AMF →
UE
Verify that 5G AMF sends IE De-registration type = ‘Switch
Off’ to UE.
2 De-registration
Accept
UE →
AMF

Verify that UE sends IE De-registration accept message identity
= ‘Deregistration accept (UE originating)’ to 5G AMF.
 5
Table 4-11 defines the KPI record table of registration/ registration testing in multiple network slices. 6
Table 4-11 Multiple Network Slices KPI record table 7
KPI Repeat Times Calculation
1 2 3 4 5 6 7 8 9 10 Minimum Maximum Average
Registration
Time
(multiple
slices)
(millisecond)

De-
registration
Time

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 48

O-RAN.TIFG.E2E-Test.0-v02.00
(multiple
slices)
(millisecond)
 1
4.9 Idle Mode Intra-O-DU mobility 2
4.9.1 Test description and applicability 3
The purpose of the test is to verify O-CU, idle mode intra O-DU mobility of a UE.  The test validates the O-CU, O-DU, 4
O-RU functionality in handling inter cell mobility when two O-RU connected to an O-DU. Test scenarios are classified 5
into two groups as Standalone (SA) Intra frequency cell reselection and as Standalone (SA) Inter frequency cell 6
reselection. 7
Idle mode Intra O-DU mobility shall follow 3GPP 38.133, Section 4.2.2.3 and Section 4.2.2.4 for i ntra frequency and 8
inter frequency cell selection measurement, respectively. And 3GPP 38.304, Section 5.2.2 for the state transition.   9
4.9.2 Test setup and configuration 10
In SA, the test setup consists of two 5G cells, each one associated with same O -DU and O-CU connected to 5G Core 11
network, refer Figure 4.9-1 for the test setup topology.  The test environment shall have single UE in Idle mode.  12
 13
Figure 4.9-1 Idle Mode Intra O-DU mobility test bed for SA mode of operation. 14
Test configuration : The test configu ration is not specified. The utilized test configuration (parameters) should be 15
recorded in the test report.  16
Laboratory setup: The radio conditions of UE can be modified by a variable attenuator/fading generator inserted between 17
the antenna connectors of O-RU and UE. The radio conditions of UE are initially set to excellent. The minimum coupling 18
loss (see Section 3.6) should not be exceeded. The UE should be placed inside RF shielded box or RF shielded room if 19
the UE is not connected via cable. The UE mobi lity between the cells can be achieved by changing radio signal strength 20
of source and target cells using variable attenuators.  21
 22
Field setup: The drive route with source and target cells should be defined. The UE is placed in the centre of cell close 23
to the radiated gNB antenna(s), where excellent radio conditions (RSRP as defined in Section 3.6) should be observed. 24
The minimum coupling loss (see 3.5) should not be exceeded. The change in radio conditions is achieved by moving the 25
UE along the drive route from source cell to target cell. 26

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 49

O-RAN.TIFG.E2E-Test.0-v02.00
4.9.3 Test Procedure 1
Below are the SA Mode Intra frequency steps  2
1. The 5G cell setup is configured following section 3.2. 3
2. Configure two 5G cells (cell 1 and cell 2) of same frequency within an O -DU according to the test 4
configuration. The cells are activated and unloaded.  5
3. Both 5G Cells are configured neighbours to each other, so that UE can use it for cell re -selection.   6
4. The source and target 5G cells for intra O-DU mobility shall be depicted as in Figure 4.9-1. 7
5. The test UE is under source cell coverage. 8
6. Power on the UE and UE shall successfully register to source 5G cell. 9
7. Wait till the UE goes in idle mode as per UE inactivity timer and then move the UE from source cell 10
to target cell. 11
8. Once UE moves to new cell, make an MO data call. 12
9. Repeat the above test steps for 10 iterations. 13
Below is the SA Mode Inter frequency steps  14
1. The 5G cell setup is configured following section 3.2. 15
2. Configure two 5G cells (cell 1 and cell 2) on different frequencies within an O -DU according to the test 16
configuration. The cells are activated and unloaded.  17
3. Both 5G Cells are configured neighbours to each other, so that UE can use it for cell re -selection.   18
4. The source and target 5G cells for intra O-DU mobility shall be depicted as in Figure 4.9-1. 19
5. The test UE is under source cell coverage. 20
6. Power on the UE and UE shall successfully register to source 5G cell. 21
7. Wait till the UE goes in idle mode as per UE inactivity timer and then move the UE from source cell to target 22
cell, move the UE from source cell to target cell. 23
8. Once UE moves to new cell, make an MO data call. 24
9. Repeat the above test steps for 10 iterations. 25
 26
4.9.4 Test requirements (expected results) 27
The Idle mode intra O-DU cell reselection shall be verified for both intra-frequency and inter-frequency. Also verify 28
call setup latency on a target cell with reference to RRC connection call flow defined in 3GPP 38.133, Section 5.3.3.1. 29
Following functionalities are also validated to declare the verdict. 30
• Cell re-selection is successful on target cell.  31
• RRC connection establishment is successful on a target cell. 32

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 50

O-RAN.TIFG.E2E-Test.0-v02.00
In addition to the common minimum set of configuration parameters defined (see 3.3), the following metrics and 1
counters shall be recorded and reported for the performance assessment.  2
gNB side: 3
• Idle mode mobility time from UE, in idle mode at source to UE in idle mode to target.   4
UE side: 5
• Radio parameters such as RSRP, RSRQ. 6
• KPI’s related to Cell re-selection failure, idle mode mobility time from UE in idle mode at source to UE in idle 7
mode to target . 8
 9
The cell re-selection failure can be found out by checking if the cell re-selection is successful or not. The Idle to 10
Connected on a target cell Time latency is measured by calculating the time between RRC_Idle mode to 11
RRC_Connected state when UE moves to new cell after initiating an MO call. Capture the cell re -selection 12
success/failure and latency for each iteration and sort the latency value ob served for each iteration in ascending order. 13
Record the Minimum, Average (Sum of all latency value/ Total Iterations, Total Iterations are 10 in this case) and 14
Maximum latency value observed in below tables:  15
Table 4-12 5G Cell Re-selection Success and Failure KPI 16
KPI Repeat Times
1 2 3 4 5 6 7 8 9 10
Cell Re-selection
Success

Cell Re-selection
Failure

 17
 18
 19
4.10 Idle mode Inter-O-DU mobility 20
4.10.1 Test description and applicability 21
The purpose of the test is to verify intra O-CU, idle mode inter O-DU mobility of a UE. The test validates the O-CU, O-22
DU, O-RU functionality in handling inter cell mobility. Test scenarios are classified into two groups as Standalone (SA) 23
Intra frequency cell reselection and as Standalone (SA) Inter frequency cell reselection. 24
Idle mode Inter O-DU mobility shall follow 3GPP 38.133, Section 4.2.2.3 and Section 4.2.2.4 for intra frequency and 25
inter frequency cell selection measurement, respectively. And 3GPP 38.304, Section 5.2.2 for the state transition. 26
4.10.2 Test setup and configuration 27
In standalone Mode, the test setup consists of two 5G cells, each one associated with different O -DU, connected to 28
same O-CU.  The test environment shall have single UE in Idle mode. 29

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 51

O-RAN.TIFG.E2E-Test.0-v02.00
 1
Figure 4.10-1 Inter O-DU mobility test bed for SA mode of operation 2
 3
Test configuration : The test configuration is not specified. The utilized test configuration (parameters) should be 4
recorded in the test report.  5
Laboratory setup: The radio conditions of UE can be modified by a variable attenuator/fading generator inserted between 6
the antenna connectors of O-RU and UE. The radio conditions of UE are initially set to excellent. The minimum coupling 7
loss (see Section 3.6) should not be exceeded. The UE should be placed inside RF shielded box or RF shielded room if 8
the UE is not connected via cable. The UE mobility between the cells can be achieved by changing radio signal strength 9
of source and target cells using variable attenuators.  10
 11
Field setup: The drive route with source and target cells should be defined. The UE is placed in the centre of cell close 12
to the radiated gNB antenna(s), where excellent radio conditions (RSRP as defined in Section 3.6) should be observed. 13
The minimum coupling loss (see 3.5) should not be exceeded. The change in radio conditions is achieved by moving the 14
UE along the drive route from source cell to target cell. 15
4.10.3 Test Procedure 16
Below are the SA Mode Intra frequency steps  17
1. The 5G cell setup is configured following section 3.2. 18
2. All the 5G cells are configured according to the test configuration. The cells are activated and unloaded   19
3. Both 5G Cells are configured neighbours to each other, so that UE can use it for cell re -selection.   20
4. The source and target 5G cells for inter O-DU mobility shall be depicted as in Figure 4.10-1. 21
5. The test UE is under source cell coverage. 22
6. Power on the UE and UE shall successfully register to source 5G cell. 23
7. Wait till the UE goes in idle mode as per UE inactivity timer and then move the UE from source cell to target 24
cell . 25
8. Once UE moves to new cell, make an MO data call. 26

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 52

O-RAN.TIFG.E2E-Test.0-v02.00
9. Repeat the above test steps for 10 iterations. 1
Below is the SA Mode Inter frequency steps  2
1. The 5G cell setup is configured following section 3.2. 3
2. All the 5G cells are configured according to the test configuration. The cells are activated and unloaded.   4
3. Both 5G Cells are configured neighbours to each other, so that UE can use it for cell re -selection.   5
4. The source and target 5G cells for inter O-DU mobility shall be depicted as in Figure 4.10-1. 6
5. The test UE is under source cell coverage. 7
6. Power on the UE and UE shall successfully register to source 5G cell. 8
7. Wait till the UE goes in idle mode as per UE inactivity timer and then move the UE from source cell to target 9
cell. 10
8. Once UE moves to new cell, make an MO data call. 11
9. Repeat the above test steps for 10 iterations. 12
 13
4.10.4 Test requirements (expected results) 14
The Idle mode inter O-DU cell reselection shall be verified for both intra-frequency and inter-frequency. Also verify 15
call setup latency on a target cell with reference to RRC connection call flow mentioned in 3GPP 38.133, Section 16
5.3.3.1. Following functionalities are also validated to declare the verdict.  17
• RRC connection establishment is complete. 18
• Cell re-selection is successful.  19
In addition to the common minimum set of configuration parameters defined (see 3.3), the following metrics and 20
counters shall be recorded and reported for the performance assessment.  21
gNB side: 22
• Idle mode mobility time from UE, in idle mode at source to UE in idle mode to target . 23
UE side: 24
• Radio parameters such as RSRP, RSRQ, SINR on PDSCH in time (average per second)  25
• KPI’s related to Cell re-selection failure, Idle mode mobility time from UE, in idle mode at source to UE in 26
idle mode to target.. 27
 28
The cell re-selection failure can be found out by checking if the cell re-selection is successful or not. The Idle to 29
Connected on a target cell Time latency is measured by calculating the time between RRC_Idle mode to 30
RRC_Connected state when UE moves to new cell after initiating an MO call. Capture the cell re -selection 31
success/failure and latency for each iteration and sort the latency value observed for each iteration in ascending order. 32
Record the Minimum, Average (Sum of all latency value/ Total Iterations, Total Iterations are 10 in this case) and 33
Maximum latency value observed in below tables:  34

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 53

O-RAN.TIFG.E2E-Test.0-v02.00
Table 4-13 5G Cell Re-selection Success and Failure KPI 1
KPI Repeat Times
1 2 3 4 5 6 7 8 9 10
Cell Re-selection
Success

Cell Re-selection
Failure

4.11 Idle mode Inter-O-CU mobility 2
4.11.1 Test description and applicability 3
The purpose of the test is to verify inter O-CU mobility of the UE.  The test validates the O-CU, O-DU functionality in 4
handling inter O-CU mobility connected to same 5G Core Network (in SA). The test validates the O-CU, O-DU, O-RU 5
functionality in handling inter cell mobility. Test scenarios are classified into two groups as Standalone (SA) Intra 6
frequency cell reselection and as Standalone (SA) Inter frequency cell reselection.  7
Idle mode Inter O-CU mobility shall follow 3GPP 38.133, Section 4.2.2.3 and Section 4.2.2.4 for intra frequency and 8
inter frequency cell selection measurement, respectively. And 3GPP 38.304, Section 5.2.2 for the state transition.  9
4.11.2 Test setup and configuration 10
In Standalone mode, the test setup consists of two 5G cells, each one associated with a different O-DU and O-CU 11
connected to same 5G Core network, refer Figure 4.9-2 for the test setup topology.  The test environment shall have 12
single UE. 13
 14

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 54

O-RAN.TIFG.E2E-Test.0-v02.00
Figure 4.11-1  Inter O-CU mobility test bed for SA mode of operation. 1
 2
Test configuration : The test configuration is not specified. The utilized test configuration (parameters) should be 3
recorded in the test report.  4
Laboratory setup: The radio conditions of UE can be modified by a variable attenuator/fading generator inserted between 5
the antenna connectors of O-RU and UE. The radio conditions of UE are initially set to excellent. The minimum coupling 6
loss (see Section 3.6) should not be exceeded. The UE should be placed  inside RF shielded box or RF shielded room if 7
the UE is not connected via cable. The UE handover between the cells can be achieved by changing radio signal strength 8
of source and target cells using variable attenuators.  9
 10
Field setup: The drive route with source and target cells should be defined. The UE is placed in the centre of cell close 11
to the radiated eNB/gNB antenna(s), where excellent radio conditions (RSRP as defined in Section 3.6) should be 12
observed. The minimum coupling loss (see 3.5) should not be exceeded. The change in radio conditions is achieved by 13
moving the UE along the drive route from source cell to target cell. 14
 15
4.11.3 Test Procedure 16
Below are the SA Mode Intra frequency steps  17
1. The 5G cell setup is configured following section 3.2. 18
2. Configure two 5G cells (cell 1 and cell 2) connected to different O-DU and O-CU according to the test 19
configuration. The cells are activated and unloaded.  20
3. Both 5G Cells are configured neighbors to each other, so that UE can use it for cell re-selection.   21
4. The source cell (source O-DU and source O-CU) is the cell where UE is initially placed as depicted in figure 22
4.11-1. 23
5. Power on the UE and UE shall successfully register to source 5G cell. 24
6. Wait till the UE goes in idle mode as per UE inactivity timer and then move the UE from source cell to target 25
cell. 26
7. Once UE moves to new cell, make an MO data call. 27
8. Repeat the above test steps for 10 iterations. 28
 29
Below are the SA Mode Inter frequency steps  30
1. The 5G cell setup is configured following section 3.2. 31
2. Configure two 5G cells (cell 1 and cell 2) connected to different O-DU and O-CU according to the test 32
configuration. The cells are activated and unloaded.  33
3. Both 5G Cells are configured neighbors to each other, so that UE can use it for cell re -selection.   34
4. The source cell (source O-DU and source O-CU) is the cell where UE is initially placed as depicted in figure 35
4.11-1. 36
5. Power on the UE and UE shall successfully register to source 5G cell. 37
6. Wait till the UE goes in idle mode as per UE inactivity timer and then move the UE from source cell to target 38
cell. 39

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 55

O-RAN.TIFG.E2E-Test.0-v02.00
7. Once UE moves to new cell, make an MO data call. 1
8. Repeat the above test steps for 10 iterations. 2
4.11.4 Test requirements (expected results) 3
The Idle mode inter O-CU cell reselection shall be verified for both intra-frequency and inter-frequency. Also verify 4
call setup latency on a target cell with reference to RRC connection call flow mentioned in 3GPP 38.133, Section 5
5.3.3.1. Following functionalities are also validated to declare the verdict.  6
• RRC connection establishment is complete. 7
• Cell re-selection is successful.  8
In addition to the common minimum set of configuration parameters defined (see 3.3), the following metrics and 9
counters shall be recorded and reported for the performance assessment.  10
gNB side: 11
• Idle mode mobility time from UE, in idle mode at source to UE in idle mode to target. 12
UE side: 13
• Radio parameters such as RSRP, RSRQ, SINR on PDSCH in time (average per second)  14
• KPI’s related to Cell re-selection failure, Idle mode mobility time from UE, in idle mode at source to UE in 15
idle mode to target.. 16
  17
The cell re-selection failure can be found out by checking if the cell re-selection is successful or not. The Idle to 18
Connected on a target cell Time latency is measured by calculating the time between RRC_Idle mode to 19
RRC_Connected state when UE moves to new cell after initiating an MO call. Capture the cell re-selection 20
success/failure and latency for each iteration and sort the latency value observed for each iteration in ascending order. 21
Record the Minimum, Average (Sum of all latency value/ Total Iterations, Total Iterations are 10 in this case) and 22
Maximum latency value observed in below tables:  23
Table 4-14 5G Cell Re-selection Success and Failure KPI 24
KPI Repeat Times
1 2 3 4 5 6 7 8 9 10
Cell Re-selection
Success

Cell Re-selection
Failure

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 56

O-RAN.TIFG.E2E-Test.0-v02.00
4.12 5G/4G Inter-RAT Mobility - 5G to LTE handover 1
4.12.1 Test description and applicability 2
The purpose of this test is to validate inter system handovers between 5G and LTE systems.  The test validates the O-3
CU, O-DU, and O-RU functionality in handling Inter RAT mobility from 5G to LTE.  This scenarios covers handover 4
of UE from 5G to LTE when UE is registered in 5G and moves from 5G coverage area to  LTE coverage area. This 5
scenario is applicable for LTE and 5G SA 6
The test focuses on the procedure of ‘5GS to EPS handover using N26 interface’ as defined in 3GPP TS 23.502 Section 7
4.2.2.2.1 and ‘Inter-RAT mobility as defined in 3GPP TS 28.331 Section 5.4 8
4.12.2 Test setup and configuration 9
The test setup shall include 5G End to End system (gNB and 5G Core) and LTE E2E   system (O-eNB and EPC). 5G and 10
4G core shall be Interconnected for Inter RAT mobility (N26 interface between the MME and AMF) and shall supports 11
a combined anchor point for 4G and 5G, i.e. SMF+PGW-C and UPF+PGW-U. The eNB connects to a 4G-5G 12
interconnected core over the 4G interfaces over S1 to provide 4G LTE service to make E2E LTE system and the O-CU-13
CP/O-CU-UP, O-DU and O-RU would connect to a 4G-5G interconnected core over the 5G interfaces over N2 and N3 14
to make E2E 5G SA system . The DUT (gNB and O-eNB) and the components shall comply to the O-RAN 15
specifications. We will need Real or emulated UE . The test setup (UE, gNB, O-eNB, 5G and 4G Core) should include 16
tools which have the ability to collect traces on the elements and/or packet captures of communication between the 17
elements. Optionally, if some of the network elements are located remotely either in a cloud or on the internet, the 18
additional latency should be calculated and accounted for. The O-eNB, gNB and their components (O-RU, O-DU and 19
O-CU-CP/O-CU-UP) need to have the right configuration and software load. 20
Test configuration : The test configuration is not specified. The utilized test configuration (parameters) should be 21
recorded in the test report.  22
Laboratory setup: The radio conditions of UE can be modified by a variable attenuator/fading generator ins erted 23
between the antenna connectors (if available) of the O-RU and the UE, or appropriately emulated using a UE emulator. 24
The radio conditions of the UE are initially set to excellent using RSRP as the metric. The minimum coupling loss (see 25
Section 3.6) should not be exceeded. The UE should be placed inside and RF shielded box or RF shielded room if the 26
UE is not connected via cable. The UE handover between the cells can be achieved by changing the radio signal  27
strength of the source (gNB) and target cells (O-eNB) using variable attenuators.  28
 29
Field setup: The drive route with source and target cells should be defined. The UE is placed in the centre of cell close 30
to the radiated eNB/gNB antenna(s), where excellent radio conditions (using RSRP as the metric as defined in Section 31
3.6) should be observed. The minimum coupling loss (see 3.5) should not be exceeded. The change in radio conditions is 32
achieved by moving the UE along the drive route from source (gNB) and target cells (O-eNB) 33
4.12.3 Test Procedure 34
Below are the 5G to LTE Inter RAT handover steps  35
1. The test setup is configured according to the test configuration. The test configuration should be recorded in 36
the test report. The serving cell under test is activated and unloaded. All other cells are pow ered off. 37

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 57

O-RAN.TIFG.E2E-Test.0-v02.00
2. 5G and LTE cells are configured as neighbours to each other, so that  the UE can trigger measurement events 1
for Inter RAT handover.   2
3. The test UE is under source cell (gNB) coverage. 3
4. Power on the UE and UE shall successfully register to source 5G cell. 4
5. The full-buffer UDP bi-directional data transmission (see 3.4) from the application server is initiated. 5
6. The UE shall move from source (gNB) and target cells (O-eNB) to perform handover. 6
4.12.4 Test requirements (expected results) 7
The Inter RAT handover call flow shall be verified between 5G and LTE systems. Following functionalities shall also 8
be validated: 9
• PDU Session is established when full-buffer bi-directional data transmission is initiated with 5G source cell 10
• Handover is successful.  11
In addition to the common minimum set of configuration parameters defined (see Section 3.3), the following metrics 12
and counters shall be recorded and reported for the performance assessment.  13
eNB/gNB/Application server side: 14
• Transmit downlink throughput measured at application server in time (average per second)  15
• Received uplink throughput measured at application server in time (average per second)  16
• Uplink packet loss percentage during handover. 17
• Uplink BLER, MCS, MIMO rank (RI) on PUSCH in time (average per second)   18
UE side: 19
• Radio parameters such as RSRP, RSRQ, SINR on PDSCH in time (average per second)  20
• Downlink BLER, MCS, MIMO rank (RI) on PDSCH in time (average per second).  21
• Received Downlink throughput (L1 and L3 PDCP layers) in time (average per second).  22
• Downlink packet loss percentage during handover 23
• Uplink throughput (L1 and L3 PDCP layers) in time (average per second) 24
• Channel utilization, i.e. Number allocated/occupied downlink and uplink RBs in time (per TTI/average per 25
second) and Number of allocated/occupied slots in time. 26
• KPIs related to Handover failure, Call drop, Handover latency, Handover interruption time.  27
 28
4.13 5G/4G Inter-RAT Mobility - LTE to 5G handover 29
4.13.1 Test description and applicability 30
The purpose of this test is to validate inter system handovers between 5G and LTE systems.  The test validates the O-31
CU, O-DU, and O-RU functionality in handling Inter RAT mobility from LTE to 5G. This scenarios covers handover of 32
UE from 5G to LTE when UE is registered in LTE and moves from LTE coverage area to  5G coverage area. This 33
scenario is applicable for LTE and 5G SA 34

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 58

O-RAN.TIFG.E2E-Test.0-v02.00
The test focuses on the procedure of ‘EPS to 5GS handover using N26 interface’ as defined in 3GPP TS 23.502 Section 1
4.2.2.2.2 and ‘Inter-RAT mobility as defined in 3GPP TS 28.331 Section 5.4 2
4.13.2 Test setup and configuration 3
The test setup shall include 5G End to End system (gNB and 5G Core) and LTE E2E   system (O-eNB and EPC). 5G and 4
4G core shall be Interconnected for Inter RAT mobility (N26 interface between the MME and AMF) and shall supports 5
a combined anchor point for 4G and 5G, i.e. SMF+PGW-C and UPF+PGW-U. The eNB connects to a 4G-5G 6
interconnected core over the 4G interfaces over S1 to provide 4G LTE service to make E2E LTE system and the O-CU-7
CP/O-CU-UP, O-DU and O-RU would connect to a 4G-5G interconnected core over the 5G interfaces over N2 and N3 8
to make E2E 5G SA system . The DUT (gNB and O-eNB) and the components shall comply to the O-RAN 9
specifications. We will need Real or emulated UE . The test setup (UE, gNB, O-eNB, 5G and 4G Core) should include 10
tools which have the ability to collect traces on the elements and/or packet captures of communication between the 11
elements. Optionally, if some of the network elements are located remotely either in a cloud or on the internet, the 12
additional latency should be calculated and accounted for. The O-eNB, gNB and their components (O-RU, O-DU and 13
O-CU-CP/O-CU-UP) need to have the right configuration and software load. 14
Test configuration : The test configuration is not specified. The utilized test configuration (parameters) should be 15
recorded in the test report.  16
Laboratory setup: The radio conditions of UE can be modified by a variable attenuator/fading generator ins erted 17
between the antenna connectors (if available) of the O-RU and the UE, or appropriately emulated using a UE emulator. 18
The radio conditions of the UE are initially set to excellent using RSRP as the metric. The minimum coupling loss (see 19
Section 3.6) should not be exceeded. The UE should be placed inside and RF shielded box or RF shielded room if the 20
UE is not connected via cable. The UE handover between the cells can be achieved by changing the radio signal  21
strength of the source (O-eNB) and target cells (gNB) using variable attenuators.  22
 23
Field setup: The drive route with source and target cells should be defined. The UE is placed in the centre of cell close 24
to the radiated eNB/gNB antenna(s), where excellent radio conditions (using RSRP as the metric as defined in Section 25
3.6) should be observed. The minimum coupling loss (see 3.5) should not be exceeded. The change in radio conditions is 26
achieved by moving the UE along the drive route from source (O-NB) and target cells (gNB) 27
4.13.3 Test Procedure 28
Below are the LTE to 5G Inter RAT handover steps  29
1. The test setup is configured according to the test configuration. The test configuration should be recorded in 30
the test report. The serving cell under test is activated and unloaded. All other cells are pow ered off. 31
2. LTE and 5G cells are configured as neighbours to each other, so that  the UE can trigger measurement events 32
for Inter RAT handover.   33
3. The test UE is under source cell (gNB) coverage. 34
4. Power on the UE and UE shall successfully register to source LTE cell. 35
5. The full-buffer UDP bi-directional data transmission (see 3.4) from the application server is initiated. 36
6. The UE shall move from source (LTE) and target cells (gNB) to perform handover. 37

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 59

O-RAN.TIFG.E2E-Test.0-v02.00
4.13.4 Test requirements (expected results) 1
The Inter RAT handover call flow shall be verified between LTE and 5G systems. Following functionalities shall also 2
be validated: 3
• PDU Session is established when full-buffer bi-directional data transmission is initiated with LTE source cell 4
• Handover is successful.  5
In addition to the common minimum set of configuration parameters defined (see Section 3.3), the following metrics 6
and counters shall be recorded and reported for the performance assessment. 7
eNB/gNB/Application server side: 8
• Transmit downlink throughput measured at application server in time (average per second)  9
• Received uplink throughput measured at application server in time (average per second)  10
• Uplink packet loss percentage during handover. 11
• Uplink BLER, MCS, MIMO rank (RI) on PUSCH in time (average per second)   12
UE side: 13
• Radio parameters such as RSRP, RSRQ, SINR on PDSCH in time (average per second)  14
• Downlink BLER, MCS, MIMO rank (RI) on PDSCH in time (average p er second). 15
• Received Downlink throughput (L1 and L3 PDCP layers) in time (average per second).  16
• Downlink packet loss percentage during handover 17
• Uplink throughput (L1 and L3 PDCP layers) in time (average per second) 18
• Channel utilization, i.e. Number allocated/occupied downlink and uplink RBs in time (per TTI/average per 19
second) and Number of allocated/occupied slots in time. 20
KPIs related to Handover failure, Call drop, Handover latency, Handover interruption time . 21
 22

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 60

O-RAN.TIFG.E2E-Test.0-v02.00
 Performance tests 1
This chapter describes the tests evaluating and assessing the performance of radio access network from network end-2
to-end perspective (see Section 3.1). The focus of the testing is on the end-user performance which is compared against 3
the target and expected performance values. The pass and fail thresholds are defined for the test wherever possible. 4
The general test methodologies and configurations are mentioned in Chapter 3.   5
Unless otherwise stated in the chapter, th e tests are suitable and can be performed in both laboratory as well as field 6
testing environments, with pros and cons for each environment. The specific lab and field test setups are mentioned in 7
each test. 8
The following end-to-end performance tests are defined in this chapter as an extension of NGMN testing framework [3] 9
• Downlink peak throughput (single cell, single UE scenario) 10
• Uplink peak throughput (single cell, single UE scenario) 11
• Downlink throughput in different radio conditions (single cell, single UE scenario) 12
• Uplink throughput in different radio conditions (single cell, single UE scenario) 13
• Bidirectional throughput in different radio conditions (single cell, single UE scenario) 14
• Downlink coverage throughput (link budget) (single cell, single UE scenario) 15
• Uplink coverage throughput (link budget) (single cell, single UE scenario) 16
• Downlink aggregated cell throughput (cell capacity) (single cell scenario, multi-UE scenario) 17
• Uplink aggregated cell throughput (cell capacity) (single cell scenario, multi-UE scenario) 18
• Impact of fronthaul latency on downlink peak throughout (single-cell, single UE scenario) 19
• Impact of fronthaul latency on uplink peak throughout (single-cell, single UE scenario) 20
• Impact of midhaul latency on downlink peak throughout (single-cell, single UE scenario) 21
• Impact of midhaul latency on uplink peak throughout (single cell, single UE scenario) 22
Future versions of this specification may add additional end-to-end performance tests not currently addressed in this 23
version, for example: 24
• Downlink throughput with inter-cell interferences (multi-cell, single UE scenario) 25
• Uplink throughput with inter-cell interferences (multi-cell, single UE scenario) 26
• Downlink drive throughput (multi-cell, single UE scenario) 27
• Uplink drive throughput (multi-cell, single UE scenario) 28
• End-to-end latency (single cell scenario, single UE scenario) 29
• Impact of fronthaul compression scheme on downlink peak throughput (single cell, single UE scenario) 30
• Impact of fronthaul compression scheme on uplink peak throughput (single  cell, single UE scenario) 31
• Resource scheduling (single cell, multi-UE scenario) 32

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 61

O-RAN.TIFG.E2E-Test.0-v02.00
5.1 Expected throughput calculation  1
This chapter provides methodology to calculate expected theoretical downlink and uplink data throughput for 4G LTE 2
as well 5G NR and for both FDD and TDD duplex modes. 3
5.1.1 4G LTE  4
The 4G LTE expected theoretical downlink or uplink throughput at the physical layer can be calculated using the 5
following formula: 6
𝐿1 𝑡ℎ𝑟𝑜𝑢𝑔ℎ𝑝𝑢𝑡 [𝑀𝑏𝑝𝑠] = 10−6 · ∑ (𝑣𝐿𝑎𝑦𝑒𝑟
𝑗 · 𝑄𝑚
𝑗 · 𝑅 · 𝑁𝑃𝑅𝐵
𝐵𝑊,𝑗 · 12
𝑇𝑠
· (1 − 𝑂𝐻𝑗 ) · 𝐷𝐿𝑈𝐿𝑟𝑎𝑡𝑖𝑜)
𝐽
𝑗=1
 7
where 8
• J is the number of aggregated LTE component carriers. 9
• 𝑅 is the coding rate corresponding to channel quality (CQI and SINR). The maximum coding rate is 0.9258. 10
For the j-th component carrier: 11
• 𝑣𝐿𝑎𝑦𝑒𝑟𝑠 is the number of MIMO layers. The maximum is 4 (8 in LTE-Advanced) in downlink and 1 (4 in LTE 12
-Advanced) in uplink. 13
• 𝑄𝑚 is the modulation order, which is equal to 2 for QPSK, 4 for 16QAM, 5 for 32QAM, 6 for 64QAM, 8 for 14
256QAM. 15
• 𝑁𝑃𝑅𝐵
𝐵𝑊  is the number of PRBs allocated in bandwidth BW – see Table 5-1. 16
Table 5-1 The number of PRBs allocated in bandwidth 17
Bandwidth [MHz] 1.4 3 5 10 15 20
Number of PRBs 6 13 25 50 75 100
• OH is overhead for control channels and signalling (i.e. Reference Signal, PSS, SSS, PBCH, PDCCH, etc. in 18
downlink or SRS, PUCCH, PRACH in uplink) within a period of 1 sec – see Table 5-2 for downlink. 19
Table 5-2 Overhead as a function of bandwidth, no. of antenna ports and CFI  20
Bandwidth 1.4 MHz 3 MHz 5 MHz 10 MHz 15 MHz 20 MHz
CFI 1 2 1 2 1 2 1 2 1 2 1 2
1 TX (ant. ports) 0.16 0.21 0.14 0.18 0.12 0.17 0.12 0.16 0.11 0.16 0.11 0.16
2 TX (ant. ports) 0.20 0.24 0.17 0.22 0.16 0.21 0.15 0.20 0.15 0.20 0.15 0.20
4 TX (ant. ports) 0.25 0.29 0.22 0.26 0.21 0.25 0.20 0.25 0.20 0.25 0.20 0.24
• 𝑇𝑠 is the average OFDM symbol duration in a subframe, and it is equal to  𝑇𝑠 = 10−3/14 for normal Cyclic 21
Prefix (14 OFDM symbols per slot) or  𝑇𝑠 = 10−3/12 for extended CP (12 OFDM symbols per slot). 22
• 𝐷𝐿𝑈𝐿𝑟𝑎𝑡𝑖𝑜 is a ratio of the symbols allocated for DL or UL data to total number of symbols in a frame (10ms) 23
– predefined patters (uplink-downlink configuration, special subframe configuration) for DL/UL allocation in 24
[25]. In case of FDD mode, the DLULratio is equal to 1. 25
Example of calculation of expected downlink throughput for the following system configuration:  26
• MIMO 2x2 → 𝑣𝐿𝑎𝑦𝑒𝑟𝑠 = 2 27

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 62

O-RAN.TIFG.E2E-Test.0-v02.00
• BW = 20 MHz → 𝑁𝑃𝑅𝐵
𝐵𝑊 =100 1
• Normal CP 2
• CFI = 1 → OH = 0.15 3
• no carrier aggregation → J = 1 4
• MCS = 28 and modulation 64QAM → 𝑄𝑚 = 6 and 𝑅 = 0.9258 5
• TDD with uplink-downlink configuration = 1 (i.e. DSUUD) and special subframe configuration = 7 (i.e. 6
10:2:2) → 𝐷𝐿𝑈𝐿𝑟𝑎𝑡𝑖𝑜 = (14 · 2 + 10)/14 · 5) =  0.543 7
𝐿1 𝑡ℎ𝑟𝑜𝑢𝑔ℎ𝑝𝑢𝑡 [𝑀𝑏𝑝𝑠] = 10−6 · ∑ (2 · 6 · 0.9258 · 100 · 12
10−3
14
· (1 − 0.15) · 0.543)
1
𝑗=1
= 86.1 𝑀𝑏𝑝𝑠 8
 9
5.1.2 5G NR 10
The 5G NR expected theoretical downlink or uplink throughput at physical layer (UE category is not assumed) can be 11
calculated using the following formula [15]: 12
𝐿1 𝑡ℎ𝑟𝑜𝑢𝑔ℎ𝑝𝑢𝑡 [𝑀𝑏𝑝𝑠] = 10−6 · ∑ (𝑣𝐿𝑎𝑦𝑒𝑟𝑠
𝑗 · 𝑄𝑚
𝑗 · 𝑓𝑗 · 𝑅 · 𝑁𝑃𝑅𝐵
𝐵𝑊𝑗,𝜇 · 12
𝑇𝑆
𝜇 · (1 − 𝑂𝐻𝑗) · 𝐷𝐿𝑈𝐿𝑟𝑎𝑡𝑖𝑜 )
𝐽
𝑗=1
 13
where 14
• J is the total number of component carriers (CC) in a band or band combination. The maximum number is 16 15
[23]. 16
• 𝑅 is the coding rate corresponding to channel quality (CQI, SINR), and it is calculated as Target_code_rate 17
[22]/1024. For LDPC code the maximum coding rate is 948/1024. 18
For the j-th component carrier: 19
• 𝑣𝐿𝑎𝑦𝑒𝑟𝑠 is the number of MIMO layers. The maximum is 8 in downlink and 4 in uplink  [23]. 20
• 𝑄𝑚 is the modulation order, which is equal to 2 for QPSK, 4 for 16QAM, 5 for 32QAM, 6 for 64QAM, 8 for 21
256QAM. 22
• 𝑓 is the scaling factor [24] which can take the values 0.4, 0.75, 0.8 or 1. The scaling factor is signalled per 23
band and per band combination as per UE capability signalling.   24
• 𝑂𝐻 is the overhead of control channels and signalling within a period of 1 sec, and it is equal to  25
o 0.14 for frequency range 1 (FR1) for DL 26
o 0.18 for frequency range 2 (FR2) for DL 27
o 0.08 for frequency range 1 (FR1) for UL 28
o 0.10 for frequency range 2 (FR2) for UL 29
• µ is 5G NR numerology (sub-carrier spacing) which can take the values from 0 to 4 [20]. The number of slots 30
per frame and sub-carrier spacing vary with numerology – see Table 5-3.  31
 32

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 63

O-RAN.TIFG.E2E-Test.0-v02.00
Table 5-3 5G NR numerology 1
5G NR
numerology µ
Sub-carrier
spacing (SCS)
Number of slots per frame Number of symbols
per slot
Total number of
symbols per frame
0 15 kHz 10 14 (normal CP) 140
1 30 kHz 20 14 (normal CP) 280
2
60 kHz 40 14 (normal CP) 560
60 kHz 40 12 (extended CP) 480
3 120 kHz 80 14 (normal CP) 1120
4 240 kHz 160 14 (normal CP) 2240
• 𝑇𝑆
𝜇 is the average OFDM symbol duration in a subframe for numerology µ,  and it is equal to 𝑇𝑆
𝜇 = 10−3/(14 ·2
2µ) for normal Cyclic Prefix (14 OFDM symbols per slot) or  𝑇𝑆
𝜇 = 10−3/(12 · 2µ) for extended CP (12 OFDM 3
symbols per slot).  4
• 𝑁𝑃𝑅𝐵
𝐵𝑊,µ is the number of PRBs allocated in bandwidth BW with numerology µ - see Table 5-4.  5
Table 5-4 The max. number of PRBs  for each supported bandwidth and 5G NR numerology µ [18], [19] 6
 Channel bandwidth BW [MHz]
µ 5 10 15 20 25 30 40 50 60 80 90 100 200 400
0 25 52 79 106 133 160 216 270 N/A N/A N/A N/A N/A N/A
1 11 24 38 51 65 78 106 133 162 217 245 273 N/A N/A
2 N/A 11 18 24 31 38 51 65 79 107 121 135 N/A N/A
3 N/A N/A N/A N/A N/A N/A N/A 66 N/A N/A N/A 132 264 N/A
4 N/A N/A N/A N/A N/A N/A N/A 32 N/A N/A N/A 66 132 264
• 𝐷𝐿𝑈𝐿𝑟𝑎𝑡𝑖𝑜 is a ratio of the symbols allocated for DL or UL d ata to total number of symbols in a time period  7
(periodicity) in TDD mode – predefined slot formats in [21]. In case of FDD mode, the DLULratio is equal to 1.   8
Example of calculation of expected downlink throughput for the following system  configuration:  9
• SCS = 15kHz → µ = 0 10
• MIMO 2x2 → 𝑣𝐿𝑎𝑦𝑒𝑟𝑠 = 2 11
• BW = 20 MHz → 𝑁𝑃𝑅𝐵
𝐵𝑊,µ=106 12
• Normal CP 13
• no carrier aggregation → J = 1 14
• MCS = 28 and modulation 64QAM → 𝑄𝑚 = 6 and Target code rate = 948 15
• scaling factor f = 1 16
• downlink throughput and FR1 → OH = 0.14 17
• TDD configuration 4:1 (DDDDSU), i.e. four DL slots (slot format 0 [21]), one Special slot (slot format 32 18
[21]) and one UL slot (slot format 1 [21]) where slot format 0 means allocation of all 14 symbols for DL, slot 19
format 1 means allocation of all 14 symbols for UL, slot format 32 means allocation of 10 symbols for DL, 2 20
symbols for Guard Period, 2 symbols for UL → 𝐷𝐿𝑈𝐿𝑟𝑎𝑡𝑖𝑜 = (14 · 4 + 10)/14 · 6) =  0.7857  21

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 64

O-RAN.TIFG.E2E-Test.0-v02.00
𝐿1 𝑡ℎ𝑟𝑜𝑢𝑔ℎ𝑝𝑢𝑡 [𝑀𝑏𝑝𝑠] = 10−6 · ∑ (2 · 6 · 1 · 948/1024 · 106 · 12
10−3
14 · 20
· (1 − 0.14) · 0.7857) =
1
𝑗=1
133.6 𝑀𝑏𝑝𝑠 1
 2
5.2 Downlink peak throughput 3
5.2.1 Test description and applicability 4
The purpose of the test is to measure the peak  (i.e. maximum achievable) user data throughput in the downlink direction 5
(i.e. data transmitted from application (traffic) server to UE).  A stationary UE is placed under excellent radio conditions 6
inside an isolated cell.   7
5.2.2 Test setup and configuration 8
The test setup is a single cell scenario (i.e.  an isolated cell without any inter -cell interference – see Section 3.7) with a 9
stationary UE (real or emulated UE) placed under excellent radio conditions as defined in Section 3.6 – using SINR as 10
the metric since this is a downlink test . Note that in this case of a single cell scenario, SINR is in fact SNR as  inter-cell 11
interference is not present. Within the cell there should be only one active UE downloading data from the application 12
server.  The application server should be placed as close as possible to the core /core emulator  and connected to the 13
core/core emulator via a transport link with sufficient capacity so as not to limit the expected data throughput. The test is 14
suitable for both lab and field environments. 15
Test configuration : The test configuration  is not specified. The utilized test configuration  (parameters) should be  16
recorded in the test report.  17
Laboratory setup: The radio conditions experienced by the UE can be  modified using a variable attenuator /fading 18
generator inserted between the antenna connectors (if available) of the O-RU and the UE, or appropriately emulated using 19
a UE emulator. The test environment should be setup  to achieve excellent radio conditions  (SINR as defined in Section 20
3.6) for the UE, but the minimum coupling loss (see Section 3.6) should not be exceeded.  The UE should be placed inside 21
an RF shielded box or RF shielded room if the UE is not connected via cable. 22
Field setup: The UE is placed in the centre of cell close to the radiated eNB/gNB antenna(s), where excellent radio 23
conditions (SINR as defined in Section 3.6) should be observed. The minimum coupling loss (see Section 3.6) should not 24
be exceeded. 25

3GPP Core
4G EPC4G eNB S1 3GPP
services
Other
services
Application server
O-RAN System under test
 UE Uu

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 65

O-RAN.TIFG.E2E-Test.0-v02.00

Figure 5-1 The test setups of 4G, 5G NSA and 5G SA 1
5.2.3 Test Procedure 2
1. The test setup is configured according to the test configuration. The test configuration should be recorded in the test 3
report. The serving cell under test is activated and unloaded. All other cells are turned off.  4
2. The UE (real or emulated UE)  is placed under excellent radio conditio ns (cell centre close to radiated eNB/gNB 5
antenna) using SINR thresholds as indicated in Section 3.6. The UE is powered on and attached to the network.  6
3. The downlink full-buffer UDP and TCP data transmission (see Section 3.4) from the application server should be 7
verified by adjusting the connection settings (cabled environment) or UE position (OTA environment). The UE under 8
excellent radio conditions that is achieving peak user throughput should see stable utilization of the highest possible 9
downlink MCS, downlink transport block size and downlink MIMO rank (number of layers). These KPIs should also 10
be verified.  11
4. The UE should be turned off or set to airplane mode, if possible, to empty the buffers. The downlink full-buffer UDP 12
data transmission from the application server to the UE is started. The UE should receive the data from the application 13
server.  14
5. All the required performance data (incl. the signalling and control data)  as specified in the “Test requirements” 15
Section below is measured and captured at the UE and Application server  sides using logging/measurement tools. 16
The duration of the test should be at least 3 minutes when the throughput is stable. The location and position of the 17
UE should remain unchanged during the entire measurement duration (capture of log data).  18
6. The capture of log data is stopped. The downlink full -buffer UDP data transmission from the application server is 19
stopped.  20
7. [Optional] Steps 4 to 6 are repeated for downlink full-buffer TCP data transmission. 21
5.2.4 Test requirements (expected results) 22
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 23
should be captured and reported in the test report for the performance assessment 24
UE side (real or emulated UE): 25
3GPP Core
4G EPC EN-DC NSA
5G en-gNB
4G eNB
(anchor)
S1
S1
3GPP
services
Other
services
Application server
X2
O-RAN System under test
 UE
Uu
Uu
3GPP Core
5GC SA5G gNB NG 3GPP
services
Other
services
Application server
O-RAN System under test
 UE Uu

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 66

O-RAN.TIFG.E2E-Test.0-v02.00
• Radio parameters such as RSRP, RSRQ, CQI, PDSCH SINR (average sample per second) 1
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second) 2
• Received downlink throughput (L1 and Application layers) (average sample per second) 3
• Downlink transmission mode 4
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 5
(average sample per second) 6
Application server side: 7
• Transmitted downlink throughput (Application layer) (average sample per second) 8
When the UE is under excellent radio conditions (cell centre), the stable utilization of the highest possible downlink MCS, 9
downlink transport block size and downlink MIMO rank should be observed.  The UE should also receive the data with 10
minimum downlink BLER. 11
The Table 5-5 gives an example of the test results record (median and standard deviation from the captured samples 12
should be calculated for each metric). In case of 5G SA and NSA, SS-RSRP and SS-SINR should be reported. In case 13
of 5G NSA and dual connectivity (EN-DC), the values should be provided separately for both LTE and 5G paths. The 14
spectral efficiency (see Section 3.8) should be calculated for benchmarking and comparison purposes in order to 15
minimize the influence of different configured parameters such as bandwidth  and TDD DL/UL ratio. 16
Table 5-5 Example record of test results (median and standard deviation from the captured samples) 17
 UDP TCP
Received L1 DL throughput [Mbps]
L1 DL Spectral efficiency [bps/Hz]
Received Application DL throughput [Mbps]
UE RSRP [dBm]
UE RSRQ [dB]
UE SINR [dB]
MIMO rank
PDSCH MSC
DL PRB number
PDSCH BLER [%]
The following figures should be also included in the test report, and the stable behavior should be observed and evaluated. 18
• Received UDP downlink throughput (L1 and Application layer) vs Time duration 19
• PDSCH SINR vs Time duration 20
• Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots vs Time duration 21
The gap analysis should be provided for the measured and the expected target downlink throughputs  which can be 22
calculated based on the procedures from Section 5.1.   23

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 67

O-RAN.TIFG.E2E-Test.0-v02.00
5.3 Uplink peak throughput 1
5.3.1 Test description and applicability 2
The purpose of the test is to measure the peak (i.e. maximum achievable) user data throughput in uplink direction (i.e. 3
data transmitted from UE to application (traffic) server).  The stationary UE is placed under excellent radio conditions 4
inside the isolated cell.   5
5.3.2 Test setup and configuration 6
The test setup is a single cell scenario (i.e. isolated cell without any inter-cell interference – see Section 3.7) with stationary 7
UE (real or emulated UE) placed under excellent radio conditions as defined in Section 3.6 – using RSRP as the metric 8
since this is an  uplink test. Within the cell there should be only one active UE uploading data to the application server.  9
The application server should be placed as close as possible to the core /core emulator and connected to the core /core 10
emulator via a transport link with sufficient capacity so as not to limit  the expected data throughput . The test is suitable 11
for both lab and field environments.  12
Test configuration : The test configuration is not specified . The utilized test configuration  (parameters) should be  13
recorded in the test report.  14
Laboratory setup : The radio conditions experienced by the UE can be modified using a variable attenuator/fading 15
generator inserted between the antenna connectors (if available) of the O-RU and the UE, or appropriately emulated using 16
a UE emulator. The test environment should be setup to achieve excellent radio conditions (RSRP as defined in Section 17
3.6) for the UE, but the minimum coupling loss (see Section 3.6) should not be exceeded.  The UE should be placed inside 18
an RF shielded box or RF shielded room if the UE is not connected via cable.  19
Field setup: The UE is placed in the centre of cell close to the radiated  eNB/gNB antenna(s), where excellent radio 20
conditions (RSRP as defined in Section 3.6) should be observed . The m inimum coupling loss (see 3.5) should not be 21
exceeded. 22
The test setups of 4G, 5G NSA and 5G SA are mentioned in Figure 5-1.  23
5.3.3 Test Procedure 24
1. The test setup is configured according to the test configuration. The test configuration should be recorded in the test 25
report. The serving cell under test is activated and unloaded. All other cells are turned off.  26
2. The UE  (real or emulated UE)  is placed under excellent  radio condition  (cell centre  close to radiated eNB/gNB 27
antenna) using RSRP thresholds as indicated in Section 3.6. The UE is powered on and attached to the network.  28
3. The uplink full-buffer UDP and TCP data transmission (see Section 3.4) from UE to the application server should be 29
verified by adjusting the connection settings (cabled environment) or UE position (OTA environment). The UE under 30
excellent radio conditions that is achieving peak user throughput should see stable utilization of the highest possible 31
uplink MCS and uplink transport block size. These KPIs should also be verified.  32
4. The UE should be turned off or set to airplane mode, if possible, to empty the buffers . The uplink full-buffer UDP 33
data transmission from UE to the application server is started. The application server should receive the sent data.  34
5. All the required performance data (incl. the signalling and control data)  as specified in the following “Test 35
requirements” section  is measured and captured at UE , eNB/gNB and Application server  sides using 36
logging/measurement tools. The duration of test should be at least 3 minutes when the throughput is stable. The 37

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 68

O-RAN.TIFG.E2E-Test.0-v02.00
location and position of the UE should remain unchanged during the entire measurement duration (capture of log 1
data).  2
6. The capture of log data is stopped. The uplink full-buffer UDP data transmission from UE to the application server 3
is stopped.  4
7. The UE should be turned off or set to airplane mode, if possible, to empty the buffers . The uplink full-buffer TCP 5
data transmission from UE to the application server is started, and Step 5 is repeated.  6
5.3.4 Test requirements (expected results) 7
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 8
should be captured and reported in the test report for the performance assessment 9
UE side (real or emulated UE): 10
• Radio parameters such as RSRP, RSRQ, CQI, PDSCH SINR (average sample per second) 11
• PUSCH BLER, PUSCH MCS (average sample per second) 12
• Transmit power on PUSCH  13
• Transmitted uplink throughput (Application layer) (average sample per second) 14
• Channel utilization, i.e. Number allocated/occupied uplink PRBs and Number of allocated/occupied slots  15
(average sample per second) 16
eNB/gNB side (if capture of logs is possible): 17
• Radio parameters such as PUSCH SINR (average per second) 18
• PUSCH BLER (average sample per second) 19
Application server side: 20
• Received uplink throughput (L1 and Application layers) (average sample per second) 21
When the UE is in excellent radio condition (cell centre), t he stable utilization of the highest possible uplink MCS and 22
uplink transport block size should be observed and evaluated.  The eNB/gNB should also receive the data with the 23
minimum uplink BLER.   24
The Table 5-6 gives an example of the test results record (median and standard deviation from the measured samples 25
should be provided for each metric ). In case of 5G SA and NSA, SS -RSRP and SS-SINR should be reported. In case of 26
5G NSA and dual connectivity (EN -DC), the values should be provided separately for both LTE and 5G. The spectral 27
efficiency (see Section 3.8) should be calculated for benchmarking and comparison in order to minimize the influence of 28
different configured parameters such as bandwidth and TDD DL/UL ratio. 29
Table 5-6 Example record of test results (median and standard deviation from the measured samples) 30
 UDP TCP
Received L1 UL throughput [Mbps]
L1 UL Spectral efficiency [bps/Hz]
Received Application UL throughput [Mbps]
UE RSRP [dBm]
UE PDSCH SINR [dB]

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 69

O-RAN.TIFG.E2E-Test.0-v02.00
PUSCH transmit power [dBm]
PUSCH MSC
UL RB number
PUSCH BLER [%]
The following figures should be also included in the test report, and the stable behavior should be observed and evaluated. 1
• Received UDP uplink throughput (L1 and Application layers) vs Time duration 2
• UE RSRP vs Time duration 3
• Number of allocated/occupied uplink PRBs and Number of allocated/occupied slots vs Time duration 4
The gap analysis should be provided for the measured and the expected target uplink throughputs which can be calculated 5
based on the procedures from Section 5.1.   6
5.4 Downlink throughput in different radio conditions 7
5.4.1 Test description and applicability 8
The purpose of the test is to measure the user experienced data throughput  in the downlink direction while varying  9
received radio signal quality (strength). The UE is placed in different stationary points inside the isolated cell.  10
5.4.2 Test setup and configuration 11
The test setup is a single cell scenario (i.e. isolated cell without any inter-cell interference – see Section 3.7) with stationary 12
UE (real or emulated UE)  placed in different radio conditions as defined in Section 3.6 - SINR should be considered in 13
case of downlink . Note that i n this case of single cell scenario, SINR is in fact SNR as inter -cell interferences are not 14
present. The UE is sequentially placed in different radio conditions ranging from good  to poor. Note that the testing of 15
peak downlink throughout in excellent radio conditions is already covered in Section 5.2 and so is skipped in this section. 16
Within the cell there should be only one active UE in time downloading data from th e application (traffic) server.  The 17
application server should be placed as close as possible to the core/core emulator and connected to the core/core emulator 18
via a transport link with sufficient capacity so as not to limit the expected data throughput. The test is suitable for lab as 19
well as field environment. 20
Test configuration : The test configuration is not specified . The utilized test configuration  (parameters) should be  21
recorded in the test report.  22
Laboratory setup : The radio conditions experienced by the UE can be modified using a variable attenuator/fading 23
generator inserted between the antenna connectors (if available) of the O-RU and the UE, or appropriately emulated using 24
a UE emulator. The radio conditions of UE are initially set to good. The minimum coupling loss (see Section 3.6) should 25
not be exceeded.  The change in radio conditions of UE, from excellent through fair and good to poor, is  achieved by 26
increasing the attenuation of radio signal. The UE should be placed inside RF shielded box or RF shielded room if the 27
UE is not connected via cable. 28
Field setup: The test points with good, fair and poor radio conditions (as defined in Section 3.6) should be defined inside 29
the serving cell. The minimum coupling loss (see 3.5) should not be exceeded.  The UE is initially placed where good 30
radio conditions (SINR as defined in Section 3.6) should be observed. The change in radio conditions is achieved by 31
moving the UE inside the serving cell from close to cell centre (with good radio conditions) to cell edge (with poor radio 32
conditions).  33
The test setups of 4G, 5G NSA and 5G SA are mentioned in Figure 5-1.  34

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 70

O-RAN.TIFG.E2E-Test.0-v02.00
5.4.3 Test Procedure 1
1. The test setup is configured according to the test configuration. The test configuration should be recorded in the test 2
report. The serving cell under test is activated and unloaded. All other cells are turned off.  3
2. The UE (real or emulated UE) is placed under good radio conditions (close to cell centre) using SINR thresholds as 4
indicated in Section 3.6. The UE is powered on and attached to the network.  5
3. The downlink full -buffer UDP and TCP data transmission (see Section 3.4) from the application server should be 6
verified by adjusting the connection settings (cabled environment) or UE position (OTA environment)  to achieve 7
good radio conditions.  8
4. The UE should be turned off or set to airplane mode, if possible, to empty the buffers. The downlink full-buffer UDP 9
data transmission from the application server to the UE is started. The UE should receive the data from the application 10
server.  11
5. All the required performance data (incl. the signalling and control data)  as specified in the following “Test 12
requirements” section is measured and captured at UE and Application server sides using logging/measurement tools. 13
The duration of test should be at least 3 minutes when the throughput is stable. The location and position of the UE 14
should remain unchanged during the entire measurement duration (capture of log data).   15
6. The capture of log data is stopped. The downlink full -buffer UDP data t ransmission from the application server is 16
stopped.  17
7. The radio conditions of UE are changed to fair using SINR thresholds as indicated  in Section 3.6. The steps 4 to 6 18
are repeated.  19
8. The radio conditions of UE are changed to poor (cell edge) radio condition  using SINR thresholds as indicated  in 20
Section 3.6. Steps 4 to 6 are repeated.  21
9. [Optional] Steps 4 to 8 are repeated for downlink full-buffer TCP data transmission. 22
5.4.4 Test requirements (expected results) 23
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 24
should be captured and reported in the test report for the performance assessment 25
UE side (real or emulated UE): 26
• Radio parameters such as RSRP, RSRQ, CQI, PDSCH SINR (average sample per second) 27
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second) 28
• Received downlink throughput (L1, and Application layers) (average sample per second) 29
• Downlink transmission mode 30
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 31
(average sample per second) 32
Application server side: 33
• Transmitted downlink throughput (Application layer) (average sample per second) 34

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 71

O-RAN.TIFG.E2E-Test.0-v02.00
As the UE moves from good (close to cell centre), to fair, and to poor (cell edge) radio conditions , the changing radio 1
conditions should cause the UE to report lower CQI and MIMO rank which results in assignment of lower MCS and 2
lower data throughput in the downlink. 3
The Table 5-7 gives an example of the test results rec ord (median and standard deviation from the captured samples 4
should be calculated for each metric). In case of 5G SA and NSA, SS-RSRP and SS-SINR should be reported. In case of 5
5G NSA and dual connectivity (EN -DC), the values should be provided separately for both LTE and 5G paths. The 6
spectral efficiency (see Section 3.8) should be calculated for benchmarking and comparison in order to minimize the 7
influence of different configured parameters such as bandwidth and TDD DL/UL ratio. 8
Table 5-7 Example record of test results (median and standard deviation from the captured samples)  9

Good Fair Poor
(cell edge)
UDP / TCP UDP / TCP UDP / TCP
Received L1 DL throughput [Mbps]
L1 DL Spectral efficiency [bps/Hz]
Received Application DL throughput [Mbps]
UE RSRP [dBm]
UE PDSCH SINR [dB]
MIMO rank
PDSCH MSC
DL RB number
PDSCH BLER [%]
The following figures should be also included in the test report, and the stable behavior should be observed and evaluated. 10
• Received UDP downlink throughput (L1 and Application layer) vs Time duration 11
• PDSCH SINR vs Time duration 12
• Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots vs Time duration 13
The gap analysis should be provided for the measured and the expected target downlink throughputs which can be 14
calculated based on the procedures from Section 5.1.    15
5.5 Uplink throughput in different radio conditions 16
5.5.1 Test description and applicability 17
The purpose of the test is to measure the user experience d data throughput in uplink while varying received radio signal 18
quality (strength). The UE is placed in different stationary points inside the isolated cell.  19
5.5.2 Test setup and configuration 20
The test setup is a single cell scenario (i.e. isolated cell without any inter-cell interference – see Section 3.7) with stationary 21
UE (real or emulated UE)  placed in different radio condit ions as defined in Section 3.6 - RSRP should be considered in 22
case of uplink. The UE is gradually placed in different radio conditions from good (close to cell centre) to poor (cell edge 23
– coverage limited cell edge in case of single cell scenario ). Note that the testing of peak uplink throughout in excellent 24
radio conditions is already covered in Section 5.3 and so is skipped in this section . Within the cell there should be only 25
one active UE in time uploading data to the application (traffic) server.  The application server should be placed as close 26

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 72

O-RAN.TIFG.E2E-Test.0-v02.00
as possible to the core/core emulator and connected to the core/core emulator via a transport link with sufficient capacity 1
so as not to limit the expected data throughput. The test is suitable for both lab and field environments. 2
Test configuration : The test configuration is no t specified . The utilized test configuration  (parameters) should be  3
recorded in the test report.  4
Laboratory setup : The radio conditions experienced by the UE can be modified using a variable attenuator/fading 5
generator inserted between the antenna connectors (if available) of the O-RU and the UE, or appropriately emulated using 6
a UE emulator. The radio conditions of UE are initially set to good. The minimum coupling loss (see Section 3.6) should 7
not be exceeded.  The change in radio conditions of UE, from good to fair to poor, is achieved by increasing the attenuation 8
of radio signal. The UE should be placed inside RF shielded box or RF shielded room if the UE is not connected via cable. 9
Field setup: The test points with good, fair and poor radio conditions (as defined in Section 3.6) should be identified 10
inside the serving c ell. The minimum coupling loss (see 3.5) should not be exceeded.  The UE is initially placed where 11
good radio conditions (RSRP as defined in Section 3.6) should be observed. The change in radio conditions is achieved 12
by moving the UE inside the serving cell from  close to cell centre (with good radio conditions) to c ell edge (with poor 13
radio conditions). 14
The test setups of 4G, 5G NSA and 5G SA are illustrated in Figure 5-1.  15
5.5.3 Test Procedure 16
1. The test setup is configured according to the test configuration. The test configuration should be recorded in the test 17
report. The serving cell under test is activated and unloaded. All other cells are turned off.  18
2. The UE (real or emulated UE) is placed under good radio condition (close to cell centre) using RSRP thresholds as 19
indicated in Section 3.6. The UE is powered on and attached to the network.  20
3. The uplink full-buffer UDP and TCP data transmission (see Section 3.4) from UE to the application server should be 21
verified by adjusting the connection settings (cabled environment) or UE position (OTA environment)  to achieve 22
good radio conditions.   23
4. The UE should be turned off or set to airplane mode, if possible, to empty the bu ffers. The uplink full-buffer UDP 24
data transmission from  UE to the application server is started. The application server should receive the data  from 25
UE.  26
5. All the required performance data (incl. the signalling and control data)  as specified in the followi ng “Test 27
requirements” section  is measured and captured at UE , eNB/gNB and application server side using 28
logging/measurement tools. The duration of test should be at least 3 minutes when the throughput is stable. The 29
location and position of the UE should remain unchanged during the entire measurement duration (capture of log 30
data).  31
6. The capture of log data is stopped. The uplink full-buffer UDP data transmission from UE to the application server 32
is stopped.  33
7. The radio conditions of UE are changed to fair using RSRP thresholds as indicated  in Section 3.6. The steps 4 to 6 34
are repeated.  35
8. The radio conditions of UE are chan ged to poor (cell edge) radio condition using RSRP thresholds as indicated  in 36
Section 3.6. Steps 4 to 6 are repeated.  37
9. [Optional] Steps 4 to 8 are repeated for uplink full-buffer TCP data transmission. 38

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 73

O-RAN.TIFG.E2E-Test.0-v02.00
5.5.4 Test requirements (expected results) 1
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 2
should be captured and reported in the test report for the performance assessment 3
UE side (real or emulated UE): 4
• Radio parameters such as RSRP, RSRQ, CQI, PDSCH SINR (average sample per second) 5
• PUSCH BLER, PUSCH MCS (average sample per second) 6
• Transmit power on PUSCH  7
• Transmitted uplink throughput (Application layer) (average sample per second) 8
• Channel utilization, i.e. Number allocated/occupied uplink PRBs and Number of allocated/occupied slots 9
(average sample per second) 10
eNB/gNB side (if capture of logs is possible): 11
• Radio parameters such as PUSCH SINR (average per second) 12
• PUSCH BLER (average per second) 13
Application server side: 14
• Received uplink throughput (L1 and Application layers) (average sample per second) 15
The Table 5-8 gives an example of the test results record (median and standard deviation from the captured samples 16
should be calculated for each metric). In case of 5G SA and NSA, SS-RSRP and SS-SINR should be reported. In case of 17
5G NSA and dual connectivity (EN -DC), the values should be provided separately for both LTE and 5G paths. The 18
spectral efficiency (see Section 3.8) should be calculated for benchmarking and comparison in order to minimize the 19
influence of different configured parameters such as bandwidth and TDD DL/UL ratio. 20
Table 5-8 Example record of test results (median and standard deviation from the measured samples) 21

Good Fair Poor
(cell edge)
UDP / TCP UDP / TCP UDP / TCP
Received L1 UL throughput [Mbps]
L1 UL Spectral efficiency [bps/Hz]
Received Application UL throughput [Mbps]
UE RSRP [dBm]
UE PDSCH SINR [dB]
PUSCH transmit power [dBm]
PUSCH MSC
UL RB number
PUSCH BLER [%]
The following figures should be also included in the test report, and the stable behavior should be observed and evaluated. 22
• Received UDP uplink throughput (L1 and Application layers) vs Time duration 23
• UE RSRP vs Time duration 24

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 74

O-RAN.TIFG.E2E-Test.0-v02.00
• Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots vs Time duration 1
The gap analysis should be provided for the measured and the expected target uplink throughputs which can be calculated 2
based on the procedures from Section 5.1.    3
5.6 Bidirectional throughput in different radio conditions 4
5.6.1 Test description and applicability 5
The purpose of the test is to measure the user experienced data throughput in  both downlink and uplink in parallel while 6
varying received radio signal quality (strength) . The UE is placed in different stationary points inside the isolated cell. 7
The test also includes the measurement of peak (maximum achievable) data throughput of UE located in the excellent 8
radio conditions, and cell edge coverage data throughput of UE located at the cell edge in the poor radio conditions.  9
5.6.2 Test setup and configuration 10
The test setup is a single cell scenario (i.e. isolated cell without any inter-cell interference – see Section 3.7) with stationary 11
UE (real or emulated) placed in different radio conditions as defined in Section 3.6 –RSRP should be considered in this  12
case. The UE is gradually placed in different radio conditions from excellent (cell centre) to poor (cell edge  – coverage 13
limited cell edge in case of single cell scenario). Within the cell there should be only one active UE in time simultaneously 14
downloading data from and uploading data to the application (traffic) server.  The application server should be placed as 15
close as possible to the core /core emulator and connected to the core /core emulator via a transport link  with sufficient 16
capacity so as not to limit the expected data throughput. The test is suitable for both lab and field environments. 17
Test configuration : The test configuration is not specified . The utilized test configuration  (parameters) should be  18
recorded in the test report.  19
Laboratory setup : The radio conditions ex perienced by the UE can be modified using a variable attenuator/fading 20
generator inserted between the antenna connectors (if available) of the O-RU and the UE, or appropriately emulated using 21
a UE emulator . The radio conditions of UE are initially set to e xcellent. The minimum coupling loss (see  Section 3.6) 22
should not be exceeded. The change in radio conditions of UE, from excellent through fair and good to poor, is achieved 23
by increasing the attenuation of radio signal. The UE should be placed inside RF shielded box or RF shielded room if the 24
UE is not connected via cable. 25
Field setup: The test points with excellent, good, fair and poor radio conditions (as defined in Section 3.6) should be 26
identified inside the serving cell. The minimum coupling loss (see 3.5) should not be exceeded. The UE is initially placed 27
in the cell center close to the eNB/gNB antenna(s), where excellent radio conditions (using RSRP as the metric as defined 28
in Section 3.6) should be observed. The change in radio conditions is achieved by moving the UE inside the serving cell 29
from cell centre (with excellent radio conditions) to cell edge (with poor radio conditions).  30
The test setups of 4G, 5G NSA and 5G SA are illustrated in Figure 5-1.  31
5.6.3 Test Procedure 32
1. The test setup is configured according to the test configuration. The test configuration should be recorded in the test 33
report. The serving cell under test is activated and unloaded. All other cells are turned off.  34
2. The UE (real or emulated) is placed under excellent radio conditions as using RSRP thresholds as indicated in Section 35
3.6. The UE is powered on and attached to the network.  36
3. The simultaneous d ownlink and uplink  full-buffer UDP  and TCP data transmission (see Section 3.4) should be 37
verified by adjusting the connection settings (cabled environment) or UE position (OTA environment). The UE under 38

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 75

O-RAN.TIFG.E2E-Test.0-v02.00
excellent radio conditions that is achieving peak uplink and downlink user throughput should see stable utilization of 1
the highest possible MCS, downl ink block size and MIMO rank (number of layers). These KPIs should also be 2
verified.  3
4. The UE should be turned off or set to airplane mode, if possible, to empty the buffers . The simultaneous downlink 4
and uplink full-buffer UDP data transmissions are started. Both the UE and application server should receive the data.  5
5. All the required performance data (incl. the signalling and control data)  as specified in the “Test requirements” 6
Section below are measured and captured at UE, eNB/gNB and application server side using logging/measurement 7
tools. The duration of test should be at least 3 minutes when the throughput is stable. The location and position of the 8
UE should remain unchanged during the entire measurement duration (capture of log data).   9
6. The capture of log data is stopped. The simultaneous downlink and up link full-buffer UDP data transmission s are 10
stopped.  11
7. The radio conditions of UE are changed to good as defined by both SINR and RSRP in Section 3.6, if possible. The 12
steps 4 to 6 are repeated.  13
8. The radio conditions of UE are changed to fair as defined by both SINR and RSRP in Section 3.6, if possible. The 14
steps 4 to 6 are repeated. 15
9. The radio conditions of UE are changed to poor (cell edge) radio condition as defined by both SINR and RSRP in 16
Section 3.6, if possible. The steps 4 to 6 are repeated.  17
10. [Optional] Steps 4 to 9 are repeated for simultaneous downlink and uplink full-buffer TCP data transmission.  18
5.6.4 Test requirements (expected results) 19
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 20
should be captured and reported in the test report for the performance assessment 21
UE side (real or emulated): 22
• Radio parameters such as RSRP, RSRQ, CQI, PDSCH SINR (average sample per second) 23
• PDSCH BLER, PDSCH MCS, PUSCH BLER, PUSCH MCS (average sample per second) 24
• DL MIMO rank (number of layers) (average sample per second) 25
• Downlink transmission mode 26
• Transmit power on PUSCH  27
• Received downlink throughput (L1, and Application layers) (average sample per second) 28
• Transmitted uplink throughput (Application layer) (average sample per second) 29
• Channel utilization, i.e. Number allocated/occupied PRBs and Number of allocated/occupied slots  in both 30
downlink and uplink directions (average sample per second) 31
eNB/gNB side (if capture of logs is possible): 32
• Radio parameters such as PUSH SINR (average per second) 33
Application server side: 34
• Received uplink throughput (L1 and Application layers) (average sample per second) 35

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 76

O-RAN.TIFG.E2E-Test.0-v02.00
• Transmitted downlink throughput (Application layer) (average sample per second)  1
When the UE is  under excellent radio conditions (cell centre), t he stable utilization of the highest possible MCS and 2
transport block size  in both uplink and downlink direction  should be observed and evaluated. The UE and eNB/gNB  3
should also receive the data with the minimum downlink and uplink BLER, respectively.  4
The Table 5-9 gives an example of the test results record (median and standard deviation from the captured samples 5
should be calculated for each metric). In case of 5G SA and NSA, SS-RSRP and SS-SINR should be reported. In case of 6
5G NSA and dual connectivity (EN -DC), the values should be provided separately for both LTE and 5G paths. The 7
spectral efficiency (see Section 3.8) should be calculated for benchmarking and comparison in order to minimize the 8
influence of different configured parameters such as bandwidth and TDD DL/UL ratio. 9
Table 5-9 Example record of test results (median and standard deviation from the captured samples) 10

Excellent
(cell centre) Good Fair Poor
(cell edge)
UDP / TCP UDP / TCP UDP / TCP UDP / TCP
Received L1 UL throughput [Mbps]
L1 UL Spectral efficiency [bps/Hz]
Received Application UL throughput [Mbps]
Received L1 DL throughput [Mbps]
L1 DL Spectral efficiency [bps/Hz]
Received Application DL throughput [Mbps]
UE RSRP [dBm]
UE PDSCH SINR [dB]
PUSCH transmit power [dBm]
DL MIMO rank
DL MSC
UL MSC
DL RB number
UL RB number
DL PDSCH BLER [%]
UL PUSCH BLER [%]
 11
The following figures should be also included in the test report, and the stable behavior should be observed and evaluated. 12
• Received UDP uplink throughput (L1 and Application layer) vs Time duration 13
• Received UDP downlink throughput (L1 and Application layer) vs Time duration 14
• UE RSRP vs Time duration 15
• UE PDSCH SINR vs Time duration 16
• Number of allocated/occupied downlink PRBs and Number of allocated/occupied downlink slots vs Time 17
duration 18
• Number of allocated/occupied uplink PRBs and Number of allocated/occupied uplink slots vs Time duration 19

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 77

O-RAN.TIFG.E2E-Test.0-v02.00
The bidirectional DL and UL throughputs should be compared with unidirectional downlink (Section 5.4) and uplink 1
(Section 5.5) throughput s. Assuming the same test conditions (radio conditions), the bidirectional and unidirectional 2
throughputs are expected to be equal.  3
The gap analysis should be provided for the measured and the expected target downlink and uplink  throughputs which 4
can be calculated based on the procedures from Section 5.1.    5
5.7 Downlink coverage throughput (link budget)  6
5.7.1 Test description and applicability 7
The purpose of the test is to measure the downlink user data throughput (i.e. data transmitted from application (traffic) 8
server to UE) when radio conditions of UE change gradually. Test is verified by moving UE from center to edge of the 9
isolated cell on the main lobe of eNB/gNB antenna until UE loses the coverage (call drop). Test assesses link adaptation 10
and effect on scheduling, CQI, MCS, Number of layers (MIMO Rank) assignment etc. during the movement of UE from 11
excellent radio conditions to poor radio conditions. 12
5.7.2 Test setup and configuration 13
The test setup is a single cell scenario (i.e. isolated cell without any inter-cell interference – see Section 3.7) with UE (real 14
or emulated) slowly moving in the main lobe of eNB/gNB antenna out from cell centre to cell edge until UE loses the 15
coverage (call drop). The drive route inside the cell should be defined to cover the whole range of SINR values from 16
excellent (cell center) to poor (cell edge) radio conditions as defined in Section 3.6. Note that in case of single cell scenario 17
SINR is in fact SNR as inter -cell interferences are not present. Within the cell there should be only one active UE 18
downloading UDP/TCP data from the application server.  The application server should be placed as close as possible to 19
the core/core emulator and connected to the core /core emulator via a transport link  with sufficient capacity so as not to 20
limit the expected data throughput. The test is suitable for both lab and field environments. 21
Test configuration : The test configuration is not specified . The utilized test configuration  (parameters) should be  22
recorded in the test report.  23
Laboratory setup : The radio conditions experienced by the UE can be modified using a variable attenuator/fading 24
generator inserted between the antenna connectors (if available) of the O-RU and the UE, or appropriately emulated using 25
a UE emulator . The radio conditions of UE are initially set to excellent . The minimum coupling loss (see  Section 3.6) 26
should not be exceeded. The movement of UE out from cell centre to cell edge can be achieved by gradually increasing 27
the attenuation of radio signal to cover the whole range of SINR from excellent through good and fair to poor (as defined 28
in Section 3.6) until UE loses the coverage (call drop). The UE should be placed inside RF shielded box or RF shielded 29
room if the UE is not connected via cable. 30
Field setup: The drive route inside the isolated cell should be defined to cover the whole range of SINR values from 31
excellent (cell center) through good and fair to poor (cell edge) (as defined in Section 3.6) until UE loses the coverage 32
(call drop). The UE is placed in the centre of cell close to the radiated eNB/gNB antenna(s). The minimum coupling loss 33
(see 3.5) should not be exceeded.  The change in radio conditions is achieved by moving the UE along the drive route out 34
from cell centre to cell edge until UE loses the coverage (call drop) – see Figure 5-2.  35

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 78

O-RAN.TIFG.E2E-Test.0-v02.00
 1
Figure 5-2 Testing of link budget in the field setup 2
5.7.3 Test Procedure 3
1. The test setup is configured according to the test configuration. The test configuration should be recorded in the test 4
report. The serving cell under test is activated and unloaded. All other cells are turned off.  5
2. The UE (real or emulated) is placed under excellent radio condition (cell centre) using SINR thresholds as indicated 6
in Section 3.6. The UE is powered on and attached to the network.  7
3. The downlink full -buffer UDP and TCP data transmission (see Section 3.4) from the application server should be 8
verified. The excellent radio conditions experiencing peak user throughput is identified with stable utilization of the 9
highest possible downlink MCS, downlink transport block size and downlink MIMO rank (number of layers). These 10
KPIs should also be verified.  11
4. The UE should be turned off or set to airplane mode, if possible, to empty the buffers. The downlink full-buffer UDP 12
data transmission from the application server to the UE is started. The UE should receive the data from the  application 13
server.  14
5. All the required performance data (incl. the signalling and control data)  as specified in the following “Test 15
requirements” section is measured and captured at UE and Application server sides using logging/measurement tools.  16
6. In the field setup, the UE is moved along the defined drive route out from cell centre (excellent radio conditions) to 17
cell edge (poor radio conditions)  on the main lobe of eNB/gNB antenna and with constant speed of around 30 kph 18
until UE loses the coverage (call drop).  19
7. In the lab setup, the attenuation between the antenna connectors of O -RU and UE is gradually increased until UE 20
losses the coverage (call drop). 21
8. The capture of log data is stopped. The downlink full-buffer UDP data transmission from the application server to 22
UE is stopped.  23
9. [Optional] Steps 5 to 7 are repeated for downlink full-buffer TCP data transmission.   24
5.7.4 Test requirements (expected results) 25
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 26
should be captured and reported in the test report for the performance assessment 27
UE side (real or emulated): 28
UE

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 79

O-RAN.TIFG.E2E-Test.0-v02.00
• Radio parameters such as RSRP, RSRQ, CQI, PDSCH SINR (average sample per second) 1
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second) 2
• Received downlink throughput (L1 and Application layers) (average sample per second) 3
• Downlink transmission mode (average sample per second) 4
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 5
(average sample per second) 6
• GPS coordinates (latitude, longitude) in the field setup 7
Application server side: 8
• Transmitted downlink throughput (Application layer) (average sample per second) 9
Initially when the UE is in the excellent radio condition s (cell centre) , the stable utilization of the highest possible 10
downlink MCS, downlink transport block size and downlink MIMO rank should be observed and evaluated. The UE 11
should also receive the data with the minimum downlink BLER. As the UE moves out from excellent radio conditions 12
(cell centre), the radio conditions of the UE change gradually from excellent  through good and fair to poor (see Section 13
3.6). The changing radio conditions may cause UE reporting lower CQI  and MIMO rank which results in assignment of 14
lower MCS and lower data throughput in downlink. Such a behavior should be observed and evaluated.  15
Note that the test results can be affected by the various coverage enhancing features (e.g. transmit/receive diversity) at 16
eNB/gNB and/or at UE sides. The used coverage enhancement features should be also listed in the test report.  17
The following figures should be included in the test report, and the behavior should be evaluated.  In case of 5G SA and 18
NSA, SS-RSRP and SS-SINR should be reported. In case of 5G NSA and dual connectivity (EN -DC), the values should 19
be provided separately f or both LTE and 5G paths. The spectral efficiency (see Section 3.8) should be calculated for 20
benchmarking and comparison in order to minimize the inf luence of different configured parameters such as bandwidth  21
and TDD DL/UL ratio. 22
• Received UDP downlink throughput (L1 and Application layer) vs PDSCH SINR (all the samples as well as 23
average curve with median values) 24
• Received UDP L1 downlink spectral efficiency vs PDSCH SINR (all the samples as well as average curve with 25
median values) 26
• Cumulative distribution function of PDSCH SINR  27
• Cumulative distribution function of PDSCH MCS  28
• Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots vs PDSCH SINR (all the 29
samples as well as average curve with median values) 30
In the field environment, the cell coverage distance in meters (i.e. a straight line from the cell site) that corresponds to the 31
cell-edge downlink application throughput of 6 Mbps for LTE and 50 Mbps for 5G NR should be recorded.  32
5.8 Uplink coverage throughput (link budget)  33
5.8.1 Test description and applicability 34
The purpose of the test is to measure the uplink user data throughput (i.e. data transmitted from UE to application (traffic) 35
server) when radio conditions of UE change gradually. Test is verified by moving UE from center to edge of the isolated 36
cell on the main lobe of eNB/gNB antenna until UE loses the coverage (call drop). Test assesses link adaptation and effect 37

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 80

O-RAN.TIFG.E2E-Test.0-v02.00
on scheduling, uplink transmit power (power control), CQI, MCS, etc. during the movement of UE from excellent radio 1
conditions to poor radio conditions. 2
5.8.2 Test setup and configuration 3
The test setup is a single cell scenario (i.e. isolated cell without any inter-cell interference – see Section 3.7) with UE (real 4
or emulated) slowly moving in the main lobe of eNB/gNB antenna out from cell centre to cell edge until UE loses the 5
coverage (call drop). The drive route inside the cell should be defined to cover the whole range of RSRP values from 6
excellent (cell center) to poor (cell edge) radio conditions as defined in Section 3.6. Within the cell there should be only 7
one active UE uploading UDP/TCP data to application server.  The application server should be placed as close as possible 8
to the core/core emulator and connected to the core/core emulator via a transport link with sufficient capacity so as not to 9
limit the expected data throughput. The test is suitable for both lab and field environments. 10
Test configuration : The test configuration is not specified . The utilized test configuration  (parameters) should be  11
recorded in the test report.  12
Laboratory setup : The radio conditions experienced by the UE can be modified using a variable attenuator/fading 13
generator inserted between the antenna connectors (if available) of the O-RU and the UE, or appropriately emulated using 14
a UE emulator . The radio conditions of UE are initially set to excellent. The minimum coupling loss (see  Section 3.6) 15
should not be exceeded.  The movement of UE out from cell centre to cell edge can be achieved by gradually increasing 16
the attenuation of radio signal to cover the whole range of RSRP from excellent through good and fair to poor (as defined 17
in Section 3.6) until UE loses the coverage (call drop). The UE should be placed inside RF shielded box or RF shielded 18
room if the UE is not connected via cable. 19
Field setup: The drive route inside the isolated cell should be defined to cover the whole range of RSRP values from 20
excellent (cell center) through good and fair to poor (cell edge) (as defined in Section 3.6) until UE loses the coverage 21
(call drop). The UE is placed in the centre of cell close to the radiated eNB/gNB antenna(s). The minimum coupling 22
loss (see 3.5) should not be exceeded.  The change in radio conditions is achieved by moving the UE along the drive 23
route out from cell centre to cell edge until UE loses the coverage (call drop) – see Figure 5-2.  24
5.8.3 Test Procedure 25
1. The test setup is configured according to the test configuration. The test configuration should be recorded in the test 26
report. The serving cell under test is activated and unloaded. All other cells are turned off.  27
2. The UE (real or emulated) is placed under excellent radio condition (cell centre) using RSRP thresholds as indicated 28
in Section 3.6. The UE is powered on and attached to the network.  29
3. The uplink full-buffer UDP and TCP data transmission (see Section 3.4) from UE to the application server should be 30
verified. The UE under excellent radio conditions that is achieving peak user throughput should see stable utilization 31
of the highest possible uplink MCS and uplink transport block size. These KPIs should also be verified.  32
4. The UE should be turned off or set to airplane mode, if possible, to empty the buffers . The uplink full-buffer UDP 33
data transmission from the application server to the UE is start ed. The application server should receive data from 34
the UE.  35
5. All the required performance data (incl. the signalling and control data)  as specified in the following “Test 36
requirements” section  is measured and captured at UE , eNB/gNB and Application server sides using 37
logging/measurement tools.  38
6. In the field setup, the UE is moved along the defined drive route out from cell centre (excellent radio conditions) to 39
cell edge (poor radio conditions) on the main lobe of eNB/gNB an tenna and with constant speed of around 30 kph 40
until UE loses the coverage (call drop).  41

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 81

O-RAN.TIFG.E2E-Test.0-v02.00
7. In the lab setup, the attenuation between the antenna connectors of O -RU and UE is gradually increased until UE 1
losses the coverage (call drop). 2
8. The capture of log data  is stopped. The uplink full-buffer UDP data transmission from UE to the application server 3
is stopped.  4
9. [Optional] Steps 4 to 7 are repeated for uplink full-buffer TCP data transmission. 5
5.8.4 Test requirements (expected results) 6
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 7
should be captured and reported in the test report for the performance assessment 8
UE side (real or emulated): 9
• Radio parameters such as RSRP, RSRQ, CQI, PDSCH SINR (average sample per second) 10
• PUSCH BLER, PUSCH MCS (average sample per second) 11
• Transmit power on PUSCH (average sample per second) 12
• Transmitted uplink throughput (Application layer) (average sample per second) 13
• Channel utilization, i.e. Number allocated/occupied uplink PRBs and Number of allocated/occupied slots 14
(average sample per second) 15
• GPS coordinates (latitude, longitude) in the field setup 16
eNB/gNB side (if capture of logs is possible): 17
• Radio parameters such as PUSCH SINR (average per second) 18
• PUSCH BLER (average per second) 19
Application server side: 20
• Received uplink throughput (L1 and Application layers) (average sample per second) 21
Initially when the UE is in the excellent radio conditions (cell centre), the stable utilization of the highest possible uplink 22
MCS and uplink transport block size should be observed and evaluated. The eNB/gNB should also receive the data with 23
the minimum uplink BLER. As the UE moves out from excellent radio conditions (cell centre) , the radio conditions of 24
the UE change gradually from excellent through good and fair to p oor (see Section 3.6). With deteriorat ing channel 25
conditions and increasing path loss, eNB/gNB would start experiencing worse PUSCH/PUCCH BLER.  To compensate 26
the path loss and limit PUSCH/PUCCH BLER, eNB/gNB shall command the UE to increate PUSCH/PUCCH transmit 27
power through closed loop Transmit Power Control (TPC) feature. The number of HARQ retransmissions may be also 28
increased. If the power control does not reduce PUSCH/PUCCH BLER bellow a threshold, eNB/gNB shall schedule UE 29
with lower MCS and lower MIMO rank in uplink which results in lower uplink data throughput. The UE moving further 30
away from eNB/gNB can cause radio link failure and call drop  (due to UL Max RLC retransmissions, failure in CQI 31
decoding, low uplink SINR, etc.)  which results in triggering of RRC connection re -establishment procedure . Such a 32
behavior should be observed and evaluated.  33
Note that the test results can be affected by the various coverage enhancing features (e.g. transmit/receive diversity) at 34
eNB/gNB and/or at UE sides. The used coverage enhancement features should be also listed in the test report.  35
The following figures should be included in the test report, and the behavior should be evaluated.  In case of 5G SA and 36
NSA, SS-RSRP and SS-SINR should be reported. In case of 5G NSA and dual connectivity (EN -DC), the values should 37
be provided separ ately for both LTE and 5G paths. The spectral efficiency (see Section 3.8) should be calculated for 38

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 82

O-RAN.TIFG.E2E-Test.0-v02.00
benchmarking and comparison in order to minimize the influence of different configured parameters such as bandwidth  1
and TDD DL/UL ratio. 2
• Received UDP uplink throughput (L1 and Application layer) vs RSRP (all the samples as well as average curve 3
with median values) 4
• Received UDP L1 uplink spectral efficiency vs RSRP (all the samples as well as average curve with median 5
values) 6
• PUSCH transmit power vs RSRP 7
• Cumulative distribution function of RSRP  8
• Cumulative distribution function of PUSCH MCS  9
• Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots vs RSRP (all the samples 10
as well as average curve with median values) 11
In the field environment, the cell coverage distance in meters (i.e. a straight line from the cell site) that corresponds to 12
the cell-edge uplink application throughput of 0.5 Mbps for LTE and 5 Mbps for 5G NR should be recorded.  13
5.9 Downlink aggregated cell throughput (cell capacity)  14
5.9.1 Test description and applicability 15
The purpose of the test is to validate the downlink aggregated cell throughput (downlink cell capacity) when the UEs are 16
distributed in a uniform or non-uniform way inside a cell. This test also captures the influence of MU -MIMO feature on 17
aggregated cell throughput. However, the test does not require support of MU-MIMO or massive MIMO. The aggregated 18
cell throughput depends on the multiple factors like spatial distribution of UEs inside a cell, radio conditions of UEs, test 19
configuration supported by both UE and eNB/gNB, etc. The test covers two spatial distribution scenarios of test UEs, i.e. 20
uniform distribution and non-uniform distribution. 21
5.9.2 Test setup and configuration 22
The test setup is a single cell scenario (i.e. an isolated cell without any inter -cell interference – see Section 3.7) with 10 23
stationary UEs (real or emulated UEs) in total where 1 UE should be placed in excellent radio conditions, 2 UEs in good 24
radio conditions, 4 UEs in fair radio conditions and 3 UEs in poor radio conditions. The radio conditions are defined in 25
Section 3.6 – SINR (range of values)  should be considered in case of downlink. Note that in this case of single cell 26
scenario, SINR is in fact SNR as inter-cell interferences are not present. 27
The UEs can be distributed in the following spatial distribution scenarios (see Figure 5-3 The distribution 28
scenarios of UEs inside the cell using horizontal, vertical or 3D beamforming 29
):    30
a) uniform distribution: ten UEs are placed uniformly with a spatial separation in both horizontal and vertical 31
directions. Uniform distribution maximizes the potential of MU-MIMO, because the spatially separated UEs can 32
be served by different beams at the same time. The channel experienced by each UE is spatially un-correlated 33
which reduces the inter-beam interference. In addition, selecting orthogonal precoders will further reduce the 34
inter-beam interference.  35
b) non-uniform distribution: the UEs are grouped in clusters. The UEs in the cluster experience very similar radio 36
conditions. In addition, the UEs in the cluster cannot be spatially separated and served by different beams at the 37

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 83

O-RAN.TIFG.E2E-Test.0-v02.00
same time. This distribution scenario restricts the scheduling options of eNB/gNB resulting in lower aggregated 1
cell throughput.  2
Figure 5-3 also shows impact of different types of beamforming (namely horizontal beamforming, vertical 3
beamforming and 3D beamforming) on aggregated cell throughput. The number of vertical beams is usually 4
lower compared to horizontal beams, i.e. the number of UEs which can be spatially  separated and served by 5
different vertical beams at the same time is also lower resulting in lower aggregated cell throughput compared to 6
horizontal beamforming. The typical scenario for vertical beamforming is a high -rise building where each floor 7
can be covered by different vertical beam. For example, in Figure 5-3 The distribution scenarios of UEs inside 8
the cell using horizontal, vertical or 3D beamforming 9
 a), UEs 2 and 4 are located in the coverage of the same horizontal beam thus cannot be served at the same time, 10
while in Figure 5-3 The distribution scenarios of UEs inside the cell using horizontal, vertical or 3D beamforming 11
 b), the same UEs 2 and 4 are located in the coverage of two different vertical beams thus can be served at the same time. 12

a) uniform distribution with horizontal beamforming

b) uniform distribution with vertical beamforming
Excellent radio conditions
Good radio conditions
Fair radio conditions
Poor radio conditions
1
5
7
9
8
4
2
3
6
beam 1
beam 2
beam 3
1
5
7
9
8
4
2
3
6

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 84

O-RAN.TIFG.E2E-Test.0-v02.00

c) uniform distribution with 3D beamforming

d) non-uniform distribution with horizontal beamforming
Figure 5-3 The distribution scenarios of UEs inside the cell using horizontal, vertical or 3D beamforming 1
Within the cell there should be 10 active UEs in total downloading data from the application server. The application server 2
should be placed as close as possible to the core/core emulator and connected to t he core/core emulator via a transport 3
link with sufficient capacity so as not to limit the expected data throughput. The test is suitable for both lab and field 4
environments.  5
Test configuration : The test configuration is not specified. The utilized test c onfiguration (parameters) should be 6
recorded in the test report.  7
 8
Laboratory setup : The radio conditions experienced by the UE can be modified using a variable attenuator/fading 9
generator inserted between the antenna connectors (if available) of the O-RU and the UE, or appropriately emulated using 10
a UE emulator. It is recommended to use a UE emulator to properly distribute the UEs inside a cell according to selected 11
distribution scenario. The minimum coupling loss (see Section 3.6) should not be exceeded. The UEs should be placed 12
inside an RF shielded box or RF shielded room if the UEs are not connected via cables.  13
 14
Field setup : The UEs should be distributed inside a cell according to selected distribution scenario. The minimum 15
coupling loss (see Section 3.6) should not be exceeded. 16
5.9.3 Test Procedure 17
1. The test setup is configured according to the test configuration. The test configuration should be recorded in the test 18
report. The serving cell under test is activated and unloaded. All other cells are turned off.  19
2. Ten UEs (real or emulated) are placed inside a serving cell according to uniform distribution scenario, and in radio 20
conditions using SINR thresholds as indicated in Section 3.6 - 1 UE should be placed in excellent radio conditions, 2 21
UEs in good radio conditions, 4 UEs in fair radio conditions and 3 UEs in poor radio conditions. The UEs are powered 22
on and attached to the network.  23
5
7
9
8
4
3
6
2
1
57
98
4
6
1
3 2

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 85

O-RAN.TIFG.E2E-Test.0-v02.00
3. The downlink full-buffer UDP and TCP data transmission s (see Section 3.4) from the application server to all UEs 1
should be verified.  2
4. The UEs should be turned off or set to airplane mode, if possible, to empty the buffers. The downlink full-buffer UDP 3
data transmissions from the application server to all UEs are started. All UEs should receive the data from application 4
server.  5
5. All the required performance data (incl. the signalling and control data)  as specified in the following “Test 6
requirements” section is measured and captured at all UEs and Application server using logging/measurement tools.  7
6. The capture of log data is stopped. The downlink full-buffer UDP data transmission from the application server to 8
UEs is stopped.  9
7. [Optional] Steps 5 to 6 are repeated for downlink full-buffer TCP data transmissions.   10
8. [Optional] Non-uniform spatial distribution scenario of UEs is setup. Steps 3 to 7 are repeated.   11
5.9.4 Test requirements (expected results) 12
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 13
should be captured and reported in the test report for the performance assessment 14
UE side (real or emulated UE): 15
• Radio parameters such as RSRP, RSRQ, CQI, PDSCH SINR (average sample per second) 16
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second) 17
• Received downlink throughput (L1, and Application layers) (average sample per second) 18
• Downlink transmission mode 19
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 20
(average sample per second) 21
• GPS coordinates (latitude, longitude) in the field setup 22
Application server side: 23
• Transmitted downlink throughput (Application layer) (average sample per second) 24
The Table 5-10 Example record of test results (median and standard deviation from the captured samples) 25
 gives an example of the test results record (median and standard deviation from the captured samples should be 26
calculated for each metric). In case of 5G SA and NSA, SS-RSRP and SS-SINR should be reported. In case of 5G NSA 27
and dual connectivity (EN-DC), the values should be provided separately for both LTE and 5G paths. The spectral 28
efficiency (see Section 3.8) should be calculated for benchmarking and comparison in order to minimize the influence 29
of different configured parameters such as bandwidth and TDD DL/UL ratio. 30
Table 5-10 Example record of test results (median and standard deviation from the captured samples) 31

For each UE (UE 1 to UE 10)
UDP / TCP
Spatial distribution scenario [uniform/non-uniform]
Received L1 DL throughput [Mbps]
Aggregated cell L1 DL throughput [Mbps] sum of Received L1 DL throughput (UE 1 to UE 10)

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 86

O-RAN.TIFG.E2E-Test.0-v02.00
L1 DL Spectral efficiency [bps/Hz]
Received Application DL throughput [Mbps]
Aggregated cell App. DL throughput [Mbps] sum of Received App. DL throughput (UE 1 to UE 10)
UE RSRP [dBm]
UE PDSCH SINR [dB]
MIMO rank
PDSCH MSC
DL RB number
Aggregated DL RB number sum of DL RB number (UE 1 to UE 10)
PDSCH BLER [%]
 1
In addition, the used spatial distribution scenario(s) should be properly described and depicted.  2
The following figures should be also included in the test report, and the stable behavior should be observed and evaluated.  3
• received total UDP/TCP downlink throughput at Application server (L1 and Application layers) vs Time 4
duration 5
For each UE: 6
• UE PDSCH SINR vs Time duration 7
• number of allocated/occupied downlink PRBs and Number of allocated/occupied slots vs Time duration 8
5.10 Uplink aggregated cell throughput (cell capacity)  9
5.10.1 Test description and applicability 10
The purpose of the test is to validate the uplink aggregated cell throughput  (uplink cell capacity)  when the UEs are 11
distributed in a uniform or non-uniform way inside a cell. This test also captures the influence of MU-MIMO feature on 12
aggregated cell throughput. The aggregated cell throughput depends on the multiple factors like spatial distribution of 13
UEs inside a cell, radio conditions  of UEs, test configuration supported by both UE and eNB/gNB, etc. The test covers 14
two spatial distribution scenarios of test UEs, i.e. uniform distribution and non-uniform distribution. 15
5.10.2 Test setup and configuration 16
The test setup is a single cell scenario (i.e. an isolated cell without any inter -cell interference – see Section 3.7) with 10 17
stationary UEs (real or emulated UEs) in total where 1 UE should be placed in excellent radio conditions, 2 UEs in good 18
radio conditions, 4 UEs in fair radio conditions and 3 UEs in poor radio conditions. The radio conditions are defined in 19
Section 3.6 – RSRP (range of values) should be considered in case of uplink.  20
The UEs can be distributed in the following spatial distribution scenarios ( see Figure 5-3 The distribution 21
scenarios of UEs inside the cell using horizontal, vertical or 3D beamforming 22
).  23
a) uniform distribution: ten UEs are placed uniformly with a spatial separation in both horizontal and vertical 24
directions. Uniform distribution maximizes the potential of MU-MIMO, because the spatially separated UEs can 25

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 87

O-RAN.TIFG.E2E-Test.0-v02.00
be served by different beams at the same time. The channel experienced by each UE is spatially un-correlated 1
which reduces the inter-beam interference.  2
b) non-uniform distribution: the UEs are grouped in clusters. The UEs in the cluster experience very similar radio 3
conditions. In addition, the UEs in the cluster cannot be spatially separated and served by different beams at the 4
same time. This distribution scenario restricts the scheduling options of eNB/gNB resulting in lower aggregated 5
cell throughput.  6
Within the cell there should be 10 active UEs in total uploading data to the application server. The application server 7
should be placed as close as possible to the core/core emulator and connected to the core/core e mulator via a transport 8
link with sufficient capacity so as not to limit the expected data throughput. The test is suitable for both lab and field 9
environments.  10
Test configuration : The test configuration is not specified. The utilized test configuration ( parameters) should be 11
recorded in the test report.  12
 13
Laboratory setup : The radio conditions experienced by the UE can be modified using a fading generator inserted 14
between the antenna connectors (if available) of the O -RU and the UE, or appropriately emula ted using a UE emulator. 15
It is recommended to use a UE emulator to properly distribute the UEs inside a cell according to selected distribution 16
scenario. The minimum coupling loss (see Section 3.6) should not be exceeded. The UEs should be placed inside an RF 17
shielded box or RF shielded room if the UEs are not connected via cables.  18
 19
Field setup : The UEs should be distributed inside a cell according to selected distri bution scenario. The minimum 20
coupling loss (see Section 3.6) should not be exceeded. 21
5.10.3 Test Procedure 22
1. The test setup is configured according to the test configuration. The test configuration should be recorded in the test 23
report. The serving cell under test is activated and unloaded. All other cells are turned off.  24
2. Ten UEs (real or emulated) in total are placed inside a serving cell according to uniform distribution scenario, and in 25
radio conditions  using RSRP thresholds as indicated in Section 3.6 - 1 UE should be placed in excellent  radio 26
conditions, 2 UEs in good radio conditions, 4 UEs in fair radio conditions and 3 UEs in poor radio conditions. The  27
UEs are powered on and attached to the network.  28
3. The uplink full-buffer UDP and TCP data transmission s (see Section 3.4) from all UEs to application server should 29
be verified.  30
4. The UEs should be turned off or set to airplane mode, if possible, to empty the buffers. The uplink full-buffer UDP 31
data transmissions from all UEs to the application server are started. The application server should receive data from 32
all UEs.  33
5. All the required performance data (incl. the signalling and control data)  as specified in the following “Test 34
requirements” section  is measured and captured at all UEs, eNB/gNB and Application server using 35
logging/measurement tools.  36
6. The capture of log data is stopped. The uplink full-buffer UDP data transmissions from the UEs to application server 37
are stopped.  38
7. [Optional] Steps 4 to 6 are repeated for uplink full-buffer TCP data transmissions.   39
8. [Optional] Non-uniform spatial distribution scenario of UEs is setup. Steps 3 to 7 are repeated.   40

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 88

O-RAN.TIFG.E2E-Test.0-v02.00
5.10.4 Test requirements (expected results) 1
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 2
should be captured and reported in the test report for the performance assessment 3
UE side (real or emulated UE): 4
• Radio parameters such as RSRP, RSRQ, CQI, PDSCH SINR (average sample per second)  5
• PUSCH BLER, PUSCH MCS (average sample per second) 6
• Transmit power on PUSCH (average sample per second) 7
• Transmitted uplink throughput (Application layer) (average sample per second)  8
• Channel utilization, i.e. Number allocated/occupied uplink PRBs and Number of allocated/occupied slots 9
(average sample per second) 10
• GPS coordinates (latitude, longitude) in the field setup 11
eNB/gNB side (if capture of logs is possible): 12
• Radio parameters such as PUSCH SINR (average per second) 13
• PUSCH BLER (average per second) 14
Application server side: 15
• Received uplink throughput (L1 and Application layers) (average sample per second) 16
The Table 5-11 gives an example of the test results record (median and standard deviation from the captured samples 17
should be calculated for each metric). In case of 5G SA and NSA, SS-RSRP and SS-SINR should be reported. In case of 18
5G NSA and dual connectivity (EN -DC), the values should be provided separately for both LTE and 5G paths. The 19
spectral efficiency (see Section 3.8) should be calculated for benchmarking and comparison in order to minimize the 20
influence of different configured parameters such as bandwidth and TDD DL/UL ratio. 21
Table 5-11 Example record of test results (median and standard deviation from the captured samples)  22

For each UE (UE 1 to UE 10)
UDP / TCP
Spatial distribution scenario [uniform/non-uniform]
Received L1 UL throughput [Mbps]
Aggregated cell L1 UL throughput [Mbps] sum of Received L1 UL throughput (UE 1 to UE 10)
L1 UL Spectral efficiency [bps/Hz]
Received Application UL throughput [Mbps]
Aggregated cell App. UL throughput [Mbps] sum of Received App. UL throughput (UE 1 to UE 10)
UE RSRP [dBm]
UE PDSCH SINR [dB]
PUSCH transmit power [dBm]
PUSCH MSC
UL RB number
Aggregated UL RB number sum of UL RB number (UE 1 to UE 10)

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 89

O-RAN.TIFG.E2E-Test.0-v02.00
PUSCH BLER [%]
 1
In addition, the used spatial distribution scenario(s) should be properly described and depicted.  2
The following figures should be also included in the test report, and the stable behavior should be observed and evaluated.  3
• received total UDP/TCP uplink throughput at Application server (L1 and Application layers) vs Time duration 4
For each UE: 5
• UE RSRP vs Time duration 6
• number of allocated/occupied uplink PRBs and Number of allocated/occupied slots vs Time duration 7
5.11 Impact of fronthaul latency on downlink peak throughout  8
5.11.1 Test description and applicability 9
The purpose of the test is to evaluate the user peak downlink throughput as a function of the fronthaul transport latency 10
(i.e. one -way t ransmission delay between O -RU and O -DU at OpenFH (CUSM -plane interface), and to identify the 11
maximum applicable fronthaul transport latency.  Since the fronthaul transport latency corresponds with the distance 12
(fiber length) between O -RU and O -DU, the results of test can be also used to identi fy the maximum distance (fiber 13
length) with an acceptable degradation of user peak downlink throughput. 14
5.11.2 Test setup and configuration 15
The network setup is single cell scenario (i.e. isolated cell without any inter -cell interference – see Section 3.7) with 16
stationary UE (real or emulated UE) placed in the excellent radio condition as defined in Section 3.6 - SINR should be 17
considered in case of downlink. Within the cell there should be only one active UE downloading data from the application 18
server. The application server should be placed as close as possible to the core/core emulat or and connected to the 19
core/core emulator via transport link with sufficient capacity not limiting the expected data throughput. The test is suitable 20
for lab as well as field environment. 21
The reference measurement points of the fronthaul latency (R1/R4 – Transmit/Receive interface at O -DU (CU-plane); 22
R2/R3 – Receive/Transmit interfaces at O -RU (CU-plane)) [16] are shown in Figure 5-4. Transmission delay between 23
O-RU and O -DU are specified as T12 (downlink  direction) and T34 (uplink  direction). Transmission delay should be 24
symmetrical and equal in both directions . The transmission delay encompasses only the time from when a bit leaves the 25
sender (R1/ R3) until it is received at the receiver (R2/ R4). 26
 27
 28
Figure 5-4 Definition of reference measurement points for fronthaul latency [16] 29
 30

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 90

O-RAN.TIFG.E2E-Test.0-v02.00
Test configuration : The test configuration is not specified. The utilized test configuration (parameters) should be 1
recorded in the test report.  2
Laboratory setup: The radio conditions of UE can be modified by a variable attenuator/fading generator inserted between 3
the antenna connectors of O-RU and UE. The minimum attenuation of radio signal should be set to achieve the excellent 4
radio conditions (SINR as defined in Section 3.6), but the minimum coupling loss (see Section 3.6) should not be 5
exceeded. The UE should be placed inside RF shielded box or RF shielded room if the UE is not connected via  cable.  6
Field setup: The UE is placed in the centre of cell close to the radiated eNB/gNB antenna(s), where excellent radio 7
conditions (SINR as defined in Section 3.6) should be observed. The minimum coupling loss (see Section 3.6) should not 8
be exceeded. 9
In both laboratory and field setups, Figure 5-5, the fronthaul latency can be modified by using of various lengths of fibre 10
(assuming typical delay of fibre around 5 us per kilometre) or by using of a network impairment emulator inserted between 11
O-RU and O-DU. It is necessary to know/measure the transmission delays produced by all fronthaul transport components 12
between O-RU and O-DU in order to properly estimate the total fronthaul latency (T12/T34). 13

Figure 5-5 The fronthaul transport latency test setups of 4G, 5G NSA and 5G SA 14
5.11.3 Test Procedure 15
1. The test setup is configured according to the test configuration. The test configuration should be recorded in the test 16
report. The serving cell under test is activated and unloaded. All other cells are turned off.  17
2. The UE (real or emulated) is placed in t he excellent radio condition (cell centre) as defined by SINR in Section 3.6. 18
The UE is powered on and attached to the network  19
3GPP Core
4G EPC
S1 3GPP
services
Other
services
Application server
O-RAN System under test
 UE Uu 4G O-RU 4G
O-DU/CU
Inserted
latency
OpenFH latency
S1
S1
 UE
Uu
Uu
O-RAN System under test
4G O-RU 4G
O-DU/CU
Inserted
latency
OpenFH latency
5G O-RU 5G
O-DU/O-CU
Inserted
latency
3GPP Core
4G EPC EN-DC NSA
3GPP
services
Other
services
Application server
X2
3GPP Core
5G SA
NG 3GPP
services
Other
services
Application server
O-RAN System under test
 UE Uu 5G O-RU 5G
O-DU/O-CU
Inserted
latency
OpenFH latency

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 91

O-RAN.TIFG.E2E-Test.0-v02.00
3. The downlink full -buffer UDP and TCP data transmission (see Section 3.4) from the application server should be 1
verified. The excellent radio conditions experiencing peak user throughput is identified with stable utilization of the 2
highest possible downlink MCS, downlink transport block size and downlink M IMO rank (number of layers). The 3
utilization of these KPIs should be also verified.  4
4. The minimum fronthaul latency ( one-way transmission delay between O -DU and O -RU) should be set using a 5
network impairment emulator or a fibre with corresponding length (as suming typical delay of fibre around 5 us per 6
kilometre).  7
5. The UEs should be turned off or set to airplane mode, if possible, to empty the buffers. The downlink full-buffer UDP 8
data transmission from the application server to the UE is started. The UE shou ld receive the data from application 9
server.  10
6. All the required performance data (incl. the signalling and control data) as specified in the following “Test 11
requirements” section is measured and captured at UE and Application server using logging/measuremen t tools.  12
7. The capture of log data is stopped. The downlink full-buffer UDP data transmission from the UE to application server 13
is stopped.  14
8. The fronthaul latency is increased by 20 us if no degradation of user peak downlink throughput was observer in the 15
previous measurement. As soon as a degradation of user peak downlink throughout will be observed, the fronthaul 16
latency is increased only by 5 us in order to capture fine-grained log data.  17
9.  Steps 5 to 8 are repeated until the total degradation of user peak  downlink throughput is less than 30%.  The KPIs 18
measured with the minim fronthaul latency are used as a baseline (100%) for calculation of the degradation.  19
10. [Optional] Steps 4 to 9 are repeated for downlink full-buffer TCP data transmission.   20
5.11.4 Test requirements (expected results) 21
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 22
should be recorded and reported for the performance assessment. 23
 24
UE side (real or emulated UE): 25
• Radio parameters such as RSRP, RSRQ, CQI, PDSCH SINR (average sample per second)  26
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second)  27
• Received downlink throughput (L1, and Application layers) (average sample per second)  28
• Downlink transmission mode 29
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 30
(average sample per second) 31
• GPS coordinates (latitude, longitude) in the field setup 32
Application server side: 33
• Transmitted downlink throughput (Application layer) (average sample per second)  34

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 92

O-RAN.TIFG.E2E-Test.0-v02.00
When the UE is in the excellent radio conditions (cell centre), the stable utilization of the highest possible downlink MCS, 1
downlink transport block size and downlink MIMO rank should be observed and evaluated. The UE should also receive 2
the data with the minimum downlink BLER. 3
 4
The Table 5-12 gives an example of the test results record (median and standard deviation from the captured samples 5
should be calculated for each metric). In case of 5G SA and NSA, SS-RSRP and SS-SINR should be reported. In case of 6
5G NSA and dual connectivity (EN-DC), the values should be provided separately for both LTE and 5G. 7
 8
Table 5-12 The example of record of test results (median and standard deviation from the captured samples ) 9

For each measured fronthaul latency value
UDP / TCP
Total fronthaul transport latency (T12/T34) [us]
Received L1 DL throughput [Mbps]
Degradation of Received L1 DL throughput [%]#
Received Application DL throughput [Mbps]
UE RSRP [dBm]
UE RSRQ [dB]
UE PDSCH SINR [dB]
MIMO rank
PDSCH MSC
DL PRB number
PDSCH BLER [%]
# The “Received L1 DL throughput” measured with the minim fronthaul latency is used as a baseline (100%) for calculation of the degradation 10
The following figures should be also included in the test report. 11
• Received UDP/TCP downlink throughput (L1 and Application layers) vs Total fronthaul latency (T12/T34) 12
5.12 Impact of fronthaul latency on uplink peak throughout  13
5.12.1 Test description and applicability 14
The purpose of the test is to evaluate the user peak uplink throughput as a function of the fronthaul transport latency (i.e. 15
one-way transmission delay between O-RU and O-DU at OpenFH (CUSM-plane interface), and to identify the maximum 16
applicable fronthaul transport latency. Since the fronthaul transport latency corresponds with the distance (fiber length) 17
between O-RU and O -DU, the results of test can be also used to identify the maximum distance (fiber lengt h) with an 18
acceptable degradation of user peak uplink throughput. 19
5.12.2 Test setup and configuration 20
The network setup is single cell scenario (i.e. isolated cell without any inter -cell interference – see Section 3.7) with 21
stationary UE (real or emulated UE) placed in the excellent radio condition as defined in Section 3.6 - RSRP should be 22
considered in case of uplink. Within the cell there should be only one active UE uploading data to the application server. 23
The application server should be placed as close as possible to the core/core emulator and connected to the core/core 24

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 93

O-RAN.TIFG.E2E-Test.0-v02.00
emulator via transport link with sufficient capacity not limiting the expected data throughput. The test is suitable for lab 1
as well as field environment. 2
The reference measurement points of the fronthaul latency (R1/R4 – Transmit/Receive interface at O -DU (CU-plane); 3
R2/R3 – Receive/Transmit interfaces at O -RU (CU-plane)) [16] are shown in Figure 5-4. Transmission delay between 4
O-RU and O -DU are specified as T12 (downlink  direction) and T34 (uplink  direction). Transmission delay should be 5
symmetrical and equal in both directions . The transmission delay encompasses only the time from when a bit leaves the 6
sender (R1/ R3) until it is received at the receiver (R2/ R4). 7
 8
Test configuration : The test configuration is not specified. The utilized test configuration (parameters) should be 9
recorded in the test report.  10
Laboratory setup: The radio conditions of UE can be modified by a variable attenuator/fading generator inserted between 11
the antenna connectors of O-RU and UE. The minimum attenuation of radio signal should be set to achieve the excellent 12
radio conditions (RSRR as defined in Section 3.6), but the minimum coupling loss (see Section 3.6) should not be 13
exceeded. The UE should be placed inside RF shielded box or RF shielded room if the UE is not connected via cable.  14
Field setup: The UE is placed in the centre of cell close to the radiated eNB/gNB antenna(s), where excellent radio 15
conditions (RSRP as defined in Section 3.6) should be observed. The minimum coupling loss (see Section 3.6) should 16
not be exceeded. 17
In both laboratory and field setups, Figure 5-5, the fronthaul latency can be modified by using of various lengths of fibre 18
(assuming typical delay of fibre around 5 us per kilometre) or by using of a network impairment emulator inserted between 19
O-RU and O-DU. It is necessary to know/measure the transmission delays produced by all fronthaul transport components 20
between O-RU and O-DU in order to properly estimate the total fronthaul latency (T12/T34). 21
5.12.3 Test Procedure 22
1. The test setup is configured according to the test configuration. The test configuration should be recorded in the test 23
report. The serving cell under test is activated and unloaded. All other cells are turned off.  24
2. The UE (real or emulated) is placed in the excellent radio condition (cell centre) as defined by RSRP in Section 3.6. 25
The UE is powered on and attached to the network  26
3. The uplink full-buffer UDP and TCP data transmission (see Section 3.4) from UE to the application server should be 27
verified. The excellent radio conditions experiencing peak user throughput is identifie d with stable utilization of the 28
highest possible uplink MCS and uplink transport block size. The utilization of these KPIs should be also verified.  29
4. The minimum fronthaul latency ( one-way transmission delay between O -DU and O -RU) should be set using a 30
network impairment emulator or a fibre with corresponding length (assuming typical delay of fibre around 5 us per 31
kilometre).  32
5. The UEs should be turned off or set to airplane mode, if possible, to empty the buffers. The uplink full-buffer UDP 33
data transmission from UE to the application server is started. The application server should receive data from UE.  34
6. All the required performance data (incl. the signalling and control data) as specified in the following “Test 35
requirements” section is measured and captured at UE, eNB/gNB and Application server using logging/measurement 36
tools.  37
7. The capture of log data is stopped. The uplink full-buffer UDP data transmission from application server to UE is 38
stopped.  39
8. The fronthaul latency is increased by 20 us if no degradat ion of user peak uplink throughput was observer in the 40
previous measurement. As soon as a degradation of user peak uplink throughout will be observed, the fronthaul 41
latency is increased only by 5 us in order to capture fine-grained log data.  42

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 94

O-RAN.TIFG.E2E-Test.0-v02.00
9.  Steps 5 to 8  are repeated until the total degradation of user peak uplink throughput is less than 30%.  The KPIs 1
measured with the minim fronthaul latency are used as a baseline (100%) for calculation of the degradation.  2
10. [Optional] Steps 4 to 9 are repeated for uplink full-buffer TCP data transmission.   3
5.12.4 Test requirements (expected results) 4
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 5
should be recorded and reported for the performance assessment. 6
 7
UE side (real or emulated UE): 8
• Radio parameters such as RSRP, RSRQ, CQI, PDSCH SINR (average sample per second) 9
• PUSHC BLER, PUSCH MCS (average sample per second) 10
• Transmit power on PUSCH (average sample per second) 11
• Transmitted uplink throughput (Application layer) (average sample per second) 12
• Channel utilization, i.e. Number allocated/occupied up link PRBs and Number of allocated/occupied slots 13
(average sample per second) 14
• GPS coordinates (latitude, longitude) in the field setup 15
eNB/gNB side (if capture of logs is possible): 16
• Radio parameters such as PUSCH SINR (average per second) 17
• PUSCH BLER (average per second) 18
Application server side: 19
• Received uplink throughput (L1 and Application layers) (average sample per second) 20
 21
When the UE is in excellent radio condition (cell centre), t he stable utilization of the highest possible uplink MC S and 22
uplink transport block size should be observed and evaluated.  The eNB/gNB should also receive the data with the 23
minimum uplink BLER.   24
The Table 5-13 gives an example of the test results record (median and standard deviation from the captured samples 25
should be calculated for each metric). In case of 5G SA and NSA, SS-RSRP and SS-SINR should be reported. In case of 26
5G NSA and dual connectivity (EN-DC), the values should be provided separately for both LTE and 5G. 27
 28
Table 5-13 The example of record of test results (median and standard deviation from the captured samples)  29

For each measured fronthaul latency value
UDP / TCP
Total fronthaul transport latency (T12/T34) [us]
Received L1 UL throughput [Mbps]
Degradation of Received L1 UL throughput [%]#
Received Application UL throughput [Mbps]
UE RSRP [dBm]

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 95

O-RAN.TIFG.E2E-Test.0-v02.00
UE RSRQ [dB]
UE PDSCH SINR [dB]
PUSCH transmit power [dBm]
PUSCH MSC
UL PRB number
PUSCH BLER [%]
# The “Received L1 UL throughput” measured with the minim fronthaul latency is used as a baseline (100%) for calculation of the degradation 1
The following figures should be also included in the test report. 2
• Received UDP/TCP uplink throughput (L1 and Application layers) vs Total fronthaul latency (T12/T34) 3
5.13 Impact of midhaul latency on downlink peak throughout  4
5.13.1 Test description and applicability 5
The purpose of the test is to evaluate the user peak downlink throughput as a function of the midhaul transport latency 6
(i.e. one -way t ransmission delay between O -DU and O -CU at F1 interface). Since the midhaul transport latency 7
corresponds with the distance (fiber length) between O-DU and O-CU, the results of test can be also used to identify the 8
maximum distance (fiber length) with an acceptable degradation of user peak downlink throughput. Note that the midhaul 9
transport exists only if  O-DU and O -CU are not a combined entity , and the interface F1 between O -DU and O -CU is 10
exposed. The test does not support LTE network architecture because an interface providing means for interconnecting 11
LTE O -DU/DU and LTE CU has not been specified yet neither in 3GPP nor in O-RAN ALLIANCE. Note that W1 12
interface [33] has been specified only for 5G NSA Options 4 and 7 network architectures. The test is suitable only for 5G 13
NSA (Option 3/3a/3x) and 5G SA network architectures as defined in Chapter 3. 14
5.13.2 Test setup and configuration 15
The network setup is single cell scenario (i.e. isolated cell without any inter-cell interference – see Section 3.7) with 16
stationary UE (real or emulated UE) placed in the excellent radio condition as defined in Section 3.6 - SINR should be 17
considered in case of downlink. Within the cell there should be only one active UE downloading data from the application 18
server. The application server should be placed as close as possible to the core/core emulator and connected to the 19
core/core emulator via transport link with sufficient capacity not limiting the expected data throughput. The test is suitable 20
for lab as well as field environment. 21
The midhaul latency encompasses only the time from when a bit leaves O-CU until it is received at O-DU. 22
 23
Test configuration : The test configuration is not specified. The utilized test configuration (parameters) should be 24
recorded in the test report.  25
Laboratory setup: The radio conditions of UE can be modified by a variable attenuator/fading generator inserted between 26
the antenna connectors of O-RU and UE. The minimum attenuation of radio signal should be set to achieve the excellent 27
radio conditions (SINR as defined in Section 3.6), but the minimum coupling loss (see Section 3.6) should not be 28
exceeded. The UE should be placed inside RF shielded box or RF shielded room if the UE is not connected via cable.  29
Field setup: The UE is placed in the centre of cell close to the radiated eNB/gNB antenna(s), where excellent radio 30
conditions (SINR as defined in Section 3.6) should be observed. The minimum coupling loss (see Section 3.6) should not 31
be exceeded. 32

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 96

O-RAN.TIFG.E2E-Test.0-v02.00
In both laboratory and field setups, Figure 5-6, the midhaul transport latency can be modified by using of a network 1
impairment emulator inserted between O -DU and O -CU. It is necessary to know/measure the transmission delays 2
produced by all midhaul transport components between O -DU and O-CU in order to properly estimate the total midh aul 3
latency. 4

Figure 5-6 The midhaul transport latency test setups of 5G NSA and 5G SA 5
5.13.3 Test Procedure 6
1. The test setup is configured according to the test configuration. The test configuration should be recorded in the test 7
report. The serving cell under test is activated and unloaded. All other cells are turned off.  8
2. The UE (real or emulated) is placed in t he excellent radio condition (cell centre) as defined by SINR in Section 3.6. 9
The UE is powered on and attached to the network  10
3. The downlink full -buffer UDP and TCP data transmission (see Section 3.4) from the application server should be 11
verified. The excellent radio conditions experiencing peak user throughput is identified wi th stable utilization of the 12
highest possible downlink MCS, downlink transport block size and downlink MIMO rank (number of layers). The 13
utilization of these KPIs should be also verified.  14
4. The minimum midhaul latency (one-way transmission delay between O-DU and O-CU) should be set using a network 15
impairment emulator.  16
5. The UEs should be turned off or set to airplane mode, if possible, to empty the buffers. The downlink full-buffer UDP 17
data transmission from the application server to UE is started. The UE should receive the data from application server.  18
6. All the required performance data (incl. the signalling and control data) as specified in the following “Test 19
requirements” section is measured and captured at UE and Application server using logging/measureme nt tools.  20
7. The capture of log data is stopped. The downlink full-buffer UDP data transmission from application server is stopped.  21
8. The midhaull latency is increased by 1000 us if no degradation of user peak downlink throughput was observer in the 22
previous measurement. As soon as a degradation of user peak downlink throughout will be observed, the midhaul 23
latency is increased only by 250 us in order to capture fine-grained log data.  24
9.  Steps 5 to 8 are repeated until the total degradation of user peak downlin k throughput is less than 30%.  The KPIs 25
measured with the minim fronthaul latency are used as a baseline (100%) for calculation of the degradation.  26
S1
S1
 UE
Uu
Uu
O-RAN System under test
4G O-RU 4G
O-DU/CU
Midhaul latency
5G
O-RU/O-DU 5G O-CUInserted
latency
3GPP Core
4G EPC EN-DC NSA
3GPP
services
Other
services
Application server
X2
F1F1
OpenFH
3GPP Core
5G SA
NG 3GPP
services
Other
services
Application server
O-RAN System under test
 UE Uu 5G
O-RU/O-DU 5G O-CUInserted
latency
Midhaul latency
F1F1

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 97

O-RAN.TIFG.E2E-Test.0-v02.00
10. [Optional] Steps 4 to 9 are repeated for downlink full-buffer TCP data transmission.   1
5.13.4 Test requirements (expected results) 2
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 3
should be recorded and reported for the performance assessment. 4
 5
UE side (real or emulated UE): 6
• Radio parameters such as RSRP, RSRQ, CQI, PDSCH SINR (average sample per second) 7
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second)  8
• Received downlink throughput (L1, and Application layers) (average sample per second)  9
• Downlink transmission mode 10
• Channel utilization, i.e. Number of alloc ated/occupied downlink PRBs and Number of allocated/occupied slots 11
(average sample per second) 12
• GPS coordinates (latitude, longitude) in the field setup 13
Application server side: 14
• Transmitted downlink throughput (Application layer) (average sample per second)  15
When the UE is in the excellent radio conditions (cell centre), the stable utilization of the highest possible downlink MCS, 16
downlink transport block size and downlink MIMO rank should be observed and evaluated. The UE should also receive 17
the data with the minimum downlink BLER. 18
 19
The Table 5-14 gives an example  of the test results record (median and standard deviation from the captured samples 20
should be calculated for each metric). In case of 5G SA and NSA, SS-RSRP and SS-SINR should be reported. In case of 21
5G NSA and dual connectivity (EN-DC), the values should be provided separately for both LTE and 5G. 22
 23
Table 5-14 The example of record of test results (median and standard deviation from the captured samples)  24

For each measured midhaul latency value
UDP / TCP
Total midhaul transport latency [us]
Received L1 DL throughput [Mbps]
Degradation of Received L1 DL throughput [%]#
Received Application DL throughput [Mbps]
UE RSRP [dBm]
UE RSRQ [dB]
UE PDSCH SINR [dB]
MIMO rank
PDSCH MSC
DL PRB number
PDSCH BLER [%]
# The “Received L1 DL throughput” measured with the minim fronthaul latency is used as a baseline (100%) for calculation of the degradation 25

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 98

O-RAN.TIFG.E2E-Test.0-v02.00
The following figures should be also included in the test report. 1
• Received UDP/TCP downlink throughput (L1 and Application layers) vs Total midhaul latency  2
5.14 Impact of midhaul latency on uplink peak throughout  3
5.14.1 Test description and applicability 4
The purpose of the test is to evaluate the user peak uplink throughput as a function of the midhaul transport latency (i.e. 5
one-way transmission delay between O-DU and O-CU at F1 interface). Since the midhaul transport latency corresponds 6
with the distance (fiber length) between O -DU and O-CU, the results of test can be also used to identify the maximum 7
distance (fiber length) with an acceptable degradation of user peak uplink throughput. Note that the midhaul transport 8
exists only if O-DU and O-CU are not a combined entity, and the interfaces between O-DU and O-CU are exposed. The 9
test does not support LTE network architecture because an interface providing means for interconnecting LTE O-DU/DU 10
and LTE CU has not been specified yet neither in 3GPP nor in O-RAN ALLIANCE. Note that W1 interface [33] has been 11
specified only for 5G NSA Options 4 and 7 network architectures. The test is suitable only for 5G NSA (Option 3/3a/3x) 12
and 5G SA network architectures as defined in Chapter 3. 13
5.14.2 Test setup and configuration 14
The network setup is single cell scenario (i.e. isolated cell without any inter -cell interference – see Section 3.7) with 15
stationary UE (real or emulated UE) placed i n the excellent radio condition as defined in Section 3.6 - RSRP should be 16
considered in case of uplink. Within the cell there should be only one active UE uploading data to application server. The 17
application server should be placed as close as possible to the core/core emulator and connected to the core/core emulator 18
via transport link with sufficient capacity not limiting the expected data throughput. The test is suitable for lab as well a s 19
field environment. 20
The midhaul latency encompasses only the time from when a bit leaves O-DU until it is received at O-CU. 21
 22
Test configuration : The test configuration is not specified. The utilized test configuration (parameters) should be 23
recorded in the test report.  24
Laboratory setup: The radio conditions of UE can be modified by a variable attenuator/fading generator inserted between 25
the antenna connectors of O-RU and UE. The minimum attenuation of radio signal should be set to achieve the excellent 26
radio conditions (RSRP as defined in Section 3.6), but the minimum coupling loss (see Section 3.6) should not be 27
exceeded. The UE should be placed inside RF shielded box or RF shielded room if the UE is not connected via cable.  28
Field setup: The UE is placed in the centre of cell close to the radiated eNB/gNB antenna(s), where excellent radio 29
conditions (RSRP as defined in Section 3.6) should be observed. The minimum coupling loss (see Section 3.6) should 30
not be exceeded. 31
In both laboratory and field setups, Figure 5-6, the midhaul transport latency can be modified by using of a network 32
impairment emulator inserted between O -DU and O -CU. It is necessary to know/measure the transmission delays 33
produced by all midhaul transport components between O -DU and O-CU in order to properly estimate the total midh aul 34
latency. 35
5.14.3 Test Procedure 36
1. The test setup is configured according to the test configuration. The test configuration should be recorded in the test 37
report. The serving cell under test is activated and unloaded. All other cells are turned off.  38

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 99

O-RAN.TIFG.E2E-Test.0-v02.00
2. The UE (real or emulated) is placed in the excellent radio condition (cell centre) as defined by RSRP in Section 3.6. 1
The UE is powered on and attached to the network  2
3. The uplink full-buffer UDP and TCP data transmission (see Section 3.4) from UE to the application server should be 3
verified. The excellent radio conditions experiencing pea k user throughput is identified with stable utilization of the 4
highest possible uplink MCS and uplink transport block size. The utilization of these KPIs should be also verified.  5
4. The minimum midhaul latency (one-way transmission delay between O-DU and O-CU) should be set using a network 6
impairment emulator.  7
5. The UEs should be turned off or set to airplane mode, if possible, to empty the buffers. The uplink full-buffer UDP 8
data transmission from UE to application server is started. The application server should receive the data from UE.  9
6. All the required performance data (incl. the signalling and control data) as specified in the following “Test 10
requirements” section is measured and captured at UE, eNB/gNB and Application server using logging/measurement 11
tools.  12
7. The capture of log data is stopped. The uplink full-buffer UDP data transmission from the UE to application server 13
is stopped.  14
8. The midhaull latency is increased by 1000 us if no degradation of user peak uplink throughput was observer in the 15
previous measurement. As soon as a degradation of user peak uplink throughout will be observed, the midhaul latency 16
is increased only by 250 us in order to capture fine-grained log data.  17
9.  Steps 5 to 8 are repeated until the total degradation of user peak uplink th roughput is less than 30%.  The KPIs 18
measured with the minim fronthaul latency are used as a baseline (100%) for calculation of the degradation.  19
10. [Optional] Steps 4 to 9 are repeated for uplink full-buffer TCP data transmission.   20
5.14.4 Test requirements (expected results) 21
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 22
should be recorded and reported for the performance assessment. 23
 24
UE side (real or emulated UE): 25
• Radio parameters such as RSRP, RSRQ, CQI, PDSCH SINR (average sample per second) 26
• PUSHC BLER, PUSCH MCS (average sample per second) 27
• Transmit power on PUSCH (average sample per second) 28
• Transmitted uplink throughput (Application layer) (average sample per second) 29
• Channel utilization, i.e. Number allocated/occupied uplink PRBs and Number of allocated/occupied slots 30
(average sample per second) 31
• GPS coordinates (latitude, longitude) in the field setup 32
eNB/gNB side (if capture of logs is possible): 33
• Radio parameters such as PUSCH SINR (average per second) 34
• PUSCH BLER (average per second) 35
Application server side: 36
• Received uplink throughput (L1 and Application layers) (average sample per second) 37

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 100

O-RAN.TIFG.E2E-Test.0-v02.00
 1
When the UE is in excellent radio condition (cell centre), t he stable utilization of the highest possible uplink MCS and 2
uplink transport block size should be observed and evaluated.  The eNB/gNB should also receive the data with the 3
minimum uplink BLER.   4
The Table 5-15 gives an example  of the test results record (median and standard deviation from the captured samples 5
should be calculated for each metric). In case of 5G SA and NSA, SS-RSRP and SS-SINR should be reported. In case of 6
5G NSA and dual connectivity (EN-DC), the values should be provided separately for both LTE and 5G. 7
 8
Table 5-15 The example of record of test results (median and standard deviation from the captured samples)  9

For each measured midhaul latency value
UDP / TCP
Total midhaul transport latency [us]
Received L1 UL throughput [Mbps]
Degradation of Received L1 UL throughput [%]#
Received Application UL throughput [Mbps]
UE RSRP [dBm]
UE RSRQ [dB]
UE PDSCH SINR [dB]
PUSCH transmit power [dBm]
PDSCH MSC
UL PRB number
PUSCH BLER [%]
# The “Received L1 UL throughput” measured with the minim fronthaul latency is used as a baseline (100%) for calculation of the degradation 10
The following figures should be also included in the test report. 11
• Received UDP/TCP uplink throughput (L1 and Application layers) vs Total midhaul latency  12
 13

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 101

O-RAN.TIFG.E2E-Test.0-v02.00
 Services 1
As a part of O-RAN testing, we need to ensure the O-RAN system can work in a telecom network, by inter-working with 2
the other sub-systems to provide End-to-End services. This section of the document outlines the services which needs to 3
be tested to validate that the O-RAN system can be deployed and optimized to deliver great end user service experience 4
in a telecom network. 5
The services which need to be tested are broadly classified as data service, video streaming service, voice service, video 6
calling service in the eMBB slice and services supported using other slices like URLLC and mMTC.   This section of the 7
document also tests different scenarios like handover and different radio conditions to assess the impact of these scenarios 8
on the end user service experience.   9
6.1 Data Services 10
Data services forms one of the basic services supported as of today on a telecom network. This includes everything 11
from web browsing, uploading/downloading content to traffic generated by all the different applications on the end  user 12
device. These services form an integral part of data traffic on a telecom network. This section comprises of two test 13
scenarios which include web browsing and file upload/download. 14
Along with the monitoring and validation of these services using user  experience KPIs, the O-RAN systems also need 15
to be monitored. The end user service experience can also be impacted by some of the features available on the O -RAN 16
system. Some of these features have been included below. As a part of the data services testi ng, details of these features 17
need to be included in the test report to get a comprehensive view of the setup used for testing. If additional features or 18
functionalities have been enabled during this testing that impact the end user experience, those need be included in the 19
test report as well.  20
• Control Channel Beamforming 21
• Connected Mode DRX 22
• Massive MIMO 23
• Coordinated Multi Point (DL and UL) 24
• 256QAM Support (DL and UL) 25
• SSB Power Boost 26
• Beam Management 27
• DL Common Channel Beamforming 28
• Single-User MIMO Beamforming (DL and UL) 29
• Multi-User MIMO Beamforming (DL and UL) 30
• Single-User MIMO, TM Switching 31
• Multi-User MIMO, TM Switching 32
• TDD configuration support 33
• RAN Slicing Framework (NR Low/Mid/High) 34
• RACH Enhancements (PRACH Format 0) 35
• PF (Proportional Fairness) Scheduling 36
• QoS Scheduling 37
• Minimal Bit Rate Scheduling 38

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 102

O-RAN.TIFG.E2E-Test.0-v02.00
• Link Adaptation (DL and UL) 1
• Uplink Data Compression UDC 2
• PUSCH Frequency Hopping 3
 4
6.1.1 Web Browsing 5
6.1.1.1 Test Description 6
Web browsing forms an integral part of the 4G and 5G data network, and this test case is applicable to both NSA and 7
SA deployments. This testing will need to be performed twice, once for NSA and repeated again for SA deployment as 8
applicable. HTTP is the protocol used for web browsing and this protocol can be transported over TCP/TLS or 9
UDP/QUIC. As a part of this effort, we will include scenarios which will include both these protocols. Testing can be 10
performed using HTTP/TLS/TCP or HTTP/QUIC/UDP or both these protocols as applicable. 11
Web browsing KPI 12
• DNS Resolution time– Time taken from when the client sends a DNS query to when the DNS responds 13
with an IP address in milliseconds/seconds.  This KPI should be recorded if DNS is used. 14
• Time To First Byte (TTFB) – Time taken from when the client makes the HTTP request to when the first 15
byte of the page is received in milliseconds/seconds. 16
• Page Load Time – Time taken from when the client places the request to when the page is completely 17
loaded in seconds. 18
6.1.1.2 Test Setup 19
The SUT in this test case would be O-eNB along with O-RU, O-DU and O-CU-CP/O-CU-UP for NSA deployments or 20
just the O-RU, O-DU and O-CU-CP/O-CU-UP for SA deployments. The O-RAN setup should support the ability to 21
perform this testing in different radio conditions as defined in Section 3.6. The 4G/5G core will be required to support 22
the basic functionality to authenticate and register an end user device in order to setup a PDN connection/PDU session. 23
The 4G core will be used for O-RAN NSA testing, whereas 5G core will be used for O-RAN SA testing. The 4G/5G 24
core could be a completely emulated, partially emulated or a real non-emulated core. The application server should 25
support web browsing and be accessible from the 4G/5G core. This application server(s) should support protocols like 26
HTTP/TCP and/or HTTP/QUIC as applicable. The end user device (UE) used for testing could be a real UE or an 27
emulated one. The test setup should include tools which have the ability to collect traces on the elements and/or packet 28
captures of communication between the elements. This could be a built -in capability of the emulated/non-emulated 29
network elements and end user device or an external tool. Optionally, if some of the network elements or applicat ion 30
server is located remotely either in a cloud or on the internet, the additional latency should be calculated and accounted 31
for.  32
The O-eNB, O-RU, O-DU and O-CU-CP/O-CU-UP need to have the right configuration and software load. Tools 33
which emulate latency need to be used and configured on the O-RAN system (between O-RU, O-DU and O-CU-CP/O-34
CU-UP) to emulate real-world deployment conditions as applicable. The end user device must be configured with the 35
right user credentials to be able to register and authenticate with the O-RAN system and the 4G/5G core. The end user 36
device also needs to be provisioned with the right application like the web browser to perform the tests. The 4G/5G core 37
network must be configured to support end user device used for testing.  This includes supporting registration, 38
authentication and PDN connection/PDU session establishment for this end user device. The application server should 39
be configured to support web browsing, along with support for HTTP/TCP and /or HTTP/QUIC protocols as applicable. 40
Web browsing has lot of variables which can impact the KPIs and in order to reduce the variables and make the test 41
outcomes consistent, it is recommended to set up a static web page of ~1.8 -2MB size. The size of the web page is 42

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 103

O-RAN.TIFG.E2E-Test.0-v02.00
comparable to the average size of web pages on the internet. A static web page does not change the content on repeated 1
requests, thus making the test results consistent and repeatable. The locations where the radio conditions are excellent, 2
good, fair and poor need to be identified within the serving cell. 3
All the elements in the network like O-RAN system, 4G/5G core and the application server need to have the ability to 4
capture traces to validate the successful execution of the test cases. The end user device needs to ha ve the capability to 5
capture traces/packet capture to calculate the web browsing KPIs. Optionally, the network could have network taps 6
deployed in various legs of the network to get packet captures to validate successful execution of the test cases. Finall y, 7
all these different components need to have connectivity with each other – the end user device should be able to connect 8
to O-RAN system, which needs to be connected to the 4G/5G core which in turn should have connectivity to the 9
application server. 10
6.1.1.3 Test Methodology/Procedure 11
Ensure the end user device, O-RAN system, 4G/5G core and the application server have all been configured as outlined 12
in Section 6.1.1.2. All traces and packet captures need to be enabled for the duration of the testing to ensure all  13
communication between network elements can be captured and validated. 14
1. Power on the end user device in excellent radio condition and ensure it registers with the 4G/5G core for data 15
services. 16
2. Once the registration is complete, the end user device has to establish a PDU session with the 5G core (SA) or PDN 17
connection with the 4G core (NSA). 18
3. Open the browser or application on the end user device and start a web browsing session to a web content using 19
HTTP/TCP and/or HTTP/QUIC protocol. If the intent is to execute this test case for both protocols, then this step 20
needs to be performed twice, once for web content using HTTP/TCP followed by web content using HTTP/QUIC 21
protocol. 22
4. Validate the end user can get the content from the application server and view it on t he end device. For every test 23
collect the KPIs included in Section 6.1.1.1. 24
5. Clear the browser cache on the end user device between tests.  25
6. Repeat the test multiple times (>10 times) and gather results. 26
7. Repeat the above steps 1 through 6 for the good, fair and poor radio conditions. 27
 28
6.1.1.4 Test Expectation (expected results) 29
As a pre-validation, use the traces to validate a successful registration and PDN connection/PDU session setup by the 30
end user device without any errors. This is a prerequisite before these tests can be validated. 31
First and foremost, the end user device should be able to view the web browsing content for the web browsing test, with 32
the content being viewable and readable. Use the packet captures to validate there is no packet drop or out of sequence 33
packets which could impact customer experience and point to a misconfigured or flawed system. Any failures 34
encountered during testing, will have to be investigated to identify the root cause.  35
Calculate the web browsing KPIs included in Section 6.1.1.1 and include it in the test report. In a lab setup, the user of 36
DNS could be bypassed by using IP Addresses instead of domain names. However, the DNS resolution time KPI must 37
be recorded if a DNS is used for testing. The average range of values for these KPIs are included below for guidance, 38
taking into consideration the size of the web page (~ 2MB). These values are applicable when the testing is performed 39
in a controlled environment in good radio condition without the interfe rence of external factors which could impact the 40
KPIs, example: use of internet to connect to remote servers/hosts could add additional latency and packet loss issues to 41
the connection, thus impacting the KPIs. As there are multiple variables which can imp act the testing in this scenario, a 42
KPI outcome outside the range does not necessarily point to a failed test case. Any test results outside the range of KPI 43
encountered during testing, will have to be investigated to identify the root cause as the issues may be due to the 44

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 104

O-RAN.TIFG.E2E-Test.0-v02.00
variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and the test has to be 1
repeated.  2
• DNS Resolution time (conditional mandatory) – < 1 second 3
• Time To First Byte (TTFB) – < 3 seconds 4
• Page Load Time – < 12 seconds 5
 6
As a part of gathering test data and reporting, ensure the minimum configuration parameters (see Section 3.3) are 7
included in the test report. The following information should also be included in the test report to provide a 8
comprehensive view of the test setup. 9
End user device side (real or emulated): 10
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second)  11
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second)  12
• Received downlink throughput for the duration of the page download (L1 and L3 PDCP layers) (average sample 13
per second) 14
• Downlink transmission mode 15
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 16
(average sample per second) 17
The table below gives an example of the test report considering the mean and standard deviation of all test results that 18
have been captured.  19
Table 6-1 Example of Test Report for Web Browsing Testing 20

Excellent
(cell centre) Good Fair Poor
(cell edge)
HTTP over
TCP/QUIC
HTTP over
TCP/QUIC
HTTP over
TCP/QUIC
HTTP over
TCP/QUIC
DNS Resolution Time (Conditional
mandatory)
Time To First Byte (TTFB)
Page Load Time
L1 DL throughput [Mbps]
L1 DL Spectral efficiency [bps/Hz]
L3 DL PDCP throughput [Mbps]
Application DL throughput [Mbps]
UE RSRP [dBm]
UE PDSCH SINR [dB]
MIMO rank
PDSCH MCS
DL RB number
UE CQI
UE RSRQ
UE PMI
UE RSSI

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 105

O-RAN.TIFG.E2E-Test.0-v02.00
UE Buffer status
UE Packet delay
PDSCH BLER [%]
The web browsing experience can also be impacted by some of the features available (see Section 6.1) on the O -eNB, 1
O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other features/functionality which 2
could impact the end user’s web browsing experience should be included in the test report to provide a comprehensive 3
view of the testing. 4
6.1.2 File upload/download 5
6.1.2.1 Test Description 6
File Transfer Protocol (FTP) is a simple application layer protocol used to transfer file between remote locations. FTP 7
along with different flavours of the protocols forms one of the fundamental methods to upload/download a file on the 8
internet. This test case is applicable to both NSA and SA deployments and will need to be performed twice, once for 9
NSA and repeated again for SA deployment as applicable. This scenario tests the end user experience to 10
upload/download files to/from an FTP server over an O-RAN system. The KPIs used to measure the user experience 11
have been included below 12
1. Download throughput – This is the average application layer throughput to download the file in kbps 13
2. Upload throughput – This is the average application layer throughput to upload the file in kbps. 14
3. Time taken to Download file – This is the time taken to download the file in seconds. 15
4. Time taken to Upload file – This is the time taken to upload the file in seconds. 16
 17
6.1.2.2 Test Setup 18
The SUT in this test case would be O-eNB along with O-RU, O-DU and O-CU-CP/O-CU-UP for NSA deployments or 19
just the O-RU, O-DU and O-CU-CP/O-CU-UP for SA deployments. The O-RAN setup should support the ability to 20
perform this testing in different radio conditions as defined in Section 3.6. A 4G/5G core will be required with the basic 21
functionality to authenticate and register an end user device in order to setup a PDN connection/PDU session. The 4G 22
core will be used for O-RAN NSA testing, whereas 5G core will be used for O-RAN SA testing. The 4G/5G core be a 23
completely emulated, partially emulated or a real non-emulated core. An FTP server acts as an application server for 24
this test case. The end user device (UE) used for testing could be a real UE or an emulated one. The test setup should 25
include tools which have the ability to collect traces on the elements and/or packet captures of communication between 26
the elements. This could be a built-in capability of the emulated/non-emulated network elements or an external tool. 27
Optionally, if some of the network elements are located remotely either in a cloud or on the internet, the additional 28
latency should be calculated and accounted for.  29
The O-eNB, O-RU, O-DU and O-CU-CP/O-CU-UP need to have the right configuration and software load. Tools 30
which emulate latency need to be used and configured on the O-RAN system (between O-RU, O-DU and O-CU-CP/O-31
CU-UP) to emulate real-world deployment conditions. The end user device must be configured with the right user 32
credentials to be able to register and authenticate with the O-RAN system and the 4G/5G core. The end user device also 33
needs to be provisioned with the right application like an FTP client to upload and download files. The 4G/5G core 34
network must be configured to support end user device used for testing. This includes supporting registration, 35
authentication and PDN connection/PDU session establishment for this end user device. The FTP server should host 36
large files (>1 GB) which can be used for the download/upload testing. The locations where the radio conditions are 37
excellent, good, fair and poor need to be identified within the serving cell. 38

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 106

O-RAN.TIFG.E2E-Test.0-v02.00
All the elements in the network like O-RAN system, 4G/5G core and the application server need to have the ability to 1
capture traces to validate the successful execution of the test cases. The end user device needs to have the capability to 2
capture traces/packet capture to calculate the file upload/download KPIs. Optionally, the network could have net work 3
taps deployed in various legs of the network to get packet captures to validate successful execution of the test cases. 4
Finally, all these different components need to have connectivity with each other – the end user device should be able to 5
connect to O-RAN system, which needs to be connected to the 4G/5G core which in turn should have connectivity to 6
the application server. 7
6.1.2.3 Test Methodology/Procedure 8
Ensure the end user device, O-RAN system, 4G/5G core and the FTP server have all been configured as o utlined in 9
Section 6.1.2.2. All traces and packet captures need to be enabled for the duration of the testing to ensure all 10
communication between network elements can be captured and validated. 11
1. Power on the end user device in excellent radio condition and ensure it registers with the 4G/5G core for data 12
services. 13
2. Once the registration is complete, the end user device has to establish a PDU session with the 5G core (SA) or PDN 14
connection with the 4G core (NSA). 15
3. Open the application on the end user device and upload the test file to the FTP server. Make a note of the time 16
taken to upload the file and the average upload throughput. 17
4. Next use the same application on the end user device to download a different test file from the FT P server. Make a 18
note of the time taken to download the file and the average download throughput. 19
5. Clear all the buffers and caches on the client and the server. Delete the test files – downloaded file on the client and 20
the uploaded file on the FTP server. 21
6. Repeat the test multiple times (>10 times) and gather results. 22
7. Repeat the above steps 1 through 6 for the good, fair and poor radio conditions.  23
 24
6.1.2.4 Test Expectation (expected results) 25
As a pre-validation, use the traces to validate a successful registration and PDN connection/PDU session setup by the 26
end user device without any errors. This is a prerequisite before these tests can be validated.  27
Validate the end user device can upload/download the complete file to/from the FTP server without interruption. 28
Validate there isn’t a drop in throughput while uploading/downloading the file to/from the FTP server. Use the packet 29
captures to validate there is no packet drop or out of sequence packets which could impact customer experience and 30
point to a misconfigured or flawed system. 31
Calculate the file upload/download KPIs included in Section 6.1.1.1 and include it in the test report. There are no target 32
values for these KPIs as they are dependent on multiple factors used in the test configuration. Some comments on each 33
of the KPIs are included below. 34
• Download throughput – This value should be comparable to the results of the downlink throughput test 35
performed in Section 5.2 and Section 5.4. 36
• Upload throughput – This value should be comparable to the results of the uplink throughput test performed in 37
Section 5.3 and Section 5.5. 38
• Time taken to Download file – This value should be comparable to the value calculated using the formula 39
included below  40
• Time taken to Upload file – This value should be comparable to the value calculated using the formula included 41
below. 42
 43

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 107

O-RAN.TIFG.E2E-Test.0-v02.00
𝑡𝑖𝑚𝑒 𝑡𝑎𝑘𝑒𝑛 𝑡𝑜 𝑢𝑝𝑙𝑜𝑎𝑑/𝑑𝑜𝑤𝑛𝑙𝑜𝑎𝑑 [𝑠𝑒𝑐𝑜𝑛𝑑𝑠] = Size of file in GB x 1000 x 1000 x 8
𝑢𝑝𝑙𝑖𝑛𝑘/𝑑𝑜𝑤𝑛𝑙𝑖𝑛𝑘/𝑢𝑝𝑙𝑖𝑛𝑘 𝑡ℎ𝑟𝑜𝑢𝑔ℎ𝑝𝑢𝑡 [𝑘𝑏𝑝𝑠] 1
These KPI values included in Section 6.1.2.1 need to be included in the test report along with the minimum 2
configuration parameters included in Section 3.3. The following information should also be included in the test report to 3
provide a comprehensive view of the test setup. 4
End user device side (real or emulated UE): 5
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second)  6
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per s econd) 7
• Received downlink throughput (L1 and L3 PDCP layers) (average sample per second)  8
• Downlink transmission mode 9
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 10
(average sample per second) 11
The table below gives an example of the test report considering the mean and standard deviation of all test results that 12
have been captured.  13
Table 6-2 Example Test Report for File Upload/Download Testing 14

Excellent
(cell centre) Good Fair Poor
(cell edge)
File
Upload/Download
File
Upload/Download
File
Upload/Download
File
Upload/Download
Upload/Download Throughput (kbps)
Time taken to Upload/Download File
L1 DL throughput [Mbps]
L1 DL Spectral efficiency [bps/Hz]
L3 DL PDCP throughput [Mbps]
Application DL throughput [Mbps]
UE RSRP [dBm]
UE PDSCH SINR [dB]
MIMO rank
PDSCH MCS
DL RB number
UE CQI
UE RSRQ
UE PMI
UE RSSI
UE Buffer status
UE Packet delay
PDSCH BLER [%]
The file upload/download experience can also be impacted by some of the features available (see Section 6.1) on the O -15
eNB, O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other features/functionality 16

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 108

O-RAN.TIFG.E2E-Test.0-v02.00
which could impact the end user’s web browsing experience should be included in the test report to provide a 1
comprehensive view of the testing. 2
6.2 Video Streaming 3
Video makes up a major part of the internet traffic today and we are seeing similar trend even on the mobile data traffic. 4
Mobile video accounts for close to two-thirds of the total mobile data traffic and is expected to increase in the coming 5
years. Video streaming has evolved over time with newer and better audio and video codecs. The protocols used to 6
stream the video packets has also been evolving over time, with Adaptive Bit Rate (ABR) being the most common 7
protocol used for streaming video on the internet today. There are multiple flavours of A BR, but at the crux they all use 8
HTTP protocol over TCP/TLS or UPD/QUIC to transfer audio-video packets to a client. A video streaming server 9
which supports ABR hosts multiple versions of the same video content with each version of the video being encoded at 10
different resolution and quality, hence different bit rates. The client uses the ABR protocol to get the list of all availabl e 11
versions of the requested video content and picks the best video content based on the available bandwidth on the client 12
side. The ABR client continuously monitors the network condition and dynamically adjusts the quality and resolution of 13
the video stream to match the available bandwidth. 14
The ABR protocol provides the user with the best video streaming experience based on the ava ilable bandwidth 15
between the client and the content server. This however adds a lot of variables to the streaming session  16
• An ABR client may start with a lower resolution video and progressively switch to higher resolution video as it 17
better estimates the bandwidth available at the client. 18
• An ABR client might notice an improvement in the bandwidth and request a higher resolution/quality video 19
content, thus improving the video quality mid-stream. 20
• An ABR client might notice a degradation in the bandwidth and request a lower resolution/quality video 21
content, thus deteriorating the video quality mid-stream. 22
 23
These variables make it challenging to quantify the video streaming experience of an end user. There have been many 24
tools and organization which have defined different methods and algorithms to quantify the end user experience 25
including QoE (Quality of Experience), SVQ (Streaming Video Quality), Video Multimethod Assessment Fusion 26
(VMAF) etc. In this document we recommend using the Mean Opinion Score (MOS) as defined by ITU P.1203.3 to 27
quantify the quality of experience of the end user. This mechanism however limits the testing to H264 video encoder 28
with a resolution of HD quality (1080p resolution – 1920 x 1080 pixels) or below. 29
Video Streaming KPIs 30
• Video start time or Time to load first video frame – Time from when the video was selected to play to 31
when the video starts playing in seconds. 32
• Number of video stalls/buffering (Optional) – Number of times the video stalled or started buffering during 33
the course of video streaming. This KPI has already been considered by ITU P.1203.3 to provide a 34
cumulative MOS score. 35
• Duration of stalls in the video (Optional) – The cumulative duration of all the stalls during the course of 36
video streaming in seconds. This KPI has already been considered by ITU P.1203.3 to provide a 37
cumulative MOS score. 38
• Video MOS score – MOS score for the video streaming session as defined by ITU P1203.3. 39
 40
Along with the monitoring and validation of these services using user experience KPIs, the O -RAN systems also need 41
to be monitored. The end user service experience can also be impacted by some of the features available on the O -RAN 42
system. Some of these features have been included below. As a part of the video streaming testing, details of these 43
features need to be included in the test report to get a comprehensive view of the setup used for testing. If additional 44

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 109

O-RAN.TIFG.E2E-Test.0-v02.00
features or functionalities have been enabled during this testing that impact the end user experience, those need be 1
included in the test report as well.  2
• NR to LTE PS Redirection/Cell Reselection/Handover 3
• Intra-frequency / Inter-Frequency Cell Reselection/Handover 4
• NR Coverage-Triggered NR Session Continuity 5
• LTE-NR & NR-NR Dual Connectivity and NR Carrier Aggregation 6
• Direct data forwarding between NG-RAN and E-UTRAN nodes for inter-system mobility 7
• Standard QCI Bearers Support 8
• Control Channel Beamforming 9
• Massive MIMO 10
• 256QAM Support (DL and UL) 11
• Beam Management 12
• TDD configuration support 13
• PF (Proportional Fairness) Scheduling 14
• Link Adaptation (DL and UL) 15
• Application Aware QoS 16
6.2.1 Video Streaming – Stationary Test 17
This scenario tests the video experience of a user streaming video over 4G and 5G network when the end user device is 18
stationary. 19
6.2.1.1 Test Description 20
Majority of the video streaming on the internet today uses ABR (Adaptive Bit Rate) streaming using HTTP protocol 21
over TCP/TLS or UDP/QUIC. As a part of this effort, we need to test the user’s video streaming experience when 22
connected to a telecom network over O-RAN system. This test case is applicable to both NSA and SA deployment, and 23
will need to be performed twice, once for NSA and repeated again for SA deployment as applicable.  24
6.2.1.2 Test Setup 25
The SUT in this test case would be O-eNB along with O-RU, O-DU and O-CU-CP/O-CU-UP for NSA deployments or 26
just the O-RU, O-DU and O-CU-CP/O-CU-UP for SA deployments. The O-RAN setup should support the ability to 27
perform this testing in different radio conditions as defined in Section 3.6. A 4G/5G core will be required to support the 28
basic functionality to authenticate and register an end user device in order to setup a PDN connection/PDU session. The 29
4G core will be used for O-RAN NSA testing, whereas 5G core will be used for O-RAN SA testing. The 4G/5G core 30
could be a completely emulated, partially emulated or a real non-emulated core. The application server(s) for this 31
testing should support video streaming using ABR protocol over HTTP/TCP and /or HTTP/QUIC as applicable. The end 32
user device (UE) used for testing could be a real UE or an emulated one. The test setup should include tools which have 33
the ability to collect traces on the elements and/or packet captures of communication between the elements. This could 34
be a built-in capability of the emulated/non-emulated network elements or an external tool. Optionally, if some of the 35
network elements or the application server are located remotely either in a cloud or on the internet, the additional 36
latency should be calculated and accounted for.  37

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 110

O-RAN.TIFG.E2E-Test.0-v02.00
The O-eNB, O-RU, O-DU and O-CU-CP/O-CU-UP need to have the right configuration and software load. Tools 1
which emulate latency need to be used and configured on the O-RAN system (between O-RU, O-DU and O-CU-CP/O-2
CU-UP) to emulate real-world deployment conditions. The end user device must be configured with the right user 3
credentials to be able to register and authenticate with the O-RAN system and the 4G/5G core. The end user device also 4
needs to be provisioned with the right application like a video streaming client to perform the tests. The 4G/5G core 5
network must be configured to support end user device used for testing. This includes supporting registration, 6
authentication and PDN connection/PDU session establishment for this end user device. The loca tions where the radio 7
conditions are excellent, good, fair and poor need to be identified within the serving cell.  8
All the elements in the network like O-RAN system, 4G/5G core and the application server need to have the ability to 9
capture traces to validate the successful execution of the test cases. The end user device needs to have the capability to 10
capture traces/packet capture to calculate the video streaming KPIs. Optionally, the network could have network taps 11
deployed in various legs of the network to get packet captures to validate successful execution of the test cases. Finally, 12
all these different components need to have connectivity with each other – the end user device should be able to connect 13
to O-RAN system, O-RAN system needs to be connected to the 4G/5G core which in turn should have connectivity to 14
the application server. 15
6.2.1.3 Test Methodology/Procedure 16
Ensure the end user device, O-RAN system, 4G/5G core and the application server have all been configured as outlined 17
in Section 6.2.1.2. All traces and packet captures need to be enabled for the duration of the testing to ensure all 18
communication between network elements can be captured and validated. 19
1. Power on the end user device in excellent radio condition and ensure it registers with the 4G/5G core for data 20
services. 21
2. Once the registration is complete, the end user device has to establish a PDN Session with the 4G core or PDU 22
session with the 5G core. 23
3. Open the video streaming client on the end user device and start a video streaming session over HTTP/TCP 24
protocol and let the video stream for at least 120 seconds.  25
4. Optionally, repeat the test by streaming a video session over the HTTP/QUIC protocol and stream the video content 26
for at least 120 seconds. 27
5. Repeat the test multiple times (> 10 times) and gather results. 28
6. Repeat the above steps 1 through 5 for the good, fair and poor radio conditions.  29
 30
6.2.1.4 Test Expectation (expected results) 31
As a pre-validation, use the traces to validate a successful registration and PDN connection/PDU session setup by the 32
end user device without any errors. This is a prerequisite before these tests can be validated.  33
Validate the end user device is able to start streaming the video without delays and watch the video content. Ensure the 34
video is able to stream without stalls or intermittent buffering. Use the packet captures to validate there is no packet 35
drop or out of sequence packets which could impact customer experience and point to a misconfigured or flawed 36
system. 37
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 38
is performed in a controlled environment in good radio condition without the interference of external factors which 39
could impact the KPIs, example: use of internet to connect to remote s ervers/hosts could add latency, jitter and packet 40
loss issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the testing in 41
this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test results outside 42
the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues may be 43

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 111

O-RAN.TIFG.E2E-Test.0-v02.00
due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to  be resolved and the test 1
has to be repeated.  2
• Video Start time or Time to load first video frame – ~1.5 seconds 3
• Number of video stalls/buffering – < 1 4
• Duration of stalls in the video – < 5 seconds 5
• Video MOS Score – > 3.5 6
 7
The values for the end user video streaming KPIs defined in Section 6.2 need to be included in the test report along with 8
the minimum configuration parameters included in Section 3.3. The following information should also be included in 9
the test report for the testing performed in different radio conditions to provide a comprehensive view of the test setup. 10
End user device side (real or emulated UE): 11
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second)  12
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average samp le per second) 13
• Downlink transmission mode 14
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 15
(average sample per second) 16
The table below gives an example of the test report considering the mean and s tandard deviation of all test results that 17
have been captured.  18
Table 6-3 Example Test Report for Video Streaming – Stationary Test 19

Excellent
(cell centre) Good Fair Poor
(cell edge)
Video Streaming
over TCP/QUIC
Video Streaming
over TCP/QUIC
Video Streaming
over TCP/QUIC
Video Streaming
over TCP/QUIC
Video Start Time
Number of Video Stalls/buffering
Duration of stalls in the video
Video MOS Score
L1 DL Spectral efficiency [bps/Hz]
UE RSRP [dBm]
UE PDSCH SINR [dB]
MIMO rank
PDSCH MCS
DL RB number
UE CQI
UE RSRQ
UE PMI
UE RSSI
UE Buffer status
UE Packet delay
PDSCH BLER [%]

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 112

O-RAN.TIFG.E2E-Test.0-v02.00
The end user video streaming experience can also be impacted by some of the features available (see Section 6.2) on the 1
O-eNB, O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other 2
features/functionality which could impact the end user’s video streaming experience should be included in the test 3
report to provide a comprehensive view of the testing. 4
6.2.2 Video Streaming – Handover between same Master eNB but different 5
O-RUs – Intra O-DU 6
This test scenario validates the user’s video streaming experience when the end user device (UE) is connected over 7
NSA to a 4G core and is in the process of a handover between two O -RAN subcomponents (two O-RUs) on the 8
Secondary gNB while remaining connected to the same Master eNB. As services like VoLTE or VoNR cannot b e used 9
to test handover of secondary gNB, video streaming is used for this testing. Hence, this test scenario is only applicable 10
in an NSA deployment. 11
6.2.2.1 Test Description 12
The 5G O-RAN system has multiple sub-components like the O-RU, O-DU and the O-CU-CP/O-CU-UP. This setup 13
leads to multiple handover scenarios when the O-RAN system is connected as a Secondary gNB in an NSA 14
deployment. This scenario tests the end user’s video streaming experience when the end user device is connected over 15
NSA to the 4G core and performs a handover between O-RUs which are connected to the same O-DU (and O-CU-16
CP/O-CU-UP) on the Secondary gNB – Intra-O-DU handover. The end user device remains connected to the same 17
Master eNB through the entire handover process. This handover is agn ostic to the 4G core as the handover occurs on 18
the O-RAN system. This test assesses the impact of the video streaming service on the end user device in this handover 19
scenario by monitoring the end user video streaming KPIs included in Section 6.2. This tes t case streams video using 20
ABR protocol over HTTP/TCP and/or HTTP/QUIC.  21
6.2.2.2 Test Setup 22
The SUT in this test case would be the Master eNB and a Secondary gNBs. The Master eNB will be an O-eNB and the 23
Secondary gNB will constitute of a pair of O-RUs – O-RU1 and O-RU2, which will connect to the same O-DU and O-24
CU-CP/O-CU-UP. The O-eNB, O-RUs, O-DU and O-CU-CP/O-CU-UP have to comply with the O-RAN 25
specifications. A 4G core will be required to support the basic functionality to authenticate and register an end user  26
device in order to setup a PDN connection. The 4G core could be a completely emulated, partially emulated or a real 27
non-emulated core. The application server(s) for this testing should support video streaming using ABR protocol over 28
HTTP/TCP and/or HTTP/QUIC as applicable. The end user device (UE) used for testing could be a real UE or an 29
emulated one. The test setup should include tools which have the ability to collect traces on the elements and/or packet 30
captures of communication between the elements. This could be a built-in capability of the emulated/non-emulated 31
network elements or an external tool. Optionally, if some of the network elements or application server(s) are located 32
remotely either in a cloud or on the internet, the additional latency sho uld be calculated and accounted for.  33
The O-eNB needs to have the right configuration and software load. The pair of O-RUs (O-RU1 and O-RU2) need to be 34
connected to the O-DU and O-CU-CP/O-CU-UP and all the components need to have the right configuration and 35
software load. Tools which emulate latency need to be used and configured on the O -RAN system (between O-RU, O-36
DU and O-CU-CP/O-CU-UP) to emulate real-world deployment conditions. The end user device must be configured 37
with the right user credentials to be able to register and authenticate with the O -RAN system and the 4G core. The end 38
user device also needs to be provisioned with the right application like a video streaming client to perform the tests. The 39
4G core network must be configured to support end user device used for testing. This includes supporting registration, 40
authentication and PDN connection establishment for this end user device. 41

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 113

O-RAN.TIFG.E2E-Test.0-v02.00
All the elements in the network like O-RAN system, 4G core and the application server need to have the ability to 1
capture traces to validate the successful execution of the test cases. The end user device needs to h ave the capability to 2
capture traces/packet capture to calculate the video streaming KPIs. Optionally, the network could have network taps 3
deployed in various legs of the network to get packet captures to validate successful execution of the test cases. Fi nally, 4
all these different components need to have connectivity with each other – the end user device should be able to connect 5
to O-RAN system(O-eNB, O-RUs, O-DU and O-CU-CP/O-CU-UP), O-RAN system needs to be connected to the 4G 6
core which in turn should have connectivity to the application server. 7
6.2.2.3 Test Methodology/Procedure 8
Ensure the end user device, O-RAN system, 4G core and the application server have all been configured as outlined in 9
Section 6.2.2.2. All traces and packet captures need to be enabled for the duration of the testing to ensure all 10
communication between network elements can be captured and validated. 11
1. Power on the end user device and ensure it registers with the 4G core for data services over NSA by connecting 12
over the O-eNB as Master eNB and O-RU1 of the O-RAN system as secondary gNB. 13
2. Once the registration is complete, the end user device has to establish a PDN connection with the 4G core. 14
3. Open the video streaming client on the end user device and start a streaming session over HTTP/TCP protocol.  15
4. Once the video streaming session has started, move the device so it can handover from O-RU1 to O-RU2 on the 16
Secondary gNB while it continues to use the O-eNB as the Master eNB.  17
5. Allow the end user device to stream video through the entire handover process. Measure the KPIs included in 18
Section 6.2 for this video streaming session. 19
6. Optionally, repeat steps 1 through 5 for a video streaming session which uses HTTP/QUIC protocol for streaming.  20
7. Repeat the test multiple times (> 10 times) and gather results. 21
 22
6.2.2.4 Test Expectation (expected results) 23
As a pre-validation, use the traces to validate a successful registration and PDN connection setup by the end user device 24
without any errors using the Master eNB and O-RU1, O-DU and O-CU-CP/O-CU-UP as the Secondary gNB. This is a 25
prerequisite before these tests can be validated. 26
Validate the end user device is able to start streaming the video without delays and watch the video content through the 27
entire handover process. Ensure there is no stalls, downgrading of video quality or intermittent buffering of the video 28
content, especially during the handover process. Use the packet captures to validate there is no packet drop or out of 29
sequence packets which could impact customer experience and point to a misconfigured or flawed system.  30
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 31
is performed in a controlled environment in good radio condition without the interference of external factors which 32
could impact the KPIs, example: use of internet to connect to remote servers/hosts could add latency, jitter and packet 33
loss issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the testing in 34
this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test res ults outside 35
the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues may be 36
due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and the te st 37
has to be repeated.  38
• Video Start time or Time to load first video frame – ~1.5 seconds 39
• Number of video stalls/buffering – < 1 40
• Duration of stalls in the video – < 5 seconds 41
• Video MOS Score – > 3.5 42
 43

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 114

O-RAN.TIFG.E2E-Test.0-v02.00
The values for the end user video streaming KPIs defined in Section 6.2 need to be included in the test report along with 1
the minimum configuration parameters included in Section 3.3. The following information should also be included in 2
the test report to provide a comprehensive view of the test setup. 3
End user device side (real or emulated UE): 4
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second)  5
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second)  6
• Received downlink throughput (L1 and L3 PDCP layers) (average sample per second) 7
• Downlink transmission mode 8
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 9
(average sample per second) 10
The table below gives an example of the test report considering the mean and  standard deviation of all test results that 11
have been captured.  12
Table 6-4 Example Test Report for Video Streaming – Handover between same Master eNB but different O-RUs 13
– Intra DU handover 14
 Video Streaming over HTTP/TCP Video Streaming over HTTP/QUIC
Video Start Time
Number of Video Stalls/buffering
Duration of stalls in the video
Video MOS Score
L1 DL Spectral efficiency [bps/Hz]
UE RSRP [dBm]
UE PDSCH SINR [dB]
MIMO rank
PDSCH MCS
DL RB number
UE CQI
UE RSRQ
UE PMI
UE RSSI
UE Buffer status
UE Packet delay
PDSCH BLER [%]
The end user video streaming experience can also be impacted by some of the features available (see Section 6.2) on the 15
O-eNB, O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other 16
features/functionality which could impact the end user’s video streaming experience should be included in the test 17
report to provide a comprehensive view of the testing. 18

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 115

O-RAN.TIFG.E2E-Test.0-v02.00
6.2.3 Video Streaming – Handover between same MeNB but different O-DUs 1
– Inter O-DU Intra O-CU 2
This test scenario validates the user’s video streaming experience when the UE is connected over NSA to a 4G core and 3
the UE is in the process of a handover between two O-RAN subcomponents (two O-RUs and O-DUs) on the Secondary 4
gNB while remaining connected to the same Master eNB. As services like VoLTE or VoNR cannot be used to test 5
handover of secondary gNB, video streaming is used for this testing. Hence, this test scenario is only applicable in an 6
NSA deployment. 7
6.2.3.1 Test Description 8
The 5G O-RAN system has multiple sub-components like the O-RU, O-DU and the O-CU-CP/O-CU-UP. This setup 9
leads to multiple handover scenarios when the O-RAN system is connected as a Secondary gNB in an NSA 10
deployment. This scenario tests the end user’s video streaming experience when the end user device is connected over 11
NSA to the 4G core and performs a handover between O-RUs which are connected to different O-DUs, which in turn 12
are connected to the same O-CU-CP/O-CU-UP on the Secondary gNB – Inter-O-DU Intra-O-CU handover. The end 13
user device remains connected to the same Master eNB through the entire handover process. This handover is agnostic 14
to the 4G core as the handover occurs on the O-RAN system. This test assesses the impact of the video streaming 15
service on the end user device in this handover scenario by monitoring the KPIs included in Section 6.2 This test case 16
streams video using ABR protocol over HTTP/TCP and/or HTTP/QUIC.  17
6.2.3.2 Test Setup 18
The SUT in this test case would be the Master eNB and a Secondary gNBs. The Master eNB will be an O-eNB and the 19
Secondary gNB will constitute of a pair of O-RUs – O-RU1 and O-RU2, which are connected to a pair of O-DUs -DU1 20
and O-DU2, which are connected to the same O-CU-CP/O-CU-UP. The O-eNB, O-RUs, O-DUs and O-CU-CP/O-CU-21
UP have to comply with the O-RAN specifications. The 4G core will be required to support the basic functionality to 22
authenticate and register an end user device in order to setup a PDN connection. The 4G/5G core could be a completely 23
emulated, partially emulated or a real non-emulated core. The application server(s) for this testing should support video 24
streaming using ABR protocol over HTTP/TCP and/or HTTP/QUIC as applicable. The end user device (UE) used for 25
testing could be a real UE or an emulated one. The test setup should include tools which have the ability to collect 26
traces on the elements and/or packet captures of communication between the elements. This could be a built -in 27
capability of the emulated/non-emulated network elements or an external tool. Optionally, if some of the network 28
elements or application server(s) are located remotely either in a cloud or on the internet, the additional latency should 29
be calculated and accounted for.  30
The O-eNB needs to have right the configuration and software load. The pair of O-RUs (O-RU1 and O-RU2) need to be 31
connected to the pair of O-DUs (O-DU1 and O-DU2), where O-RU1 is connected to O-DU1 and O-RU2 is connected 32
to O-DU2. Both the O-DUs, O-DU1 and O-DU2 need to be connected to the same O-CU-CP/O-CU-UP. All the O-33
RAN components (O-RUs, O-DUs and O-CU-CP/O-CU-UP) need to have the right configuration and software load. 34
The end user device must be configured with the right user credentials to re gister and authenticate with the Master eNB 35
and the 4G core. The end user device also needs to be provisioned with the right application like a video streaming 36
client to perform the tests. The 4G core network must be configured to support end user device u sed for testing. This 37
includes supporting registration, authentication and PDN connection establishment for this end user device.  38
All the elements in the network like O-RAN system, 4G core and the application server need to have the ability to 39
capture traces to validate the successful execution of the test cases. The end user device needs to have the capability to 40
capture traces/packet capture to calculate the video streaming KPIs. Optionally, the network could have network taps 41
deployed in various legs of the network to get packet captures to validate successful execution of the test cases. Finally, 42
all these different components need to have connectivity with each other – the end user device should be able to connect 43

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 116

O-RAN.TIFG.E2E-Test.0-v02.00
to O-RAN system (O-eNB, O-RUs, O-DUs and O-CU-CP/O-CU-UP), O-RAN system needs to be connected to the 4G 1
core which in turn should have connectivity to the application server. 2
6.2.3.3 Test Methodology/Procedure 3
Ensure the end user device, O-RAN system, 4G core and the application server have all been configured as outlined in 4
Section 6.2.3.2. All traces and packet captures need to be enabled for the duration of the testing to ensure all 5
communication between network elements can be captured and validated. 6
1. Power on the end user device and ensure it registers with the 4G core for data services over NSA by connecting 7
over the O-eNB as Master eNB and O-RU1 of the O-RAN system as secondary gNB. 8
2. Once the registration is complete, the end user device has to establish a PDN connection with the 4G core. 9
3. Open the video streaming client on the end user device and start a streaming session over HTTP/TCP protocol.  10
4. Once the video streaming session has started, move the device so it can handover from O-RU1 to O-RU2 (and in 11
turn O-DU1 to O-DU2) on the Secondary gNB while it continues to use the O-eNB as the Master eNB.  12
5. Allow the end user device to stream video through the entire handover process. Measure the KPIs included in 13
Section 6.2 for this video streaming session. 14
6. Optionally, repeat steps 1 through 5 for a video streaming session which uses HTTP/QUIC protocol for streaming. 15
7. Repeat the test multiple times (> 10 times) and gather results. 16
 17
6.2.3.4 Test Expectation (expected results) 18
As a pre-validation, use the traces to validate a successful registration and PDN connec tion setup by the end user device 19
without any errors using the O-eNB as Master eNB and O-RU1, O-DU1 and O-CU-CP/O-CU-UP as the Secondary 20
gNB. This is a prerequisite before these tests can be validated. 21
Validate the end user device is able to start streaming the video without delays and watch the video content through the 22
entire handover process. Ensure there is no stalls, downgrading of video quality or intermittent buffering of the video 23
content, especially during the handover process. Use the packet captures to validate there is no packet drop or out of 24
sequence packets which could impact customer experience and point to a misconfigured or flawed system.  25
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 26
is performed in a controlled environment in good radio condition without the interference of external factors which 27
could impact the KPIs, example: use of internet to connect to remote servers/hosts could add latency, jitter and packet 28
loss issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the testing in 29
this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test results outside 30
the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues may be 31
due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and the test 32
has to be repeated.  33
• Video Start time or Time to load first video frame – ~1.5 seconds 34
• Number of video stalls/buffering – < 1 35
• Duration of stalls in the video – < 5 seconds 36
• Video MOS Score – > 3.5 37
 38
The values for the end user video streaming KPIs defined in Section 6.2 need to be included in the test report along with 39
the minimum configuration parameters included in Section 3.3. The following information should also be included in 40
the test report to provide a comprehensive view of the test setup. 41
End user device side (real or emulated UE): 42

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 117

O-RAN.TIFG.E2E-Test.0-v02.00
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second)  1
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second)  2
• Downlink transmission mode 3
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 4
(average sample per second) 5
The table below gives an example of the test report considering the mean and standard deviati on of all test results that 6
have been captured.  7
Table 6-5 Example Test Report for Video Streaming – Handover between same Master eNB but different O-RUs 8
and O-DUs – Inter-O-DU Intra-O-CU handover 9
 Video Streaming over HTTP/TCP Video Streaming over HTTP/QUIC
Video Start Time
Number of Video Stalls/buffering
Duration of stalls in the video
Video MOS Score
L1 DL Spectral efficiency [bps/Hz]
UE RSRP [dBm]
UE PDSCH SINR [dB]
MIMO rank
PDSCH MCS
DL RB number
UE CQI
UE RSRQ
UE PMI
UE RSSI
UE Buffer status
UE Packet delay
PDSCH BLER [%]
The end user video streaming experience can also be impacted by some of the features available (see Section 6.2) on the 10
O-eNB, O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other 11
features/functionality which could impact the end user’s video streaming experience should be included in the test 12
report to provide a comprehensive view of the testing. 13
6.2.4 Video Streaming – Handover between same MeNB but different O-CUs 14
– Inter O-CU 15
This test scenario validates the user’s video streaming experience when the end user device (UE) is connected over 16
NSA to a 4G core and is in the process of a handover between two Secondary gNB (O-RUs, O-DUs and O-CU-CP/O-17
CU-UPs) while remaining connected to the same Master eNB. As services like VoLTE or VoNR cannot be used to test 18
handover of secondary gNB, video streaming is used for this testing. Hence, this test scenario is only applicable in an 19
NSA deployment. 20

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 118

O-RAN.TIFG.E2E-Test.0-v02.00
 1
6.2.4.1 Test Description 2
This scenario tests the impact of a handover on the video streaming service. The end user device is connected over NSA 3
to the 4G core and streaming video, while it performs a handover of the Secondary gNB (O-RU, O-DU and O-CU-4
CP/O-CU-UP) to a new Secondary gNB (O-RU, O-DU and O-CU-CP/O-CU-UP), i.e. an inter-O-CU handover, when 5
still being connected to the same Master eNB. This test assesses the impact of the video streaming ser vice on the end 6
user device in this handover scenario by monitoring the end user video streaming KPIs included in Section 6.2. This test 7
case streams video using ABR protocol over HTTP/TCP and/or HTTP/QUIC. 8
6.2.4.2 Test Setup 9
The SUT in this test case would be the O-eNB which would be the Master eNB and a pair of Secondary gNBs – O-10
RU1, O-DU1, O-CU-CP/O-CU-UP1 and O-RU2, O-DU2, O-CU-CP/O-CU-UP2. The O-eNB, O-RUs, O-DUs and O-11
CU-CP/O-CU-UPs have to comply with the O-RAN specifications. A 4G core will be required to support the basic 12
functionality to authenticate and register an end user device in order to setup a PDN connection. The 4G/5G core could 13
be a completely emulated, partially emulated or a real non-emulated core. The application server(s) for this testing 14
should support video streaming using ABR protocol over HTTP/TCP and/or HTTP/QUIC as applicable. The end user 15
device (UE) used for testing could be a real UE or an emulated one. The test setup should include tools which have the 16
ability to collect traces on the elements and/or packet captures of communication between the elements. This could be a 17
built-in capability of the emulated/non-emulated network elements or an external tool. Optionally, if some of the 18
network elements are located remotely either in a cloud or on the internet, the additional latency should be calculated 19
and accounted for.  20
The O-eNB, pair of gNBs (O-RUs, O-DUs and O-CU-CP/O-CU-UPs) need to have the right configuration and software 21
load. The pair of gNBs will be connected – O-RU1 will be connected to O-DU1, which in turn will be connected to O-22
CU-CP/O-CU-UP1 and similarly O-RU2 will be connected to O-DU2, which in turn will be connected to O-CU-CP/O-23
CU-UP2. The end user device must be configured with the right user credentials to be able to  register and authenticate 24
with the Master eNB and the 4G core. The end user device also needs to be provisioned with the right application like a 25
video streaming client to perform the tests. The 4G core network must be configured to support end user devic e used for 26
testing. This includes supporting registration, authentication and PDN connection establishment for this end user device.  27
All the elements in the network like O-RAN (O-eNB and gNBs), 4G core and the application server need to have the 28
ability to capture traces to validate the successful execution of the test cases. The end user device needs to have the 29
capability to capture traces/packet capture to calculate the video streaming KPIs. Optionally, the network could have 30
network taps deployed in various legs of the network to get packet captures to validate successful execution of the test 31
cases. Finally, all these different components need to have connectivity with each other – the end user device should be 32
able to connect to O-RAN system(O-eNB and gNBs), O-RAN system needs to be connected to the 4G core which in 33
turn should have connectivity to the application server. 34
6.2.4.3 Test Methodology/Procedure 35
Ensure the end user device, O-RAN system, 4G core and the application server have all been configured as outlined in 36
Section 6.2.4.2. All traces and packet captures need to be enabled for the duration of the testing to ensure all 37
communication between network elements can be captured and validated. 38
1. Power on the end user device and ensure it registers with the 4G core for data services over NSA by connecting 39
over the O-eNB as Master eNB and O-RU1, O-DU1, O-CU-CP/O-CU-UP1 as secondary gNB. 40
2. Once the registration is complete, the end user device has to establish a PDN connection with the 4G core. 41
3. Open the video streaming client on the end user device and start a streaming session over HTTP/TCP protocol.  42

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 119

O-RAN.TIFG.E2E-Test.0-v02.00
4. Once the video streaming session has started, move the device so the secondary gNB is handed over from O-RU1 1
to O-RU2 (i.e. from O-RU1, O-DU1 and O-CU-CP/O-CU-UP1, to O-RU2, O-DU2 and O-CU-CP/O-CU-UP2), 2
while it continues to use the O-eNB as the Master eNB.  3
5. Allow the end user device to stream video through the entire handover process. Measure the KPIs included in 4
Section 6.2 for this video streaming session. 5
6. Optionally, repeat steps 1 through 5 for a video streaming session which uses HTTP/QUIC protocol for streaming.  6
7. Repeat the test multiple times (> 10 times) and gather results. 7
 8
6.2.4.4 Test Expectation (expected results) 9
As a pre-validation, use the traces to validate a successful registration and PDN connection setup by the end user device 10
without any errors using the Master eNB and the O-RU1, O-DU1 and O-CU-CP/O-CU-UP1 as secondary gNB. This is 11
a prerequisite before these tests can be validated. 12
Validate the end user device is able to start streaming the video without delays and watch the video content through the 13
entire handover process. Ensure there is no stalls, downgrading of video quality or intermittent buffering of the video 14
content, especially during the handover process. Use the packet captures to validate there is no packet drop or out of 15
sequence packets which could impact customer experience and point to a misconfigured or flawed system.  16
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 17
is performed in a controlled environment in good radio condition without the interference of external factors which 18
could impact the KPIs, example: use of internet to connect to remote servers/hosts c ould add latency, jitter and packet 19
loss issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the testing in 20
this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any  test results outside 21
the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues may be 22
due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved a nd the test 23
has to be repeated.  24
• Video Start time or Time to load first video frame – ~1.5 seconds 25
• Number of video stalls/buffering – < 1 26
• Duration of stalls in the video – < 5 seconds 27
• Video MOS Score – > 3.5 28
 29
The values for the end user video streaming KPIs defined in Section 6.2 need to be included in the test report along with 30
the minimum configuration parameters included in Section 3.3. The following information should also be included in 31
the test report to provide a comprehensive view of the test setup. 32
End user device side (real or emulated UE): 33
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second)  34
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second)  35
• Downlink transmission mode 36
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 37
(average sample per second) 38
The table below gives an example of the test report considering the mean and standard deviation of al l test results that 39
have been captured.  40
Table 6-6 Example Test Report for Video Streaming – Handover between same Master eNB but different O-RUs 41
– Inter CU handover 42

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 120

O-RAN.TIFG.E2E-Test.0-v02.00
 Video Streaming over HTTP/TCP Video Streaming over HTTP/QUIC
Video Start Time
Number of Video Stalls/buffering
Duration of stalls in the video
Video MOS Score
L1 DL Spectral efficiency [bps/Hz]
UE RSRP [dBm]
UE PDSCH SINR [dB]
MIMO rank
PDSCH MCS
DL RB number
UE CQI
UE RSRQ
UE PMI
UE RSSI
UE Buffer status
UE Packet delay
PDSCH BLER [%]
The end user video streaming experience can also be impacted by some of the features available (see Section 6.2) on the 1
O-eNB, O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other 2
features/functionality which could impact the end user’s video streaming experience should be included in the test 3
report to provide a comprehensive view of the testing. 4
6.2.5 Video Streaming – Handover between different MeNB while staying 5
connected to same SgNB 6
This test scenario validates the user’s video streaming experience when the end user device (UE) is connected over 7
NSA to a 4G core and the UE is in the process of a handover between two Master eNBs (two O-eNB) while staying 8
connected to the same Secondary gNB (O-RU, O-DU and O-CU-CP/O-CU-UP). As services like VoLTE or VoNR 9
cannot be used to test handover of secondary gNB, video streaming is used for this testing. Hence, this t est scenario is 10
only applicable in an NSA deployment. 11
 12
6.2.5.1 Test Description 13
This scenario tests the impact of a handover on the video streaming service. The end user device is connected over NSA 14
to the 4G core and streaming video while it performs handover of the Master eNB to a new Master eNB, i.e. O-eNB1 to 15
O-eNB2, while staying connected to the same Secondary gNB (O-RU, O-DU and O-CU-CP/O-CU-UP).This test 16
assesses the impact of the video streaming service on the end user device in this handover s cenario by monitoring the 17
end user video streaming KPIs included in Section 6.2. This test case streams video using ABR protocol over 18
HTTP/TCP and/or HTTP/QUIC. 19

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 121

O-RAN.TIFG.E2E-Test.0-v02.00
6.2.5.2 Test Setup 1
The SUT in this test case would be a pair of O-eNBs (O-eNB1 and O-eNB2) which act as Master eNBs and a 2
Secondary gNBs – (O-RU, O-DU and O-CU-CP/O-CU-UP). The pair of O-eNBs, O-RU, O-DU and O-CU-CP/O-CU-3
UP have to comply with the O-RAN specifications. A 4G core will be required to support the basic functionality to 4
authenticate and register an end user device in order to setup a PDN connection. The 4G/5G core could be a completely 5
emulated, partially emulated or a real non-emulated core. The application server(s) for this testing should support video 6
streaming using ABR protocol over HTTP/TCP and/or HTTP/QUIC as applicable. The end user device (UE) used for 7
testing could be a real UE or an emulated one. The test setup should include tools which have the ability to collect 8
traces on the elements and/or packet captures of communication between the elements. This could be a built-in 9
capability of the emulated/non-emulated network elements or an external tool. Optionally, if some of the network 10
elements are located remotely either in a cloud or on the internet, the additional latency should be ca lculated and 11
accounted for.  12
The pair of O-eNBs (O-eNB1 and O-eNB2) and the gNB (O-RU, O-DU and O-CU-CP/O-CU-UP) need to have the 13
right configuration and software load. Tools which emulate latency need to be used and configured on the O -RAN 14
system (between O-RU, O-DU and O-CU-CP/O-CU-UP) to emulate real-world deployment conditions. The end user 15
device must be configured with the right user credentials to be able to register and authenticate with the O -RAN (O-16
eNBs and gNB) and the 4G core. The end user device also needs to be provisioned with the right application like a 17
video streaming client to perform the tests. The 4G core network must be configured to support end user device used for 18
testing. This includes supporting registration, authentication and PDN session establishment for this end user device.  19
All the elements in the network like O-RAN system, 4G core and the application server need to have the ability to 20
capture traces to validate the successful execution of the test cases. The end user device ne eds to have the capability to 21
capture traces/packet capture to calculate the video streaming KPIs. Optionally, the network could have network taps 22
deployed in various legs of the network to get packet captures to validate successful execution of the test c ases. Finally, 23
all these different components need to have connectivity with each other – the end user device should be able to connect 24
to O-RAN system(O-eNBs and gNB), O-RAN system needs to be connected to the 4G core which in turn should have 25
connectivity to the application server. 26
6.2.5.3 Test Methodology/Procedure 27
Ensure the end user device, O-RAN system, 4G core and the application server have all been configured as outlined in 28
Section 6.2.5.2. All traces and packet captures need to be enabled for the duration  of the testing to ensure all 29
communication between network elements can be captured and validated. 30
1. Power on the end user device and ensure it registers with the 4G core for data services over NSA by connecting 31
over the O-eNB1 as Master eNB and O-RU, O-DU and O-CU-CP/O-CU-UP as Secondary gNB. 32
2. Once the registration is complete, the end user device has to establish a PDN connection with the 4G core. 33
3. Open the video streaming client on the end user device and start a streaming session over HTTP/TCP protocol.  34
4. Once the video streaming session has started, move the end user device so it performs a handover of the Master 35
eNB from O-eNB1 to O-eNB2, while staying connected to the same Secondary gNB i.e. O-RU, O-DU and O-CU-36
CP/O-CU-UP. 37
5. Allow the end user device to stream video through the entire handover process. Measure the KPIs included in 38
Section 6.2 for this video streaming session. 39
6. Optionally, repeat steps 1 through 6 for a video streaming session which uses HTTP/QUIC protocol for streaming.  40
7. Repeat the test multiple times (> 10 times) and gather results. 41
 42

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 122

O-RAN.TIFG.E2E-Test.0-v02.00
6.2.5.4 Test Expectation (expected results) 1
As a pre-validation, use the traces to validate a successful registration and PDN connection setup by the end user device 2
without any errors using O-eNB1 as the Master eNB and the O-RU, O-DU and O-CU-CP/O-CU-UP as secondary gNB. 3
This is a prerequisite before these tests can be validated. 4
Validate the end user device is able to start streaming the video without delays and watch the video content through the 5
entire handover process. Ensure there is no stalls, downgrading of video quality or intermittent buffering of the video 6
content, especially during the handover process. Use the packet captures to validate there is no packet drop or out of 7
sequence packets which could impact customer experience and point to a misconfigured or flawed system. 8
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 9
is performed in a controlled environment in good radio condition without the interference of external factors which 10
could impact the KPIs, example: use of internet to connect to remote servers/hosts could add latency, jitter and packet 11
loss issues to the connection, thus impacting the KPIs. As there are multiple var iables which can impact the testing in 12
this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test results outside 13
the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues may be 14
due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and the test 15
has to be repeated.  16
• Video Start time or Time to load first video frame – ~1.5 seconds 17
• Number of video stalls/buffering – < 1 18
• Duration of stalls in the video – < 5 seconds 19
• Video MOS Score – > 3.5 20
 21
The values for the end user video streaming KPIs defined in Section 6.2 need to be included in the test report along with 22
the minimum configuration parameters included in Section 3.3. The following information should also be included in 23
the test report to provide a comprehensive view of the test setup. 24
End user device side (real or emulated UE): 25
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second)  26
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second)  27
• Downlink transmission mode 28
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 29
(average sample per second) 30
The table below gives an example of the test report considering the mean and standard deviati on of all test results that 31
have been captured.  32
Table 6-7 Example Test Report for Video Streaming – Handover between Master eNB while staying connected 33
to the same Secondary gNB 34
 Video Streaming over TCP/QUIC Video Streaming over TCP/QUIC
Video Start Time
Number of Video Stalls/buffering
Duration of stalls in the video
Video MOS Score
L1 DL Spectral efficiency [bps/Hz]

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 123

O-RAN.TIFG.E2E-Test.0-v02.00
UE RSRP [dBm]
UE PDSCH SINR [dB]
MIMO rank
PDSCH MCS
DL RB number
UE CQI
UE RSRQ
UE PMI
UE RSSI
UE Buffer status
UE Packet delay
PDSCH BLER [%]
The end user video streaming experience can also be impacted by some of the features available (see Section 6.2) on the 1
O-eNB, O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other 2
features/functionality which could impact the end user’s video streaming experience should be included in the test 3
report to provide a comprehensive view of the testing. 4
6.3 Voice Services – Voice over LTE (VoLTE) 5
Voice service forms another basic service which is provided to a customer on a telecom network. Even though the 6
earlier 2G/3G network provide voice service through circuit-switched network, this document focuses on voice service 7
provided using a packet switched network. This Section of the document includes test case to validate the Voice over 8
LTE service in different scenarios. The KPIs which will be monitored to assess the voice service are included below  9
• CSSR – Call Setup Success Rate % – Total number of calls which were successful by the total number of 10
calls made as a percentage. 11
• CST – Call Setup Time – Time taken from the initial SIP INVITE to when the SIP 180 Ringing is received 12
in seconds. 13
• MOS Score – Mean Opinion Score for the voice call. This needs to be measured on both ends of the Voice 14
call – mobile originated and mobile terminated. 15
• Mute Rate – This is the percentage of calls which were muted in both directions (calls with RTP loss of > 16
3-4s in both directions are considered muted call). This needs to be measured on both ends of the voice call 17
– mobile originated & mobile terminated and counted only once per call. 18
• One Way Calls – This is the percentage of calls which were muted in any one direction (calls with RTP 19
loss of > 3-4s in one direction only are considered one-way calls). This needs to be monitored on both ends 20
of the voice call – mobile originated & mobile terminated and counted only once per call. 21
• RTP Packet Loss % - Number of RTP packets which were dropped/lost in uplink/downlink direction as a 22
percentage of total packets. This needs to be measured on both ends of the voice call – mobile originated 23
and mobile terminated. 24
 25
Along with the monitoring and validation of these services using user experience KPIs, the O -RAN systems also need 26
to be monitored. The end user service experience can also be impacted by some of the features availab le on the O-RAN 27
system. Some of these features have been included below. As a part of the voice services testing, details of these 28
features need to be included in the test report to get a comprehensive view of the setup used for testing. If additional 29
features or functionalities have been enabled during this testing that impact the end user voice experience, those need be 30
included in the test report as well.  31
• RLC in Unacknowledged Mode 32

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 124

O-RAN.TIFG.E2E-Test.0-v02.00
• Robust Header Compression 1
• DRX 2
• Dynamic GBR Admission Control 3
• TTI Bundling 4
• VoLTE Inactivity Timer 5
• Frequency Hopping 6
• Multi-Target RRC Connection Re-Establishment 7
• VoLTE HARQ 8
• Coordinated Multi Point (DL and UL) 9
• VoLTE Quality Enhancement 10
• Packet Loss Detection 11
• Voice Codec Aware scheduler 12
• NR to LTE PS Redirection/Cell Reselection/Handover 13
• LTE to NR PS Redirection/Cell Reselection/Handover 14
 15
6.3.1 VoLTE Stationary Test 16
This scenario tests the voice service experience on an LTE network – Voice over LTE (VoLTE) when the end user 17
device is stationary.  18
6.3.1.1 Test Description 19
With penetration of LTE, VoLTE has become the primary method of providing voice service. VoLTE uses IP pac kets 20
to send and receive voice packets, with the IP packets being transferred over LTE. IP Multimedia System (IMS) forms 21
an important part of VoLTE as it is used to setup control and data plane needed for VoLTE communication. As voice 22
service is latency sensitive, the 4G core interacts with the IMS core to setup different bearers for Voice traffic – QCI-5 23
for VoLTE control plane and QCI-1 for VoLTE data plane. This test case is applicable when UE is connected over an 24
O-RAN system to a 4G core in an NSA deployment. 25
6.3.1.2 Test Setup 26
The SUT in this test case would be a O-eNB which would be the Master eNB and a Secondary gNB. As most of the 27
current NSA deployment use the 4G eNB to provide voice services, the use of a secondary gNB is optional. We have 28
however included the Secondary gNB in this test scenario, but it is only applicable if the gNB plays a role in 29
establishing the control plane or data plane for a voice call. The O-eNB, gNB and the components within these have to 30
comply with the O-RAN specifications. The O-RAN setup should support the ability to perform this testing in different 31
radio conditions as defined in Section 3.6. A 4G core will be required to support the basic functionality to authenticate 32
and register an end user device in order to setup a PDN connection. An IMS core will be required to register the end 33
user device to support voice services on a 4G network. The 4G and IMS cores could be a completely emulated, partially 34
emulated or real non-emulated cores. We will need at least two end user devices (UE) which can be a real UEs or an 35
emulated one, and both have to support voice service using VoLTE. The end user devices will serve as Mobile 36
Originated (MO) and Mobile Terminated (MT) end user devices forming the two ends of the voice call. Going forward 37
in this section, these end user devices will be referred to as MO end user device and MT end user devices to represent 38

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 125

O-RAN.TIFG.E2E-Test.0-v02.00
the role they plan in the voice call. The test setup should include tools which have the ability to collect traces on the 1
elements and/or packet captures of communication between the elements. This could be a built -in capability of the 2
emulated/non-emulated network elements or an external tool. Optionally, if some of the network elements are located 3
remotely either in a cloud or on the internet, the additional latency should be calculated and accounted for.  4
The Master eNB, Secondary gNB and their components (O-eNB, O-RU, O-DU and O-CU-CP/O-CU-UP) need to have 5
the right configuration and software load. The end user device must be configured with the right user credentials to be 6
able to register and authenticate with the O-RAN system and the 4G core. The end user devices also need to be 7
provisioned with the user credentials to attach to the 4G core and register with the IMS core to perform voice call using 8
VoLTE. The 4G core network and IMS core must be configured to support end user devices used for testing. This 9
includes supporting registration, authentication and PDN connection establishment for these end user devices.  This also 10
includes provisioning the IMS core to support registration of the end user devices to make voice calls over VoLTE, and 11
dynamically setting up dedicated bearers for voice calls. The locations where the radio conditions are excellent, good, 12
fair and poor need to be identified within the serving cell. 13
All the elements in the network like O-RAN system, 4G core and the IMS Core need to have the ability to capture 14
traces to validate the successful execution of the test cases. The end user devices need to have the capability to capture 15
traces/packet capture to calculate the VoLTE KPIs. Optionally, the network could have network taps deployed in 16
various legs of the network to get packet captures to validate successful execution of the test cases. Finally, all these 17
different components need to have connectivity with each other – the end user device should be able to connect to O-18
RAN system(O-eNBs and gNBs), O-RAN system needs to be connected to the 4G core which in turn should have 19
connectivity to the IMS Core. 20
6.3.1.3 Test Methodology/Procedure 21
Ensure the end user devices, O-RAN system, 4G core and the IMS Core have all been configured as outlined in Section 22
6.3.1.2. In this test scenario, both the mobile originated and mobile terminated end user devices are going to use the 23
same O-RAN (i.e. same O-eNB), 4G and IMS core to perform the end-to-end voice call. All traces and packet captures 24
need to be enabled for the duration of the testing to ensure all communication between network elements can be 25
captured and validated. 26
1. Power on the two end user devices in excellent radio condition and ensure both of the end user devices connect to 27
the 4G core over the Master O-eNB and optionally secondary gNB.  28
2. Ensure both the MO and MT end user devices can establish a PDN connection with the 4G core. Once the PDN 29
connection has been setup, both the end user devices have to register with the IMS core to support voice services.  30
3. Use the MO end user device to call the MT end user device. Validate the MT end user device is able to receive and 31
answer the call. 32
4. Continue to have two-way voice communication on the voice call for at least 5 minutes before terminating it.  33
5. Repeat the test multiple times (>10 times) and gather results. 34
6. Repeat the above steps 1 through 5 for the MO and MT end user devices in good, fair and poor radio conditions.  35
 36
6.3.1.4 Test Expectation (expected results) 37
As a pre-validation, use the traces to validate a successful PDN connection setup by the  end user devices without any 38
errors using the Master eNB and optionally the secondary gNB. Also validate both the end user devices are able to 39
register with the IMS core for voice services. This is a prerequisite before these tests can be validated.  40
Validate the end user devices are able to make a voice call between each other by dynamically setting up a QCI -1 bearer 41
to transfer voice packets over RTP. Ensure the Call Setup Time is reasonable, the voice quality from both parties are 42
clear and audible without one-way or intermittent muting. Use the packet captures to validate there is no RTP packet 43

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 126

O-RAN.TIFG.E2E-Test.0-v02.00
drops or high RTP packet jitter which could cause voice muting issues. Use the packet captures to ensure there are no 1
out-of-sequence packets which could impact customer’s voice experience.  2
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 3
is performed in a controlled environment in good radio condition without the interference of external f actors which 4
could impact the KPIs, example: use of internet to connect to remote network nodes could add latency, jitter and packet 5
loss issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the testing in 6
this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test results outside 7
the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues may be 8
due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and the test 9
has to be repeated.  10
• CSSR – Call Setup Success Rate % –> 99%. 11
• CST – Call Setup Time – < 2.5s 12
• MOS Score – > 3.5 13
• Mute Rate % – < 1% 14
• One Way Call % - < 1% 15
• RTP Packet Loss % - < 1% 16
 17
These end user voice KPI values included in Section 6.3 need to be included in the test report along with the minimum 18
configuration parameters included in Section 3.3. The following information should also be included in the test report 19
for the testing performed in different radio conditions to provide a comprehensive view of the test setup.  20
End user device side (real or emulated UE): 21
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second)  22
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second)  23
• Downlink transmission mode 24
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 25
(average sample per second) 26
The table below gives an example of the test report considering the mean and standard deviation of all test results that 27
have been captured.  28
Table 6-8 Example Test Report for Voice over LTE Testing – Stationary Test 29

Excellent
(cell centre) Good Fair Poor
(cell edge)
VoLTE MO/MT VoLTE MO/MT VoLTE MO/MT VoLTE MO/MT
Call Setup Success Rate
Call Setup time
MOS Score
Mute Rate
One Way Call
RTP Packet Loss
L1 DL Spectral efficiency [bps/Hz]
UE RSRP [dBm]
UE PDSCH SINR [dB]

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 127

O-RAN.TIFG.E2E-Test.0-v02.00
MIMO rank
PDSCH MCS
DL RB number
UE CQI
UE RSRQ
UE PMI
UE RSSI
UE Buffer status
UE Packet delay
PDSCH BLER [%]
The end user VoLTE experience can also be impacted by some of the features available (see Section 6.3) on the O -eNB, 1
O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other features/func tionality which 2
could impact the end user’s voice service experience should be included in the test report to provide a comprehensive 3
view of the testing. 4
6.3.2 VoLTE Handover Test 5
This test section is FFS 6
6.3.3 Voice Service - LTE and NR handover tests 7
This test scenario validates the user’s voice experience, when the UE is in a voice call, and performs a handover from 8
LTE network to a 5G network and vice versa. This test scenario is applicable for a 5G SA deployment.  9
6.3.3.1 Test Description 10
Voice service is one of the basic services provided on the telecommunication network. Voice service on the 4G network 11
is provided using VoLTE. Similarly, voice service on the 5G network is provided using packet switch technology called 12
Voice over New Radio (VoNR). As 5G network is being deployed by a telecommunication service provider, the service 13
provider will need to support 4G and 5G network, and thus support VoLTE, VoNR and the handover between the two 14
voice services. This scenario tests the end user’s voice experience when the end  user device performs a handover from 15
VoLTE to VoNR and vice versa. 16
6.3.3.2 Test Setup 17
The SUT in this test case would be an O-eNB along with a gNB (O-RU, O-DU and O-CU-CP/O-CU-UP). We would 18
also need an interworking 4G-5G core (referred to as 4G-5G core going forward) which supports a combined anchor 19
point for 4G and 5G, i.e. SMF+PGW-C and UPF+PGW-U. The eNB connects to a 4G-5G core over the 4G interfaces 20
like S1 to provide 4G LTE service and the O-CU-CP/O-CU-UP, O-DU and O-RU would connect to a 4G-5G core over 21
the 5G interfaces like N2 and N3 to provide 5G SA service. The O-eNB, gNB and the components within these have to 22
comply with the O-RAN specifications. The 4G-5G core will be required to support the basic functionality to 23
authenticate and register an end user device in order to setup a PDN connection/PDU session. An IMS core will be 24
required to register the end user device to support voice services on a 4G and 5G network. The 4G and 5G core have to 25
interwork using the N26 interface between the MME and AMF to support seamless handover. We are recommending 26
the use of N26 interface to ensure better customer experience when the end user device performs a handover between 27
4G and 5G. The 4G-5G and IMS cores could be a completely emulated, partially emulated or real non-emulated cores. 28
We will need at least two end user devices (UE) which can be a real UEs or an emulated one, and both have to support 29

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 128

O-RAN.TIFG.E2E-Test.0-v02.00
voice service using VoLTE, VoNR and the capability to handover from VoLTE to VoNR and vice versa. The end user 1
devices will serve as Mobile Originated (MO) and Mobile Terminated (MT) end user devices forming the two ends of 2
the voice call. Going forward in this section, these end user devices will be referred to as MO end user device and MT 3
end user devices to represent their role in the voice call. The test setup should include tools which have the ability to 4
collect traces on the elements and/or packet captures of communication between the elements. This could be a built -in 5
capability of the emulated/non-emulated network elements or an external tool. Optionally, if some of the network 6
elements are located remotely either in a cloud or on the internet, the additional latency should be calculated and 7
accounted for.  8
The O-eNB, gNB and their components (O-RU, O-DU and O-CU-CP/O-CU-UP) need to have the right configuration 9
and software load. The end user devices must be configured with the right user credentials to be able to register and 10
authenticate with the O-RAN system and the 4G-5G core. The end user devices also need to be provisioned with the 11
user credentials to support attach procedure to the 4G-5G core, registering to the IMS core and performing a voice call 12
using VoLTE and VoNR. This includes supporting registration, authentication and PDN connection/PDU Session 13
establishment for these end user devices. This also includes provisioning the IMS core to support registration of the end 14
user devices to make voice calls over VoLTE and VoNR, and dynamically setting up dedicated bearers/QoS Flows for 15
voice calls.  16
All the elements in the network like O-RAN system, 4G-5G core and the IMS Core need to have the ability to capture 17
traces to validate the successful execution of the test cases. The end user devices need to have the capability to capture 18
traces/packet capture to calculate the VoLTE and VoNR KPIs. Optionally, the network could have network taps 19
deployed in various legs of the network to get packet captures to validate successful execution of the test cases. Finally, 20
all these different components need to have connectivity with each other – the end user device should be able to connect 21
to O-RAN system(eNBs and gNBs), O-RAN system needs to be connected to the 4G-5G core which in turn should 22
have connectivity to the IMS Core. 23
6.3.3.3 Test Methodology/Procedure 24
Ensure the end user devices, O-RAN system, 4G-5G core and the IMS Core have all been configured as outlined in 25
Section 6.3.3.2. In this test scenario, both the mobile originated and mobile terminated end user devices are going to use 26
the eNB and gNB to connect to the same 4G-5G and IMS core to perform the end-to-end voice call. All traces and 27
packet captures need to be enabled for the duration of the testing to ensure all communication between network 28
elements can be captured and validated. 29
The below section gives the steps to perform a VoLTE to VoNR handover, followed by VoNR to VoLTE handover 30
1. Power on the two end user devices and ensure both of the end user devices can connect to the 4G-5G core over the 31
O-eNB.  32
2. Ensure both the MO and MT end user devices can establish a PDN connection with the 4G-5G core. Once the PDN 33
connection has been setup, both the end user devices have to register with the IMS core to support voice services.  34
3. Use the MO end user device to call the MT end user device. Validate the MT end user device is able to  receive and 35
answer the call. 36
4. Once the two-way call has been setup and communication is going back and forth between the two end user 37
devices, move the MO device to the O-RU coverage of the gNB, thus forcing a handover from 4G to 5G, hence 38
forcing a VoLTE to VoNR handover. Continue two-way voice communication through the handover process and 39
terminate the call once the handover process is complete. Measure the voice KPIs included in Section 6.3.  40
5. At this point in time, the MO end user device is in 5G coverage and registered to the 4G-5G core. The MT end user 41
device is in 4G coverage and registered to the 4G-5G core. Both end user devices should still be registered to the 42
IMS core. 43
6. Use the MO end user device to call the MT end user device. Validate the MT end user device is able to receive and 44
answer the call. 45
7. Once the two-way call has been setup and communication is going back and forth between the two end user 46
devices, move the MO device to the O-eNB coverage, thus forcing a handover from 5G to 4G, hence forcing a 47

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 129

O-RAN.TIFG.E2E-Test.0-v02.00
VoNR to VoLTE handover. Continue two-way voice communication through the handover process and terminate 1
the call once the handover process is complete. Measure the voice KPIs included in Section 6.3.  2
8. Repeat the test multiple times (> 10 times) and gather results. 3
 4
6.3.3.4 Test Expectation (expected results) 5
As a pre-validation, use the traces to validate a successful registration and PDN connection setup by the end user 6
devices without any errors using the O-eNB when in 4G coverage. Similarly, use traces to validate successful 7
registration and PDU session setup by the end user device without any errors using O -RU, O-DU and O-CU-CP/O-CU-8
UP when in 5G coverage. Also validate both the end user devices are able to register with the IMS core for voice 9
services. This is a prerequisite before these tests can be validated. 10
Validate the end user devices are able to make a voice call between each other by dynamically setting up a QCI -1/5QI-1 11
bearer to transfer voice packets over RTP. Ensure the Call Setup Time is  reasonable, the voice quality from both parties 12
are clear and audible without one-way or intermittent muting. Ensure the voice quality is not impacted during the 13
handover process – VoLTE to VoNR and VoNR to VoLTE. Use the packet captures to validate there  is no RTP packet 14
drops or high RTP packet jitter which could cause voice muting issues, especially during the handover process. Use the 15
packet captures to ensure there are no out-of-sequence packets which could impact customer’s voice experience.  16
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 17
is performed in a controlled environment in good radio condition without the interference of external factors which 18
could impact the KPIs, example: use of internet to connect to remote network nodes could add latency, jitter and packet 19
loss issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the testing in 20
this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test results outside 21
the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues may be 22
due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and the test 23
has to be repeated.  24
• CSSR – Call Setup Success Rate % –> 99%. 25
• CST – Call Setup Time – < 2.5s 26
• MOS Score – > 3.5 27
• Mute Rate % – < 1% 28
• One Way Call % - < 1% 29
• RTP Packet Loss % - < 1% 30
 31
These end user voice KPI values included in Section 6.3 need to be included in the test report along with the minimum 32
configuration parameters included in Section 3.3. The following information should also be included in the test report 33
for the testing performed in different radio conditions to provide a comprehensive view of the test setup. 34
End user device side (real or emulated UE): 35
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second)  36
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second) 37
• Received downlink throughput (L1 and L3 PDCP layers) (average sample per second)  38
• Downlink transmission mode 39
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 40
(average sample per second) 41

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 130

O-RAN.TIFG.E2E-Test.0-v02.00
The table below gives an example of the test report considering the mean and standard deviation of all test results that 1
have been captured.  2
Table 6-9 Example Test Report for Voice service handover testing – LTE and NR 3
 VoLTE to VoNR handover VoNR to VoLTE handover
Call Setup Success Rate
Call Setup time
MOS Score
Mute Rate
One Way Call
RTP Packet Loss
L1 DL Spectral efficiency [bps/Hz]
UE RSRP [dBm]
UE PDSCH SINR [dB]
MIMO rank
PDSCH MCS
DL RB number
UE CQI
UE RSRQ
UE PMI
UE RSSI
UE Buffer status
UE Packet delay
PDSCH BLER [%]
The end user VoLTE/VoNR experience can also be impacted by some of the features available (see Section 6.3) on the 4
O-eNB, O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other 5
features/functionality which could impact the end user’s voice service experience should be included in the test report to 6
provide a comprehensive view of the testing. 7
6.4 Voice Service – EPS Fallback 8
Voice service is one of the basic services which must be supported on every telecom network. Upgrade of telecom 9
network occurs in phases, and this is no different for 5G. The telecom networ k may not be able to support voice service 10
on 5G during this phase for multiple reasons – the 5G network may not be deployed nationwide, or 5G network may not 11
be tuned to support voice service or the devices may not be able to support voice service on 5G. However, there can be 12
no interruption to voice service during this transition phase. EPS fallback is the method used to support voice services 13
during this phase, where voice services are continued to be supported on the legacy LTE network using VoLTE by 14
forcing the device to fallback to LTE to make or receive a call. The KPIs which will be monitored to assess the voice 15
service are included below 16
• CSSR – Call Setup Success Rate % – Total number of calls which were successful by the total number of 17
calls made as a percentage. 18
• CST – Call Setup Time – Time taken from the initial SIP INVITE to when the SIP 180 Ringing is received 19
in seconds. 20

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 131

O-RAN.TIFG.E2E-Test.0-v02.00
• MOS Score – Mean Opinion Score for the voice call. This needs to be measured on both ends of the Voice 1
call – mobile originated and mobile terminated. 2
• Mute Rate % – This is the percentage of calls which were muted in both directions (calls with RTP loss of 3
> 3-4s in both directions are considered muted call). This needs to be measured on both ends of the Voice 4
call – mobile originated and mobile terminated and counted only once per call. 5
• One Way Calls % – This is the percentage of calls which were muted in any one direction (calls with RTP 6
loss of > 3-4s in one direction only are considered one-way calls). This needs to be monitored on both ends 7
of the Voice call – mobile originated and mobile terminated and counted only once per call. 8
• RTP Packet Loss % - Number of RTP packets which were dropped/lost in uplink/downlink direction as a 9
percentage of total packets. This needs to be measured on both ends of the Voice call – mobile originated 10
and mobile terminated. 11
 12
Along with the monitoring and validation of these services using user experience KPIs, the O -RAN systems also need 13
to be monitored. The end user service experience can also be impacted by some of the features available on the O -RAN 14
system. Some of these features have been included below. As a part of the voice services testing, details of these 15
features need to be included in the test report to get a comprehensive view of the setup used for testing. If additional 16
features or functionalities have been enabled during this testing that impact the end user voice experience, those need be 17
included in the test report as well.  18
• EPS Fallback for IMS Voice 19
• NR to EPS Mobility 20
• RLC in Unacknowledged Mode 21
• Robust Header Compression 22
• DRX 23
• Dynamic GBR Admission Control 24
• TTI Bundling 25
• VoLTE Inactivity Timer 26
• Frequency Hopping 27
• Multi-Target RRC Connection Re-Establishment 28
• VoLTE HARQ 29
• Coordinated Multi Point (DL and UL) 30
• VoLTE Quality Enhancement 31
• Packet Loss Detection 32
• Voice Codec Aware scheduler 33
 34
6.4.1 EPS Fallback Test 35
This scenario tests the voice service when an end user device (UE) is in 5G SA coverage and performs an EPS fallback 36
to 4G to make or receive a voice call. 37

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 132

O-RAN.TIFG.E2E-Test.0-v02.00
6.4.1.1 Test Description 1
This section tests the voice service on a 5G SA network when it uses EPS fallback mechanism to fallback to the LTE 2
network and use VoLTE to support voice service. This testing only applies to 5G SA deployment and in this scenario 3
the UE is connected to the 5G SA Core and registered with the IMS core for voice service. When the end user device 4
(UE) wants to make a voice call or receive a voice call, the network informs the device to fallback to the LTE network 5
to make/receive the voice call. The EPS fallback does increase the Call Setup Time due to the time needed to fallback 6
before making/receiving the voice call.  7
There are two mechanism in which EPS fallback is supported on the 5G core, and the methodology used impacts the 8
time taken to perform the fallback to LTE, and hence impacts the Call Setup Time. The two mechanisms that could be 9
used to perform EPS fallback is included below. These two mechanisms do change the test setup but does not change 10
the testing procedure. 11
• With N26 interface – The AMF in the 5G core communicates with the MME in the 4G over the N26 interface. 12
In this scenario the UE performs a handover from the 5G network to the 4G network. 13
• Without N26 interface – The AMF in the 5G core does not communicate directly with the 4G core, instead 14
uses the UDM/HSS to store and transfer relevant session information to the 4G core. In this scenario the UE  15
performs a Release with Redirect from the 5G network to the 4G network.  16
6.4.1.2 Test Setup 17
The SUT in this test case would be an O-eNB along with a gNB (O-RU, O-DU and O-CU-CP/O-CU-UP). We would 18
also need an interworking 4G-5G core (referred to as 4G-5G core going forward) which supports a combined anchor 19
point, i.e. SMF+PGW-C and UPF+PGW-U. The O-eNB connects to a 4G-5G core over the 4G interfaces like S1 to 20
provide 4G LTE service and the gNB (O-RU, O-DU and O-CU-CP/O-CU-UP) would connect to the 4G-5G core over 21
the 5G interfaces like N2 and N3 to provide 5G SA service. The O-eNB, gNB and the components within these have to 22
comply with the O-RAN specifications and support EPS fallback. The 4G-5G core will be required to support the basic 23
functionality to authenticate and register an end user device in order to setup a PDN connection/PDU session. T he IMS 24
core will be required to register the end user device and needs to be integrated with the 4G -5G core to support voice 25
services over VoLTE and EPS fallback. The 4G-5G core have to interwork either using the N26 interface or without 26
using N26 interface depending on the desired core network configuration. The existence of the N26 interface does 27
reduce the EPS fallback time, hence reduce Call Setup Time and provide better end user experience. The 4G -5G and 28
IMS cores could be a completely emulated, partially emulated or real non-emulated cores. We will need at least two end 29
user devices (UE) which can be a real UEs or an emulated one, and both have to support voice service using VoLTE 30
and EPS fallback procedure. The end user devices will serve as Mobile Or iginated (MO) and Mobile Terminated (MT) 31
end user devices forming the two ends of the voice call. For the sake of clarity, we will address these end user devices 32
as UE-1 and UE-2 in this section. The test setup should include tools which have the ability to collect traces on the 33
elements and/or packet captures of communication between the elements. This could be a built -in capability of the 34
emulated/non-emulated network elements or an external tool. Optionally, if some of the network elements are located 35
remotely either in a cloud or on the internet, the additional latency should be calculated and accounted for.  36
The O-eNB, gNB and their components (O-RU, O-DU and O-CU-CP/O-CU-UP) need to have the right configuration 37
and software load. The end user device must be configured with the right user credentials to be able to register and 38
authenticate with the O-RAN system and the 4G-5G core. The end user devices also need to be provisioned with the 39
user credentials to attach to the 4G-5G core and register with the IMS core to perform voice call using VoLTE and EPS 40
fallback. The 4G-5G core network and IMS core must be configured to support voice service on the end user devices 41
used for testing, which includes dynamically setting up dedicated bearers for voice calls.  42
All the elements in the network like O-RAN system, 4G-5G core and the IMS Core need to have the ability to capture 43
traces to validate the successful execution of the test cases. The end user devices need to have the capability to capture 44
traces/packet capture to calculate the VoLTE KPIs. Optionally, the network could have network taps deployed in 45

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 133

O-RAN.TIFG.E2E-Test.0-v02.00
various legs of the network to get packet captures to validate successful execution of the test cases. Finally, all these 1
different components need to have connectivity with each other – the end user device should be able to connect to O-2
RAN system(O-eNBs and gNBs), O-RAN system needs to be connected to the 4G-5G core which in turn should have 3
connectivity to the IMS Core. 4
6.4.1.3 Test Methodology/Procedure 5
Ensure the end user devices, O-RAN system, 4G-5G core and the IMS Core have all been configured as outlined in 6
Section 6.4.1.2. In this test scenario, one of the end user device is going to be connected over the 4G O -eNB to the 4G-7
5G core, while the other end user device is going to be connected over 5G gNB (O-RU, O-DU and O-CU-CP/O-CU-8
UP) to the 4G-5G core. All traces and packet captures need to be enabled for the duration of the testing to ensure all 9
communication between network elements can be captured and validated. 10
1. Power on the two end user devices and ensure UE-1 is in LTE coverage, when UE-2 is in the 5G coverage. 11
Validate both end user devices register to the 4G-5G core. 12
2. Once the registration is complete, UE-1 and UE-2 have to establish a PDN connection and PDU session 13
respectively with the 4G-5G core. Once the PDN connection and PDU session have been setup, both the end user 14
devices have to register with the IMS core to support voice services. 15
3. Use UE-1 as the MO end user device to call UE-2. Validate the UE-2 performs EPS fallback to LTE to receive the 16
call.  17
4. Answer the call on UE-2 and continue to have two-way communication for at least 5 minutes and terminate the 18
call. Measure the voice KPIs included in Section 6.4.  19
5. At this point UE-2 should have completed the call and moved back to connect to the 4G-5G Core over 5G gNB. 20
6. Use UE-2 as the MO end user device to call UE-1. Validate the UE-2 performs EPS fallback to LTE to before 21
making the call.  22
7. Answer the call on UE-1 and continue to have two-way communication for at least 5 minutes and terminate the 23
call. Measure the voice KPIs included in Section 6.4.  24
8. Repeat the test multiple times (>10 times) and gather results. 25
 26
6.4.1.4 Test Expectation (expected results) 27
As a pre-validation, use the traces to validate end user device UE-1 is able to perform a successful registration and PDN 28
connection setup while using the O-eNB in LTE coverage. Similarly, use the traces to validate end user device UE -2 is 29
able to perform a successful registration and PDU session setup while using gNB in 5G coverage. Also validate both the 30
end user devices are able to register with the IMS core for voice services. This is a prerequisite before these tests can be 31
validated. 32
Validate the end user devices are able to make a voice call between each other by dynamically setting up a QCI-1 bearer 33
to transfer voice packets over RTP. Validate the device which is in the 5G coverage falls back to 4G before receiving or 34
making a voice call, in other words the EPS fallback procedure has been executed successfully. The Call Setup Time 35
will be higher than VoLTE due to the delay associated with executing the EPS fallback procedure. Even though this test 36
case does not directly impact the quality of the voice call, for the sake of consistency, ensure the vo ice quality from both 37
parties are clear and audible without one-way or intermittent muting. Use the packet captures to validate there is no 38
RTP packet drops or high RTP packet jitter which could cause voice muting issues. Use the packet captures to ensure 39
there are no out-of-sequence packets which could impact customer’s voice experience.  40
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 41
is performed in a controlled environment in good radio condition without the interference of external factors which 42
could impact the KPIs, example: use of internet to connect to remote network nodes could add latency, jitter and packet 43
loss issues to the connection, thus impacting the KPIs. As there are mu ltiple variables which can impact the testing in 44
this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test results outside 45

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 134

O-RAN.TIFG.E2E-Test.0-v02.00
the range of KPI encountered during testing, will have to be investigated to identify  the root cause as the issues may be 1
due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and the test 2
has to be repeated.  3
Table 6-10 Typical Voice KPI values in controlled environments with good radio conditions 4
KPI With N26 Interface Without N26 Interface
CSSR – Call Setup Success Rate % >99% >99%
CST – Call Setup Time 3.5s 4 s
MOS Score 3.5 3.5
Mute Rate % < 1% < 1%
One Way Call % < 1% < 1%
RTP Packet Loss % < 1% < 1%
 5
These end user voice KPI values included in Section 6.3 need to be included in the test report along with the minimum 6
configuration parameters included in Section 3.3. The following information should also be included in the t est report to 7
provide a comprehensive view of the test setup. 8
End user device side (real or emulated UE): 9
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second)  10
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second) 11
• Downlink transmission mode 12
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 13
(average sample per second) 14
The table below gives an example of the test report considering the mean and standard deviation of all test results that 15
have been captured.  16
Table 6-11 Example Test Report for Voice Service Testing – EPS Fallback 17
 VoLTE MO VoLTE MT
Call Setup Success Rate
Call Setup time
MOS Score
Mute Rate
One Way Call
RTP Packet Loss
L1 DL Spectral efficiency [bps/Hz]
UE RSRP [dBm]
UE PDSCH SINR [dB]
MIMO rank
PDSCH MCS

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 135

O-RAN.TIFG.E2E-Test.0-v02.00
DL RB number
UE CQI
UE RSRQ
UE PMI
UE RSSI
UE Buffer status
UE Packet delay
PDSCH BLER [%]
The end user VoLTE experience can also be impacted by some of the features available (see Section 6.3) on the O -eNB, 1
O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other features/functionality which 2
could impact the end user’s voice service experience should be included in the test report to provide a comprehensive 3
view of the testing. 4
6.5 Voice Service – Voice over NR (VoNR) 5
Voice services on 5G are called Voice over New Radio (VoNR). VoNR also packetizes voice and uses IP packets for 6
voice communication. On the Core and IMS side, VoNR is very similar to VoLTE. VoNR also uses IMS system for 7
voice service, and the IMS interacts with the 5G core to setup separate bearer for voice service. This section tests the 8
voice service over VoNR in different scenarios. The KPIs which will be monitored to assess the voice service are 9
included below 10
• CSSR – Call Setup Success Rate % – Total number of calls which were successful by the total number of 11
calls made as a percentage. 12
• CST – Call Setup Time – Time taken from the initial SIP INVITE to when the SIP 180 Ringing is received 13
in seconds. 14
• MOS Score – Mean Opinion Score for the voice call. This needs to be measured on both ends of the Voice 15
call – mobile originated and mobile terminated. 16
• Mute Rate % – This is the percentage of calls which were muted in both directions (calls with RTP loss of 17
> 3-4s in both directions are considered muted call). This needs to be measured on both ends of the Voice 18
call – mobile originated and mobile terminated and counted only once per call. 19
• One Way Calls % – This is the percentage of calls which were muted in any one direction (calls with RTP 20
loss of > 3-4s in one direction only are considered one-way calls). This needs to be monitored on both ends 21
of the Voice call – mobile originated and mobile terminated and counted only once per call. 22
• RTP Packet Loss % - Number of RTP packets which were dropped/lost in uplink/downlink direction as a 23
percentage of total packets. This needs to be measured on both ends of the Voice call – mobile originated 24
and mobile terminated. 25
 26
End user voice service experience can also be impacted by some of the features available on the O-RU, O-DU and O-27
CU-CP/O-CU-UP. Some of these features have been included below. As a part of the voice services testing, details of 28
these features need to be included in the test report to get a comprehensive view of  the setup used for testing. If 29
additional features or functionalities have been enabled during this testing that impact the end user voice experience, 30
those need be included in the test report as well. 31
• Basic Voice over NR 32
• RLC in Unacknowledged Mode 33
• Robust Header Compression 34
• DRX 35

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 136

O-RAN.TIFG.E2E-Test.0-v02.00
• Dynamic GBR Admission Control 1
• TTI Bundling 2
• Automated Neighbor Relations ANR 3
• Connected mode mobility (Handover) 4
• idle mode reselection 5
• Intra-frequency Cell Reselection/Handover 6
• Inter-frequency Redirection/Cell Reselection/Handover 7
• NR Coverage-Triggered NR Session Continuity 8
 9
6.5.1 Voice over NR Test 10
This test scenario validates the user’s voice experience when the end user device is in 5G coverage and the user makes a 11
voice call over NR – VoNR. 12
6.5.1.1 Test Description 13
This section tests VoNR user experience on an O-RAN system. VoNR is similar to VoLTE where it uses IP packets to 14
send and receive voice packets, with the IP packets being transferred over NR or 5G radio. Just like VoLTE, IMS is 15
used to setup the control and data plane for the voice communication. As voice service is latency sensitive, the 5G core 16
interacts with the IMS core to setup different QoS flows for voice traffic – 5QI-5 for VoNR control plane and 5QI-1 for 17
VoNR data plane. This test case is applicable when end user device (UE) is connected over a 5G SA network.  18
6.5.1.2 Test Setup 19
The SUT in this test case would be a gNB (O-RU, O-DU and O-CU-CP/O-CU-UP) which is used to test the voice 20
service. A 5G SA Core would be required to support basic functionality to authenticate and register the end user device 21
to establish a PDU session. An IMS core will be required to register the end user device to support voice services on a 22
5G network. The 5G and IMS cores could be a completely emulated, partially emulated or real non-emulated cores. We 23
will need at least two end user devices (UE) which can be a real UEs or an emulated one, and both have to support 24
voice service using VoNR. The end user devices will serve as Mobile Originated (MO) and Mobile Terminated  (MT) 25
end user devices forming the two ends of the voice call. Going forward in this section, these end user devices will be 26
referred to as MO end user device and MT end user device to represent their role in the voice call. The test setup should 27
include tools which have the ability to collect traces on the elements and/or packet captures of communication between 28
the elements. This could be a built-in capability of the emulated/non-emulated network elements or an external tool. 29
Optionally, if some of the network elements are located remotely either in a cloud or on the internet, the additional 30
latency should be calculated and accounted for.  31
The O-RU, O-DU and O-CU-CP/O-CU-UP need to have the right configuration and software load. The SUT will also 32
need to be setup to run this testing in different radio conditions as outlined in Section 3.6. The end user device must be 33
configured with the right user credentials to be able to register and authenticate with the O-RAN system and the 5G 34
core. The end user devices also need to be provisioned with the user credentials to register and setup PDU session with 35
the 5G core and register with the IMS core to perform voice call using VoNR. The 5G core network and IMS core must 36
be configured to support voice service on the end user devices used for testing, which includes the ability to 37
dynamically set up QoS Flows for voice calls. The locations where the radio conditions are excellent,  good, fair and 38
poor need to be identified within the serving cell. 39

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 137

O-RAN.TIFG.E2E-Test.0-v02.00
All the elements in the network like O-RAN system, 5G core and the IMS Core need to have the ability to capture 1
traces to validate the successful execution of the test cases. The end user devices need to have the capability to capture 2
traces/packet capture to calculate the VoNR KPIs. Optionally, the network could have network taps deployed in various 3
legs of the network to get packet captures to validate successful execution of the test cas es. Finally, all these different 4
components need to have connectivity with each other – the end user device should be able to connect to O-RAN 5
system(O-RU, O-DU and O-CU-CP/O-CU-UP), O-RAN system needs to be connected to the 5G core which in turn 6
should have connectivity to the IMS Core. 7
6.5.1.3 Test Methodology/Procedure 8
Ensure the end user device, O-RAN system, 5G core and the IMS core have all been configured as outlined in Section 9
6.5.1.2. In this test scenario, both the mobile originated and mobile terminated end user devices are going to use the 10
same O-RAN system, 5G and IMS core to perform the end-to-end voice call. All traces and packet captures need to be 11
enabled for the duration of the testing to ensure all communication between network elements can be cap tured and 12
validated. 13
1. Power on the two end user devices in excellent radio condition and ensure both of devices registers with the 5G 14
core for voice services over SA by connecting over the O-RAN (O-RU, O-DU and O-CU-CP/O-CU-UP).  15
2. Once the registration is complete, the MO and MT end user devices have to establish a PDU session with the 5G 16
core. Once the PDU session has been setup, both the end user devices have to register with the IMS core to support 17
voice services. 18
3. Use the MO end user device to call the MT end user device. Validate the MT end user device is able to receive and 19
answer the call. 20
4. Continue to have two-way voice communication on the voice call for at least 5 minutes before terminating it.  21
5. Repeat the test multiple times (> 10 times) and gather results. 22
6. Repeat the above steps 1 through 5 for the good, fair and poor radio conditions.  23
 24
6.5.1.4 Test Expectation (expected results) 25
As a pre-validation, use the traces to validate a successful registration and PDU session setup by the end user devices 26
without any errors over O-RU, O-DU and O-CU-CP/O-CU-UP. Also validate both the end user devices are able to 27
register with the IMS core for voice services. This is a prerequisite before these tests can be validated.  28
Validate the end user devices are able to make a voice call between each other by dynamically setting up a 5QI -1 bearer 29
to transfer voice packets over RTP. Ensure the Call Setup Time is reasonable, the voice quality from both parties are 30
clear and audible without one-way or intermittent muting. Use the packet captures to validate there is no RTP packet 31
drops or high RTP packet jitter which could cause voice muting issues. Use the packet captures to ensure there are no 32
out-of-sequence packets which could impact customer’s voice experience.  33
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 34
is performed in a controlled environment in good radio condition without the interference of external factors which 35
could impact the KPIs, example: use of internet to connect to remote servers/hosts could add latency, jitter and 36
reliability issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the 37
testing in this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test results 38
outside the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues 39
may be due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and 40
the test has to be repeated.  41
• CSSR – Call Setup Success Rate % –> 99%. 42
• CST – Call Setup Time – < 2.5s 43
• MOS Score – > 3.5 44

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 138

O-RAN.TIFG.E2E-Test.0-v02.00
• Mute Rate % – < 1% 1
• One Way Call % - < 1% 2
• RTP Packet Loss % - < 1% 3
 4
As a part of gathering data, ensure the minimum configuration parameters (see Section 3.3) are included in the test 5
report. The following information should also be included in the test report to provide a comprehensive view of the test 6
setup. 7
UE side (real or emulated UE): 8
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second)  9
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second)  10
• Received downlink throughput (L1 and L3 PDCP layers) (average sample per second)  11
• Downlink transmission mode 12
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs  and Number of allocated/occupied slots 13
(average sample per second) 14
Table 6-12 Example Test Report for VoNR – Stationary Testing 15

Excellent
(cell centre) Good Fair Poor
(cell edge)
VoNR MO/MT VoNR MO/MT VoNR MO/MT VoNR MO/MT
Call Setup Success Rate
Call Setup Time
MOS Score
Mute Rate %
One Way Call %
RTP Packet Loss %
L1 DL Spectral efficiency [bps/Hz]
UE RSRP [dBm]
UE PDSCH SINR [dB]
MIMO rank
PDSCH MCS
DL RB number
UE CQI
UE RSRQ
UE PMI
UE RSSI
UE Buffer status
UE Packet delay
PDSCH BLER [%]
The end user voice service experience can also be impacted by some of the features available (see Section 6.5) on the 16
O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other features/functionality which 17

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 139

O-RAN.TIFG.E2E-Test.0-v02.00
could impact the end user’s voice service experience should be included in the test report to provide a comprehensive 1
view of the testing. 2
6.5.2 VoNR – Intra-Distributed Unit (O-DU) handover 3
This test scenario validates the user’s voice experience when the end user device (UE) is on a VoNR call and performs a 4
handover between two O-RUs which connect to the same O-DU (Intra-O-DU handover) 5
6.5.2.1 Test Description 6
The 5G O-RAN system has multiple sub-components like the O-RU, O-DU and the O-CU-CP/O-CU-UP. This setup 7
leads to multiple handover scenarios between the O-RAN sub-components. This particular scenario tests the end user’s 8
voice experience during the handover between two O-RUs which are connected to the same O-DU (and O-CU-CP/O-9
CU-UP), hence an Intra-O-DU handover. This handover will be agnostic to the 5G core as the handover occurs on the 10
O-RAN system. This test assesses the impact on the end user’s voice service in this handover scenario by monitoring 11
the KPIs included in Section 6.5. 12
6.5.2.2 Test Setup 13
The SUT in this test case would be a pair of O-RUs which connect to the same O-DU and O-CU-CP/O-CU-UP (refer 14
Section 4.4). This O-RAN setup is used to test the voice service during a handover. A 5G SA Core would be required to 15
support basic functionality to authenticate and register the end user device to establish a PDU session. An IMS core will 16
be required to register the end user device to support voice services on a 5G network. The 5G and IMS cores could be a 17
completely emulated, partially emulated or real non-emulated cores. We will need at least two end user devices (UE) 18
which can be a real UEs or an emulated one, and both have to support voice service usi ng 5G – Voice over NR(VoNR). 19
The end user devices will serve as Mobile Originated (MO) and Mobile Terminated (MT) end user devices forming the 20
two ends of the voice call. Going forward in this section, these end user devices will be referred to as MO end u ser 21
device and MT end user device to represent their role in the voice call. The test setup should include tools which have 22
the ability to collect traces on the elements and/or packet captures of communication between the elements. This could 23
be a built-in capability of the emulated/non-emulated network elements or an external tool. Optionally, if some of the 24
network elements are located remotely either in a cloud or on the internet, the additional latency should be calculated 25
and accounted for.   26
The pair of O-RUs (O-RU1 and O-RU2) need to be connected to the O-DU and O-CU-CP/O-CU-UP and all the 27
components need to have the right configuration and software load. The end user devices must be configured with the 28
right user credentials to be able to register and authenticate with the O-RAN system and the 5G core. The end user 29
devices also need to be provisioned with the user credentials to register and setup PDU session with the 5G core and 30
register with the IMS core to perform voice call using VoNR. The 5G core network and IMS core must be configured to 31
support voice service on the end user devices used for testing, which includes the ability to dynamically set up QoS 32
Flows for voice calls. 33
All the elements in the network like O-RAN system, 5G core and the IMS Core need to have the ability to capture 34
traces to validate the successful execution of the test cases. The end user devices need to have the capability to capture 35
traces/packet capture to calculate the VoNR KPIs. Optionally, the network could ha ve network taps deployed in various 36
legs of the network to get packet captures to validate successful execution of the test cases. Finally, all these different 37
components need to have connectivity with each other – the end user device should be able to connect to O-RAN 38
system(O-RUs connected to O-DU and O-CU-CP/O-CU-UP), O-RAN system needs to be connected to the 5G core 39
which in turn should have connectivity to the IMS Core. 40

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 140

O-RAN.TIFG.E2E-Test.0-v02.00
6.5.2.3 Test Methodology/Procedure 1
Ensure the end user devices, O-RAN system, 5G core and the IMS Core have all been configured as outlined in Section 2
6.5.2.2. In this test scenario, both the mobile originated and mobile terminated end user devices are going to use the 3
same O-RAN system, 5G and IMS core to perform the end-to-end voice call. All traces and packet captures need to be 4
enabled for the duration of the testing to ensure all communication between network elements can be captured and 5
validated. 6
1. Power on the two end user devices and ensure both of the devices registers with the 5G core for voice services over 7
SA by connecting over the O-RAN (O-RU, O-DU and O-CU-CP/O-CU-UP). Ensure both the MO & MT end user 8
devices are in the coverage area of the same O-RU – O-RU1. 9
2. Once the registration is complete, the MO and MT end user devices have to es tablish PDU session with the 5G 10
core. Once the PDU session has been setup, both the end user devices have to register with the IMS core to support 11
voice services. 12
3. Use the MO end user device to call the MT end user device. Validate the MT end user device is  able to receive and 13
answer the call. 14
4. Once the call has been setup, move the MO end user device from the coverage area of O -RU1 to coverage area of 15
O-RU2, thus causing a handover from O-RU1 to O-RU2.  16
5. Continue the two-way voice communication between MO and MT end user devices until the handover procedure is 17
complete before terminating the voice call.  18
6. Repeat the test (steps 1 through 5) multiple times (> 10 times) and gather results.  19
 20
6.5.2.4 Test Expectation (expected results) 21
As a pre-validation, use the traces to validate a successful registration and PDU session setup by the end user devices 22
without any errors over O-RU, O-DU and O-CU-CP/O-CU-UP. Also validate both the end user devices are able to 23
register with the IMS core for voice services. This is a prerequisite before these tests can be validated. 24
Validate the end user devices are able to make a voice call between each other by dynamically setting up a 5QI -1 bearer 25
to transfer voice packets over RTP. Ensure the Call Setup Time is reasonable, the voice qualit y from both parties are 26
clear and audible without one-way or intermittent muting through the duration of the call, especially during the 27
handover process. Use the packet captures to validate there is no RTP packet drops or high RTP packet jitter which 28
could cause voice muting issues. Use the packet captures to ensure there are no out -of-sequence packets which could 29
impact customer’s voice experience.  30
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 31
is performed in a controlled environment in good radio condition without the interference of external factors which 32
could impact the KPIs, example: use of internet to connect to remote servers/hosts could add latency, jitter and 33
reliability issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the 34
testing in this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test results 35
outside the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues 36
may be due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and 37
the test has to be repeated.  38
• CSSR – Call Setup Success Rate % –> 99%. 39
• CST – Call Setup Time – < 2.5s 40
• MOS Score – > 3.5 41
• Mute Rate % – < 1% 42
• One Way Call % - < 1% 43
• RTP Packet Loss % - < 1% 44
 45

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 141

O-RAN.TIFG.E2E-Test.0-v02.00
As a part of gathering data, ensure the minimum configuration parameters (see Section 3.3) are included in the test 1
report. The following information should also be included in the test report to provide a comprehensive view of the test 2
setup. 3
UE side (real or emulated UE): 4
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second)  5
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second)  6
• Received downlink throughput (L1 and L3 PDCP layers) (average sample per second)  7
• Downlink transmission mode 8
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs  and Number of allocated/occupied slots 9
(average sample per second) 10
Table 6-13 Example Test Report for VoNR – Intra-O-DU Handover 11
 VoNR MO VoNR MT
Call Setup Success Rate
Call Setup Time
MOS Score
Mute Rate %
One Way Call %
RTP Packet Loss %
L1 DL Spectral efficiency [bps/Hz]
UE RSRP [dBm]
UE PDSCH SINR [dB]
MIMO rank
PDSCH MCS
DL RB number
UE CQI
UE RSRQ
UE PMI
UE RSSI
UE Buffer status
UE Packet delay
PDSCH BLER [%]
The end user voice service experience can also be impacted by some of the features available (see Section 6.5) on the 12
O-RUs, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other features/functi onality which 13
could impact the end user’s voice service experience should be included in the test report to provide a comprehensive 14
view of the testing. 15

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 142

O-RAN.TIFG.E2E-Test.0-v02.00
6.5.3 VoNR – Intra-Central Unit (O-CU) Inter-Distributed Unit (O-DU) 1
handover  2
This test scenario validates the user’s voice experience when the end user device (UE) is on a VoNR call and performs a 3
handover between O-RUs which connect to different O-DUs, which in turn are connected to the same O-CU-CP/O-CU-4
UP (Intra-O-CU Inter-O-DU handover) 5
6.5.3.1 Test Description 6
The 5G O-RAN system has multiple sub-components like the O-RU, O-DU and the O-CU-CP/O-CU-UP. This setup 7
leads to multiple handover scenarios between the O-RAN sub-components. This particular scenario tests the end user’s 8
voice experience during the handover between two O-RUs, where the O-RUs are connected to different O-DUs which 9
in turn are connected to the same O-CU-CP/O-CU-UP, hence an Intra-O-CU Inter-O-DU handover. This handover will 10
be agnostic to the 5G core as the handover occurs on the O-RAN system. This test assesses the impact on the end user’s 11
voice service in this handover scenario by monitoring the KPIs included in Section 6.5.  12
6.5.3.2 Test Setup 13
The SUT in this test case would be a pair of O-RUs which connect to a different pair of O-DUs which in turn connect to 14
the same O-CU (refer Section 4.5). This O-RAN setup is used to test the voice service during a handover. A 5G SA 15
Core would be required to support basic functionality to authenticate and register the end user device to establish a PDU 16
session. An IMS core will be required to register the end user device to support voice services on a 5G network. The 5G 17
and IMS cores could be a completely emulated, partially emulated or real non-emulated cores. We will need at least two 18
end user devices (UE) which can be a real UEs or an emulated one, and both have to support voice service using VoNR. 19
The end user devices will serve as Mobile Originated (MO) and Mobile Terminated (MT) end user devices forming the 20
two ends of the voice call. Going forward in this section, these end user devices will be referred to as MO end user 21
device and MT end user device to represent their role in the voice call. The test setup should include tools which have 22
the ability to collect traces on the elements and/or packet captures of communication between the elements. This could 23
be a built-in capability of the emulated/non-emulated network elements or an external tool. Optionally, if some of the 24
network elements are located remotely either in a cloud or on the internet, the addi tional latency should be calculated 25
and accounted for.  26
The pair of O-RUs (O-RU1 and O-RU2) need to be connected to a pair of O-DUs (O-DU1 and O-DU2) and O-CU-27
CP/O-CU-UP. As for the O-RU to O-DU connection, ensure O-RU1 connects to O-DU1, and O-RU2 connects to O-28
DU2, and both the O-DUs connect to the same O-CU-CP/O-CU-UP. All the O-RAN components need to have the right 29
configuration and software load. The end user devices also need to be provisioned with the user credentials to register 30
and setup PDU session with the 5G core and register with the IMS core to perform voice call using VoNR. The 5G core 31
network and IMS core must be configured to support voice service on the end user devices used for testing, which 32
includes the ability to dynamically set up QoS Flows for voice calls. 33
All the elements in the network like O-RAN system, 5G core and the IMS Core need to have the ability to capture 34
traces to validate the successful execution of the test cases. The end user devices need to have the capability to capture 35
traces/packet capture to calculate the VoNR KPIs. Optionally, the network could have network taps deployed in various 36
legs of the network to get packet captures to validate successful execution of the test cases. Finally, all these different 37
components need to have connectivity with each other – the end user device should be able to connect to O-RAN 38
system(O-RUs connected to O-DU and O-CU), O-RAN system needs to be connected to the 5G core which in turn 39
should have connectivity to the IMS Core. 40

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 143

O-RAN.TIFG.E2E-Test.0-v02.00
6.5.3.3 Test Methodology/Procedure 1
Ensure the end user devices, O-RAN system, 5G core and the IMS core have all been configured as outlined in Section 2
6.5.3.2. In this test scenario, both the mobile originated and mobile terminated end user devices are going to use th e 3
same O-RAN system, 5G and IMS core to perform the end-to-end voice call. All traces and packet captures need to be 4
enabled for the duration of the testing to ensure all communication between network elements can be captured and 5
validated. 6
1. Power on the two end user devices and ensure both of devices registers with the 5G core for voice services over SA 7
by connecting over the O-RAN (O-RU, O-DU and O-CU). Ensure both the MO & MT end user devices are in the 8
coverage area of the same O-RU – O-RU1. 9
2. Once the registration is complete, the MO and MT end user devices have to establish a PDU session with the 5G 10
core. Once the PDU session has been setup, both the end user devices have to register with the IMS core to support 11
voice services. 12
3. Use the MO end user device to call the MT end user device. Validate the MT end user device is able to receive and 13
answer the call. 14
4. Once the call has been setup, move the MO end user device from the coverage area of O -RU1 to coverage area of 15
O-RU2, thus causing a handover from O-RU1 to O-RU2, and O-DU1 to O-DU2.  16
5. Continue the two-way voice communication between the end user devices until the handover procedure is complete 17
before terminating the voice call.  18
6. Repeat the test (steps 1 through 5) multiple times (> 10 times) and gather res ults. 19
 20
6.5.3.4 Test Expectation (expected results) 21
As a pre-validation, use the traces to validate a successful registration and PDU session setup by the end user devices 22
without any errors over O-RU, O-DU and O-CU. Also validate both the end user devices are able to register with the 23
IMS core for voice services. This is a prerequisite before these tests can be validated.  24
Validate the end user devices are able to make a voice call between each other by dynamically setting up a 5QI -1 bearer 25
to transfer voice packets over RTP. Ensure the Call Setup Time is reasonable, the voice quality from both parties are 26
clear and audible without one-way or intermittent muting through the duration of the call, especially during the 27
handover process. Use the packet captures to validate there is no RTP packet drops or high RTP packet jitter which 28
could cause voice muting issues. Use the packet captures to ensure there are no out -of-sequence packets which could 29
impact customer’s voice experience.  30
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 31
is performed in a controlled environment in good radio condition without the interference of external factors which 32
could impact the KPIs, example: use of internet to connect to remote servers/hosts could add latency, jitter and 33
reliability issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the 34
testing in this scenario, a KPI outcome outside the range does not necessarily point t o a failed test case. Any test results 35
outside the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues 36
may be due to the variables outside the SUT. If the root cause is not in the SUT, then the iss ues have to be resolved and 37
the test has to be repeated.  38
• CSSR – Call Setup Success Rate % –> 99%. 39
• CST – Call Setup Time – < 2.5s 40
• MOS Score – > 3.5 41
• Mute Rate % – < 1% 42
• One Way Call % - < 1% 43
• RTP Packet Loss % - < 1% 44
 45

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 144

O-RAN.TIFG.E2E-Test.0-v02.00
As a part of gathering data, ensure the minimum configuration parameters (see Section 3.3) are included in the test 1
report. The following information should also be included in the test report to provide a comprehensive view of the test 2
setup. 3
UE side (real or emulated UE): 4
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second)  5
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second)  6
• Downlink transmission mode 7
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 8
(average sample per second) 9
Table 6-14 Example Test Report for VoNR – Inter-O-DU Intra-O-CU Handover 10
 VoNR MO VoNR MT
Call Setup Success Rate
Call Setup Time
MOS Score
Mute Rate %
One Way Call %
RTP Packet Loss %
L1 DL Spectral efficiency [bps/Hz]
UE RSRP [dBm]
UE PDSCH SINR [dB]
MIMO rank
PDSCH MCS
DL RB number
UE CQI
UE RSRQ
UE PMI
UE RSSI
UE Buffer status
UE Packet delay
PDSCH BLER [%]
The end user voice service experience can also be impacted by some of the features available (see Section 6.5) on the 11
O-RUs, O-DU and O-CU. The details of these features, along with any other features/functionality whi ch could impact 12
the end user’s voice service experience should be included in the test report to provide a comprehensive view of the 13
testing. 14
6.5.4 VoNR – Inter-Central Unit (O-CU) handover  15
This test scenario validates the user’s voice experience when the end u ser device (UE) is on a VoNR call and performs a 16
handover between O-RUs which connect to different O-DUs and different O-CUs (Inter-O-CU Inter-O-DU handover) 17

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 145

O-RAN.TIFG.E2E-Test.0-v02.00
6.5.4.1 Test Description 1
The 5G O-RAN system has multiple sub-components like the O-RU, O-DU and the O-CU. This setup leads to multiple 2
handover scenarios between the O-RAN sub-components. This particular scenario tests the end user’s voice experience 3
during the handover between two O-RUs, where the O-RUs are connected to different O-DUs which in turn are 4
connected to different O-CUs, hence an Inter-O-CU Inter-O-DU handover. This handover occurs on the O-RAN system 5
and the 5G core is aware of the handover as it needs to send data to a new O -CU as a part of the handover process. This 6
test assesses the impact on the end user’s voice service in this handover scenario by monitoring the KPIs included in 7
Section 6.5. 8
6.5.4.2 Test Setup 9
The SUT in this test case would be a pair of O-RAN subsystems, a set of O-RU, O-DU and O-CU which interconnects 10
with another set of O-RU, O-DU and O-CU (refer Section 4.6). This O-RAN setup is used to test the voice service 11
during a handover. A 5G SA Core would be required to support basic functionality to authenticate and register the end 12
user device to establish a PDU session. An IMS core will be required to register the end user device to support voice 13
services on a 5G network. The 5G and IMS cores could be a completely emulated, partially emulated or real non-14
emulated cores. We will need at least two end user devices (UE) which can be a real UEs or an emulated one, and both 15
have to support voice service using VoNR. The end user devices will serve as Mobile Originated (MO) and Mobile 16
Terminated (MT) end user devices forming the two ends of the voice call. Going forward in this section, the se end user 17
devices will be referred to as MO end user device and MT end user device to represent their role in the voice call. The 18
test setup should include tools which have the ability to collect traces on the elements and/or packet captures of 19
communication between the elements. This could be a built-in capability of the emulated/non-emulated network 20
elements or an external tool. Optionally, if some of the network elements are located remotely either in a cloud or on the 21
internet, the additional latency should be calculated and accounted for.  22
The pair of O-RAN (O-RAN1 and O-RAN2) subsystems will be connected – O-RU1 will be connected to O-DU1, 23
which in turn will be connected to O-CU1 and similarly O-RU2 will be connected to O-DU2, which in turn will be 24
connected to O-CU2. The O-CU1 and O-CU2 nodes will be connected to each other and the 5G core. All the O-RAN 25
components need to have the right configuration and software load. The end user devices must be configured with the 26
right user credentials to be able to register and authenticate with the O-RAN system and the 5G core. The end user 27
devices also need to be provisioned with the user credentials to register and setup PDU session with the 5G core and 28
register with the IMS core to perform voice call using VoNR. The 5G core network and IMS core must be configured to 29
support voice service on the end user devices used for testing, which includes the ability to dynamically set up QoS 30
Flows for voice calls. 31
All the elements in the network like O-RAN system, 5G core and the IMS Core need to have the ability to capture 32
traces to validate the successful execution of the test cases. The end user devices need to have the capability to capture 33
traces/packet capture to calculate the VoNR KPIs. Optionally, the network cou ld have network taps deployed in various 34
legs of the network to get packet captures to validate successful execution of the test cases. Finally, all these different 35
components need to have connectivity with each other – the end user device should be able to connect to O-RAN 36
system(O-RUs connected to O-DUs which are connected to the O-CUs), O-RAN system needs to be connected to the 37
5G core which in turn should have connectivity to the IMS Core. 38
6.5.4.3 Test Methodology/Procedure 39
Ensure the end user devices, O-RAN system, 5G core and the IMS Core have all been configured as outlined in Section 40
6.5.4.2. In this test scenario, both the mobile originated and mobile terminated end user devices are going to use the 41
same O-RAN system, 5G and IMS core to perform the end-to-end voice call. All traces and packet captures need to be 42
enabled for the duration of the testing to ensure all communication between network elements can be captured and 43
validated. 44

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 146

O-RAN.TIFG.E2E-Test.0-v02.00
1. Power on the two end user devices and ensure both of devices registers with the 5G core for voice services over SA 1
by connecting over the O-RAN1 system (O-RU1, O-DU1 and O-CU1). Ensure both the MO & MT end user 2
devices are in the coverage area of the same O-RU – O-RU1. 3
2. Once the registration is complete, the MO and MT end user devices have to establish a PDU session with the 5G 4
core. Once the PDU session has been setup, both the end user devices have to register with the IMS core to support 5
voice services. 6
3. Use the MO end user device to call the MT end user device. Validate the MT end user device is able to receive and 7
answer the call. 8
4. Once the call has been setup, move the MO end user device from the coverage area of O -RU1 to coverage area of 9
O-RU2, thus causing a handover from O-RAN1 to O-RAN2 subsystem - O-RU1 to O-RU2, O-DU1 to O-DU2 and 10
O-CU1 to O-CU2.  11
5. Continue the two-way voice communication between MO and MT end user devices until the handover procedure is 12
complete before terminating the voice call.  13
6. Repeat the test (steps 1 through 5) multiple times (> 10 times) and gather results. 14
 15
6.5.4.4 Test Expectation (expected results) 16
As a pre-validation, use the traces to validate a successful registration and PDU session setup by the end user devices 17
without any errors over O-RU, O-DU and O-CU. Also validate both the end user devices are able to register with the 18
IMS core for voice services. This is a prerequisite before these tests can be validated.  19
Validate the end user devices are able to make a voice call between each other by dynamically setting up a 5QI-1 bearer 20
to transfer voice packets over RTP. Ensure the Call Setup Time is reasonable, the voice quality from both parties are 21
clear and audible without one-way or intermittent muting through the duration of the call, especially during the 22
handover process. Use the packet captures to validate there is no RTP packet drops or high RTP packet jitter which 23
could cause voice muting issues. Use the packet captures to ensure there are no out -of-sequence packets which could 24
impact customer’s voice experience.  25
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 26
is performed in a controlled environment in good radio condition without the interference of external factors which 27
could impact the KPIs, example: use of internet to connect to remote servers/hosts could add latency, jitter and 28
reliability issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the 29
testing in this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test results 30
outside the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues 31
may be due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and 32
the test has to be repeated.  33
• CSSR – Call Setup Success Rate % –> 99%. 34
• CST – Call Setup Time – < 2.5s 35
• MOS Score – > 3.5 36
• Mute Rate % – < 1% 37
• One Way Call % - < 1% 38
• RTP Packet Loss % - < 1% 39
 40
As a part of gathering data, ensure the minimum configuration parameters (see Section 3.3) are included in the test 41
report. The following information should also be included in the test report to provide a comprehensive view of the test 42
setup. 43
UE side (real or emulated UE): 44
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second)  45

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 147

O-RAN.TIFG.E2E-Test.0-v02.00
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second)  1
• Received downlink throughput (L1 and L3 PDCP layers) (average sample per second) 2
• Downlink transmission mode 3
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 4
(average sample per second) 5
Table 6-15 Example Test Report for VoNR – Inter-O-CU Handover 6
 VoNR MO VoNR MT
Call Setup Success Rate
Call Setup Time
MOS Score
Mute Rate %
One Way Call %
RTP Packet Loss %
L1 DL Spectral efficiency [bps/Hz]
UE RSRP [dBm]
UE PDSCH SINR [dB]
MIMO rank
PDSCH MCS
DL RB number
UE CQI
UE RSRQ
UE PMI
UE RSSI
UE Buffer status
UE Packet delay
PDSCH BLER [%]
The end user voice service experience can also be impacted by some of the features available (see Section 6.5) on the 7
O-RUs, O-DU and O-CU. The details of these features, along with any other features/functionality which could impact 8
the end user’s voice service experience should be included in the test report to provide a comprehensive view of the 9
testing. 10
6.6 Video Service – Video over LTE (ViLTE) 11
Voice service has been one of the basic services provided on the mobile device since launch. With increasing LTE 12
penetration and the availability of higher speeds, video calls are slowly replacing voice calls. With the launch of 5G 13
which promises much higher throughput, the shift towards using video calls is only going to get faster. This section of 14
the document validates the user’s video calling experience in different conditions on the LTE network.  This section of 15
the document only applies to the video calling service provided by the telecom service provider, which uses the telecom 16
operator’s IMS core to establish dedicated bearer to provide superior video calling experience. The KPIs which will be 17
monitored to assess the voice service are included below 18
• CSSR – Call Setup Success Rate % – Total number of calls which were successful by the total number of 19
calls made as a percentage. 20

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 148

O-RAN.TIFG.E2E-Test.0-v02.00
• CST – Call Setup Time – Time taken from the initial SIP INVITE to when the SIP 180 Ringing is received 1
in seconds. 2
• MOS Score – Mean Opinion Score for the video call. This needs to be measured on both ends of the video 3
call – mobile originated and mobile terminated. 4
• Mute Rate % – This is the percentage of video calls which were muted or video freezes in both directions 5
(calls with RTP loss of > 3-4s in both directions are considered muted call). This needs to be measured on 6
both ends of the video call – mobile originated and mobile terminated and counted only once per call. 7
• One Way Calls % – This is the percentage of video calls which were muted, or video is not transmitted in 8
any one direction only (video calls with RTP loss of > 3-4s in one direction only are considered one-way 9
calls). This needs to be monitored on both ends of the Voice call – mobile originated and mobile terminated 10
and counted only once per call. 11
• RTP Packet Loss % - Number of RTP packets which were dropped/lost in uplink/downlink direction as a 12
percentage of total packets. This needs to be measured on both ends of the Voice call – mobile originated 13
and mobile terminated. 14
 15
Along with the monitoring and validation of these services using user experience KPIs, the O-RAN systems also need 16
to be monitored. The end user service experience can also be impacted by some of the features available on the O -RAN 17
system. Some of these features have been included below. As a part of the video calling services testing, details of these 18
features need to be included in the test report to get a comprehensive view of the setup used for testing. If additional 19
features or functionalities have been enabled during this testing that impact the end user voice experience, those need be 20
included in the test report as well.  21
• RLC in Unacknowledged Mode 22
• Robust Header Compression 23
• DRX 24
• Dynamic GBR Admission Control 25
• TTI Bundling 26
• VoLTE Inactivity Timer 27
• Frequency Hopping 28
• Multi-Target RRC Connection Re-Establishment 29
• VoLTE HARQ 30
• Coordinated Multi Point (DL and UL) 31
• VoLTE Quality Enhancement 32
• Packet Loss Detection 33
• Voice Codec Aware scheduler 34
• NR to LTE PS Redirection/Cell Reselection/Handover 35
• LTE to NR PS Redirection/Cell Reselection/Handover 36
 37
6.6.1 ViLTE Stationary Test 38
This test scenario validates the user’s video calling experience when the end user device (UE) is in LTE coverage and 39
performs a video call. 40

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 149

O-RAN.TIFG.E2E-Test.0-v02.00
6.6.1.1 Test Description 1
Telecom service providers are providing video calling service to their customers and it has been gaining popularity 2
replacing voice calls. Video over LTE (ViLTE) uses the same mechanism as VoLTE, i.e. IP packets to send and receive 3
video & audio packets, with the IP packets being transferred over LTE. Just like VoLTE, I MS is used to setup control 4
and data plane needed for ViLTE communication. As two-way video service is latency sensitive, the EPC core interacts 5
with the IMS core to setup different bearers for Video traffic – QCI-5 for ViLTE control plane and QCI-2 for ViLTE 6
data plane. This test case is applicable when UE is connected over an NSA network. This section tests ViLTE user 7
experience on an O-RAN system. 8
6.6.1.2 Test Setup 9
The SUT in this test case would be a O-eNB which would the Master eNB and a Secondary gNB. As most of the 10
current NSA deployment use the 4G eNB to provide video calling services, the use of a secondary gNB is optional. We 11
will however include the Secondary gNB in this test scenario, but it is only applicable if the gNB plays a role in 12
establishing the control plane or data plane for a video call. The O-eNB, gNB and the components within these have to 13
comply with the O-RAN specifications. The O-RAN setup should support the ability to perform this testing in different 14
radio conditions as defined in Section 3.6. A 4G core will be required to support the basic functionality to authenticate 15
and register an end user device in order to setup a PDN connection. An IMS core  will be required to register the end 16
user device to support video calling services on a 4G network. The 4G and IMS cores could be a completely emulated, 17
partially emulated or real non-emulated cores. We will need at least two end user devices (UE) which c an be a real UEs 18
or an emulated one, and both have to support video calling service using ViLTE. The end user devices will serve as 19
Mobile Originated (MO) and Mobile Terminated (MT) end user devices forming the two ends of the video call  Going 20
forward in this section, these end user devices will be referred to as MO end user device and MT end user devices to 21
represent the role they plan in the video call. The test setup should include tools which have the ability to collect traces 22
on the elements and/or packet captures of communication between the elements. This could be a built -in capability of 23
the emulated/non-emulated network elements or an external tool. Optionally, if some of the network elements are 24
located remotely either in a cloud or on the internet, the additional latency should be calculated and accounted for.  25
The Master eNB, Secondary gNB and their components (O-eNB, O-RU, O-DU and O-CU) need to have the right 26
configuration and software load. The end user device must be configured with the right user credentials to be able to 27
register and authenticate with the O-RAN system and the 4G core. The end user devices also need to be provisioned 28
with the user credentials to attach to the 4G core and register with the IMS core to perform video call using V iLTE. The 29
4G core network and IMS core must be configured to support end user devices used for testing. This includes 30
supporting registration, authentication and PDN connection establishment for these end user devices. The also includes 31
provisioning the IMS core to support registration of the end user devices to make video calls over ViLTE. The locations 32
where the radio conditions are excellent, good, fair and poor need to be identified within the serving cell.  33
All the elements in the network like O-RAN system, 4G core and the IMS Core need to have the ability to capture 34
traces to validate the successful execution of the test cases. The end user devices need to have the capability to capture 35
traces/packet capture to calculate the VoLTE KPIs. Optionally, the network could have network taps deployed in 36
various legs of the network to get packet captures to validate successful execution of the test cases. Finally, all these 37
different components need to have connectivity with each other – the end user device should be able to connect to O-38
RAN system(O-eNBs and gNBs), O-RAN system needs to be connected to the 4G core which in turn should have 39
connectivity to the IMS Core. 40
6.6.1.3 Test Methodology/Procedure 41
Ensure the end user devices, O-RAN system, 4G core and the IMS Core have all been configured as outlined in Section 42
6.6.1.2. In this test scenario, both the mobile originated and mobile terminated end user devices are going to use the 43
same O-RAN (i.e. same O-eNB), 4G and IMS core to perform the end-to-end video call. All traces and packet captures 44

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 150

O-RAN.TIFG.E2E-Test.0-v02.00
need to be enabled for the duration of the testing to ensure all communication between network elements can be 1
captured and validated. 2
1. Power on the two end user devices in excellent radio condition and ensure both of the end user devices connect to 3
the 4G core over the Master O-eNB and optionally secondary gNB.  4
2. Ensure both the MO and MT end user devices can establish a PDN connection with the 4G core. Once the PDN 5
connection has been setup, both the end user devices have to register with the IMS core to support video calling 6
services. 7
3. Use the MO end user device to video call the MT end user device. Validate the MT end user device is able to 8
receive and answer the call. 9
4. Continue to have two-way voice and video communication on the video call for at least 5 minutes before 10
terminating it.  11
5. Repeat the test multiple times (>10 times) and gather results. 12
6. Repeat the above steps 1 through 5 for the MO and MT end user devices in good, fair and poor radio conditions.  13
 14
6.6.1.4 Test Expectation (expected results) 15
As a pre-validation, use the traces to validate a successful PDN connection setup by the end user devices without any 16
errors using the Master eNB and optionally the secondary gNB. Also validate both the end user devices are able to 17
register with the IMS core for voice services. This is a prerequisite before these tests can be validated.  18
Validate the end user devices are able to make a video call between each other by dynamically setting up a QCI -2 19
bearer to transfer voice and video packets over RTP. Ensure the Call Setup Time is reasonable, the video and voice 20
quality from both parties are clear and audible without one-way or intermittent muting or video freezing. Use the packet 21
captures to validate there is no RTP packet drops or high RTP packet jitter which could cause voice muting, video 22
freezing or video lag issues. Use the packet captures to ensure there are no out -of-sequence packets which could impact 23
customer’s video calling experience.  24
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 25
is performed in a controlled environment in good radio condition without the interference of external factors which 26
could impact the KPIs, example: use of internet to connect to remote network nodes could add latency, jitter and packet 27
loss issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the testing in 28
this scenario, a KPI outcome outside the range does not necessarily point to a f ailed test case. Any test results outside 29
the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues may be 30
due to the variables outside the SUT. If the root cause is not in the SUT, then the issues h ave to be resolved and the test 31
has to be repeated.  32
• CSSR – Call Setup Success Rate % –> 99%. 33
• CST – Call Setup Time – < 2.5s 34
• MOS Score – > 3.5 35
• Mute Rate % – < 1% 36
• One Way Call % - < 1% 37
• RTP Packet Loss % - < 1% 38
 39
These end user video calling KPI values included in Section 6.6 need to be included in the test report along with the 40
minimum configuration parameters included in Section 3.3. The following information should also be included in the 41
test report for the testing performed in different radio conditions to provide a comprehensive view of the test setup.  42
End user device side (real or emulated UE): 43
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second)  44

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 151

O-RAN.TIFG.E2E-Test.0-v02.00
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second)  1
• Received downlink throughput (L1 and L3 PDCP layers) (average sample pe r second) 2
• Downlink transmission mode 3
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 4
(average sample per second) 5
The table below gives an example of the test report considering the mean and standa rd deviation of all test results that 6
have been captured.  7
Table 6-16 Example Test Report for Video over LTE Testing – Stationary Test 8

Excellent
(cell centre) Good Fair Poor
(cell edge)
ViLTE MO/MT ViLTE MO/MT ViLTE MO/MT ViLTE MO/MT
Call Setup Success Rate
Call Setup time
MOS Score
Mute Rate
One Way Call
RTP Packet Loss
L1 DL Spectral efficiency [bps/Hz]
UE RSRP [dBm]
UE PDSCH SINR [dB]
MIMO rank
PDSCH MCS
DL RB number
UE CQI
UE RSRQ
UE PMI
UE RSSI
UE Buffer status
UE Packet delay
PDSCH BLER [%]
The end user ViLTE experience can also be impacted by some of the features available (see Section 6.3) on the O-eNB, 9
O-RU, O-DU and O-CU. The details of these features, along with any other features/functionality which could impact 10
the end user’s video calling service experience should be included in the test report to provide a comprehensive view of 11
the testing. 12
6.6.2 ViLTE Handover Test 13
This test section is for FFS. 14

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 152

O-RAN.TIFG.E2E-Test.0-v02.00
6.6.3 ViLTE - LTE to NR handover test 1
This test scenario validates the user’s video calling experience when the UE is in video call on LTE and performs a 2
handover from LTE network to a 5G network and vice versa. This test scenario is applicable for a 5G SA deployment. 3
6.6.3.1 Test Description 4
Voice service is one of the basic services provided on the telecommunication network. Video service on the 4G network 5
was provided using ViLTE. Similarly, video service on the 5G network is provided using packet switch technology 6
called Video over New Radio. As 5G network is being deployed by a telecommunication service provider, the service 7
provider will need to support 4G and 5G network, and thus support Video over LTE, Video  over NR and the handover 8
between the two video calling services. This scenario tests the end user’s video calling experience when the end user 9
device performs a handover from video over LTE to Video over NR and vice versa.  10
6.6.3.2 Test Setup 11
The SUT in this test case would be an O-eNB along with a gNB (O-RU, O-DU and O-CU). We would also need an 12
interworking 4G-5G core (referred to as 4G-5G core going forward) which supports a combined anchor point for 4G 13
and 5G, i.e. SMF+PGW-C and UPF+PGW-U. The eNB connects to a 4G-5G core over the 4G interfaces like S1 to 14
provide 4G LTE service and the O-CU, O-DU and O-RU would connect to a 4G-5G core over the 5G interfaces like N2 15
and N3 to provide 5G SA service. The O-eNB, gNB and the components within these have to comply with the O-RAN 16
specifications. The 4G-5G core will be required to support the basic functionality to authenticate and register an end 17
user device in order to setup a PDN connection/PDU session. An IMS core will be required to register the end user 18
device to support video calling services on a 4G and 5G network. The 4G and 5G core have to interwork using the N26 19
interface between the MME and AMF to support seamless handover. We are recommending the use of N26 interface to 20
ensure better customer experience when the end user device performs a handover between 4G and 5G. The 4G-5G and 21
IMS cores could be a completely emulated, partially emulated or real non-emulated cores. We will need at least two end 22
user devices (UE) which can be a real UEs or an emulated one, an d both have to support video calling service using 23
ViLTE, Video over NR and the capability to handover from ViLTE to Video over NR and vice versa. The end user 24
devices will serve as Mobile Originated (MO) and Mobile Terminated (MT) end user devices forming  the two ends of 25
the video call. Going forward in this section, these end user devices will be referred to as MO end user device and MT 26
end user devices to represent their role in the video call. The test setup should include tools which have the ability to 27
collect traces on the elements and/or packet captures of communication between the elements. This could be a built -in 28
capability of the emulated/non-emulated network elements or an external tool. Optionally, if some of the network 29
elements are located remotely either in a cloud or on the internet, the additional latency should be calculated and 30
accounted for. 31
The O-eNB, gNB and their components (O-RU, O-DU and O-CU) need to have the right configuration and software 32
load. The end user devices must be configured with the right user credentials to be able to register and authenticate with 33
the O-RAN system and the 4G-5G core. The end user devices also need to be provisioned with the user credentials to 34
support attach procedure to the 4G-5G core, registering to the IMS core and performing a video call over LTE and NR. 35
This includes supporting registration, authentication and PDN connection/PDU Session establishment for these end user 36
devices. This also includes provisioning the IMS core to support registration o f the end user devices to make video calls 37
over ViLTE and Video over NR, which includes dynamically setting up dedicated bearers/QoS Flows for video calls.  38
All the elements in the network like O-RAN system, 4G-5G core and the IMS Core need to have the ability to capture 39
traces to validate the successful execution of the test cases. The end user devices need to have the capability to capture 40
traces/packet capture to calculate the ViLTE and Video over NR KPIs. Optionally, the network could have network taps 41
deployed in various legs of the network to get packet captures to validate successful execution of the test cases. Finally, 42
all these different components need to have connectivity with each other – the end user device should be able to connect 43

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 153

O-RAN.TIFG.E2E-Test.0-v02.00
to O-RAN system(O-eNBs and gNBs), O-RAN system needs to be connected to the 4G-5G core which in turn should 1
have connectivity to the IMS Core. 2
6.6.3.3 Test Methodology/Procedure 3
Ensure the end user devices, O-RAN system, 4G-5G core and the IMS Core have all been configured as outlined in 4
Section 6.6.3.2. In this test scenario, both the mobile originated and mobile terminated end user devices are going to use 5
the O-eNB and gNB to connect to the same 4G-5G and IMS core to perform the end-to-end video call. All traces and 6
packet captures need to be enabled for the duration of the testing to ensure all communication between network 7
elements can be captured and validated. 8
The below section gives the steps to perform a video over LTE to video over NR handover, followed by video over NR 9
to video over LTE handover 10
1. Power on the two end user devices and ensure both of the end user devices can connect to the 4G-5G core over the 11
O-eNB.  12
2. Ensure both the MO and MT end user devices can establish a PDN connection with the 4G-5G core. Once the PDN 13
connection has been established, both the end user devices have to register with the IMS core to support video 14
calling services. 15
3. Use the MO end user device to video call the MT end user device. Validate the MT end user device is able to 16
receive and answer the video call. 17
4. Once the two-way call has been setup and communication is going back and forth between the two end user 18
devices, move the MO device to the O-RU coverage of the gNB, thus forcing a handover from 4G to 5G, hence 19
forcing a ViLTE to Video over NR handover. Continue two-way voice and video communication through the 20
handover process and terminate the call once the handover process is complete. Measure the video KPIs included in 21
Section 6.6.  22
5. At this point in time, the MO end user device is in 5G coverage and registered to the 4G-5G core. The MT end user 23
device is in 4G coverage and registered to the 4G-5G core. Both end user devices should still be registered to the 24
IMS core. 25
6. Use the MO end user device to video call the MT end user device. Validate the MT end user device is able to 26
receive and answer the call. 27
7. Once the two-way call has been setup and communication is going back and forth between the two end user 28
devices, move the MO device to the O-eNB coverage, thus forcing a handover from 5G to 4G, hence forcing a 29
Video over NR to ViLTE handover. Continue the two-way voice and video communication through the handover 30
process and terminate the call once the handover process is complete. Measure the video KPIs included in Section 31
6.6.  32
8. Repeat the test multiple times (> 10 times) and gather results. 33
 34
6.6.3.4 Test Expectation (expected results) 35
As a pre-validation, use the traces to validate a successful registration and PDN connection setup by the end user 36
devices without any errors using the O-eNB when in 4G coverage. Similarly, use traces to validate successful 37
registration and PDU session setup by the end user device without any errors using O -RU, O-DU and O-CU when in 5G 38
coverage. Also validate both the end user devices are able to register with the IMS core for voice services. This is a 39
prerequisite before these tests can be validated. 40
Validate the end user devices are able to make a video call between each other by dynamically setting up a QCI -2/5QI-2 41
bearer to transfer voice and video packets over RTP. Ensure the Call Setup Time is reasonable, the video and voice 42
quality from both parties are clear and audible without one-way or intermittent muting or video freezing. Ensure the 43
voice and video quality is not impacted during the handover process – ViLTE to Video over NR and Video over NR to 44
ViLTE. Use the packet captures to validate there is no RTP packet drops or high RTP packet jitter which could cause 45

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 154

O-RAN.TIFG.E2E-Test.0-v02.00
voice muting, video freezing or video lag issues. Use the packet captures to ensure ther e are no out-of-sequence packets 1
which could impact customer’s video calling experience.  2
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 3
is performed in a controlled environment in good radio condition without the interference of external factors which 4
could impact the KPIs, example: use of internet to connect to remote network nodes could add latency, jitter and packet 5
loss issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the testing in 6
this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test res ults outside 7
the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues may be 8
due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and the te st 9
has to be repeated.  10
• CSSR – Call Setup Success Rate % –> 99%. 11
• CST – Call Setup Time – < 2.5s 12
• MOS Score – > 3.5 13
• Mute Rate % – < 1% 14
• One Way Call % - < 1% 15
• RTP Packet Loss % - < 1% 16
 17
These end user video calling KPI values included in Section 6.6 need to be included in the test report along with the 18
minimum configuration parameters included in Section 3.3. The following information should also be included in the 19
test report for the testing performed in different radio conditions to provide a comprehensive view of the test setup.  20
End user device side (real or emulated UE): 21
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second)  22
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second)  23
• Downlink transmission mode 24
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 25
(average sample per second) 26
The table below gives an example of the test report considering the mean and standard deviation of all test results that 27
have been captured.  28
Table 6-17 Example Test Report for Video over LTE Testing – Stationary Test 29

Excellent
(cell centre) Good Fair Poor
(cell edge)
ViLTE MO/MT ViLTE MO/MT ViLTE MO/MT ViLTE MO/MT
Call Setup Success Rate
Call Setup time
MOS Score
Mute Rate
One Way Call
RTP Packet Loss
L1 DL Spectral efficiency [bps/Hz]
UE RSRP [dBm]
UE PDSCH SINR [dB]
MIMO rank

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 155

O-RAN.TIFG.E2E-Test.0-v02.00
PDSCH MCS
DL RB number
UE CQI
UE RSRQ
UE PMI
UE RSSI
UE Buffer status
UE Packet delay
PDSCH BLER [%]
The end user ViLTE experience can also be impacted by some of the features available (see Section 6.3) on the O -eNB, 1
O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other features/func tionality which 2
could impact the end user’s video calling service experience should be included in the test report to provide a 3
comprehensive view of the testing. 4
6.7 Video Service – EPS Fallback 5
Video service is quickly replacing voice service and turning into one of the basic services offered by telecommunication 6
service providers. Upgrade of telecom network occurs in phases, and this is no different for 5G. The telecom network 7
may not be able to support video calling service on 5G during this phase for multiple reasons – the 5G network may not 8
be deployed nationwide, or 5G network may not be tuned to support voice/video service or the devices may not be able 9
to support video/voice service on 5G. EPS fallback is the method used to support voice/video services during this 10
transition phase, where voice/video services are continued to be supported on the legacy LTE network using video over 11
LTE (ViLTE). This section of the document only applies to the video calling service provided by the telecom service 12
provider, which uses the telecom operator’s IMS core to establish dedicated bearer to provide superior video calling 13
experience. The KPIs which will be monitored to assess the video calling service are included below 14
• CSSR – Call Setup Success Rate % – Total number of calls which were successful by the total number of 15
calls made as a percentage. 16
• CST – Call Setup Time – Time taken from the initial SIP INVITE to when the SIP 180 Ringing is received 17
in seconds. 18
• MOS Score – Mean Opinion Score for the video call. This needs to be measured on both ends of the video 19
call – mobile originated and mobile terminated. 20
• Mute Rate % – This is the percentage of video calls which were muted or video freezes in both directi ons 21
(calls with RTP loss of > 3-4s in both directions are considered muted call). This needs to be measured on 22
both ends of the video call – mobile originated and mobile terminated and counted only once per call. 23
• One Way Calls % – This is the percentage of video calls which were muted, or video is not transmitted in 24
any one direction only (video calls with RTP loss of > 3-4s in one direction only are considered one-way 25
calls). This needs to be monitored on both ends of the Voice call – mobile originated and mobile terminated 26
and counted only once per call. 27
• RTP Packet Loss % - Number of RTP packets which were dropped/lost in uplink/downlink direction as a 28
percentage of total packets. This needs to be measured on both ends of the Voice call – mobile originated 29
and mobile terminated. 30
 31
Along with the monitoring and validation of these services using user experience KPIs, the O -RAN systems also need 32
to be monitored. The end user service experience can also be impacted by some of the features available on the O -RAN 33
system. Some of these features have been included below. As a part of the voice services testing, details of these 34
features need to be included in the test report to get a comprehensive view of the setup used for testing. If additional 35

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 156

O-RAN.TIFG.E2E-Test.0-v02.00
features or functionalities have been enabled during this testing that impact the end user voice experience, those need be 1
included in the test report as well.  2
• EPS Fallback for IMS Voice 3
• NR to EPS Mobility 4
• RLC in Unacknowledged Mode 5
• Robust Header Compression 6
• DRX 7
• Dynamic GBR Admission Control 8
• TTI Bundling 9
• VoLTE Inactivity Timer 10
• Frequency Hopping 11
• Multi-Target RRC Connection Re-Establishment 12
• VoLTE HARQ 13
• Coordinated Multi Point (DL and UL) 14
• VoLTE Quality Enhancement 15
• Packet Loss Detection 16
• Voice Codec Aware scheduler 17
 18
6.7.1 Video Service – EPS Fallback testing 19
This scenario tests the video calling service when an end user device (UE) is in 5G SA coverage and performs EPS 20
fallback to 4G to make or receive a video call. 21
6.7.1.1 Test Description 22
This section tests the video service on a 5G SA network when it uses EPS fallback mechanism to fallback to the LTE 23
network to use video over LTE to support video calling service. This testing only applies to 5G SA deployment and in 24
this scenario the UE is connected to the 5G SA Core and registered with the IMS core for video calling service. As 25
video calling service is not supported on 5G yet, the UE is forced to fallback to 4G when it makes/receives a video call. 26
The EPS fallback does increase the Call Setup Time due to the time needed to fallback befo re making/receiving the 27
video call.  28
There are two mechanism in which EPS fallback is supported on the 5G core, and the methodology used impacts the 29
time taken to perform the fallback to LTE, and hence impacts the Call Setup Time. The two mechanisms that c ould be 30
used to perform EPS fallback is included below. These two mechanisms do change the test setup but does not change 31
the testing procedure. 32
• With N26 interface – The AMF in the 5G core communicates with the MME in the 4G over the N26 interface. 33
In this scenario the UE performs a handover from the 5G network to the 4G network. 34
• Without N26 interface – The AMF in the 5G core does not communicate directly with the 4G core, instead 35
uses the UDM/HSS to store and transfer relevant session information to the 4G  core. In this scenario the UE 36
performs a Release with Redirect from the 5G network to the 4G network.  37
 38

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 157

O-RAN.TIFG.E2E-Test.0-v02.00
6.7.1.2 Test Setup 1
The SUT in this test case would be an O-eNB along with a gNB (O-RU, O-DU and O-CU-CP/O-CU-UP). We would 2
also need an interworking 4G-5G core (referred to as 4G-5G core going forward) which supports a combined anchor 3
point, i.e. SMF+PGW-C and UPF+PGW-U. The O-eNB connects to a 4G-5G core over the 4G interfaces like S1 to 4
provide 4G LTE service and the gNB (O-RU, O-DU and O-CU-CP/O-CU-UP) would connect to the 4G-5G core over 5
the 5G interfaces like N2 and N3 to provide 5G SA service. The O-eNB, gNB and the components within these have to 6
comply with the O-RAN specifications and support EPS fallback. The 4G-5G core will be required to support the basic 7
functionality to authenticate and register an end user device in order to setup a PDN connection/PDU session. The IMS 8
core will be required to register the end user device and needs to be integrated with the 4G -5G core to support video 9
calling services over LTE (ViLTE) and EPS fallback. The 4G-5G core has to interwork either using the N26 interface 10
or without using N26 interface depending on the desired core network configuration. The existence of the N26 interface 11
does reduce the EPS fallback time, hence reduce Call Setup Time and provide better end user experience. The 4G -5G 12
and IMS cores could be a completely emulated, partially emulated or real non-emulated cores. We will need at least two 13
end user devices (UE) which can be a real UEs or an emulated one, and both have to support video calling service using 14
ViLTE and EPS fallback procedure. The end user devices will serve as Mobile Originated (MO) and Mobile 15
Terminated (MT) end user devices forming the two ends of the video call. For the sake of clarit y, we will address these 16
end user devices as UE-1 and UE-2 in this section. The test setup should include tools which have the ability to collect 17
traces on the elements and/or packet captures of communication between the elements. This could be a built -in 18
capability of the emulated/non-emulated network elements or an external tool. Optionally, if some of the network 19
elements are located remotely either in a cloud or on the internet, the additional latency should be calculated and 20
accounted for.  21
The O-eNB, gNB and their components (O-RU, O-DU and O-CU-CP/O-CU-UP) need to have the right configuration 22
and software load. The end user device must be configured with the right user credentials to be able to register and 23
authenticate with the O-RAN system and the 4G-5G core. The end user devices also need to be provisioned with the 24
user credentials to attach to the 4G-5G core and register with the IMS core to perform video call using ViLTE and EPS 25
fallback. The 4G-5G core network and IMS core must be configured to support video calling service on the end user 26
device used for testing, which includes dynamically setting up dedicated bearers for voice calls.  27
All the elements in the network like O-RAN system, 4G-5G core and the IMS Core need to have the ability to capture 28
traces to validate the successful execution of the test cases. The end user devices need to have the capability to capture 29
traces/packet capture to calculate the ViLTE KPIs. Optionally, the network could have network taps deployed in 30
various legs of the network to get packet captures to validate successful execution of the test cases. Finally, all these 31
different components need to have connectivity with each other – the end user device should be able to connect to O-32
RAN system(O-eNBs and gNBs), O-RAN system needs to be connected to the 4G-5G core which in turn should have 33
connectivity to the IMS Core. 34
6.7.1.3 Test Methodology/Procedure 35
Ensure the end user devices, O-RAN system, 4G-5G core and the IMS Core have all been configured as outlined in 36
Section 6.7.1.2. In this test scenario, one of the end user devices (UE-1) is going to be connected over the 4G O-eNB to 37
the 4G-5G core, while the other end user device (UE-2) is going to be connected over 5G gNB (O-RU, O-DU and O-38
CU-CP/O-CU-UP) to the 4G-5G core. All traces and packet captures need to be enabled for the duration of the testing 39
to ensure all communication between network elements can be captured and validated.  40
1. Power on the two end user devices and ensure UE-1 is in LTE coverage, when UE-2 is in the 5G coverage. 41
Validate both end user devices register to the 4G-5G core. 42
2. Once the registration is complete, UE-1 and UE-2 have to establish a PDN connection and PDU session 43
respectively with the 4G-5G core. Once the PDN connection and PDU session have been setup, both the end user 44
devices have to register with the IMS core to support video calling services. 45

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 158

O-RAN.TIFG.E2E-Test.0-v02.00
3. Use UE-1 as the MO end user device to video call UE-2. Validate the UE-2 performs EPS fallback to LTE to 1
receive the video call.  2
4. Answer the call on UE-2 and continue to have two-way audio/video communication for at least 5 minutes and 3
terminate the call. Measure the voice KPIs included in Section 6.7.  4
5. At this point UE-2 should have completed the video call and moved back to connect to the 4G -5G Core over 5G 5
gNB (i.e. O-RU). 6
6. Use UE-2 as the MO end user device to call UE-1. Validate the UE-2 performs EPS fallback to LTE to before 7
making the video call.  8
7. Answer the call on UE-1 and continue to have two-way communication for at least 5 minutes and terminate the 9
call. Measure the voice KPIs included in Section 6.7.  10
8. Repeat the test multiple times (>10 times) and gather results. 11
 12
6.7.1.4 Test Expectation (expected results) 13
As a pre-validation, use the traces to validate end user device UE-1 is able to perform a successful registration and PDN 14
connection setup while using the O-eNB in LTE coverage. Similarly, use the traces to validate end user device UE -2 is 15
able to perform a successful registration and PDU session setup while using gNB in 5G coverage. Also validate both the 16
end user devices are able to register with the IMS core for voice services. This is a prerequisite before these tests can be 17
validated. 18
Validate the end user devices are able to make a video call between each other by dynamically setting up a QCI -2 19
bearer to transfer voice and video packets over RTP. Validate the device which is in the 5G coverage falls back to 4G 20
before receiving or making a video call, in other words the EPS fallback procedure has been executed successfully. The 21
Call Setup Time will be higher than Video over LTE due to the delay associated with executing the EPS fallback 22
procedure. Even though this test case does not directly impact the q uality of the video call, for the sake of consistency, 23
ensure the voice and video quality from both parties are clear and audible without one -way or intermittent muting and 24
video freezing. Use the packet captures to validate there is no RTP packet drops or  high RTP packet jitter which could 25
cause voice muting, video freezing and video lag issues. Use the packet captures to ensure there are no out -of-sequence 26
packets which could impact customer’s video calling experience.  27
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 28
is performed in a controlled environment in good radio condition without the interference of external factors which 29
could impact the KPIs, example: use of internet to connect to remote network nodes could add latency, jitter and packet 30
loss issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the testing in 31
this scenario, a KPI outcome outside the range does not necessarily poin t to a failed test case. Any test results outside 32
the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues may be 33
due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and the test 34
has to be repeated.  35
Table 6-18 Typical Video KPI values in controlled environments with good radio conditions  36
KPI With N26 Interface Without N26 Interface
CSSR – Call Setup Success Rate % >99% >99%
CST – Call Setup Time 3.5s 4 s
MOS Score 3.5 3.5
Mute Rate % < 1% < 1%
One Way Call % < 1% < 1%

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 159

O-RAN.TIFG.E2E-Test.0-v02.00
RTP Packet Loss % < 1% < 1%
 1
These end user video calling KPI values included in Section 6.7 need to be included in the test report along with the 2
minimum configuration parameters included in Section 3.3. The following information should also be included in the 3
test report for the testing performed to provide a comprehensive view of the test setup.  4
End user device side (real or emulated UE): 5
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second) 6
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second)  7
• Received downlink throughput (L1 and L3 PDCP layers) (average sample per second)  8
• Downlink transmission mode 9
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 10
(average sample per second) 11
The table below gives an example of the test report considering the mean and standard deviation of all test results that 12
have been captured.  13
Table 6-19 Example Test Report for Video Calling Service Testing – EPS Fallback 14
 EPS Fallback MO EPS Fallback MT
Call Setup Success Rate
Call Setup time
MOS Score
Mute Rate
One Way Call
RTP Packet Loss
L1 DL Spectral efficiency [bps/Hz]
UE RSRP [dBm]
UE PDSCH SINR [dB]
MIMO rank
PDSCH MCS
DL RB number
UE CQI
UE RSRQ
UE PMI
UE RSSI
UE Buffer status
UE Packet delay
PDSCH BLER [%]
The end user ViLTE experience can also be impacted by some of the features available (see Section 6.7) on the O -eNB, 15
O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other features/functionality which 16
could impact the end user’s video calling experience should be included in the test report to provide a comprehensive 17
view of the testing. 18

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 160

O-RAN.TIFG.E2E-Test.0-v02.00
6.8 Video Service – Video over NR 1
Video calling service is slowly replacing voice as a basic calling service. The improvement in end user throughput on 2
LTE has facilitated the move from voice call to video, and the high speeds on 5G is only going to accelerate this 3
migration process. This section of the document validates the user’s video calling experience in different conditions on 4
the 5G network. This section of the document only applies to the video calling service provided by the telecom service 5
provider, which uses the telecom operator’s IMS core to establish dedicated QoS flows to provide superior video calling 6
experience. The KPIs which will be monitored to assess the video calling service are included below 7
• CSSR – Call Setup Success Rate % – Total number of calls which were successful by the total number of 8
calls made as a percentage. 9
• CST – Call Setup Time – Time taken from the initial SIP INVITE to when the SIP 180 Ringing is received 10
in seconds. 11
• MOS Score – Mean Opinion Score for the video call. This needs to be measured on both ends of the video 12
call – mobile originated and mobile terminated. 13
• Mute Rate % – This is the percentage of video calls which were muted or video freezes in both directions 14
(calls with RTP loss of > 3-4s in both directions are considered muted call). This needs to be measured on 15
both ends of the video call – mobile originated and mobile terminated and counted only once per call. 16
• One Way Calls % – This is the percentage of video calls which were muted, or video is not transmitted in 17
any one direction only (video calls with RTP loss of > 3-4s in one direction only are considered one-way 18
calls). This needs to be monitored on both ends of the Voice call – mobile originated and mobile terminated 19
and counted only once per call. 20
• RTP Packet Loss % - Number of RTP packets which were dropped/lost in uplink/downlink direction as a 21
percentage of total packets. This needs to be measured on both ends of the Voice call – mobile originated 22
and mobile terminated. 23
 24
End user video calling service experience can also be impacted by some of the features available on the O -RU, O-DU 25
and O-CU-CP/O-CU-UP. Some of these features have been included below. As a part of the video calling services 26
testing, details of these features need to be included in the test report to get a comprehensive view of the setup used for 27
testing. If additional features or functionalities have been enabled during this testing that impact the end user voice 28
experience, those need be included in the test report as well. 29
• Basic Voice over NR 30
• RLC in Unacknowledged Mode 31
• Robust Header Compression 32
• DRX 33
• Dynamic GBR Admission Control 34
• TTI Bundling 35
• Automated Neighbor Relations ANR 36
• Connected mode mobility (Handover) 37
• idle mode reselection 38
• Intra-frequency Cell Reselection/Handover 39
• Inter-frequency Redirection/Cell Reselection/Handover 40
• NR Coverage-Triggered NR Session Continuity 41
 42

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 161

O-RAN.TIFG.E2E-Test.0-v02.00
6.8.1 Video over NR – Stationary Testing 1
This test scenario validates the user’s video calling experience when the end user device (UE)  makes a video call on the 2
5G network. 3
6.8.1.1 Test Description 4
This section of the document tests video over NR user experience on an O -RAN system. Video over NR is similar to 5
ViLTE where it uses IP packets to send and receive audio/video packets, with the IP packets being transferred over NR 6
or 5G radio. Just like ViLTE, IMS is used to setup the control and data plane for the video communication. As video 7
calling service is latency sensitive, the 5G core interacts with the IMS core to setup different QoS flows for video traffic 8
– 5QI-5 for control plane and 5QI-2 for data plane. This test case is applicable when UE is connected over a 5G SA 9
network.  10
6.8.1.2 Test Setup 11
The SUT in this test case would be a gNB (O-RU, O-DU and O-CU-CP/O-CU-UP) which is used to test the video 12
calling service. A 5G SA Core would be required to support basic functionality to authenticate and register the end user 13
device to establish a PDU session. An IMS core will be required to register the end user device to support video calling 14
services on a 5G network. The 5G and IMS cores could be a completely emulated, partially emulated or real non-15
emulated cores. We will need at least two end user devices (UE) which can be a real UEs or an emulated one, and both 16
have to support video calling service using Video over NR. The end user devices will serve as Mob ile Originated (MO) 17
and Mobile Terminated (MT) end user devices forming the two ends of the video call. Going forward in this section, 18
these end user devices will be referred to as MO end user device and MT end user device to represent their role in the 19
video call. The test setup should include tools which have the ability to collect traces on the elements and/or packet 20
captures of communication between the elements. This could be a built -in capability of the emulated/non-emulated 21
network elements or an external tool. Optionally, if some of the network elements are located remotely either in a cloud 22
or on the internet, the additional latency should be calculated and accounted for.  23
The O-RU, O-DU and O-CU-CP/O-CU-UP need to have the right configuration and software load. The SUT will also 24
need to be setup to run this testing in different radio conditions as outlined in Section 3.6. The end user device must be 25
configured with the right user credentials to be able to register and authenticate with the O-RAN system and the 5G 26
core. The end user devices also need to be provisioned with the user credentials to register and setup PDU session with 27
the 5G core and register with the IMS core to perform video call using Video over NR. The 5G core network and IMS 28
core must be configured to support video calling service on the end user devices used for testing, which includes the 29
ability to dynamically set up QoS Flows for video calls. The locations where the radio conditions are excellent, good, 30
fair and poor need to be identified within the serving cell. 31
All the elements in the network like O-RAN system, 5G core and the IMS Core need to have the ability to capture 32
traces to validate the successful execution of the test cases. The end user devices need to have the capability to capture 33
traces/packet capture to calculate the Video over NR KPIs. Optionally, the network could have network taps deployed 34
in various legs of the network to get packet captures to validate successful execution of the test cases. Finally, all these 35
different components need to have connectivity with each other – the end user device should be able to connect to O-36
RAN system(O-RU, O-DU and O-CU-CP/O-CU-UP), O-RAN system needs to be connected to the 5G core which in 37
turn should have connectivity to the IMS Core. 38
6.8.1.3 Test Methodology/Procedure 39
Ensure the end user devices, O-RAN system, 5G core and the IMS core have all been configured as outlined in Section 40
6.8.1.2. In this test scenario, both the mobile originated and mobile terminated end user devices are going to use the 41
same O-RAN system, 5G and IMS core to perform the end-to-end video call. All traces and packet captures need to be 42

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 162

O-RAN.TIFG.E2E-Test.0-v02.00
enabled for the duration of the testing to ensure all communication between network elements can be captured and 1
validated. 2
1. Power on the two end user devices in excellent radio condition and ensure both of devices registers with the 5G 3
core for video calling services over SA by connecting over the O-RAN (O-RU, O-DU and O-CU-CP/O-CU-UP).  4
2. Once the registration is complete, the MO and MT end user devices have to establish a PDU session with the 5G 5
core. Once the PDU session has been setup, both the end user devices have to register with the I MS core to support 6
video calling services. 7
3. Use the MO end user device to video call the MT end user device. Validate the MT end user device is able to 8
receive and answer the call. 9
4. Continue to have two-way audio/video communication on the video call for at least 5 minutes before terminating it.  10
5. Repeat the test multiple times (> 10 times) and gather results. 11
6. Repeat the above steps 1 through 5 for the good, fair and poor radio conditions.  12
 13
6.8.1.4 Test Expectation (expected results) 14
As a pre-validation, use the traces to validate a successful registration and PDU session setup by the end user devices 15
without any errors over O-RU, O-DU and O-CU-CP/O-CU-UP. Also validate both the end user devices are able to 16
register with the IMS core for video calling services. This is a prerequisite before these tests can be validated. 17
Validate the end user devices are able to make a video call between each other by dynamically setting up a 5QI -2 bearer 18
to transfer voice and video packets over RTP. Ensure the Call Setup Time is reasona ble, the voice and video quality 19
from both parties are clear and audible without one-way or intermittent muting and video freezing. Use the packet 20
captures to validate there is no RTP packet drops or high RTP packet jitter which could cause voice muting, v ideo 21
freezing or video lag issues. Use the packet captures to ensure there are no out -of-sequence packets which could impact 22
customer’s voice experience.  23
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 24
is performed in a controlled environment in good radio condition without the interference of external factors which 25
could impact the KPIs, example: use of internet to connect to remote servers/hosts could add latency, jitter and 26
reliability issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the 27
testing in this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test results 28
outside the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues 29
may be due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and 30
the test has to be repeated.  31
• CSSR – Call Setup Success Rate % –> 99%. 32
• CST – Call Setup Time – < 2.5s 33
• MOS Score – > 3.5 34
• Mute Rate % – < 1% 35
• One Way Call % - < 1% 36
• RTP Packet Loss % - < 1% 37
 38
As a part of gathering data, ensure the minimum configuration parameters (see Section 3.3) are included in the test 39
report. The following information should also be included in the test report to provide a comprehensive view of the test 40
setup. 41
UE side (real or emulated UE): 42
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second)  43
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second)  44

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 163

O-RAN.TIFG.E2E-Test.0-v02.00
• Downlink transmission mode 1
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 2
(average sample per second) 3
Table 6-20 Example Test Report for Video over NR – Stationary Testing 4

Excellent
(cell centre) Good Fair Poor
(cell edge)
Video over NR
MO/MT
Video over NR
MO/MT
Video over NR
MO/MT
Video over NR
MO/MT
Call Setup Success Rate
Call Setup Time
MOS Score
Mute Rate %
One Way Call %
RTP Packet Loss %
L1 DL Spectral efficiency [bps/Hz]
UE RSRP [dBm]
UE PDSCH SINR [dB]
MIMO rank
PDSCH MCS
DL RB number
UE CQI
UE RSRQ
UE PMI
UE RSSI
UE Buffer status
UE Packet delay
PDSCH BLER [%]
The end user video calling service experience can also be impacted by some of the features available (see Section 6.8) 5
on the O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other features/functionality 6
which could impact the end user’s video calling service experience should be incl uded in the test report to provide a 7
comprehensive view of the testing. 8
6.8.2 Video over NR – Intra-Distributed Unit (DU) handover 9
This test scenario validates the user’s video calling experience when the end user device (UE) is on a video call over 10
NR and performs a handover between O-RUs which connect to the same O-DU (Intra-O-DU handover). 11
6.8.2.1 Test Description 12
The 5G O-RAN system has multiple sub-components like the O-RU, O-DU and the O-CU-CP/O-CU-UP. This setup 13
leads to multiple handover scenarios between the O-RAN sub-components. This particular scenario tests the end user’s 14
video calling experience during the handover between two O-RUs which are connected to the same O-DU (and O-CU-15
CP/O-CU-UP), hence an Intra-O-DU handover. This handover will be agnostic to the 5G core as the handover occurs 16

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 164

O-RAN.TIFG.E2E-Test.0-v02.00
on the O-RAN system. This test assesses the impact on the end user’s voice service in this handover scenario by 1
monitoring the KPIs included in Section 6.8. 2
6.8.2.2 Test Setup 3
The SUT in this test case would be a pair of O-RUs which connect to the same O-DU and O-CU-CP/O-CU-UP. This O-4
RAN setup is used to test the video calling service during a handover  (refer Section 4.4). A 5G SA Core would be 5
required to support basic functionality to authenticate and register the end user dev ice to establish a PDU session. An 6
IMS core will be required to register the end user device to support video calling services on a 5G network. The 5G and 7
IMS cores could be a completely emulated, partially emulated or real non-emulated cores. We will need at least two end 8
user devices (UE) which can be a real UEs or an emulated one, and both have to support video calling service using 5G 9
– Video over NR. The end user devices will serve as Mobile Originated (MO) and Mobile Terminated (MT) end user 10
devices forming the two ends of the video call. Going forward in this section, these end user devices will be referred to 11
as MO end user device and MT end user device to represent their role in the video call. The test setup should include 12
tools which have the ability to collect traces on the elements and/or packet captures of communication between the 13
elements. This could be a built-in capability of the emulated/non-emulated network elements or an external tool. 14
Optionally, if some of the network elements are located remotely either in a cloud or on the internet, the additional 15
latency should be calculated and accounted for.   16
The pair of O-RUs (O-RU1 and O-RU2) need to be connected to the O-DU and O-CU-CP/O-CU-UP and all the 17
components need to have the right configuration and software load. The end user devices must be configured with the 18
right user credentials to be able to register and authenticate with the O -RAN system and the 5G core. The end user 19
devices also need to be provisioned with the user credentials to register and setup PDU session with the 5G core and 20
register with the IMS core to perform video call using Video over NR. The 5G core network and IMS core must be 21
configured to support video service on the end user devices used for testing, which includes the ability to dynamically 22
set up QoS Flows for voice calls. 23
All the elements in the network like O-RAN system, 5G core and the IMS Core need to have the ability to capture 24
traces to validate the successful execution of the test cases. The end user devices  need to have the capability to capture 25
traces/packet capture to calculate the Video over NR KPIs. Optionally, the network could have network taps deployed 26
in various legs of the network to get packet captures to validate successful execution of the test c ases. Finally, all these 27
different components need to have connectivity with each other – the end user device should be able to connect to O-28
RAN system(O-RUs connected to O-DU and O-CU-CP/O-CU-UP), O-RAN system needs to be connected to the 5G 29
core which in turn should have connectivity to the IMS Core. 30
6.8.2.3 Test Methodology/Procedure 31
Ensure the end user devices, O-RAN system, 5G core and the IMS core have all been configured as outlined in Section 32
6.8.2.2. In this test scenario, both the mobile originated and mobile terminated end user devices are going to use the 33
same O-RAN system, 5G and IMS core to perform the end-to-end video call. All traces and packet captures need to be 34
enabled for the duration of the testing to ensure all communication between network ele ments can be captured and 35
validated. 36
1. Power on the two end user devices and ensure both of the devices registers with the 5G core for video calling 37
services over SA by connecting over the O-RAN (O-RU, O-DU and O-CU-CP/O-CU-UP). Ensure both the MO & 38
MT end user devices are in the coverage area of the same O-RU – O-RU1. 39
2. Once the registration is complete, the MO and MT end user devices have to establish PDU session with the 5G 40
core. Once the PDU session has been setup, both the end user devices have to register  with the IMS core to support 41
voice services. 42
3. Use the MO end user device to video call the MT end user device. Validate the MT end user device is able to 43
receive and answer the call. 44

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 165

O-RAN.TIFG.E2E-Test.0-v02.00
4. Once the call has been setup, move the MO end user device from the covera ge area of O-RU1 to coverage area of 1
O-RU2, thus causing a handover from O-RU1 to O-RU2.  2
5. Continue the two-way video and voice communication between MO and MT end user devices until the handover 3
procedure is complete before terminating the video call.  4
6. Repeat the test (steps 1 through 5) multiple times (> 10 times) and gather results.  5
 6
6.8.2.4 Test Expectation (expected results) 7
As a pre-validation, use the traces to validate a successful registration and PDU session setup by the end user devices 8
without any errors over O-RU, O-DU and O-CU-CP/O-CU-UP. Also validate both the end user devices are able to 9
register with the IMS core for video calling services. This is a prerequisite before these tests can be validated.  10
Validate the end user devices are able to make a video call between each other by dynamically setting up a 5QI -2 bearer 11
to transfer voice and video packets over RTP. Ensure the Call Setup Time is reasonable, the voice and video 12
qualityfrom both parties are clear and audible without one-way or intermittent muting and video freezing, through the 13
duration of the call, especially during the handover process. Use the packet captures to validate there is no RTP packet 14
drops or high RTP packet jitter which could cause voice muting, video freezing or video lag issues. Use the packet 15
captures to ensure there are no out-of-sequence packets which could impact customer’s voice experience.  16
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 17
is performed in a controlled environment in good radio condition without the interference of external factors which 18
could impact the KPIs, example: use of internet to connect to remote servers/hosts could add latency, jitter and 19
reliability issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the 20
testing in this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test results 21
outside the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues 22
may be due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and 23
the test has to be repeated.  24
• CSSR – Call Setup Success Rate % –> 99%. 25
• CST – Call Setup Time – < 2.5s 26
• MOS Score – > 3.5 27
• Mute Rate % – < 1% 28
• One Way Call % - < 1% 29
• RTP Packet Loss % - < 1% 30
 31
As a part of gathering data, ensure the minimum configuration parameters (see Section 3.3) are included in the test 32
report. The following information should also be included in the test report to provide a comprehensive view of the test 33
setup. 34
UE side (real or emulated UE): 35
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second)  36
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per  second) 37
• Downlink transmission mode 38
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 39
(average sample per second) 40
Table 6-21 Example Test Report for Video over NR – Intra-O-DU Handover 41

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 166

O-RAN.TIFG.E2E-Test.0-v02.00
 Video over NR MO Video over NR MT
Call Setup Success Rate
Call Setup Time
MOS Score
Mute Rate %
One Way Call %
RTP Packet Loss %
L1 DL Spectral efficiency [bps/Hz]
UE RSRP [dBm]
UE PDSCH SINR [dB]
MIMO rank
PDSCH MCS
DL RB number
UE CQI
UE RSRQ
UE PMI
UE RSSI
UE Buffer status
UE Packet delay
PDSCH BLER [%]
The end user video calling service experience can also be impacted by some of the features available (see Section 6.8) 1
on the O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other features/functionality 2
which could impact the end user’s video calling service experience should be included in the test report to provide a 3
comprehensive view of the testing. 4
6.8.3 Video over NR – Intra-Central Unit (CU) Inter-Distributed Unit (DU) 5
handover  6
This test scenario validates the user’s video calling experience when the end user device (UE) is on a video call and 7
performs a handover between O-RUs which connect to different O-DUs which in turn are connected to the same O-CU-8
CP/O-CU-UP (Intra-O-CU Inter-O-DU handover) 9
6.8.3.1 Test Description 10
The 5G O-RAN system has multiple sub-components like the O-RU, O-DU and the O-CU-CP/O-CU-UP. This setup 11
leads to multiple handover scenarios between the O-RAN sub-components. This particular scenario tests the end user’s 12
video calling experience during the handover between two O-RUs, where the O-RUs are connected to different O-DUs 13
which in turn are connected to the same O-CU-CP/O-CU-UP, hence an Intra-O-CU Inter-O-DU handover. This 14
handover will be agnostic to the 5G core as the handover occurs on the O -RAN system. This test assesses the impact on 15
the end user’s voice service in this handover scenario by monitoring the KPIs included in Section 6.8. 16

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 167

O-RAN.TIFG.E2E-Test.0-v02.00
6.8.3.2 Test Setup 1
The SUT in this test case would be a pair of O-RUs which connect to a different pair of O-DUs which in turn connect to 2
the same O-CU-CP/O-CU-UP (refer Section 4.5). This O-RAN setup is used to test the voice service during a handover. 3
A 5G SA Core would be required to support basic functionality to authenticate and register the end user device to 4
establish a PDU session. An IMS core will be required to register the end user device to suppo rt voice services on a 5G 5
network. The 5G and IMS cores could be a completely emulated, partially emulated or real non-emulated cores. We 6
will need at least two end user devices (UE) which can be a real UEs or an emulated one, and both have to support 7
video calling service using Video over NR. The end user devices will serve as Mobile Originated (MO) and Mobile 8
Terminated (MT) end user devices forming the two ends of the video call. Going forward in this section, these end user 9
devices will be referred to as MO end user device and MT end user device to represent their role in the voice call. The 10
test setup should include tools which have the ability to collect traces on the elements and/or packet captures of 11
communication between the elements. This could be a built-in capability of the emulated/non-emulated network 12
elements or an external tool. Optionally, if some of the network elements are located remotely either in a cloud or on the 13
internet, the additional latency should be calculated and accounted for.  14
The pair of O-RUs (O-RU1 and O-RU2) need to be connected to a pair of O-DUs (O-DU1 and O-DU2) and O-CU-15
CP/O-CU-UP. As for the O-RU to O-DU connection, ensure O-RU1 connects to O-DU1, and O-RU2 connects to O-16
DU2, and both the O-DUs connect to the same O-CU-CP/O-CU-UP. All the O-RAN components need to have the right 17
configuration and software load. The end user devices also need to be provisioned with the user credentials to register 18
and setup PDU session with the 5G core and register with the IMS core to per form video call using Video over NR. The 19
5G core network and IMS core must be configured to support video calling service on the end user devices used for 20
testing, which includes the ability to dynamically set up QoS Flows for video calls.  21
All the elements in the network like O-RAN system, 5G core and the IMS Core need to have the ability to capture 22
traces to validate the successful execution of the test cases. The end user devices need to have the capability to capture 23
traces/packet capture to calculate the Video over NR KPIs. Optionally, the network could have network taps deployed 24
in various legs of the network to get packet captures to validate successful execution of the test cases. Finally, all these 25
different components need to have connectivity with each other – the end user device should be able to connect to O-26
RAN system(O-RUs connected to O-DU and O-CU-CP/O-CU-UP), O-RAN system needs to be connected to the 5G 27
core which in turn should have connectivity to the IMS Core. 28
6.8.3.3 Test Methodology/Procedure 29
Ensure the end user devices, O-RAN system, 5G core and the IMS Core have all been configured as outlined in Section 30
6.8.3.2. In this test scenario, both the mobile originated and mobile terminated end user device s are going to use the 31
same O-RAN system, 5G and IMS core to perform the end-to-end video call. All traces and packet captures need to be 32
enabled for the duration of the testing to ensure all communication between network elements can be captured and 33
validated. 34
1. Power on the two end user devices and ensure both of devices registers with the 5G core for video calling services 35
over SA by connecting over the O-RAN (O-RU, O-DU and O-CU-CP/O-CU-UP). Ensure both the MO & MT end 36
user devices are in the coverage area of the same O-RU – O-RU1. 37
2. Once the registration is complete, the MO and MT end user devices have to establish a PDU session with the 5G 38
core. Once the PDU session has been setup, both the end user devices have to register with the IMS core to support 39
video calling services. 40
3. Use the MO end user device to video call the MT end user device. Validate the MT end user device is able to 41
receive and answer the call. 42
4. Once the video call has been setup, move the MO end user device from the coverage area of O-RU1 to coverage 43
area of O-RU2, thus causing a handover from O-RU1 to O-RU2, and O-DU1 to O-DU2.  44
5. Continue the two-way video and voice communication between the end user devices until the handover p rocedure 45
is complete before terminating the video call.  46

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 168

O-RAN.TIFG.E2E-Test.0-v02.00
6. Repeat the test (steps 1 through 5) multiple times (> 10 times) and gather results.  1
 2
6.8.3.4 Test Expectation (expected results) 3
As a pre-validation, use the traces to validate a successful registration and PDU session setup by the end user devices 4
without any errors over O-RU, O-DU and O-CU-CP/O-CU-UP. Also validate both the end user devices are able to 5
register with the IMS core for video calling services. This is a prerequisite before these tests can be va lidated. 6
Validate the end user devices are able to make a video call between each other by dynamically setting up a 5QI -2 bearer 7
to transfer voice and video packets over RTP. Ensure the Call Setup Time is reasonable, the voice and video quality 8
from both parties are clear and audible without one-way or intermittent muting and video freezing, through the duration 9
of the call, especially during the handover process. Use the packet captures to validate there is no RTP packet drops or 10
high RTP packet jitter which could cause voice muting, video freezing or video lag issues. Use the packet captures to 11
ensure there are no out-of-sequence packets which could impact customer’s voice experience.  12
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 13
is performed in a controlled environment in good radio condition without the interference of external factors which 14
could impact the KPIs, example: use of internet to connect to remote servers/hosts could add  latency, jitter and 15
reliability issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the 16
testing in this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test re sults 17
outside the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues 18
may be due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and 19
the test has to be repeated.  20
• CSSR – Call Setup Success Rate % –> 99%. 21
• CST – Call Setup Time – < 2.5s 22
• MOS Score – > 3.5 23
• Mute Rate % – < 1% 24
• One Way Call % - < 1% 25
• RTP Packet Loss % - < 1%  26
 27
As a part of gathering data, ensure the minimum configuration parameters (see Section 3.3) are included in the test 28
report. The following information should also be included in the test report to provide a comprehensive view of the test 29
setup. 30
UE side (real or emulated UE): 31
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second)  32
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second)  33
• Downlink transmission mode 34
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 35
(average sample per second) 36
Table 6-22 Example Test Report for Video over NR – Intra-O-CU Inter-O-DU Handover 37
 Video over NR MO Video over NR MT
Call Setup Success Rate
Call Setup Time

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 169

O-RAN.TIFG.E2E-Test.0-v02.00
MOS Score
Mute Rate %
One Way Call %
RTP Packet Loss %
L1 DL Spectral efficiency [bps/Hz]
UE RSRP [dBm]
UE PDSCH SINR [dB]
MIMO rank
PDSCH MCS
DL RB number
UE CQI
UE RSRQ
UE PMI
UE RSSI
UE Buffer status
UE Packet delay
PDSCH BLER [%]
The end user video calling service experience can also be impacted by some of the features available (see Section 6.8) 1
on the O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other features/functionality 2
which could impact the end user’s video calling service experience should be included in the test report to provide a 3
comprehensive view of the testing. 4
6.8.4 Video over NR – Intra-Central Unit (CU) handover  5
This test scenario validates the user’s video calling experience when the end user device (UE) is on a video call and 6
performs a handover between O-RUs which connected to different O-DUs and O-CU-CP/O-CU-UPs (Inter-O-CU Inter-7
O-DU handover) 8
6.8.4.1 Test Description 9
The 5G O-RAN system has multiple sub-components like the O-RU, O-DU and the O-CU-CP/O-CU-UP. This setup 10
leads to multiple handover scenarios between the O-RAN sub-components. This particular scenario tests the end user’s 11
video calling experience during the handover between two O-RUs, where the O-RUs are connected to different O-DUs 12
which in turn are connected to different O-CU-CP/O-CU-UPs, hence an Inter-O-CU Inter-O-DU handover. This 13
handover occurs on the O-RAN system and the 5G core is aware of the handover as it needs to send data to a new O-14
CU-CP/O-CU-UP as a part of the handover process. This test assesses the impact on the end user’s voice service in this 15
handover scenario by monitoring the KPIs included in Section 6.8. 16
6.8.4.2 Test Setup 17
The SUT in this test case would be a pair of O-RAN subsystems, a set of O-RU, O-DU and O-CU-CP/O-CU-UP which 18
interconnects with another set of O-RU, O-DU and O-CU-CP/O-CU-UP (refer Section 4.6). This O-RAN setup is used 19
to test the video calling service during a handover. A 5G SA Core would be required to support basic functionality to 20
authenticate and register the end user device to establish a PDU session. An IMS core will be required to regis ter the 21
end user device to support voice services on a 5G network. The 5G and IMS cores could be a completely emulated, 22
partially emulated or real non-emulated cores. We will need at least two end user devices (UE) which can be a real UEs 23

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 170

O-RAN.TIFG.E2E-Test.0-v02.00
or an emulated one, and both have to support video calling service using Video over NR. The end user devices will 1
serve as Mobile Originated (MO) and Mobile Terminated (MT) end user devices forming the two ends of the video call. 2
Going forward in this section, these end user devices will be referred to as MO end user device and MT end user device 3
to represent their role in the video call. The test setup should include tools which have the ability to collect traces on th e 4
elements and/or packet captures of communication between the elements. This could be a built-in capability of the 5
emulated/non-emulated network elements or an external tool. Optionally, if some of the network elements are located 6
remotely either in a cloud or on the internet, the additional latency should be  calculated and accounted for.  7
The pair of O-RAN (O-RAN1 and O-RAN2) subsystems will be connected – O-RU1 will be connected to O-DU1, 8
which in turn will be connected to O-CU-CP/O-CU-UP1 and similarly O-RU2 will be connected to O-DU2, which in 9
turn will be connected to O-CU-CP/O-CU-UP2. The O-CU-CP/O-CU-UP1 and O-CU-CP/O-CU-UP2 nodes will be 10
connected to each other and the 5G core. All the O-RAN components need to have the right configuration and software 11
load. The end user devices must be configured with the right user credentials to be able to register and authenticate with 12
the O-RAN system and the 5G core. The end user devices also need to be provisioned with the user credentials to 13
register and setup PDU session with the 5G core and register with the IMS  core to perform video call using Video over 14
NR. The 5G core network and IMS core must be configured to support video calling service on the end user devices 15
used for testing, which includes the ability to dynamically set up QoS Flows for voice calls.  16
All the elements in the network like O-RAN system, 5G core and the IMS Core need to have the ability to capture 17
traces to validate the successful execution of the test cases. The end user devices need to have the capability to capture 18
traces/packet capture to calculate the Video over NR KPIs. Optionally, the network could have network taps deployed 19
in various legs of the network to get packet captures to validate successful execution of the test cases. Finally, all these 20
different components need to have connectivity with each other – the end user device should be able to connect to O-21
RAN system(O-RUs connected to O-DUs which are connected to the O-CU-CP/O-CU-UPs), O-RAN system needs to 22
be connected to the 5G core which in turn should have connectivity to the IM S Core. 23
6.8.4.3 Test Methodology/Procedure 24
Ensure the end user devices, O-RAN system, 5G core and the IMS Core have all been configured as outlined in Section 25
6.8.4.2. In this test scenario, both the mobile originated and mobile terminated end user devices are goi ng to use the 26
same O-RAN system, 5G and IMS core to perform the end-to-end video call. All traces and packet captures need to be 27
enabled for the duration of the testing to ensure all communication between network elements can be captured and 28
validated. 29
1. Power on the two end user devices and ensure both of devices registers with the 5G core for video calling services 30
over SA by connecting over the O-RAN1 system (O-RU1, O-DU1 and O-CU-CP/O-CU-UP1). Ensure both the 31
MO & MT end user devices are in the coverage area of the same O-RU – O-RU1. 32
2. Once the registration is complete, the MO and MT end user devices have to establish a PDU session with the 5G 33
core. Once the PDU session has been setup, both the end user devices have to register with the IMS core to support 34
video calling services. 35
3. Use the MO end user device to video call the MT end user device. Validate the MT end user device is able to 36
receive and answer the video call. 37
4. Once the call has been setup, move the MO end user device from the coverage area of O -RU1 to coverage area of 38
O-RU2, thus causing a handover from O-RAN1 to O-RAN2 subsystem - O-RU1 to O-RU2, O-DU1 to O-DU2 and 39
O-CU-CP/O-CU-UP1 to O-CU-CP/O-CU-UP2.  40
5. Continue the two-way video and voice communication between MO and MT end user devices until the h andover 41
procedure is complete before terminating the video call.  42
6. Repeat the test (steps 1 through 5) multiple times (> 10 times) and gather results.  43
 44

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 171

O-RAN.TIFG.E2E-Test.0-v02.00
6.8.4.4 Test Expectation (expected results) 1
As a pre-validation, use the traces to validate a successful registration and PDU session setup by the end user devices 2
without any errors over O-RU, O-DU and O-CU-CP/O-CU-UP. Also validate both the end user devices are able to 3
register with the IMS core for video calling services. This is a prerequisite before these tests can be validated. 4
Validate the end user devices are able to make a video call between each other by dynamically setting up a 5QI -2 bearer 5
to transfer voice and video packets over RTP. Ensure the Call Setup Time is reasonable, the voice and video quality 6
from both parties are clear and audible without one-way or intermittent muting and video freezing, through the duration 7
of the call, especially during the handover process. Use the packet captures to validate there is no RTP packet drops or 8
high RTP packet jitter which could cause voice muting, video freezing or video lag issues. Use the packet captures to 9
ensure there are no out-of-sequence packets which could impact customer’s voice experience.  10
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 11
is performed in a controlled environment in good radio condition without the interference of external factors which 12
could impact the KPIs, example: use of internet to connect to remote servers/hosts could add latency, jitter and 13
reliability issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the 14
testing in this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. An y test results 15
outside the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues 16
may be due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and 17
the test has to be repeated.  18
• CSSR – Call Setup Success Rate % –> 99%. 19
• CST – Call Setup Time – < 2.5s 20
• MOS Score – > 3.5 21
• Mute Rate % – < 1% 22
• One Way Call % - < 1% 23
• RTP Packet Loss % - < 1% 24
 25
As a part of gathering data, ensure the minimum configuration parameters (see Section 3.3) are included in the test 26
report. The following information should also be included in the test report to provide a comprehensive view of the test 27
setup. 28
UE side (real or emulated UE): 29
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second)  30
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second) 31
• Downlink transmission mode 32
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 33
(average sample per second) 34
Table 6-23 Example Test Report for Video over NR – Inter-O-CU Handover 35
 Video over NR MO Video over NR MT
Call Setup Success Rate
Call Setup Time
MOS Score
Mute Rate %

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 172

O-RAN.TIFG.E2E-Test.0-v02.00
One Way Call %
RTP Packet Loss %
L1 DL Spectral efficiency [bps/Hz]
UE RSRP [dBm]
UE PDSCH SINR [dB]
MIMO rank
PDSCH MCS
DL RB number
UE CQI
UE RSRQ
UE PMI
UE RSSI
UE Buffer status
UE Packet delay
PDSCH BLER [%]
The end user video calling service experience can also be impacted by some of the features available (see Section 6.8) 1
on the O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other features/functionality 2
which could impact the end user’s video calling service experience should be included in the test report to provide a 3
comprehensive view of the testing. 4
6.9 URLLC 5
The 5G network has been built not just to support the data and voice use -cases of the past, but a wide variety of use-6
cases to support connectivity across a diverse set of verticals like industrial automation, health care, gaming etc. 5G 7
network was built with flexibility and efficiency with additional enhancements being added in every release to enable 8
ultra-low latency and/or ultra-reliability to cater to a diverse set of use-cases such as automotive applications, augment 9
and virtual reality etc. Network Slicing in 5G allows the separation of the network where services with different 10
characteristics and requirements can be run on its own slice of the network. These enhancements in 5G paired up with 11
network slicing capability allows a telecommunication service provider to support all the diverse set of use-cases using 12
the same 5G network on different network slices.   13
Ultra-Reliable Low Latency Communication (URLLC) is one such slice which allows service which require low E2E 14
latency to be deployed. As this is a new concept, new services and use-cases are being created to use this slice of the 15
network. The generic KPIs which will be monitored to assess the URLLC use cases are included below. As the URLLC 16
use cases vary, the KPI used and the values for these KPIs also vary by use case. Not all KPIs are used for all use cases 17
and similarly some KPIs are used only in specific use cases. 18
• Reliability – This is the percentage of packets or messages sent which were successfully delivered to the 19
end node. 20
• End to End Latency – Time taken to transfer a packet/message from the end user device to the application 21
or vice versa. 22
• Jitter – This is the variation of the end-to-end latency values seen in the network setup. 23
• Position Service Accuracy – This is the distance between the location provided by the location service and 24
the real location of the target object.  25
• Position Service Latency – Time elapsed between the request to get location information to when the 26
location information is available. 27
• User experienced throughput – The throughput as measured on the application layer.  28

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 173

O-RAN.TIFG.E2E-Test.0-v02.00
• Mean time between failures – This is the duration of time the service was available before a failure 1
condition which causes the service to be unavailable. 2
 3
This section of the document gives the procedure to test the URLLC applications in a telecommunication network. 4
URLLC refers to a diverse set of applications where the end-user device communicates to the application server over 5
the 5G network. The use-case itself has stringent requirements for the end-to-end latency and/or reliability between the 6
end-user device and application server. 5G has enhancements to its RAN and core to support these requirements to 7
support these use cases. The 5G RAN allows for lower downlink data to uplink Ack, thus redu cing the latency on the air 8
interface. Similarly, 5G Core allows for the data plane anchor (UPF) to be deployed close to the end user device in the 9
edge location along with the application server to eliminate part of the backhaul latency.   10
6.9.1 Augmented Reality 11
This test scenario validates the end user’s augmented reality experience when using an AR device over the 5G network.  12
6.9.1.1 Test Description 13
This section of the document is using Augmented Reality (AR) as an example, where the end user device is an AR 14
device which communicates with an image processing server which acts as the application server. AR itself has a wide 15
range of applications including gaming, augmented worker as a part of smart factory, remote maintenance, ad -hoc 16
support from remote expert and control of heavy equipment etc. AR is one such use case where AR device has a camera 17
and continuously transmits images in real time to an application server, along with the position of the AR device. The 18
application server in this case would be an image processing server, which render the augmented image and transmit 19
down the augmented video stream back to the AR device, which is then displayed on the AR device. Here the entire 20
process of uploading the image from the AR device to the processing on the applicat ion server, and the displaying of the 21
augmented image on the AR device has to occur within milliseconds with high reliability to ensure the end user has a 22
realistic augmented reality experience.  23
The 5G URLLC network slice does not have requirements on the protocol, or the method used to communicate between 24
the end user device and the application server. The 5G core acts as a pipe to allow communication between these two 25
end points, AR device and application server, with high data rate, reliability and late ncy requirements as dictated by the 26
application. Optionally, depending on the AR use case two additional KPIs could also be monitored  when audio-visual 27
interaction is characterized by a human being interacting with entities or humans by relying on audio -visual feedback. 28
• Motion to photon – This is the latency between the physical movement of the user’s head and the updated 29
image on the AR device. 30
• Motion to audio – This is the latency between the physical movement of the user’s head and the updated sound 31
waves from the AR device’s speakers. 32
 33
6.9.1.2 Test Setup 34
The SUT in this test case would be a gNB (O-RU, O-DU and O-CU-CP/O-CU-UP) which is used to test the URLLC 35
use case. A 5G SA Core would be required to support basic functionality to authenticate and register the end user device 36
to establish a PDU session. An image processing server would act as an application server for this test scenario. The 5G 37
core and application server could be a completely emulated, partially emulated or real non -emulated core and image 38
processing server. We will need an end user device which would be an AR device for this test scenario, which could a 39
customized device like an AR based eyeglass, or an AR based head mounted device or a generic handset which support 40
AR or any AR end user device which is needed to support this use case. For the testing of this use case, we could use a 41
real end user device or an emulated one. The test setup should include tools which have the ability to collect traces on 42
the elements and/or packet captures of communication between the elements. This could be a built-in capability of the 43

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 174

O-RAN.TIFG.E2E-Test.0-v02.00
emulated/non-emulated network elements or an external tool. Optionally, if some of the network elements are located 1
remotely either in a cloud or on the internet, the additional latency should be calculated and accounted for.  2
The O-RU, O-DU and O-CU-CP/O-CU-UP need to have the right configuration and software load. The SUT will also 3
need to be setup to run this testing in different radio conditions as outlined in Section 3.6. The end user device must be 4
configured with the right user credentials to be able to register and authenticate with the O -RAN system and the 5G 5
core. The 5G core maybe distributed with the UPF deployed at the edge location along with the application server to 6
support the stringent latency requirement of the AR use case. The end user device also needs to be provisioned with the 7
user credentials to register and setup PDU session with the 5G core and authenticate/communicate with the application 8
server. The application server must be configured and provisioned with all the necessary information, including relevant 9
images and data to support the AR use case. The locations where the radio conditions are excellent, good, fair and poor 10
need to be identified within the serving cell. 11
All the elements in the network like O-RAN system, 5G core and the application server need to have the ability to 12
capture traces to validate the successful execution of the test cases. The end user devices need to have the capability to 13
capture traces/packet capture to calculate the KPIs. Optionally, the network could have network taps deployed in 14
various legs of the network to get packet captures to validate successful execution of the test cases. Finally, all these 15
different components need to have connectivity with each other – the end user device should be able to connect to O-16
RAN system (O-RU, O-DU and O-CU-CP/O-CU-UP), O-RAN system needs to be connected to the 5G core which in 17
turn should have connectivity to the application server. 18
6.9.1.3 Test Methodology/Procedure 19
Ensure the end user devices, O-RAN system, 5G core and the application server have all been configured as outlined in 20
section 6.9.1.2. All traces and packet captures need to be enabled for the duration of the testing to ensure all 21
communication between network elements can be captured and validated. 22
1. Power on the end user device in excellent radio condition and ensure the device registers with the 5G core over SA 23
by connecting over the O-RAN (O-RU, O-DU and O-CU-CP/O-CU-UP). 24
2. Once the registration is complete, the end user device has to establish a PDU session over the URLLC network 25
slice, followed by connection to the application server. 26
3. Use the end user device to communicate with the application server for an AR session. Depending on  the end user 27
device, this could be done using a finger and/or hand to select the right AR application, or using a button or touch 28
activated system to select the right AR application.   29
4. Once the application has started, complete the designated task which c ould be playing a game or performing a pre-30
defined task on a machinery, troubleshooting etc. 31
5. Repeat the test multiple times (> 10 times) and gather results. 32
6. Repeat the above steps 1 through 5 for the good, fair and poor radio conditions.  33
 34
6.9.1.4 Test Expectation (expected results) 35
As a pre-validation, use the traces to validate a successful registration and PDU session setup to the URLLC network 36
slice by the end user device without any errors over O-RU, O-DU and O-CU-CP/O-CU-UP. Also validate the end user 37
device is able to connect to the application server for the AR session. This is a prerequisite before these tests can be 38
validated. 39
Validate the end user device is able to setup the AR session and perform the desired task eg: Remote maintenance. 40
Ensure there is no noticeable video freezing, jitter or lag in the AR session for the end user. Use packet captures to 41
validate there is no jitter or packet drops between the end user device and the application server. Use the packet captures 42
to ensure there are no out-of-sequence packets which could impact the end user’s experience. Calculate the metrics like 43
latency and throughput using the packet captures. 44

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 175

O-RAN.TIFG.E2E-Test.0-v02.00
The KPIs for an Augmented Reality session varies by the use-case type, the vertical that is being supported and even the 1
product that is being used for testing. As there is no standard application layer protocol or method used for these use -2
cases, vendors have developed products which has been optimized using their own propriety mechanism. The specific 3
values for these KPIs depends on the product used for testing and has to be provided by the product vendor. As there are 4
multiple variables which can impact the testing in this scenario, a KPI outcome outside the range does not necessarily 5
point to a failed test case. Any test results outside the range of KPI encountered during testing, will have to be 6
investigated to identify the root cause as the issues may be due to the variables outside the SUT. If the root cause is not 7
in the SUT, then the issues have to be resolved and the test has to be repeated.  8
The KPIs for the URLLC session varies by use-case category, use-case type and even by the vendor products used for 9
testing. The table below [34] gives a generic list of KPI values defined by use-case category. The specific values for 10
these KPIs depends on the product used for testing and has to be provided by the product vendor. 11
Table 6-24 - URLLC KPIs 12
Use case
group

Use case example e2e
latency

jitter round
trip time
e2e
reliabilit
y
network
reliabilit
y
user
experienced
throughput
network
throughput

availability
time
synchronous
accuracy
device/
connection
density

AR/VR
Augmented
worker 10ms   99.9999%
VR view broadcast   <20ms 99.999%  40 -
700Mbps    3000/km2
Tactile
interactio
n
Cloud Gaming <7ms
(uplink)   99.999%  1 Gbps    3000/km2

Energy
Differential
protection <15ms <160us  99.999%  2.4Mbps   10us 10-100/km2
FISR <25ms     10 Mbps    10/km2
Fault location
identification 140ms 2ms  99.9999%  100 Mbps   5 us 10/km2
fault mgmt. in
distr. Power
generation

<30ms

99.999%

1Mbps

99.999%

<2000/km2

Factory
of the
future

Advanced
industrial robotics

<2ms
 <30ms
task
planner;
<1-5ms
robot ctrl
99.9999%
to
99.999999
%

AGV control

5ms

99.999%
 100 kbps
(downlink)
3-8Mbps
uplink

Robot tooling
1ms
robotic
motion
ctrl;
1-10ms
machine
ctrl

<50%

99.9999%

UAV
UTM connectivity    99.999%  < 128 bps
Cmnd & Ctrl <100ms   99.999%
Payload application dependent

Position
measure-
ment
delivery

for AR in
smart factory

<15ms

99.9%

for inbound
logistics in
manufacturing

<10ms

99.9%

 13
The capability to capture data is dependent on the end user device’s capability. As a part of gathering data, it is 14
recommended that minimum configuration parameters (see Section 3.3) are included in the test report. The following 15
information is also recommended to be included in the test report to provide a comprehensive view of the test setup.  16
End user device (real or emulated AR device): 17

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 176

O-RAN.TIFG.E2E-Test.0-v02.00
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second)  1
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second)  2
• Downlink transmission mode 3
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 4
(average sample per second) 5
Table 6-25 4 Example Test Report for URLLC Testing 6
URLLC KPIs
Excellent
Good Fair
Poor
(cell centre) (cell edge)
Reliability
End to End Latency
User Experienced
Throughput
Motion-to-Photon
latency
Motion-to-sound delay
HARQ Retransmission
DL/UL Latency – User
Plane
DL/UL Latency –
Control Plane
L1 DL throughput
[Mbps]
L1 DL Spectral
efficiency [bps/Hz]
L3 DL PDCP
throughput [Mbps]
Application DL
throughput [Mbps]
UE RSRP [dBm]
UE RSRQ
UE PDSCH SINR [dB]
MIMO rank
PDSCH MCS
CQI
PMI
RSSI
Packet delay
PDSCH BLER [%]
 7
6.10 mMTC 8
The 5G network also supports a wide set of use cases which allow machines type devices to communicate with network 9
to realise the Internet of Things use cases. These use cases fall under a category called mMTC – massive Machine Type 10

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 177

O-RAN.TIFG.E2E-Test.0-v02.00
Communication. There are a diverse set of use cases under this category which supports a range of verticals including 1
security, healthcare, remote control, metering, smart city to name of few. The 5G network has enhanced the RAN and 2
the core to support mMTC requirements. Some of these enhancements include support large number of mMTC devices 3
by improving coverage and density of devices. Enhancements also include reducing the ove rhead for small intermittent 4
data transmission, thus ensuring low energy usage, and extending the battery life of the mMTC devices.  5
Network slicing in 5G allows for a telecommunication service provider to support these diverse set of mMTC use cases 6
by dedicating a separate slice of the network for these use cases. 3GPP has also defined a massive Machine Type 7
Communication (mMTC) as a slice which is used to support the use case to support very large number of small devices 8
(million to billions) in an efficient way so as to ensure optimal energy utilization. This section of the document includes 9
scenarios to test service(s) which use the mMTC slice. 10
6.10.1 Sensors 11
This test scenario validates the working of sensors over an mMTC slice on a 5G network. 12
6.10.1.1 Test Description 13
This section of the document gives the procedure to test the mMTC applications in a telecommunication network. As 14
already mentioned, mMTC comprises of a diverse set of use cases supported across a various vertical. Most of the 15
mMTC use cases include an end user equipment which communicates with an application server over the 5G network. 16
The end user device varies based on the use case from a sensor which measure temperature or humidity or weight etc to 17
a gauge which measures usage of electricity or water etc to a monitoring device which monitor vital statistics. These 18
end user devices collect data and update the application server periodically. The application server collects the data, 19
stores it, analyses it, and can also perform additional tasks if needed based on the outcome of the analysis eg: turn off 20
equipment when the temperature is high etc.  21
This section of the document is using sensors to perform mMTC testing. The test case includes deploying 5G sensors in 22
the field to collect data and upload it to an application server. The sensors itself could vary based on the use case needed 23
to support. These sensors could be air humidity sensor, temperature sensor and soil moisture sensor used in conjunction 24
to identify the need to irrigate a farm. These sensors could also be used to identify gas leaks to alert the authorities or 25
turn off gas lines. These sensors could also be light/image sensors which can be used to identify if a parking spot is 26
occupied. There are numerous sensors which support a varied set of use cases, but this test case has been intentionally 27
left generic without specifying the type of sensor that will be used for testing.  28
The 5G mMTC network slice does not have requirements on the protocol, or the method used to communicate betwee n 29
the end user device and the application server. The 5G core acts as a pipe to allow communication between these two 30
end points, the sensors and application server, with the low energy utilization and the ability to support massive number 31
of such sensors to connect. 32
6.10.1.2 Test Setup 33
The SUT in this test case would be a gNB (O-RU, O-DU and O-CU-CP/O-CU-UP) which is used to test the mMTC use 34
case. A 5G SA core would be required to support basic functionality to authenticate and register the end user device(s) 35
to establish a PDU session. An application server which has the capability to collect data from different end user 36
devices(s)/sensor(s) which will be used for testing. The 5G core and application server could be a completely emulated, 37
partially emulated or real non-emulated core. The end user device(s) in this scenario would be a single or a group of 38
sensors which can communicate over 5G. For the testing of this use case, we could use real end user device(s) or we 39
could use emulated devices which emulate the different sensor(s). The test setup should include tools which have the 40
ability to collect traces on the elements and/or packet captures of communication between the elements. This could be a 41
built-in capability of the emulated/non-emulated network elements or an external tool. Optionally, if some of the 42

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 178

O-RAN.TIFG.E2E-Test.0-v02.00
network elements are located remotely either in a cloud or on the internet, the additional latency should be calculated 1
and accounted for.  2
The O-RU, O-DU and O-CU-CP/O-CU-UP need to have the right configuration and software load. The SUT will also 3
need to be setup to run this testing in different radio conditions as outlined in Section 3.6. The end user device must be 4
configured with the right user credentials to be able to register and authenticate with the O-RAN system and the 5G 5
core. The end user device also needs to be provisioned with the user credentials to register and setup PDU session with 6
the 5G core and authenticate/communicate with the application server. The application server must be configured and 7
provisioned with all the necessary information to connect to the different sensors and the values. The locations where 8
the radio conditions are excellent, good, fair and poor need to be identified within the serving cell.  9
All the elements in the network like O-RAN system, 5G core and the application server need to have the ability to 10
capture traces to validate the successful execution of the test cases. The end user devices need to have the capability to 11
capture traces/packet capture to calculate the KPIs. Optionally, the network could have network taps deployed in 12
various legs of the network to get packet captures to validate successful execution of the test cases. Finally, all these 13
different components need to have connectivity with each other – the end user device should be able to connect to O-14
RAN system (O-RU, O-DU and O-CU-CP/O-CU-UP), O-RAN system needs to be connected to the 5G core which in 15
turn should have connectivity to the application server. 16
6.10.1.3 Test Methodology/Procedure 17
Ensure the end user devices, O-RAN system, 5G core and the application server have all been configured as outlined in 18
section 6.10.1.2. All traces and packet captures need to be enabled for the duration of the testing to ensure all 19
communication between network elements can be captured and validated. 20
1. Power on the end user device(s) in excellent radio condition and ensure the device registers with the 5G core over 21
SA by connecting over the O-RAN (O-RU, O-DU and O-CU-CP/O-CU-UP). Depending on the type of mMTC 22
device this could be done by pushing the power button on the end user device or could be by connecting the end 23
user device to a battery or power source.  24
2. Once the registration is complete, the end user device(s) has to establish a PDU session over the mMTC network 25
slice, followed by connection to the application server. 26
3. Based on the type of mMTC device, additional steps may be required the first time the device is connected to the 27
network, to enable the device to register with the application server. This could include adding t he end user device 28
to the application server using a unique ID or/and unique security code etc. 29
4. Validate the end user device are able to communicate with the application server periodically and update the sensor 30
data.  31
5. Let the test run for long enough duration of time so that the end user device(s) is able to upload enough data to the 32
application server. 33
6. Repeat the above steps 1 through 5 for the good, fair and poor radio conditions.  34
 35
6.10.1.4 Test Expectation (expected results) 36
As a pre-validation, use the traces to validate a successful registration and PDU session setup to the mMTC network 37
slice by the end user devices without any errors over O-RU, O-DU and O-CU-CP/O-CU-UP. Also validate the end user 38
devices are able to connect to the application server. This is a prerequisite before these tests can be validated. 39
Validate the end user devices are able to connect to the application server and update the sensor data periodically. 40
Validate the application server is able to receive the data from the sensors, process it and take the necessary action – 41
updating charts and report, triggering secondary actions etc. Use packet captures to validate there is no packet drops 42
between the end user device and the application server. Calculate the metrics like latency and t hroughput using the 43
packet captures. 44

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 179

O-RAN.TIFG.E2E-Test.0-v02.00
The KPIs for the mMTC session varies by use-case category, use-case type and even by the vendor products used for 1
testing. As there is no standard application layer protocol or method used for these use -cases, vendors have developed 2
products which has been optimized using their own propriety mechanism. The specific values for these KPIs depends 3
on the product used for testing and has to be provided by the product vendor. As there are multiple variables which can 4
impact the testing in this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any 5
test results outside the range of KPI encountered during testing, will have to be investigated to identify the root cause as 6
the issues may be due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be 7
resolved and the test has to be repeated.  8
The capability to capture data is dependent on the end user device’s capability. As a part of gathering data,  it is 9
recommended that minimum configuration parameters (see Section 3.3) are included in the test report. The following 10
information is also recommended to be included in the test report to provide a comprehensive view of the test setup.  11
End user device (real or emulated sensors): 12
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second)  13
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second)  14
• Downlink transmission mode 15
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 16
(average sample per second) 17
Table 6-26 Example Test Report for mMTC Testing 18
MMTC KPIs Excellent Good Fair Poor
(cell centre) (cell edge)
RACH Success
Rate
Paging Success
Rate
UE RSRP [dBm]
UE RSRQ
UE PDSCH SINR
[dB]
CQI
Packet delay
Buffer Status
DRX Sleep Time
 19
 20
  21

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 180

O-RAN.TIFG.E2E-Test.0-v02.00
 Security 1
This chapter describes the tests evaluating and assessing the security aspects of the E2E of a radio access network. The 2
general test methodologies and configurations outlined in Chapter 3 should be followed.  3
Because the whole O-RAN system is the System under Test (SUT) and can be viewed as an integrated black box in the 4
context of the E2E testing, the security test cases listed here fall into the following categories: 5
1. gNB security assurance specification required by 3GPP SA3 (applicable to 5G-NR NSA/SA); 6
2. Optional: Denial of Service (DoS), fuzzing, and exploitation types of security test cases against the O -RAN 7
system executed on L2 (Ethernet) and L3 (IP) layers with easy to access target information (MAC/IP address)  8
(applicable to 5G-NR NSA/SA); 9
3. Optional: Resource exhaustion types of security test case against the underlying cloud infrastructure hosting 10
the O-RAN system and its network slice(s) (applicable to 5G-NR NSA/SA)   11
Any other security test cases requiring specific access to a major interface or an internal functionality of the O -RAN 12
component(s) is out of scope. 13
7.1 gNB Security Assurance Specification (SCAS) required by 14
3GPP SA3 15
An E2E O-RAN system (SUT) is equivalent to a gNB from both network architecture and functional perspectives, 16
therefore any gNB specific security requirements, threats and test cases outlined in 3GPP TS 33.511  [27] shall be 17
followed in this E2E of radio access network security evaluation and assessment. The following table illustrates all the 18
required test cases listed in Section 4.2.2 of 3GPP TS 33.511[27]. 19
Table 7-1 List of gNB SCAS Test Cases 20
Test Case
(O-RAN
Ref. #)
Test Case
(3GPP
Ref. #)
Test Name Description
7.1.1 4.2.2.1.1 Integrity protection
of RRC-signalling
Verify that the RRC-signaling data sent between UE and gNB over
the NG RAN air interface are integrity protected
7.1.2 4.2.2.1.2 Integrity protection
of user data between
the UE and the gNB
Verify that the user data packets are integrity protected over the NG
RAN air interface.
7.1.3 4.2.2.1.4 RRC integrity check
failure
Verify that RRC integrity check failure is handled correctly by the
gNB
7.1.4 4.2.2.1.5 UP integrity check
failure
Verify that UP integrity check failure is handled correctly by the
gNB.
7.1.5 4.2.2.1.6 Ciphering of RRC-
signalling
Verify that the RRC-signaling data sent between UE and gNB over
the NG RAN air interface are confidentiality protected.
7.1.6 4.2.2.1.7 Ciphering of user
data between the UE
and the gNB
Verify that the user data packets are confidentiality protected over
the NG RAN air interface.
7.1.7 4.2.2.1.8 Replay protection of
user data between
Verify that the user data packets are replay protected over the NG
RAN air interface.

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 181

O-RAN.TIFG.E2E-Test.0-v02.00
the UE and the gNB
7.1.8 4.2.2.1.9 Replay protection of
RRC-signalling
Verify the replay protection of RRC-signaling between UE and gNB
over the NG RAN air interface.
7.1.9 4.2.2.1.10 Ciphering of user
data based on the
security policy sent
by the SMF
Verify that the user data packets are confidentiality protected based
on the security policy sent by the SMF via AMF
7.1.10 4.2.2.1.11 Integrity of user data
based on the security
policy sent by the
SMF
Verify that the user data packets are integrity protected based on the
security policy sent by the SMF.
7.1.11 4.2.2.1.12 AS algorithms
selection
Verify that the eNB/gNB selects the algorithms with the highest
priority in its configured list.
7.1.12 4.2.2.1.13 Key refresh at the
gNB
Key refresh at gNB
7.1.13 4.2.2.1.14 Bidding down
prevention in Xn-
handovers
Verify that bidding down is prevented in Xn-handovers.
7.1.14 4.2.2.1.15 AS protection
algorithm selection
in gNB change
Verify that AS (Algorithm Selection) protection algorithm is selected
correctly
7.1.15 4.2.2.1.16 Control plane data
confidentiality
protection over
N2/Xn interface
Verify the control plane data confidentiality protection over N2/Xn
interface
7.1.16 4.2.2.1.17 Control plane data
integrity protection
over N2/Xn
interface
Verify the control plane data integrity protection over N2/Xn
interface
7.1.17 4.2.2.1.18 Key update at the
gNB on dual
connectivity
Key update at the gNB on dual connectivity – 2 test cases
 1
7.2 Optional: DoS, fuzzing and blind exploitation types of security 2
test 3
Due to the open and disaggregated nature of the O-RAN system (SUT), the attack surfaces associated with some of its 4
critical transport protocols and major interfaces of the O-RAN system become easy targets for potential attackers. 5
Cyberattacks like DoS, fuzzing and blind exploitation types are easy to launch, require little information on the target 6
system, and could cause significant performance degradation, or even the service interruption if not properly mitigated.  7
As the WG1 Security Task Group (STG) is continuously working on the O -RAN system security analysis and security 8
requirements specifications, the proposed security test cases in this section are optional for this E2E test specification 9
release.  10
As we learn more about the O-RAN system security posture against these types of attacks through the proposed test 11
cases, we will reevaluate their necessity in the subsequent E2E test specification releases. 12

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 182

O-RAN.TIFG.E2E-Test.0-v02.00
7.2.1 S-Plane PTP DoS Attack (Network layer) 1
7.2.1.1 Test description & applicability 2
The purpose of the test is to verify that a predefined volumetric DoS attack against O-DU S-Plane will not degrade 3
service availability or performance of the SUT in a meaningful way. 4
7.2.1.2 Test setup and configuration 5
The test requires easy to access MAC address information of the O-DU’s open fronthaul interface and L2 connectivity 6
(e.g. over L2 network switching device) to the target from the emulated attacker.  7
Please refer to the diagram below for the test setup and configuration: 8
 9
7.2.1.3 Test procedure 10
• Ensure normal UE procedures1 and user-plane traffic can be handled properly through the  SUT. It is 11
recommended to use Section 5.6 Bidirectional throughput in different radio conditions and  Section 6.1 Data 12
Services tests as a benchmark for indicating correct behavior of the SUT. 13
• Use test tool to generate various level of volumetric DoS attack against the MAC address of the O -DU S-Plane 14
o Volumetric tiers: 10Mbps, 100Mbps, 1Gbps 15
o DoS Traffic types: generic Ethernet frames, PTP announce/sync message 16
o DoS source address: spoofed MAC of PTPGM, random source MACs and broadcast MAC  17
• Observe the functional and performance impact of the SUT 18

1 UE procedures include initial acquisition, registration/de-registration, attach/detach, paging, service
request, handover, …
5GC or Emulated 5GC
UE or Emulated UE
S-Plane DoS attack

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 183

O-RAN.TIFG.E2E-Test.0-v02.00
7.2.1.4 Test requirements (expected results) 1
• No meaningful degradation of service availability and performance of the SUT. It is recommended to use  Sec.  2
5.6 Bidirectional throughput in different radio conditions and Section 6.1 Data Services tests as a benchmark 3
for indicating correct behavior of the SUT. 4
• DoS attacks should be mitigated by O-DU 5
7.2.2 C-Plane eCPRI DoS Attack (Network layer) 6
7.2.2.1 Test description & applicability 7
The purpose of the test is to verify that a predefined volumetric DoS attack against O -DU C-Plane will not degrade 8
service availability or performance of the SUT in a meaningful way. 9
7.2.2.2 Test setup and configuration 10
The test requires easy to access MAC address information of the O-DU’s open fronthaul interface and L2 connectivity 11
(e.g. over L2 network switching device) to the target from the emulated attacker.  12
Please refer to the diagram below for the test setup and configuration: 13
 14
7.2.2.3 Test procedure 15
• Ensure normal UE procedures1 and user-plane traffic can be handled properly through the SUT. It is 16
recommended to use Section 5.6 Bidirectional throughput in different radio conditions and  Section 6.1 Data 17
Services tests as a benchmark for indicating correct behavior of the SUT. 18
• Use test tool to generate various level of volumetric DoS attack against the MAC address of the O -DU C-Plane 19
o Volumetric tiers: 10Mbps, 100Mbps, 1Gbps 20
o DoS Traffic types: eCPRI real-time ctrl data message over Ethernet 21
o DoS source address: spoofed MAC of O-RU(s), random source MACs or broadcast MAC 22
• Observe the functional and performance impact of the SUT 23
5GC or Emulated 5GC
UE or Emulated UE
C-Plane DoS attack

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 184

O-RAN.TIFG.E2E-Test.0-v02.00
7.2.2.4 Test requirements (expected results) 1
• No meaningful degradation of service availability and performance of the SUT. It is recommended to use 2
Section  5.6 Bidirectional throughput in different radio conditions and Section 6.1 Data Services tests as a 3
benchmark for indicating correct behavior of the SUT. 4
• DoS attacks should be mitigated by O-DU. 5
7.2.3 Near-RT RIC A1 Interface DoS Attack (Network layer) 6
7.2.3.1 Test description & applicability 7
The purpose of the test is to verify that a predefined volumetric DoS attack against N ear-RT RIC A1 interface will not 8
degrade service availability or performance of the SUT in a meaningful way  9
7.2.3.2 Test setup and configuration 10
The test requires easy to access IP address information of the Near-RT RIC’s A1 interface and a routable path to the 11
target from the emulated attacker.  12
Please refer to the diagram below for the test setup and configuration: 13
 14
7.2.3.3 Test procedure 15
• Ensure normal UE procedures1 and user-plane traffic can be handled properly through the SUT. It is 16
recommended to use Section 5.6 Bidirectional throughput in different radio conditions and  Section 6.1 Data 17
Services tests as a benchmark for indicating correct behavior of the SUT. 18
• Use test tool to generate various level of volumetric DoS attack against the IP address of the Near -RT RIC A1 19
interface 20
o Volumetric tiers: 10Mbps, 100Mbps, 1Gbps 21
o DoS Traffic types: generic UDP packets, HTTP/HTTPs REST API calls 22
o DoS source address: spoofed IP of Non-RT RIC, random source IPs or broadcast IP (UDP only) 23
• Observe the functional and performance impact of the SUT 24
5GC or Emulated 5GC
UE or Emulated UE
A1 interface DoS attack

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 185

O-RAN.TIFG.E2E-Test.0-v02.00
7.2.3.4 Test requirements (expected results) 1
• No meaningful degradation of service availability and performance of the SUT. It is recommended to use 2
Section 5.6 Bidirectional throughput in different radio conditions and Section 6.1 Data Services tests as a 3
benchmark for indicating correct behavior of the SUT. DoS attacks should be mitigated by O-RAN system 4
perimeter security or Near-RT RIC. 5
7.2.4 S-Plane PTP Unexpected Input (Network layer) 6
7.2.4.1 Test description & applicability 7
The purpose of the test is to verify that an unexpected (not in-line with protocol specification) input sent towards O-DU 8
S-Plane will not compromise the security of the SUT 9
7.2.4.2 Test setup and configuration 10
The test requires easy to access MAC address information of the O-DU’s open fronthaul interface and L2 connectivity 11
(e.g. over L2 network switching device) to the target from the emulated attacker.  12
Please refer to the diagram below for the test setup and configuration: 13
 14
7.2.4.3 Test procedure 15
• Ensure normal UE procedures1 and user-plane traffic can be handled properly through the SUT. It is 16
recommended to use Section 5.6 Bidirectional throughput in different radio conditions and  Section 6.1 Data 17
Services tests as a benchmark for indicating correct behavior of the SUT. 18
• Use packet capture tool to capture sample of legitimate PTP message sent towards the O-DU S-Plane 19
• Use fuzzing tool to replay the captured PTP message while mutating its content and keeping original 20
source/destination MAC address. Send at least 250,000 iterations of mutated PTP message based on a random 21
seed 22
• Observe the functional and performance impact of the SUT 23
5GC or Emulated 5GC
UE or Emulated UE
S-Plane Unexpected Input

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 186

O-RAN.TIFG.E2E-Test.0-v02.00
7.2.4.4 Test requirements (expected results) 1
• No meaningful degradation of service availability and performance of the SUT. It is recommended to use 2
Section  5.6 Bidirectional throughput in different radio conditions and Section 6.1 Data Services tests as a 3
benchmark for indicating correct behavior of the SUT. 4
7.2.5 C-Plane eCPRI Unexpected Input (Network layer) 5
7.2.5.1 Test description & applicability 6
The purpose of the test is to verify that an unexpected (not in-line with protocol specification) input sent towards O-DU 7
C-Plane will not compromise the security of the SUT 8
7.2.5.2 Test setup and configuration 9
The test requires easy to access MAC address information of the O-DU’s open fronthaul interface and L2 connectivity 10
(e.g. over L2 network switching device) to the target from the emulated attacker.  11
Please refer to the diagram below for the test setup and configuration: 12
 13
7.2.5.3 Test procedure 14
• Ensure normal UE procedures1 and user-plane traffic can be handled properly through the SUT. It is 15
recommended to use Section 5.6 Bidirectional throughput in different radio conditions and  Section 6.1 Data 16
Services tests as a benchmark for indicating correct behavior of the SUT. 17
• Use packet capture tool to capture sample of legitimate eCPRI message sent towards the O -DU C-Plane 18
• Use fuzzing tool to replay the captured eCPRI message while mutating its content and keeping original 19
source/destination MAC address. Send at least 250,000 iterations of mutated eCPRI message based on a random 20
seed 21
• Observe the functional and performance impact of the SUT 22
5GC or Emulated 5GC
UE or Emulated UE
C-Plane Unexpected Input

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 187

O-RAN.TIFG.E2E-Test.0-v02.00
7.2.5.4 Test requirements (expected results) 1
• No meaningful degradation of service availability and performance of the SUT. It is recommended to use 2
Section  5.6 Bidirectional throughput in different radio conditions and Section 6.1 Data Services tests as a 3
benchmark for indicating correct behavior of the SUT. 4
7.2.6 Near-RT RIC A1 Interface Unexpected Input (Network layer) 5
7.2.6.1 Test description & applicability 6
The purpose of the test is to verify that an unexpected (not in-line with protocol specification) input sent towards Near-7
RT RIC A1 interface will not compromise the security of the SUT 8
7.2.6.2 Test setup and configuration 9
The test requires easy to access IP address information of the Near-RT RIC’s A1 interface and a routable path to the 10
target from the emulated attacker.  11
Please refer to the diagram below for the test setup and configuration: 12
 13
7.2.6.3 Test procedure 14
• Ensure normal UE procedures1 and user-plane traffic can be handled properly through the SUT. It is 15
recommended to use Section 5.6 Bidirectional throughput in different radio conditions and  Section 6.1 Data 16
Services tests as a benchmark for indicating correct behavior of the SUT 17
• Use packet capture tool to capture sample of legitimate HTTP/HTTPs REST API message sent towards the 18
Near-RT RIC A1 interface 19
• Use fuzzing tool to replay the captured HTTP/HTTPs REST API message while mutating its content and 20
keeping original source/destination IP/port. Send at least 250,000 iterations of mutated HTTP/HTTPs REST 21
API message based on a random seed 22
• Observe the functional and performance impact of the SUT 23
5GC or Emulated 5GC
UE or Emulated UE
A1 interface Unexpected Input

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 188

O-RAN.TIFG.E2E-Test.0-v02.00
7.2.6.4 Test requirements (expected results) 1
• No meaningful degradation of service availability and performance of the SUT. It is recommended to use 2
Section  5.6 Bidirectional throughput in different radio conditions and Section 6.1 Data Services tests as a 3
benchmark for indicating correct behavior of the SUT. 4
7.2.7 Blind exploitation of well-known vulnerabilities over Near-RT RIC A1 5
interface (Network layer) 6
7.2.7.1 Test description & applicability 7
The purpose of the test is to verify that exploitation attempts of well-known vulnerabilities executed blindly against 8
Near-RT RIC A1 interface will not compromise security of the SUT 9
7.2.7.2 Test setup and configuration 10
The test requires easy to access IP address information of the Near-RT RIC’s A1 interface and a routable path to the 11
target from the emulated attacker.  12
Please refer to the diagram below for the test setup and configuration: 13
 14
7.2.7.3 Test procedure 15
• Ensure normal UE procedures1 and user-plane traffic can be handled properly through the SUT. It is 16
recommended to use Section 5.6 Bidirectional throughput in different radio conditions and  Section 6.1 Data 17
Services tests as a benchmark for indicating correct behavior of the SUT 18
• Make sure that vulnerability scanning tool has up-to-date database of well-known vulnerabilities 19
(signatures/plugins) based on Common Vulnerabilities and Exposures (CVE). Document the actual version of 20
vulnerability database (signatures/plugins) for further reference 21
• Use vulnerability scanning tool to execute a scan against the IP address of the N ear-RT RIC A1 interface. The 22
scan should have the following parameters defined: 23
o TCP Ports: 0-65535 24
o UDP Ports: Top 1000 25
5GC or Emulated 5GC
UE or Emulated UE
Exploitation over A1 interface

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 189

O-RAN.TIFG.E2E-Test.0-v02.00
o Safe Checks: Disabled (to make sure that exploitation attempts of the vulnerabilities will be 1
performed) 2
• Observe the functional and performance impact of the SUT 3
7.2.7.4 Test requirements (expected results) 4
• No meaningful degradation of service availability and performance of the SUT. It is recommended to use 5
Section  5.6 Bidirectional throughput in different radio conditions and Section 6.1 Data Services tests as a 6
benchmark for indicating correct behavior of the SUT. 7
• Identification of well-known vulnerabilities is out-of-scope of the test 8
 9
7.3 O-Cloud resource exhaustion type of security test (Virtualization 10
layer) 11
Because the O-RAN system is running on a virtualized platform, O-Cloud, security attacks against this underlying 12
infrastructure could have profound performance and service impacts on the O -RAN system. 13
Among various types of threats to the O-Cloud infrastructure, resource exhaustion attack, such as the O-Cloud side-14
channel noisy neighbor attack, is one of the easiest to launch and could impact the network slice(s) performance, 15
especially on a small size edge cloud with shared physical hosts/resources. 16
As the WG1 Security Task Group (STG) is continuously working on the O -RAN system security analysis and security 17
requirements specifications, the proposed security test case in this section will be optional for this E2E test spec ification 18
release.  19
As we learn more about the O-RAN system security posture against these types of attacks through the proposed test 20
case, we will re-evaluate its necessity in the following E2E test specification release. 21
7.3.1 O-Cloud side-channel DoS attack 22
7.3.1.1 Test description & applicability 23
The purpose of the test is to verify that a noisy neighbor DoS attack against O -Cloud for resource starvation will not 24
degrade service availability or performance of the O-RAN system served by the network slice under test. 25
7.3.1.2 Test setup and configuration 26
The test requires access to the O-Cloud platform hosting the network slice(s) of the O-RAN system.  27
Please refer to the diagram below for the test setup and configuration: 28

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 190

O-RAN.TIFG.E2E-Test.0-v02.00
 1
Noisy Neighbor VNF(s) can be deployed into an existing slice or a new slice of the shared resources with the existing 2
slice under test 3
7.3.1.3 Test procedure 4
• Ensure normal UE procedures1 and user-plane traffic can be handled properly through the E2E O-RAN system 5
served by the O-Cloud RAN slice under test. It is recommended to use Section 5.6 Bidirectional throughput in 6
different radio conditions and  Section 6.1 Data Services tests as a benchmark for indicating correct behavior 7
of the SUT 8
• Use test tool (through O-Cloud MANO) to instantiate noisy neighbor VNFs  9
o Noisy Neighbor tenant: existing slice or a new slice of the shared resources with the existing slice  10
• Observe the functional and performance impact of the E2E O-RAN system served by the O-Cloud RAN slice 11
under test 12
7.3.1.4 Test requirements (expected results) 13
• No degradation of service availability and performance of the E2E O-RAN system served by the network slice 14
under test. It is recommended to use Section 5.6 Bidirectional throughput in different radio conditions and  15
Section 6.1 Data Services tests as a benchmark for indicating correct behavior of the SUT 16
• The Noisy Neighbor attack should be properly logged and alerted by the O-Cloud 17
 18
 19
5GC or Emulated 5GC
UE or Emulated UE
O-Cloud
Side-channel DoS attack

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 191

O-RAN.TIFG.E2E-Test.0-v02.00
 Load and Stress Tests 1
This chapter describes the tests evaluating and assessing the load and stress tests of the radio access network from a 2
network end-to-end perspective. The focus of the testing is on the tolerability of the eNB/gNB under load based on 3
3GPP and O-RAN specifications. 4
 5
The eNB/gNB should have an ability to process the various traffic patterns which could happen in a real field. Load and 6
stress tests are used to evaluate the tolerability of eNB/gNB against load. Load and stress tests are a test methodology to 7
be performed in the laboratory to simulate the actual traffic load that eNB/gNB are subjected to in a real field. Load and 8
stress tests help to detect performance-related problems of the entire system, which are "less reproducible" and difficult 9
to solve in general. In some cases, even problems that could not be found by functional testing are discovered. Load and 10
stress tests in addition to Functional testing will help to ensure the final quality of the eNB/gNB products. It will lead to 11
improve the overall quality of the system, which in turn will improve the user experience.  12
8.1 Simultaneous RRC_CONNECTED UEs 13
8.1.1 Test description and applicability 14
The purpose of this test is to connect multiple UEs to the eNB/gNB and measure the maximum number of UEs that can 15
be simultaneously maintained in RRC_CONNECTED state. By connecting multiple UEs at the same time, a basic 16
eNB/gNB capacity is verified. This test is valid with either LTE or 5G NSA/SA. The test procedure is to stack the 17
number of connected UEs to the eNB/gNB one by one and check the maximum number of simultaneous connected 18
UEs. To achieve this test, the connected UE needs to transmit a minimum size of U-Plane packet periodically such as 19
ping to keep the RRC_CONNECTED state. The transmit interval of the packets should be shorter than RRC inactivity 20
timer value in eNB/gNB. Figure 8-1 Simultaneous RRC_CONNECTED UEsillustrates how the test works.  21
 22
  23
Figure 8-1 Simultaneous RRC_CONNECTED UEs 24
UE s
1,200
1,000
800
600
400
200
Time
Call interval Connection holding time
ReleaseRRC_CONNECTED
ReleaseRRC_CONNECTED
ReleaseRRC_CONNECTED
ReleaseRRC_CONNECTED
ReleaseRRC_CONNECTED
Call attempt
Release
Failure
Call attempt
Call attempt
Call attempt
Call attempt
Call attempt
Failure
Number of Maximum connected UE

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 192

O-RAN.TIFG.E2E-Test.0-v02.00
8.1.2 Test setup and configuration 1
The test setup is a single cell scenario (i.e. isolated cell without any inter -cell interference – see Section 3.7) with 2
multiple UEs (emulated) placed under excellent radio conditions as defined in Section 3.6, using LTE RSRP (for LTE) 3
or 5G SS-RSRP (for 5G NSA/SA) as the metric. 4
Since this test scenario requires a large number of UE connections, using an equipment that emulates a large number of 5
UEs and performing the test is recommended. 6
This test is primarily C-Plane (i.e. RRC and RLC) capacity benchmarking and SUT capacity should be limited by O -7
DU and O-CU capacities, not O-RU. If the O-DU and O-CU are not fully utilized by the configured RF cell and 8
associated UEs, the optional OFH and/or F1 interface and test equipment as described in Section 3.1.1 should be used in 9
addition to the configured RF cell and UEs to add more UE traffic directly at the interfaces to O-DU and O-CU 10
comprising the SUT. 11
Test configuration: The test configuration is not specified. The utilized test configuration (connection diagram between 12
SUT and test equipment, parameters) should be recorded in the test report.  13
Laboratory setup : The radio conditions experienced by the UE can be modified using a variable attenua tor inserted 14
between the antenna connectors (if available) of the O -RU and the UE, or appropriately emulated using a UE emulator.  15
The test environment should be setup to achieve excellent radio conditions (LTE RSRP (for LTE) or 5G SS -RSRP (for 16
5G NSA/SA) a s defined in Section 3.6) for the UE, but t he minimum coupling loss (see  Section 3.6) should not be 17
exceeded.  18
8.1.3 Test Procedure 19
The test steps below are applicable for either LTE or 5G NSA/SA: 20
1. The test setup is configured according to the test configuration. The test configuration should be recorded in the test 21
report. The serving cell under test is activated and unloaded. All other cells are powered off. 22
2. The UEs (emulated UE) are placed under excellent radio conditions (Cell centre close to radiated eNB/gNB 23
Antenna) as defined by LTE RSRP (for LTE) or 5G SS-RSRP (for 5G NSA/SA) in Section 3.6.  24
3. The End-to-end setup shall be operational for LTE or 5G NSA/SA as applicable for the test scenario, and there 25
should not be any connectivity issues.  26
4. Required performance data (incl. the signalling and control data) as specified in the “Test requirements” Section 27
below is measured and captured at the UE(s) and eNB/gNB side using logging/measurement too ls. 28
5. "Power ON" the UEs one by one, connect them to the LTE or 5G NSA/SA cell, and confirm that they are in the 29
RRC_CONNECTED state normally. The “Power ON” intervals may be better to longer than the attach latency 30
described in Chapter4. Increase the number of UEs until the newly powered UE fails to connect to LTE or 5G 31
NSA/SA cell. The UE in  RRC_CONNECTED state, to keep their RRC connection, should send some data packets 32
like Ping. At this time, it is necessary to set the connection holding time at l east 3 minutes in Figure 8 -1 to be 33
sufficiently long. 34
6. Check the measured RRC Connected UEs for the SUT and if the measured value is not consistent with expectations 35
for the entire SUT (O-DU/O-CU) being fully loaded, use the optional OFH and/or F1 interface and test equipment 36
(see Section 3.1.1) to add more UEs maintaining RRC_CONNECTED state until the entire SUT is fully utilized 37
(e.g. >80%) during execution of the test case procedure. After the holding time, check the maximum number of 38
UEs that were in the state of RRC_CONNECTED simultaneously. 39
7. Lost connections shall be re-established automatically to maximize number of RRC Connected UEs. 40
8. Stop and save the test logs. Check the log to make sure that the test runs successfully and that no unexpected 41
behavior such as unexpected call release is recorded. The logs should be captured and kept for test result reference 42
and measurements. 43

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 193

O-RAN.TIFG.E2E-Test.0-v02.00
8.1.4 Test requirements (expected results) 1
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 2
should be captured and reported in the test report for performance assessment. 3
• Radio parameters such as RSRP, RSRQ 4
• KPIs mentioned in Table 8-1 5
• SUT load/capacity/performance related KPIs (e.g. CPU and MEMORY utilization) if any  6
Validate the successful procedures from the collected logs. Check the maximum number of UE connections.  For the 7
UE(s) which call loss has occurred i n this test more than a certain percentage ( e.g. 2%), the validity of the cause of the 8
call loss shall be confirmed. 9
Table 8-1 Maximum Number of simultaneous RRC_CONNECTED UEs 10
 Maximum Number of simultaneous
RRC_CONNECTED UEs
LTE
NR（NSA）
NR（SA）
 11
8.2 Benchmark of UE State Transition 12
8.2.1 Test description and applicability 13
 14
The purpose of this test is to verify the benchmark value of the number of UE state transitions that can be processed per 15
unit time by connecting multiple UEs to an eNB/gNB. 16
As shown in Figure 8-2, a certain number of UEs (e.g. 100 UEs, 200 UEs … etc. Attach them at reasonable stack speed 17
one by one, 10 UEs per second for example.) repeatedly performs UE state transitions (transition RRC_IDLE to/from 18
RRC_CONNECTED) for a certain period of time (X seconds). The number of UE calls is increased by a fixed number 19
(e.g. 100 UEs). If a finer resolution is required, it is increased by 10 UEs. The number of UE calls in the X seconds 20
interval is incrementally increased, and the test is performed until a fail to connect occurs as shown in the example of 21
the right end of Figure 8-2. By connecting multiple UEs at the same time, the basic eNB/gNB control plane processing 22
capability is verified. This test is valid with either LTE or 5G NSA/SA. 23
The test procedure is to gradually increase the number of UEs connected to the eNB/gNB and check the maximum 24
number of simultaneously connected UEs. In order to transition the RRC_CONNECTED state and the RRC_IDLE state 25
of the UE alternately and continuously, the UE needs to transmit a minimum size of U-Plane packet periodically such as 26
ping. The transmit interval of the packets should be longer than RRC inactivity timer value in eNB/gNB. By repeating 27
state transitions by many UEs, test a limit of eNB/gNB processing capacity.  In addition, by repeating X seconds the call 28
generation and call release, confirm that there are no problems such as processing delays and memory release leaks 29
shall be confirmed. 30
 31

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 194

O-RAN.TIFG.E2E-Test.0-v02.00
  1
Figure 8-2 Benchmark of UE State Transition 2
8.2.2 Test setup and configuration 3
The test setup is a single cell scenario (i.e. isolated cell without any inter -cell interference – see Section 3.7) with UEs 4
(emulated) placed under excellent radio conditions as defined in Section 3.6, using LTE RSRP (for LTE) or 5G SS-5
RSRP (for 5G NSA/SA) as the metric.   6
Since this test scenario requires a large number of UE connections, using an equipment that emulates a large number of 7
UEs and performing the test is recommended. 8
This test is primarily C-Plane (i.e. RRC and RLC) performance benchmarking and SUT performance should be limited 9
by O-DU and O-CU performance, not O-RU. If the O-DU and O-CU are not fully loaded to expectations using the 10
configured RF cell and associated UEs, the optional OFH and/or F1 interface and test equipment as described in Section 11
3.1.1 should be used in addition to the configured RF cell and UEs to add more UE traffic directly at the interfaces to O -12
DU and O-CU comprising the SUT. 13
Test configuration: The test configuration is not specified. The utilized test configuration (connection diagram between 14
SUT and test equipment, parameters) should be recorded in the test report.  15
Laboratory setup : The radio conditions experienced by the UE can be modified using a variable attenuator inserted 16
between the antenna connectors (if available) of the O -RU and the UE, or appropriately emulated using a UE emulator.  17
The test environment should be setup to achieve excellent radio conditions (LTE RSRP (for LTE) or 5G S S-RSRP (for 18
5G NSA/SA) as defined in Section 3.6) for the UE, but t he minimum coupling loss (see  Section 3.6) should not be 19
exceeded.  20
8.2.3 Test Procedure 21
The test steps below are applicable for either LTE or 5G NSA/SA: 22
1. The test setup is configured according to the test configuration. The test configurati on should be recorded in the test 23
report. The serving cell under test is activated and unloaded. All other cells are powered off.  24
2. The UEs (emulated UE) are placed under excellent radio conditions (Cell centre close to radiated eNB/gNB 25
Antenna) as defined by LTE RSRP (for LTE) or 5G SS-RSRP (for 5G NSA/SA) in Section 3.6.  26
3. The End-to-end setup shall be operational for LTE or 5G NSA/SA as applicable for the test scenario, and there 27
should not be any connectivity issues.  28
4. Required performance data (incl. the signalling and control data) as specified in the “Test requirements” Section 29
below is measured and captured at the UE(s) and eNB/gNB side using logging/measurement tools.  30
UE s
500
400
300
200
100
Time
X Sec X SecX SecX Sec

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 195

O-RAN.TIFG.E2E-Test.0-v02.00
5. “Power ON” the UEs (emulated UE) to attach to the LTE or 5G NSA/SA cell one by one at reasonable stack speed, 1
10 UEs per second for example. Wait for a successful attach. Each UEs state transition can be checked by the state 2
of receiving control plane messages. 3
6. Ensure that all UEs perform state transitions at regular intervals for X seconds  (at least 3minutes) and verify that 4
they are processed correctly. All UEs are then "Power off". It should be checked whether there are any UEs that 5
have failed to make the RRC state transition at the eNB/gNB. 6
7. Check the number of UEs that can be processed per unit time. Increase the number of UEs and repeat step 5 -6 in 7
the above until processes such as state transition is no longer working prope rly to identify the upper limit of 8
processing capacity. The number of UE calls is increased by a fixed number (100 UEs). If a finer resolution is 9
required, it is increased by 10 UEs. 10
8. Check the maximum measured RRC Connected UEs for the SUT in step 5-7 and if the measured value is not 11
consistent with expectations for the entire SUT (O-DU/O-CU) being fully loaded, use the optional OFH and/or F1 12
interface and test equipment (see Section 3.1.1) to add more UEs and RRC Connected state transitions until the 13
entire SUT is fully loaded (e.g. >80%) during execution of the test case procedure. 14
9. Stop and save the test logs. Check the log to make sure that the test runs successfully and that no unexpected 15
behavior such as unexpected call release is recorded. The logs should be captured and kept for test result reference 16
and measurements.  17
8.2.4 Test requirements (expected results) 18
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 19
should be captured and reported in the test report for performance assessment. 20
• Radio parameters such as RSRP, RSRQ 21
• KPIs mentioned in Table 8-2 22
• SUT load/capacity/performance related KPIs (e.g. CPU and MEMORY utilization) if any  23
Validate the successful procedures from the collected logs. Check the rate of UE state transition that can be processed per 24
unit time. For the UE(s) in which call loss has occurred in a X seconds test more than a certain percentage (e.g. 2%), the 25
validity of the cause of the call loss shall be confirmed. 26
Table 8-2 Maximum Rate of UE State Transition Benchmark 27
 Maximum Rate of UE State Transition
(per second)
Number of UEs
LTE
NR（NSA）
NR（SA）
 28
8.3 Traffic Load Testing 29
8.3.1 Test description and applicability 30
The purpose of this test is to check the stability of the eNB/gNB under load, with a large number of UEs sending and 31
receiving user data. By connecting a large number of UEs at the same time, the eNB/gNB processing capacity and 32
stability are verified. The maximum cell throughput is discussed in Chapter 5 and is not a scope of this Chapter. This 33
test is valid with either LTE or 5G SA. 34

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 196

O-RAN.TIFG.E2E-Test.0-v02.00
The test procedure is to connect X UEs (e.g. 10UEs) per second to the eNB/gNB with UDP download/upload traffic 1
until reach N UEs (e.g. 1,000UEs) connected. Then disconnect X UEs per second and connected X new UEs to the 2
eNB/gNB with UDP download/upload traffic. It is to continue disconnecting and connecting UEs near the maximum 3
number of UEs to verify a stability of eNB/gNB.  It is recommended that the total UDP traffic be less than the 4
maximum cell throughput, as maximum cell throughput is out of scope for this test.  5
In this test, the number of UEs is assumed to be large, which can reach 1000. For the values of X and N, the test results 6
in the previous Section 8.1 and 8.2 may be helpful. 7
8.3.2 Test setup and configuration 8
The test setup is a single cell scenario (i.e. isolated cell without any inter -cell interference – see Section 3.7) with UEs 9
(emulated) placed under excellent radio conditions as defined in Section 3.6, using LTE RSRP (for LTE) or 5G SS-10
RSRP (for 5G SA) as the metric.   11
Since this test scenario requires a large number of UE connections, using an equipment that emulates a large number of 12
UEs and performing the test is recommended. 13
The performance of the SUT 3GPP stack components in all of the O-RU, O-DU, and O-CU sub-nodes should be 14
measured while all sub-nodes are under high load. To achieve this it may be recommended to run an additional tes t 15
execution step adding additional traffic utilizing the optional OFH and/or F1 interface and test equipment as described 16
in Section 3.1.1 in addition to the traffic applied over the configured RF cell. 17
Test configuration: The test configuration is not specified. The utilized test configuration (connection diagram between 18
SUT and test equipment, parameters) should be recorded in the test report.  19
Laboratory setup : The radio conditions experienced by the UE can be modified using a variable attenuator inserte d 20
between the antenna connectors (if available) of the O -RU and the UE, or appropriately emulated using a UE emulator.  21
The test environment should be setup to achieve excellent radio conditions (LTE RSRP (for LTE) or 5G SS -RSRP (for 22
5G SA) as defined in Section 3.6) for the UE, but the minimum coupling loss (see Section 3.6) should not be exceeded.  23
 24
8.3.3 Test Procedure 25
The test steps below are applicable for either LTE or 5G SA: 26
1. The test setup is configured according to the test configuration. The test configuration should be recorded in the test 27
report. The serving cell under test is activated and unloaded. All other cells are powered off.  28
2. The UEs (emulated UE) are placed under excellent radio conditions (Cell centre close to radiated eNB/gNB 29
Antenna) as defined by LTE RSRP (for LTE) or 5G SS-RSRP (for 5G SA) in Section 3.6.  30
3. The End-to-end setup shall be operational for LTE or 5G SA as applicable for the test scenario, and there should 31
not be any connectivity issues.  32
4. Required performance data (incl. the signalling and control data) as specified in the “Test requirements” Section 33
below is measured and captured at the UE(s) and eNB/gNB side using log ging/measurement tools.  34
5. Keep X UEs per second to access the eNB/gNB by signalling, initiate UDP upload/download for each UE after UEs 35
get access. In this step and later steps, lost connections shall be re-established automatically to maintain number of 36
RRC Connected UEs. 37
6. Continue Step 5 until total UEs reach maximum active number of UEs N per cell. 38
7. Release X UEs per second and keep X UEs per second to access the eNB/gNB and initiate UDP upload/download 39
for each new access UE. 40
8. Repeat Step 7 for at least 5 minutes and record the KPI values and RRC access success rate.  41

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 197

O-RAN.TIFG.E2E-Test.0-v02.00
9. Release all UEs, keep X UEs per second to access the eNB/gNB and initiate UDP upload/download for each new 1
access UE until total UEs reach maximum active number of UEs N per cell. Keep for at least 5 minutes testing. 2
Record KPI values and packet error rate. 3
10. Stop and save the test logs. Check the log to make sure that the test runs successfully and that no unexpected 4
behavior such as unexpected call release is recorded. The logs should be captured and kept for test result reference 5
and measurements. 6
11. If any of measured RLC, RRC, and PDCP KPIs are not consistent with expectations for a highly loaded O -DU and 7
O-CU, repeat steps 4 through 10 with similar incremental traffic being appli ed using the optional OFH and/or F1 8
interface and test equipment (see Section 3.1.1) in addition to the traffic described in steps 5 through 9 being 9
applied at the RF interface. 10
 11
8.3.4 Test requirements (expected results) 12
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 13
should be captured and reported in the test report for performance assessment. 14
• Radio parameters such as RSRP, RSRQ 15
• N (maximum active number of UEs per cell), X (UEs per second) 16
• KPIs mentioned in Table 8-3 and Table 8-4 17
• SUT load/capacity/performance related KPIs (e.g. CPU and MEMORY utilization) if any  18
 19
Validate the successful procedures from the collected logs. Check the rate of UE state transition that can be processed per 20
unit time. For the UE(s) in which call loss has occurred, the validity of the cause of the call loss shall be confirmed.  21
Table 8-3 RRC Access Success Rate  22
 RRC Access Success Rate
LTE
NR（SA）
 23
Table 8-4 Packet Error Rate 24
 Packet Error Rate
Uplink Downlink
LTE
NR（SA）
 25
 26
8.4 Traffic Model Testing 27
8.4.1 Test description and applicability 28
The purpose of this test is to check the performance under varying load for the eNB/gNB by generating various UE 29
behavior such as Attach/Detach, Registration/Deregistration, Data upload/download, and Mobility based on the traffic 30
model to load the eNB/gNB. Traffic models can vary depending on factors such as geographic characteristics of the 31
target area, UE type, number of UEs, UE mobility, time of day, and so on. It is important to consider the maximu m 32

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 198

O-RAN.TIFG.E2E-Test.0-v02.00
number of UEs and the behavior that it is expected in the field, and develop the appropriate traffic model. This test is 1
valid for either LTE or 5G NSA/SA. The test procedure is to cause various UE behavior such as Attach/Detach, 2
Registration/Deregistration, Data upload/download, and Mobility to the eNB/gNB. Repeat various scenarios based on 3
the traffic model. By processing complex combinations of scenario, the processing capacity of the eNB/gNB is 4
confirmed. Review the KPIs for each scenario to see if there are any issues that depend on a particular scenario. 5
 6
8.4.1.1 Traffic model 7
The traffic model is a model of the set of scenarios that an eNB/gNB is expected to undergo in the actual field. The test 8
items in Chapter 4 and 6 can be part of the traffic model scenario as they are or by simplifying them. Each scenario may 9
be repeated continuously at a certain frequency, or many may be executed simultaneously. The pattern of possible 10
scenarios may be differs depending on the characteristics of deployment area in which the eNB/gNB is deployed and 11
can be categorized into several typical types. It is necessary to have eNB/gNB capabilities that are appropriate for the 12
deployment area. See Table 8.4-1 for examples of typical traffic model types. 13
Table 8-5 Example of typical traffic model types 14
No. Traffic Model Type  Major Characteristics
1 Indoor Hotspot  The number of UEs may be very large, not much fluctuation in
the number of UEs (less handover).
 High traffic volume in data transmission and reception.
2 Dense Urban  The number of UEs is very large.
 High traffic volume in data transmission and reception.
 High in mobility.
 Less active at night
3 Office area  The number of UEs is large.
 High traffic volume in data transmission and reception.
 High in mobility. Slower speed.
 Less active at night
4 Residential area  The number of UEs is small.
 Data traffic volume is low during daytime, may peaks at night in
area where Wi-Fi is not widespread.
 Low in mobility
 Somehow active during daytime
5 Rural  The number of UEs is very small.
 Low traffic volume in data.
 Low in mobility.
6 High Speed
(Train Station.
Along the railroad
tracks.)
 The number of UEs may be temporarily concentrated in large
numbers. Burst when a train arrives or departs, or when a train
passes by.
 As a train passes, many UEs pass at high speed at the same time.
 15
The scenarios included in each traffic model type and the frequency (per UE per hour) should be appropriately 16
determined considering the characteristics of that traffic model type. 17
In this testing, one or more traffic models can be used to evaluate the capacity , stability and load durability of the 18
eNB/gNB. 19
 20
Table 8-6 shows an example of traffic model. Traffic model defines a large number of UEs and scenarios, and this 21
model can present traffic observed in real field. 22
 23
Table 8-6 Example of Traffic Model 24

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 199

O-RAN.TIFG.E2E-Test.0-v02.00
Total number of UEs in a cell  1,000
(The numbers are just example)  RAT
No Scenario (baseline) LTE 5G
NSA
5G
SA
Number
of UEs
Attempts
(per UE
per hour)
Description
1 Attach 〇 〇 N/A 50 0.50 Power ON
2 Detach 〇 〇 N/A 50 0.50 Power OFF
3 Tracking area update 〇 〇 N/A 100 1.00
4 Registration N/A N/A 〇 50 0.50 Power ON
5 Deregistration N/A N/A 〇 50 0.50 Power OFF
6 Paging 〇 〇 〇 200 1.00 Total of paging in this
model.
7 Data
upload/download
Stationary 〇 〇 〇 100 1.00 One RRC_CONNECTED
period is defined as "1
attempt". At least
RRC_CONNECTED
period and avg. DL/UL bit
rate should be specified. If
mobility is involved, one
hand over per attempt
assumed, mobility type and
when to move should be
specified.
With
Mobility *)
5 0.50
Additional scenario if any
8 Web Browsing Stationary 〇 〇 〇 120 2.00 At least hold time,
RRC_CONNECTED
period/interval/number of
times per call, and avg.
DL/UL Mbytes per
RRC_CONNECTED
period should be specified.
If mobility is involved, one
hand over per attempt
assumed, mobility type and
when to move should be
specified.
With
Mobility *)
〇 〇 〇 6 0.50
9 File
upload/download
Stationary 〇 〇 〇 30 1.50 At least file size,
upload/download and hold
time should be specified. If
mobility is involved, one
hand over per attempt
assumed, mobility type and
when to move should be
specified.
With
Mobility *)
〇 〇 〇 2 0.50
10 Voice Service Stationary 〇 〇 〇 80 0.20 50% of calls Mobile
Terminated, 50% of calls
Mobile Originated
assumed. At least avg.
DL/UL bit rate and avg.
hold time should be
specified. If mobility is
involved, one hand over per
attempt assumed, mobility
type and when to move
should be specified.
With
Mobility *)
〇 〇 〇 4 0.50
11 Video Streaming Stationary 〇 〇 〇 40 1.00 At least avg. DL/UL bit rate
and avg. hold time should
be specified. With mobility,
one handover per one call
attempt. If mobility is
involved, one hand over per
attempt assumed, mobility
type and when to move
should be specified.
With
Mobility *)
〇 〇 〇 2 0.50
*) e.g. Intra-cell handover. Other mobilities can be included if multiple cells are configured.  1
 2

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 200

O-RAN.TIFG.E2E-Test.0-v02.00
The performance of the SUT 3GPP stack components in all of the O-RU, O-DU, and O-CU sub-nodes should be 1
measured while all sub-nodes are under high load. To achieve this it may be recommended to run an additional test 2
execution step adding additional traffic utilizing the optional OFH and/or F1 interface and test equipment as described 3
in Section 3.1.1 in addition to the traffic applied over the configured RF cell. 4
 5
 6
8.4.2 Test setup and configuration 7
The test setup is a single cell scenario (i.e. isolated cell without any inter -cell interference – see Section 3.7) with UEs 8
(emulated) placed under excellent radio conditions as defined in Section 3.6, using LTE RSRP (for LTE) or 5G SS-9
RSRP (for 5G NSA/SA) as the metric. When inter-eNB/gNB mobility is included in the traffic model, the target/source 10
cells and UE behavior/sequence should be simulated by the UE emulator and Core emulator without using additional 11
cells.  12
Since this test scenario requires a large number of UE connections, using an equipment that emulates a large number of 13
UEs and performing the test is recommended. 14
Test configuration: The test configuration is not specified. The utilized test configuration (connection diagram between 15
SUT and test equipment, parameters) should be recorded in the test report.  16
Laboratory setup : The radio conditions experienced by the UE can be modified using a variable attenuator/fading 17
generator inserted between the antenna connectors (if available) of the O-RU and the UE, or appropriately emulated using 18
a UE emulator. The test environment should be setup to achieve excellent radio conditions (LTE RSRP (for LTE) or 5G 19
SS-RSRP (for 5G NSA/SA) as defined in Section 3.6) for the UE, but the minimum coupling loss (see Section 3.6) should 20
not be exceeded.  21
8.4.3 Test Procedure 22
The test steps below are applicable for either LTE or 5G NSA and SA: 23
1. The test setup is configured according to the test configuration. The test configuration should be recorded in the test 24
report. The serving cell under test is activated and unloaded. All other cells are powered off.  25
2. The UEs (emulated UE) are placed under excellent radio conditions (Cell centre close to radiated eNB/gNB 26
Antenna) as defined by LTE RSRP (for LTE) or 5G SS-RSRP (for 5G NSA/SA) in Section 3.6.  27
3. The End-to-end setup shall be operational for LTE or 5G NSA/SA as applicable for the  test scenario, and there 28
should not be any connectivity issues.  29
4. Required performance data (incl. the signalling and control data) as specified in the “Test requirements” Sect ion 30
below is measured and captured at the UE(s) and eNB/gNB side using logging/measurement tools.  31
5. Start the operation of each UE according to the traffic model.  32
6. Check KPIs for each scenario included in the traffic model during continuous operation (severa l tens of minutes to 33
several hours). 34
7. Stop and save the test logs. Check the log to make sure that the test runs successfully and that no unexpected 35
behavior such as unexpected call release is recorded. The logs should be captured and kept for test result reference 36
and measurements. 37
8. If measured RLC, RRC, and and/or PDCP KPIs are not consistent with expectations for a highly loaded O -DU and 38
O-CU, repeat steps 4 through 7 with similar incremental traffic being applied  using the optional OFH and/or F1 39
interface and test equipment (see Section 3.1.1) as was used for Section 8.3.3 (Traffic Load Testing) in addition to 40
the modelled traffic applied at the RF interface. 41

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 201

O-RAN.TIFG.E2E-Test.0-v02.00
8.4.4 Test requirements (expected results) 1
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 2
should be captured and reported in the test report for performance assessment. 3
• Radio parameters such as RSRP, RSRQ 4
• Traffic model 5
• KPIs mentioned in Table 8-7 6
• SUT load/capacity/performance related KPIs (e.g. CPU and MEMORY utilization) if any 7
If a scenario in the Traffic Model is the same as the scenario described in Chapter 4  and 6, it is recommended to select 8
and evaluate some of the corresponding KPIs described in Chapter 4 and 6. 9
 10
Validate the successful procedures from the collected logs. It is desirable that the test duration shall be more than several 11
tens of minutes to several hours. Check the completion rate of each scenario and the adequacy of the completion rate for 12
each scenario shall be confirmed.  For the UE(s) in which call loss has occurred, the validity of the cause of the call loss 13
shall be confirmed. 14
Table 8-7 KPIs in traffic model scenario (example)  15
 RAT Success
Rate (%)
Optional KPI if any
No. Scenario(baseline) LTE 5G
NSA
5G SA
1 Attach 〇 〇 N/A  e.g.  SgNB addition
Success rate in NSA
2 Detach 〇 〇 N/A
3 Tracking area update 〇 〇 N/A
4 Registration N/A N/A 〇
5 Deregistration N/A N/A 〇
6 Paging 〇 〇 〇
7 Data
upload/download
Stationary 〇 〇 〇  e.g.  Avg. UL/DL
throughput
With Mobility  e.g.  Avg. UL/DL
throughput
Additional scenario if any
8 Web Browsing Stationary 〇 〇 〇  e.g.  Avg. UL/DL
throughput
With Mobility 〇 〇 〇  e.g.  Avg. UL/DL
throughput
9 File
upload/download
Stationary 〇 〇 〇  e.g.  Avg. UL/DL
throughput
With Mobility 〇 〇 〇  e.g.  Avg. UL/DL
throughput
10 Voice Service Stationary 〇 〇 〇  e.g.  Packet loss rate
With Mobility 〇 〇 〇  e.g.  Packet loss rate
11 Video Streaming Stationary 〇 〇 〇  e.g.  Avg. UL/DL
throughput
With Mobility 〇 〇 〇  e.g.  Avg. UL/DL
throughput
 16
 17

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 202

O-RAN.TIFG.E2E-Test.0-v02.00
8.5 Long hours stability Testing 1
8.5.1 Test description and applicability 2
The purpose of this test is to conduct a load test using the traffic model described in Section 8.4.1.1 for a long period 3
(e.g. 24 to 48 hours) to confirm the continuous operation for a long period of time. This test is valid with either LTE or 4
5G NSA and SA. The test procedure is to cause various UE behavior such as Attach/Detach, 5
Registration/Deregistration, and Mobility based on the traffic model to the eNB/gNB, and maintains its performance for 6
a long time. Verify the KPIs for each scenario by repeating call generation and call release for various traffic. 7
8.5.2 Test setup and configuration 8
The test setup is a single RF cell scenario (i.e. isolated cell without any inter -cell interference – see Section 3.7) with 9
UEs(emulated) placed under excellent radio conditions as defined in Section 3.6, using LTE RSRP (for LTE) or 5G SS-10
RSRP (for 5G NSA/SA) as the metric.   11
Since this test scenario requires a large number of UE connections, using an equipment that emulates a large number of 12
UEs and performing the test is recommended.  13
The extended length performance and stability of the SUT 3GPP stack components in all of the O -RU, O-DU, and O-14
CU sub-nodes should be measured while all sub-nodes are under high load. To achieve this it may be recommended to 15
apply additional traffic utilizing the optional OFH and/or F1 interface and test equipment as described in Section 3.1.1 16
in addition to the traffic applied at the configured RF cell. 17
 18
Test configuration: The test configuration is not specified. The utilized test configuration (connection diagram between 19
SUT and test equipment, parameters) should be recorded in the test report.  20
Laboratory setup : The radio conditions experienced by the UE can be modified using a variable attenuator inserted 21
between the antenna conn ectors (if available) of the O -RU and the UE, or appropriately emulated using a UE emulator.  22
The test environment should be setup to achieve excellent radio conditions (LTE RSRP (for LTE) or 5G SS -RSRP (for 23
5G NSA/SA) as defined in Section 3.6) for the UE, but t he minimum coupling loss (see  Section 3.6) should not be 24
exceeded.  25
8.5.3 Test Procedure 26
The test steps below are applicable for either LTE or 5G NSA and SA: 27
1. The test setup is configured according to the test configuration. The test configuration should be recorded in the test 28
report. The serving cell under test is activated and unloaded. All other RF cells are powered off. 29
2. If during execution of Section 8.3.3 (Traffic Load Testing) or Section 8.4.3 (Traffic Model Testing) incremental 30
traffic was applied using the optional OFH and/or F1 interface and test equipment (see Section 3.1.1) in addition to 31
the traffic being applied at the RF interface, the same level of traffic should be applied again using the  optional 32
OFH and/or F1 interface and test equipment to ensure O-DU and O-CU exposed to adequate “base load” before 33
execution of Step 6.  34
3. The UE (emulated UE) is placed under excellent radio conditions (Cell centre close to radiated eNB/gNB Antenna) 35
as defined by LTE RSRP (for LTE) or 5G SS-RSRP (for 5G NSA/SA) in Section 3.6.  36
4. The End-to-end setup shall be operational for LTE or 5G NSA/SA as applicable for the test scenario, and there 37
should not be any connectivity issues.  38

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 203

O-RAN.TIFG.E2E-Test.0-v02.00
5.  Required performance data (incl. the signalling and control data) as specified in the “Test requirements” Section 1
below is measured and captured at the UE(s) and eNB/gNB side using logging/measurement tools  (optional). 2
6. Start the operation of each UE according to the traffic model. Keep the operation for a long period of several tens of 3
hours. 4
7. Monitor KPIs for each scenario included in the traffic model during this test. A scenario is considered successful if 5
the KPIs for each scenario is equivalent to the traffic model testing case. if not, it is a failure. 6
8. Stop and save the test logs. Check the log to make sure that the test runs successfully and that no unexpected 7
behavior such as unexpected call release is recorded. The logs should be captured and kept for test result reference 8
and measurements (optional). 9
 10
8.5.4 Test requirements (expected results) 11
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 12
should be captured and reported in the test report for performance assessment. 13
• Radio parameters such as RSRP, RSRQ 14
• Traffic Model 15
• KPIs mentioned in Table 8-8, Table 8-7. 16
• SUT load/capacity/performance related KPIs (e.g. CPU and MEMORY utilization) if any  17
Validate the successful procedures from the collected logs. Normally this test should be a minimum of  12 hours with  60 18
hours as a recommended maximum. Check the completion rate of each scenario and the adequacy of the completion rate 19
for each scenario shall be co nfirmed. For the UE(s) in which call loss has occurred, the validity of the cause of the call 20
loss shall be confirmed. 21
Table 8-8 Successfully tested hours 22
 Successfully tested hours (hours)
LTE
NR(NSA)
NR（SA）
 23
8.6 Multi-cell Testing 24
8.6.1 Test description and applicability 25
The purpose of this test is to test the eNB/gNB's processing capacity by applying a load to some or all of its cells 26
simultaneously based on the traffic model. Inter-cell interference is out of scope, so the DUT cells should be configured 27
there is no inter-cell interference. This test is valid with either LTE or 5G NSA and SA. The test procedure is to cause 28
various UE behavior such as Attach/Detach, Registration/Deregistration, and Mobility based on the Traffic Model for 29
some or all of the Cells in the eNB/gNB. Verify the KPIs for each scenario in each cell by repeating call generation and 30
call release for various traffic (scenarios). This test is a multi-cell test of Section 8.4 and is intended to increase the load 31
on eNB/gNB. In this testing, various mobilities can be included among DUT cells. 32
 33
The performance of the SUT 3GPP stack components in all of the O-RU, O-DU, and O-CU sub-nodes should be 34
measured while all sub-nodes are under high load. To achieve this it may be recommended to run an additional test 35
execution step adding additional traffic utilizing the optional OFH and/or F1 interface and test equipment as described 36
in Section 3.1.1 in addition to the traffic applied over the two or more configured RF cells. 37

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 204

O-RAN.TIFG.E2E-Test.0-v02.00
 1
 2
8.6.2 Test setup and configuration 3
The test setup is multiple cells scenario (i.e. isolated cell without any inter -cell interference – see Section 3.7) with UEs 4
(emulated) placed under excellent radio conditions as defined in Section 3.6, using LTE RSRP (for LTE) or 5G SS-5
RSRP (for 5G NSA/SA) as the metric.   6
Since this test scenario requires a large number of UE connections, using an equipment that emulates a large number of 7
UEs and performing the test is recommended.  8
Test configuration: The test configuration is not specified. The utilized test configuration (connection diagram between 9
SUT and test equipment, parameters) should be recorded in the test report.  10
Laboratory setup : The radio conditions experienced by the UE can be modified using a variable attenuator inserted 11
between the antenna connectors (if available) of the O -RU and the UE, or appropriately emula ted using a UE emulator.  12
The test environment should be setup to achieve excellent radio conditions (LTE RSRP (for LTE) or 5G SS -RSRP (for 13
5G NSA/SA) as defined in Section 3.6) for the UE, but t he minimum coupling loss (see  Section 3.6) should not be 14
exceeded.  15
8.6.3 Test Procedure 16
The test steps below are applicable for either LTE or 5G NSA and SA: 17
1. The test setup is configured according to the test configuration. The test configuration should be recorded in the test 18
report. The target cells under test are activated and unloaded. All other cells are powered off.  19
2. The UE (emulated UE) is placed under excellent radio conditions (Cell centre close to radiated eNB/gNB Antenna) 20
as defined by LTE RSRP (for LTE) or 5G SS-RSRP (for 5G NSA/SA) in Section 3.6.  21
3. The End-to-end setup shall be operational for LTE or 5G NSA/SA as applicable for the test scenario, and there 22
should not be any connectivity issues.  23
4. Required performance data (incl. the signalling and control data) as specified in the “Test requirements” Section 24
below is measured and captured at the UE(s) and eNB/gNB side using logging/measurement tools. 25
5. Start the operation of each UE according to the traffic model for all of target cells. 26
6. Check KPIs for each scenario included in the traffic model. A scenario is considered successful if the KPIs for each 27
scenario is equivalent to the single-cell case. if not, it is a failure. 28
7. Stop and save the test logs. Check the log to make sure that the test runs successfully and that no unexpected 29
behavior such as unexpected call release is recorded. The logs should be captured and kept for test result reference 30
and measurements. 31
8. If measured RLC, RRC, and and/or PDCP KPIs are not consistent with expectations for a highly loaded O -DU and 32
O-CU, repeat steps 4 through 7 with similar incremental traffic being applied  using the optional OFH and/or F1 33
interface and test equipment (see Section 3.1.1) as was used for Section 8.3.3 (Traffic Load Testing) or 8.4.3 34
(Traffic Model Testing) in addition to the modelled traffic applied at the RF interface  35
8.6.4 Test requirements (expected results) 36
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 37
should be captured and reported in the test report for performance assessment. 38
• Radio parameters such as RSRP, RSRQ 39
• Traffic Model 40

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 205

O-RAN.TIFG.E2E-Test.0-v02.00
• KPIs mentioned in Table 8-9, Table 8-7 1
• SUT load/capacity/performance related KPIs (e.g. CPU and MEMORY utilization) if any  2
Validate the successful procedures from the collected logs. The number of cells to be communicated is preferably 2 cells 3
or more. Check the KPIs of each scenario and the adequacy of the completion rate for each scenario shall be confirmed. 4
For the UE(s) in which call loss has occurred, the validity of the cause of the call loss shall be confirmed. 5
Table 8-9 Successfully tested number of Cells 6
 Successfully tested number of Cells
LTE
NR(NSA)
NR（SA）
 7
8.7 Emergency call 8
8.7.1 Test description and applicability 9
The purpose of this test is to ensure that emergency calls, such as 911/112, are preferentially established even if the 10
eNB/gNB is overloaded. This test is valid with either LTE or 5G NSA and SA. The test procedure is to further increase 11
the heavy traffic model to load the eNB/gNB, creating an overload condition where some behavior of the UE, such as 12
data upload/download, fails. Verify that emergency calls, such as 911/112, is able to communicate correctly in such a 13
situation. 14
 15
The Emergency Call performance of the SUT 3GPP stack components in all of the O -RU, O-DU, and O-CU sub-nodes 16
should be measured while all sub-nodes are under high load. If additional traffic utilizing the optional OFH and/or F1 17
interface and test equipment as described in Section 3.1.1 was applied in Section 8.3.3 (Traffic Load Testing) or Section 18
8.4.3 (Traffic Model Testing) in addition to the traffic applied at the configured RF cell, the same additional traffic 19
should be applied for the Emergency Call test case. 20
 21
8.7.2 Test setup and configuration 22
The test setup is a single cell scenario (i.e. isolated cell without any inter -cell interference – see Section 3.7) with UEs 23
(real or emulated) placed under excellent radio conditions as defined in Section 3.6, using LTE RSRP (for LTE) or 5G 24
SS-RSRP (for 5G NSA/SA) as the metric.   25
Since this test scenario requires a large amount of non-emergency traffics to overload SUT, so it is recommended to run 26
the test with equipment that emulates a large amount of non-emergency traffics. 27
Test configuration: The test configuration is not specified. The utilized test configuration (connection diagram between 28
SUT and test equipment, parameters) should be recorded in the test report.  29
Laboratory setup : The radio conditions experienced by the UE can be modified using a variable attenuator inserted 30
between the antenna connectors (if available) of  the O-RU and the UE, or appropriately emulated using a UE emulator.  31
The test environment should be setup to achieve excellent radio conditions (LTE RSRP (for LTE) or 5G SS -RSRP (for 32
5G NSA/SA) as defined in Section 3.6) for the UE, but t he minimum coupling loss (see  Section 3.6) should not be 33
exceeded. 34

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 206

O-RAN.TIFG.E2E-Test.0-v02.00
8.7.3 Test Procedure 1
The test steps below are applicable for either LTE or 5G NSA and SA: 2
1. The test setup is configured according to the test configuration. The test configuration should be recorded in the test 3
report. The serving cell under test is activated and unloaded. All other cel ls are powered off. 4
2. The UE (real or emulated UE) is placed under excellent radio conditions (Cell centre close to radiated eNB/gNB 5
Antenna) as defined by LTE RSRP (for LTE) or 5G SS-RSRP (for 5G NSA/SA) in Section 3.6.  6
3. The End-to-end setup shall be operational for LTE or 5G NSA/SA as applicable for the test scenario, and there 7
should not be any connectivity issues.  8
4. Required performance data (incl. the signalling and control data) as specified in the “Test requirements” Section 9
below is measured and captured at the UE(s) and eNB/gNB side using logging/measurement tools.  10
5. If during execution of Section 8.3.3 (Traffic Load Testing) or 8.4.3 (Traffic Model Testing) incremental traffic 11
was applied using the optional OFH and/or F1 interface and test equipment (see Section 3.1.1) in addition to the 12
traffic being applied at the RF interface, the same level of traffic should be applied again using the  optional OFH 13
and/or F1 interface and test equipment to ensure O-DU and O-CU exposed to adequate “base load” before 14
executing Step 6.  15
6. Start the operation of each UE according to the traffic model. 16
7. Review the KPIs for each scenario in the traffic model to ensure that the KPIs are degraded due to overload. If there 17
is no degradation of the KPI, increase the traffic model until there is degradation of the KPI (until it becomes 18
overloaded). 19
8. Make multiple emergency calls (100 times suggested) such as 911/112 to confirm if the call can be established. 20
9. Stop and save the test logs. Check the log to make sure that the test runs successfully and that no unexpected 21
behavior such as unexpected call release is recorded. The logs should be captured and kept for test result refe rence 22
and measurements. 23
8.7.4 Test requirements (expected results) 24
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 25
should be captured and reported in the test report for performance assessment. 26
• Radio parameters such as RSRP, RSRQ 27
• Traffic Model and related KPIs 28
• KPIs mentioned in Table 8-10 29
• SUT load/capacity/performance related KPIs (e.g. CPU and MEMORY utilization) if any  30
Validate the successful procedures from the collected logs. Confirm the emergency call communication. 31
Table 8-10 Emergency Call Success Rate 32
 Emergency Call Success Rate
LTE
NR(NSA)
NR（SA）
 33

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 207

O-RAN.TIFG.E2E-Test.0-v02.00
8.8 ETWS (Earthquake and Tsunami Warning System) 1
8.8.1 Test description and applicability 2
The purpose of this test is to verify that the ETWS notification is successful even if the eNB/gNB is overloaded. This 3
test is valid with either LTE or 5G NSA and SA. The test procedure is to further increase the heavy traffic model to load 4
the eNB/gNB, creating an overload condition where some behavior of the UE, such as data upload/download, fails. 5
Verify that the ETWS Primary and Secondary notifications can communicate correctly under these conditions. 6
The ETWS performance of the SUT 3GPP stack components in all of the O -RU, O-DU, and O-CU sub-nodes should be 7
measured while all sub-nodes are under high load. If additional traffic utilizing the optional OFH and/or F1 interface 8
and test equipment as described in Section 3.1.1 was applied in Section 8.3.3 (Traffic Load Testing) or 8.4.3 (Traffic 9
Model Testing) in addition to the traffic applied at the configured RF cell, the same additional traffic should be applied 10
for the ETWS test case. 11
 12
8.8.2 Test setup and configuration 13
The test setup is a single cell scenario (i.e. isolated cell without any inter -cell interference – see Section 3.7) with UEs 14
(real or emulated) placed under excellent radio conditions as defined in Section 3.6, using LTE RSRP (for LTE) or 5G 15
SS-RSRP (for 5G NSA/SA) as the metric.   16
Since this test scenario requires a large amount of non-emergency traffics to overload SUT, so it is recommended to run 17
the test with equipment that emulates a large amount of non-emergency traffics. 18
Test configuration: The test configuration is not specified. The utilized test configuration (connection diagram between 19
SUT and test equipment, parameters) should be recorded in the test report.  20
Laboratory setup : The radio conditions experienced by the UE can be modified using a variable attenuator inserted 21
between the antenna connectors (if available) of the O -RU and the UE, or appropriately emulated using a UE emulator.  22
The test environment should be setup to achieve excellent radio conditions (LTE RSRP (for LTE) or 5G SS -RSRP (for 23
5G NSA/SA) as defined in S ection 3.6) for the UE, but t he minimum coupling loss (see  Section 3.6) should not be 24
exceeded.  25
8.8.3 Test Procedure 26
The test steps below are applicable for either LTE or 5G NSA and SA: 27
1. The test setup is configured according to the test configuration. The test configuration should be recorded in the test 28
report. The serving cell under test is activated and unloaded. All other cells are powered off. 29
2. The UE (real or emulated UE) is placed under excellent radio conditions (Cell centre close to radiated eNB/gNB 30
Antenna) as defined by LTE RSRP (for LTE) or 5G SS-RSRP (for 5G NSA/SA) in Section 3.6.  31
3. The End-to-end setup shall be operational for LTE or 5G NSA/SA as applicable for the test scenario, and there 32
should not be any connectivity issues.  33
4. Required performance data (incl. the signalling and control data) as specified in the “Test requirements” Section 34
below is measured and captured at the UE(s) and eNB/gNB side using logging/measurement tools.  35
5. If during execution of Section 8.3.3 (Traffic Load Testing) or 8.4.3 (Traffic Model Test ing) incremental traffic 36
was applied using the optional OFH and/or F1 interface and test equipment (see Section 3.1.1) in addition to the 37
traffic being applied at the RF interface, the same level of traffic should be applied again using the  optional OFH 38
and/or F1 interface and test equipment to ensure O-DU and O-CU exposed to adequate “base load” before 39
executing Step 6.  40

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 208

O-RAN.TIFG.E2E-Test.0-v02.00
6. Start the operation of each UE according to the traffic model. 1
7. Review the KPIs for each scenario in the traffic model to ensure that the KPIs are degraded due to overload. If there 2
is no degradation of the KPI, increase the traffic model until there is degradation of the KPI (until it becomes 3
overloaded). 4
8. Confirm the ETWS Primary and Secondary notifications communication multiple times (100 times suggested). 5
9. Stop and save the test logs. Check the log to make sure that the test runs successfully and that no unexpected 6
behavior such as unexpected call release is recorded. The logs should be captured and kept for test result reference 7
and measurements. 8
8.8.4 Test requirements (expected results) 9
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 10
should be captured and reported in the test report for performance assessment. 11
• Radio parameters such as RSRP, RSRQ 12
• Traffic Model and related KPIs 13
• KPIs mentioned in Table 8-11 14
• SUT load/capacity/performance related KPIs (e.g. CPU and MEMORY utilization) if any  15
Validate the successful procedures from the collected logs.  Make sure that the ETWS function works correctly and 16
confirm that the ETWS notification is sent to the UE. 17
Table 8-11 ETWS Notification Success Rate 18
 ETWS Notification Success Rate
Primary Secondary
LTE
NR
 19
 20
 21
 22
 23
 24
 25
 26

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 209

O-RAN.TIFG.E2E-Test.0-v02.00
Annex A: Template of test report 1
A. GENERAL INFORMATION 2
A1 Name of test campaign

A2 Version of the report – reference ID

A3 Date(s) of testing

A4 Contact person (tester) – incl. Name, Organization, E-mail address

A4 Test location (lab) – incl. the address

A5 Description of test campaign, summary of test results, conclusions

List of tests - details of each test can be found in Section E. 3
Test No. Test name  Test status [ PASS / FAIL / - ]
01
02
03
 4
B. TEST AND MEASUREMENT EQUIPMENT AND TOOLS 5
# Equipment or tool Type Manufacture Version (HW/SW) Notes#
01
02
03
# Specific details such as frequency range, date of last calibration, Type and version of Operating System at end-user device/application server, etc. 6

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 210

O-RAN.TIFG.E2E-Test.0-v02.00
 1
C. SYSTEM UNDER TEST 2
C1 Total number of DUTs included in SUT

C2 Deployment architecture (indoor/outdoor, micro/pico/macro)

C3 Description of SUT – block diagram – connection scheme

DUT 1# 3
C3 Type

C4 Serial Number C5 Supplier (manufacture)
C6 SW version

C7 HW version

C8 Interface/IOT profile(s) if applied

C9 Description incl. parameters, setting/configuration

# If SUT contains more DUTs, please copy the table. 4
 5
D. TEST CONFIGURATION 6
D1 Carrier (central) frequency – (NR-)ARFCN

D2 Total transmission (effective) bandwidth

D3 Number of total Resource Blocks

D4 Sub-carrier spacing

D5 Carrier prefix length

D6 Slot length

D5 Duplex mode

D6 TDD DL/UL ratio (configuration) D7 IPv4 / IPv6
eNB/gNB UE
D8 Number of supported MIMO layers at eNB/gNB

D9 Number of supported MIMO layers at UE

D10 Antenna configuration - number of Tx/Rx at eNB/gNB

D11 Antenna configuration - number of Tx/Rx at UE

D12 Total antenna gain at eNB/gNB D13 Total antenna gain at UE

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 211

O-RAN.TIFG.E2E-Test.0-v02.00
D14 Total transmit power at antenna connectors at eNB/gNB

D15 Total transmit power at antenna connectors at UE

 1
E. TEST RESULTS 2
E1 Test No.

E2 Test name

E3 Date(s) of test execution

E4 Reference to test specification

E5 Utilized test and measurement equipment and tools, incl. the specific setting/configuration – reference to Section B

E6 Test setup – connection/block diagram – deployment scenario (dense urban/urban/rural, LOS/NLOS/nLOS)

E7 Test procedure – describe differences in comparison with the test procedure defined in test spec. – limitations

E8 Test results – measured KPIs – incl. attachment of raw log file(s)

E9 Notes, incl. observed issues with the solutions

E10 Conclusions – pass/fail – assessment of measured KPIs (in comparison with the expected KPIs) – gap analysis

 3
 4

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 212

O-RAN.TIFG.E2E-Test.0-v02.00
Annex ZZZ: O-RAN Adopter License Agreement 1
BY DOWNLOADING, USING OR OTHERWISE ACCESSING ANY O -RAN SPECIFICATION, ADOPTER 2
AGREES TO THE TERMS OF THIS AGREEMENT. 3
This O-RAN Adopter License Agreement (the “Agreement”) is made by and between the O-RAN Alliance and the entity 4
that downloads, uses or otherwise accesses any O-RAN Specification, including its Affiliates (the “Adopter”). 5
This is a license agreement for entities who wish to adopt any O-RAN Specification. 6
Section 1: DEFINITIONS 7
1.1 “Affiliate” means an entity that directly or indirectly controls, is controlled by, or is under common control with 8
another entity, so long as such control exists. For the purpose of this Section, “Control” means beneficial ownership of 9
fifty (50%) percent or more of the voting stock or equity in an entity. 10
1.2 “Compliant Implementation” means any system, device, method or operation (whether implement ed in hardware, 11
software or combinations thereof) that fully conforms to a Final Specification. 12
1.3 “Adopter(s)” means all entities, who are not Members, Contributors or Academic Contributors, including their 13
Affiliates, who wish to download, use or otherwise access O-RAN Specifications. 14
1.4 “Minor Update” means an update or revision to an O-RAN Specification published by O-RAN Alliance that does not 15
add any significant new features or functionality and remains interoperable with the prior version of an O -RAN 16
Specification. The term “O-RAN Specifications” includes Minor Updates. 17
1.5 “Necessary Claims” means those claims of all present and future patents and patent applications, other than design 18
patents and design registrations, throughout the world, which ( i) are owned or otherwise licensable by a Member, 19
Contributor or Academic Contributor during the term of its Member, Contributor or Academic Contributorship; (ii) such 20
Member, Contributor or Academic Contributor has the right to grant a license without the  payment of consideration to a 21
third party; and (iii) are necessarily infringed by a Compliant Implementation (without considering any Contributions not 22
included in the Final Specification). A claim is necessarily infringed only when it is not possible on technical (but not 23
commercial) grounds, taking into account normal technical practice and the state of the art generally available at the date 24
any Final Specification was published by the O -RAN Alliance or the date the patent claim first came into existenc e, 25
whichever last occurred, to make, sell, lease, otherwise dispose of, repair, use or operate a Compliant Implementation 26
without infringing that claim. For the avoidance of doubt in exceptional cases where a Final Specification can only be 27
implemented by technical solutions, all of which infringe patent claims, all such patent claims shall be considered 28
Necessary Claims. 29
1.6 “Defensive Suspension” means for the purposes of any license grant pursuant to Section 3, Member, Contributor, 30
Academic Contributor, Adopter, or any of their Affiliates, may have the discretion to include in their license a term 31
allowing the licensor to suspend the license against a licensee who brings a patent infringement suit against the licensing 32
Member, Contributor, Academic Contributor, Adopter, or any of their Affiliates. 33
Section 2: COPYRIGHT LICENSE 34
2.1 Subject to the terms and conditions of this Agreement, O -RAN Alliance hereby grants to Adopter a nonexclusive, 35
nontransferable, irrevocable, non -sublicensable, worldwide copyright  license to obtain, use and modify O -RAN 36
Specifications, but not to further distribute such O -RAN Specification in any modified or unmodified way, solely in 37
furtherance of implementations of an O-RAN Specification. 38
2.2 Adopter shall not use O -RAN Specifications except as expressly set forth in this Agreement or in a separate written 39
agreement with O-RAN Alliance. 40

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 213

O-RAN.TIFG.E2E-Test.0-v02.00
Section 3: FRAND LICENSE 1
3.1 Members, Contributors and Academic Contributors and their Affiliates are prepared to grant based on a separate 2
Patent License Agreement to each Adopter under Fair Reasonable And Non - Discriminatory (FRAND) terms and 3
conditions with or without compensation (royalties) a nonexclusive, non -transferable, irrevocable (but subject to 4
Defensive Suspension), non-sublicensable, worldwide patent license under their Necessary Claims to make, have made, 5
use, import, offer to sell, lease, sell and otherwise distribute Compliant Implementations; provided, however, that such 6
license shall not extend: (a) to any part or function of a product in which a Compliant Implementation is incorporated that 7
is not itself part of the Compliant Implementation; or (b) to any Adopter if that Adopter is not making a reciprocal grant 8
to Members, Contributors and Academic Contributors, as set forth in Sec tion 3.3. For the avoidance of doubt, the 9
foregoing licensing commitment includes the distribution by the Adopter’s distributors and the use by the Adopter’s 10
customers of such licensed Compliant Implementations. 11
3.2 Notwithstanding the above, if any Member , Contributor or Academic Contributor, Adopter or their Affiliates has 12
reserved the right to charge a FRAND royalty or other fee for its license of Necessary Claims to Adopter, then Adopter 13
is entitled to charge a FRAND royalty or other fee to such Member,  Contributor or Academic Contributor, Adopter and 14
its Affiliates for its license of Necessary Claims to its licensees. 15
3.3 Adopter, on behalf of itself and its Affiliates, shall be prepared to grant based on a separate Patent License Agreement 16
to each Members, Contributors, Academic Contributors, Adopters and their Affiliates under Fair Reasonable And Non -17
Discriminatory (FRAND) terms and conditions with or without compensation (royalties) a nonexclusive, non -18
transferable, irrevocable (but subject to Defensi ve Suspension), non-sublicensable, worldwide patent license under their 19
Necessary Claims to make, have made, use, import, offer to sell, lease, sell and otherwise distribute Compliant 20
Implementations; provided, however, that such license will not extend: (a) to any part or function of a product in which a 21
Compliant Implementation is incorporated that is not itself part of the Compliant Implementation; or (b) to any Members, 22
Contributors, Academic Contributors, Adopters and their Affiliates that is not makin g a reciprocal grant to Adopter, as 23
set forth in Section 3.1. For the avoidance of doubt, the foregoing licensing commitment includes the distribution by the 24
Members’, Contributors’, Academic Contributors’, Adopters’ and their Affiliates’ distributors and the use by the 25
Members’, Contributors’, Academic Contributors’, Adopters’ and their Affiliates’ customers of such licensed Compliant 26
Implementations. 27
Section 4: TERM AND TERMINATION 28
4.1 This Agreement shall remain in force, unless early terminated according to this Section 4. 29
4.2 O-RAN Alliance on behalf of its Members, Contributors and Academic Contributors may terminate this Agreement 30
if Adopter materially breaches this Agreement and does not cure or is not capable of curing such breach within thirty (30) 31
days after being given notice specifying the breach. 32
4.3 Sections 1, 3, 5 - 11 of this Agreement shall survive any termination of this Agreement. Under surviving Section 3, 33
after termination of this Agreement, Adopter will continue to grant licenses (a) to entities who become Adopters after the 34
date of termination; and (b) for future versions of O-RAN Specifications that are backwards compatible with the version 35
that was current as of the date of termination. 36
Section 5: CONFIDENTIALITY 37
Adopter will use the  same care and discretion to avoid disclosure, publication, and dissemination of O -RAN 38
Specifications to third parties, as Adopter employs with its own confidential information, but no less than reasonable care. 39
Any disclosure by Adopter to its Affiliates,  contractors and consultants should be subject to an obligation of 40
confidentiality at least as restrictive as those contained in this Section. The foregoing obligation shall not apply to any 41
information which is: (1) rightfully known by Adopter without any limitation on use or disclosure prior to disclosure; (2) 42
publicly available through no fault of Adopter; (3) rightfully received without a duty of confidentiality; (4) disclosed by 43
O-RAN Alliance or a Member, Contributor or Academic Contributor to a third party without a duty of confidentiality on 44
such third party; (5) independently developed by Adopter; (6) disclosed pursuant to the order of a court or other authorized 45

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 214

O-RAN.TIFG.E2E-Test.0-v02.00
governmental body, or as required by law, provided that Adopter provides reasonable pri or written notice to O -RAN 1
Alliance, and cooperates with O -RAN Alliance and/or the applicable Member, Contributor or Academic Contributor to 2
have the opportunity to oppose any such order; or (7) disclosed by Adopter with O-RAN Alliance’s prior written approval. 3
Section 6: INDEMNIFICATION 4
Adopter shall indemnify, defend, and hold harmless the O -RAN Alliance, its Members, Contributors or Academic 5
Contributors, and their employees, and agents and their respective successors, heirs and assigns (the “Indemnitees ”), 6
against any liability, damage, loss, or expense (including reasonable attorneys’ fees and expenses) incurred by or imposed 7
upon any of the Indemnitees in connection with any claims, suits, investigations, actions, demands or judgments arising 8
out of Adopter’s use of the licensed O-RAN Specifications or Adopter’s commercialization of products that comply with 9
O-RAN Specifications. 10
Section 7: LIMITATIONS ON LIABILITY; NO WARRANTY 11
EXCEPT FOR BREACH OF CONFIDENTIALITY, ADOPTER’S BREACH OF SECTION 3, AND ADOPTER’S 12
INDEMNIFICATION OBLIGATIONS, IN NO EVENT SHALL ANY PARTY BE LIABLE TO ANY OTHER PARTY 13
OR THIRD PARTY FOR ANY INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE OR CONSEQUENTIAL 14
DAMAGES RESULTING FROM ITS PERFORMANCE OR NON -PERFORMANCE UNDER THIS AGREEMENT, 15
IN EACH CASE WHETHER UNDER CONTRACT, TORT, WARRANTY, OR OTHERWISE, AND WHETHER OR 16
NOT SUCH PARTY HAD ADVANCE NOTICE OF THE POSSIBILITY OF SUCH DAMAGES. O -RAN 17
SPECIFICATIONS ARE PROVIDED “AS IS” WITH NO WARRANTIES OR CONDITIONS WHATSOEVER, 18
WHETHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE. THE O -RAN ALLIANCE AND THE 19
MEMBERS, CONTRIBUTORS OR ACADEMIC CONTRIBUTORS EXPRESSLY DISCLAIM ANY WARRANTY 20
OR CONDITION OF MERCHANTABILITY, SECURITY, SATISFACTORY QUALITY, NONINFRINGEMENT, 21
FITNESS FOR ANY PARTICULAR PURPOSE , ERROR -FREE OPERATION, OR ANY WARRANTY OR 22
CONDITION FOR O-RAN SPECIFICATIONS. 23
Section 8: ASSIGNMENT 24
Adopter may not assign the Agreement or any of its rights or obligations under this Agreement or make any grants or 25
other sublicenses to this Agreement, ex cept as expressly authorized hereunder, without having first received the prior, 26
written consent of the O -RAN Alliance, which consent may be withheld in O -RAN Alliance’s sole discretion. O -RAN 27
Alliance may freely assign this Agreement. 28
Section 9: THIRD-PARTY BENEFICIARY RIGHTS 29
Adopter acknowledges and agrees that Members, Contributors and Academic Contributors (including future Members, 30
Contributors and Academic Contributors) are entitled to rights as a third -party beneficiary under this Agreement, 31
including as licensees under Section 3. 32
Section 10: BINDING ON AFFILIATES 33
Execution of this Agreement by Adopter in its capacity as a legal entity or association constitutes that legal entity’s or 34
association’s agreement that its Affiliates are likewise bound to t he obligations that are applicable to Adopter hereunder 35
and are also entitled to the benefits of the rights of Adopter hereunder. 36
Section 11: GENERAL 37
This Agreement is governed by the laws of Germany without regard to its conflict or choice of law provisions.  38

______________________________________________________________________________________________
Copyright © 2021 by the O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ         Page 215

O-RAN.TIFG.E2E-Test.0-v02.00
This Agreement constitutes the entire agreement between the parties as to its express subject matter and expressly 1
supersedes and replaces any pr ior or contemporaneous agreements between the parties, whether written or oral, relating 2
to the subject matter of this Agreement.  3
Adopter, on behalf of itself and its Affiliates, agrees to comply at all times with all applicable laws, rules and regulations 4
with respect to its and its Affiliates’ performance under this Agreement, including without limitation, export control and 5
antitrust laws. Without limiting the generality of the foregoing, Adopter acknowledges that this Agreement prohibits any 6
communication that would violate the antitrust laws. 7
By execution hereof, no form of any partnership, joint venture or other special relationship is created between Adopter, 8
or O -RAN Alliance or its Members, Contributors or Academic Contributors. Except as expressly  set forth in this 9
Agreement, no party is authorized to make any commitment on behalf of Adopter, or O -RAN Alliance or its Members, 10
Contributors or Academic Contributors. 11
In the event that any provision of this Agreement conflicts with governing law or if any provision is held to be null, void 12
or otherwise ineffective or invalid by a court of competent jurisdiction, (i) such provisions will be deemed stricken from 13
the contract, and (ii) the remaining terms, provisions, covenants and restrictions of this Agr eement will remain in full 14
force and effect. 15
Any failure by a party or third party beneficiary to insist upon or enforce performance by another party of any of the 16
provisions of this Agreement or to exercise any rights or remedies under this Agreement or otherwise by law shall not be 17
construed as a waiver or relinquishment to any extent of the other parties’ or third party beneficiary’s right to assert or 18
rely upon any such provision, right or remedy in that or any other instance; rather the same shall be a nd remain in full 19
force and effect. 20