O-RAN-WG6.CAD-V01.00.00
Technical Report
Cloud Architecture and Deployment Scenarios 
 for O-RAN Virtualized RAN 
This is a re-published version of the attached final specification. 
For this re-published version, the prior versions of the IPR Policy will apply, except that the previous 
requirement for Adopters (as defined in the earlier IPR Policy) to agree to an O-RAN Adopter License 
Agreement to access and use Final Specifications shall no longer apply or be required for these Final 
Specifications after 1st July 2022.
The copying or incorporation into any other work of part or all of the material available in this 
specification in any form without the prior written permission of O-RAN ALLIANCE e.V.  is prohibited, 
save that you may print or download extracts of the material on this site for your personal use, or copy 
the material on this site for the purpose of sending to individual third parties for their information 
provided that you acknowledge O-RAN ALLIANCE as the source of the material and that you inform the 
third party that these conditions apply to them and that they must comply with them.

 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 
 
 
O-RAN-WG6.CAD-V01.00.00 
Technical Report 
Cloud Architecture and Deployment Scenarios 
 for O-RAN Virtualized RAN 
 
Prepared by the O-RAN Alliance e.V. Copyright © 2019 by the O-RAN Alliance e.V.  
By using, accessing or downloading any part of this O-RAN specification document, including by copying, saving, 
distributing, displaying or preparing derivatives of, you agree to be and are bound to the terms of the O-RAN Adopter 
License Agreement contained in the Annex ZZZ of this specification. All other rights reserved.  

                                                                                                           O-RAN WG6 Technical Report 
 
2 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
Revision History 1 
Date Revision Company Description 
2019.01.18 V0000.00 AT&T, Orange, 
Lenovo, … 
Template with initial scenarios. 
2019.01.29 V00.00.01 Editor (AT&T) Updates to terminology, miscellaneous other updates 
2019.02.07 V00.00.02 Editor (AT&T)  More definitions in 2.1, New Sec 4 on Overall Architecture, 
expansion/ updates of sec 5 Profiles, added Sec 6 OAM 
placeholder.  
2019.03.18 V00.00.03 Editor (AT&T) Many additions in content and section structure. 
2019.04.01 V00.00.04 Editor (AT&T) Some restructuring and combining of early sections, and more 
discussion on scope and context.  Addition of implementation 
consideration section, including performance.  Added optional 
Fronthaul GW. Provided framework discussion in each 
scenario’s subsection.  Other updates.   
2019.04.10 V00.00.05 Aricent, Red Hat, 
KDDI, Ciena 
Updates to include comments before April 11 review.  
Comments from RaviKanth (Aricent), Pasi (Red Hat), Shinobu 
(KDDI), and Lyndon (Ciena).  
2019.04.15 V00.00.06 Editor (AT&T) Updates to include some updates from comments from April 
11 review. 
2019.04.24 V00.00.07 Editor (AT&T) Updates of diagrams to address comments, additional figures 
on scope, and other changes to address April 11 review 
comments. 
2019.05.01 V00.00.08 KDDI Updates to diagrams for Scenarios A and B.  Modifications per 
KDDI regarding C.2.  
2019.05.12 V00.00.09 KDDI, Red Hat, 
Editor (AT&T) 
Updates based on meeting discussions, subsection additions 
based on proposals. 
2019.05.15 V00.00.10 Editor (AT&T) Clean-up in preparation of creating a baseline document – 
marking of many comments as done, adding editor notes where 
needed, and other clarifications. 
2019.05.20 V00.00.11 Editor (AT&T) Continued clean-up in preparation of a baseline. 
2019.05.29 V00.00.12 Editor (AT&T) Continued clean-up in preparation of a baseline. 
2019.06.04 V00.00.13 Wind River, China 
Mobile 
Major additions to the Cloud requirements in section 5.4 and 
Appendix B by Wind River, plus updates to the Fronthaul 
section from China Mobile. Various additional minor updates. 
2019.06.13 V00.01.00 Editor (AT&T) This is the same as V00.00.13, but with renumbering to 
indicate this is the initial baseline for comment, V00.01.00  
2019.06.14  V00.01.01 Wind River, AT&T This includes updates from CRs discussed and agreed to on the 
June 13 call:   
 Wind River contributions on adding a figure for NUMA 
illustration and a major enhancement of Sec 9.1 on 
cache 
 AT&T contribution to add material on centralization of 
O-DU/O-CU resources, to Sections 5.1 and 6.2   
 Update of figures to address Open Fronthaul comments 
(discussed June 6)  

                                                                                                           O-RAN WG6 Technical Report 
 
3 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
2019.07.05 V00.01.02 Editor (AT&T), 
based on meeting 
discussion 
Updates to address several CRs: 
 Multiple editorial items: 
o Draft text to address 5G/4G scope in Sec 1.2 – 
further discussion via separate CR 
o Statement in 5.2 about performance to focus on 
delay 
o Statement in 5.7 about transport 
o 5.8; update of Figure 13 to indicate cloud locations.  
Added MEC text that to address MEC comment 
during call. 
o Delay and loss table updates in 6, and statement in 
5.2 
 Former 9.1 and 9.3 sections of Appendix B (on cache 
and storage details) will be transferred to Tong’s 
document (Reference Design).   
 Update the O-DU pooling analysis in Section 5.1.3. 
2019.07.18 V00.01.03 AT&T, Red Hat, 
TIM, Intel, 
Ericsson 
Updates to address multiple CRs, through July 18: 
 Address NSA aspects in scope 
 Addition of 5.3 (Acceleration) 
 Removal of Scale up/down appendix, and note for future 
study 
 Update of delay figure in 5.2. 
 Update of Figure 4 
 Replacement of Zbox concept with O-Cloud, and all 
related updates. 
2019.08.02 V00.01.04 AT&T, Wind 
River, Red Hat 
Updates to address multiple CRs, discussed on Aug 1: 
 Update Section 5.6, merge in sec 7, explain some 
fundamental operations concepts. 
 Update the sync section to point to work in other WGs, 
and say that text will wait until CAD version 2. 
 Update the delay section (5.2.1)  
 Remove notes that refer to items that will not receive 
contributions in version 1.  Remove comments that are 
no longer relevant. 
 Remove Appendix A 
2019.08.09 V00.01.05 Red Hat, TIM, DT, 
Editor (AT&T) 
Updates to address multiple CRs and DT review comments, 
discussed on Aug 8.   
 Update 5.2.1 to address non-optimal fronthaul, and to 
correct some equations 
 Update 5.6 to add a figure showing the O1* interface 
 Addressed a range of comments by DT, some editorial, 
some more involved. 
2019.08.16 V00.01.06 Ericsson, Wind 
River, AT&T 
Updates to address multiple CRs and DT review comments, 
discussed on Aug 15.   
 Updates to address Ericsson’s comments 
 Update to address DT’s request to define vO-DU tile 
 Update of the Cloud Considerations section (5.4), mostly 
for restructuring to remove duplication, but to also add 
material for VMs or Containers where necessary to 
provide balanced coverage. 
 Additional updates:  Many resolved and obsolete Word 
comments have been removed in anticipation of 
finalization. 
 References to documents that are not finalized have been 
removed. 
2019.08.23 V00.01.07 AT&T Updates to reflect:  
 Updates of the O-DU pooling section based on Aug 20 
discussion 
 Management section updates are to address comments 
made on Aug 15 discussion, particularly regarding the 

                                                                                                           O-RAN WG6 Technical Report 
 
4 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
use of the term domain manager and its role in an ME, 
and the location of O1 terminations 
 Edits to remove references to O-RAN WGs, and make 
updates of the revision history. 
 Addition of standard O-RAN Annex ZZZ 
2019.08.26 V00.01.08 Editor (AT&T)  Clean up of references and cross references to them 
 Removed Word comments 
 Removed cardinality questions in Scenarios A (removed 
6.1.1) and Scenario B 
2019.08.26 V00.01.09 Editor (AT&T) Final minor comments during Aug 27 WG6 call, in preparation 
for vote. 
2019.10.01 V01.00.00 Editor (AT&T) Update of Annex ZZZ, page footers, and addition of title page 
disclaimer.  
  2 

                                                                                                           O-RAN WG6 Technical Report 
 
5 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 3 
Table of Contents 4 
Revision History ................................................................................................................................................. 2 5 
Table of Contents ............................................................................................................................................... 5 6 
Table of Figures .................................................................................................................................................. 6 7 
Table of Tables ................................................................................................................................................... 7 8 
1. Scope ........................................................................................................................................................ 8 9 
1.1. Context; Relationship to Other O-RAN Work ................................................................................................... 8 10 
1.2. Objectives .......................................................................................................................................................... 8 11 
2. References .............................................................................................................................................. 10 12 
3. Definitions and Abbreviations ............................................................................................................... 11 13 
3.1. Definitions ....................................................................................................................................................... 11 14 
3.2. Abbreviations ................................................................................................................................................... 12 15 
4. Overall Architecture ............................................................................................................................... 13 16 
4.1. RAN Component Definitions .......................................................................................................................... 13 17 
4.2. Degree of Openness ......................................................................................................................................... 14 18 
4.3. Decoupling of Hardware and Software ............................................................................................................ 15 19 
5. Deployment Scenarios:  Common Considerations ................................................................................. 16 20 
5.1. Mapping Logical Functionality to Physical Implementations ......................................................................... 16 21 
 Technical Constraints that Affect Hardware Implementations................................................................... 16 22 
 Service Requirements that Affect Implementation Design ........................................................................ 17 23 
 Rationalization of Centralizing O-DU Functionality ................................................................................. 17 24 
5.2. Performance Aspects ....................................................................................................................................... 20 25 
 User Plane Delay ........................................................................................................................................ 20 26 
5.3. Hardware Acceleration Options ....................................................................................................................... 23 27 
 HW Acceleration Abstraction .................................................................................................................... 24 28 
 HW Accelerator Deployment Model .............................................................................................. 24 29 
 HW Accelerator Application APIs ................................................................................................. 24 30 
 HW Accelerator Management and Orchestration Considerations .............................................................. 24 31 
5.4. Cloud Considerations ....................................................................................................................................... 24 32 
 Networking requirements ........................................................................................................................... 25 33 
 Support for Multiple Networking Interfaces ................................................................................... 25 34 
 Support for High Performance N-S Data Plane .............................................................................. 25 35 
 Support for High-Performance E-W Data Plane ............................................................................. 26 36 
 Support for Service Function Chaining .......................................................................................... 26 37 
 Assignment of Acceleration Resources ...................................................................................................... 26 38 
 Real-time / General Performance Feature Requirements ........................................................................... 27 39 
 Host Linux OS ................................................................................................................................ 27 40 
 Support for Pre-emptive Scheduling .................................................................................. 27 41 
 Support for Node Feature Discovery .............................................................................................. 27 42 
 Support for CPU Affinity and Isolation .......................................................................................... 27 43 
 Support for Dynamic HugePages Allocation .................................................................................. 27 44 
 Support for Topology Manager ...................................................................................................... 28 45 
 Support for Scale In/Out ................................................................................................................. 28 46 
 Support for Device Plugin .............................................................................................................. 29 47 
 Support for Direct IRQ Assignment ............................................................................................... 29 48 
 Support for No Over Commit CPU ................................................................................................ 29 49 
 Support for Specifying CPU Model ................................................................................................ 29 50 
 Storage Requirements ................................................................................................................................ 29 51 
5.5. Sync Architecture ............................................................................................................................................ 30 52 
5.6. Operations and Maintenance Considerations ................................................................................................... 30 53 
5.7. Transport Network Architecture ...................................................................................................................... 32 54 
 Fronthaul Gateways ................................................................................................................................... 32 55 

                                                                                                           O-RAN WG6 Technical Report 
 
6 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
5.8. Overview of Deployment Scenarios ................................................................................................................ 33 56 
6. Deployment Scenarios and Implementation Considerations .................................................................. 34 57 
6.1. Scenario A ....................................................................................................................................................... 34 58 
 Key Use Cases and Drivers ........................................................................................................................ 34 59 
6.2. Scenario B ........................................................................................................................................................ 34 60 
 Key Use Cases and Drivers ........................................................................................................................ 35 61 
6.3. Scenario C ........................................................................................................................................................ 35 62 
 Key Use Cases and Drivers ........................................................................................................................ 36 63 
 Scenario C.1, and Use Case and Drivers .................................................................................................... 36 64 
 Scenario C.2, and Use Case and Drivers .................................................................................................... 37 65 
6.4. Scenario D ....................................................................................................................................................... 39 66 
6.5. Scenario E ........................................................................................................................................................ 39 67 
 Key Use Cases and Drivers ........................................................................................................................ 40 68 
6.6. Scenario F ........................................................................................................................................................ 40 69 
 Key Use Cases and Drivers ........................................................................................................................ 40 70 
6.7. Scenarios of Initial Interest .............................................................................................................................. 40 71 
7. Appendix A (informative):  Extensions to Current Deployment Scenarios to Include NSA ................. 41 72 
7.1. Scenario A ....................................................................................................................................................... 41 73 
7.2. Scenario B ........................................................................................................................................................ 41 74 
7.3. Scenario C ........................................................................................................................................................ 42 75 
7.4. Scenario C.2 ..................................................................................................................................................... 42 76 
7.5. Scenario D ....................................................................................................................................................... 42 77 
Annex ZZZ:  O-RAN Adopter License Agreement ......................................................................................... 43 78 
 79 
Table of Figures  80 
Figure 1:  Relationship of this Document to Scenario Documents and O-RAN Management Documents ........................ 8 81 
Figure 2:  Major Components Related to the Orchestration and Cloudification Effort  ...................................................... 9 82 
Figure 3:  Different Clouds/ Sites ..................................................................................................................................... 10 83 
Figure 4:  Architecture Overview ..................................................................................................................................... 14 84 
Figure 5:  Decoupling, and Illustration of the O-Cloud Concept ...................................................................................... 15 85 
Figure 6:  Relationship Between RAN Functions and Demands on Cloud Infrastructure and Hardware  ........................ 16 86 
Figure 7:  Simple Centralization of O-DU Resources ...................................................................................................... 18 87 
Figure 8:  Pooling of Centralized O-DU Resources ......................................................................................................... 19 88 
Figure 9:  Comparison of Merit of Centralization Options vs. Number of Cell Sites in a Pool  ........................................ 19 89 
Figure 10:  Major User Plane Latency Components, by 5G Service Slice and Function Placement  ................................ 21 90 
Figure 11:  HW Abstraction Considerations ..................................................................................................................... 24 91 
Figure 12:  Illustration of the Network Interfaces Attached to a Pod, as Provisioned by Multus CNI  ............................. 25 92 
Figure 13:  Illustration of the Userspace CNI Plugin ........................................................................................................ 26 93 
Figure 14:  Example Illustration of Two NUMA Regions ............................................................................................... 28 94 
Figure 15:  RAN OAM Logical Architecture – One Example ......................................................................................... 30 95 
Figure 16:  O1 Termination and MFs in an ME ............................................................................................................... 31 96 
Figure 17:  Three types of O1 Terminations in MEs/MFs ................................................................................................ 31 97 
Figure 18:  O1* Interface to Manage Cloud Platform Resources (in addition to O1 for RAN MEs)  ............................... 32 98 
Figure 19:  High-Level Comparison of Scenarios ............................................................................................................ 33 99 
Figure 20:  Scenario A ...................................................................................................................................................... 34 100 
Figure 21:  Scenario B ...................................................................................................................................................... 35 101 
Figure 22:  Scenario C ...................................................................................................................................................... 36 102 
Figure 23:  Treatment of Network Slices:  MEC for URLLC at Edge Cloud, Centralized Control, Single vO -DU ........ 37 103 
Figure 24:  Scenario C.1 ................................................................................................................................................... 37 104 
Figure 25:  Treatment of Network Slice: MEC for URLLC at Edge Cloud, Separate vO -DUs ....................................... 38 105 
Figure 26:  Single O-RU Being Shared by More than One Operator ............................................................................... 38 106 
Figure 27:  Scenario C.2 ................................................................................................................................................... 39 107 
Figure 28:  Scenario D ...................................................................................................................................................... 39 108 
Figure 29:  Scenario E ...................................................................................................................................................... 40 109 
Figure 30:  Scenario F ....................................................................................................................................................... 40 110 
Figure 31:  Scenario A, Including NSA ............................................................................................................................ 41 111 

                                                                                                           O-RAN WG6 Technical Report 
 
7 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
Figure 32:  Scenario B, Including NSA ............................................................................................................................ 41 112 
Figure 33:  Scenario C, Including NSA ............................................................................................................................ 42 113 
Figure 34:  Scenario C.2, Including NSA ......................................................................................................................... 42 114 
Figure 35:  Scenario D, Including NSA ............................................................................................................................ 42 115 
Table of Tables 116 
Table 1:  Service Delay Constraints and Major Delay Contributors ................................................................................. 21 117 
Table 2:  Cardinality and Delay Performance for Scenario B........................................................................................... 35 118 
Table 3:  Cardinality and Delay Performance for Scenario C........................................................................................... 36 119 
Table 4:  Cardinality and Delay Performance for Scenario C.1........................................................................................ 37 120 
 121 
122 

                                                                                                           O-RAN WG6 Technical Report 
 
8 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
1. Scope  123 
This Technical Report has been produced by the O-RAN Alliance. 124 
The contents of the present document are subject to continuing work within O -RAN and may change following formal 125 
O-RAN approval. Should O-RAN modify the contents of the present document, it will be re-released by O-RAN with 126 
an identifying change of release date and an increase in version number as follows:  127 
Version x.y.z 128 
where: 129 
x the first digit is incremented for all changes of substance, i.e. technical enhancements, corrections, updates, 130 
etc. (the initial approved document will have x=01). 131 
y the second digit is incremented when editorial only changes have been incorporated in the document. 132 
z the third digit included only in working versions of the document indicating incremental changes during the 133 
editing process. 134 
1.1. Context; Relationship to Other O-RAN Work 135 
This document introduces and examines different scenarios and use cases for O-RAN deployments of Network 136 
Functionality into Cloud Platforms and proprietary equipment.  Deployment scenarios are associated with meeting 137 
customer and service requirements, while considering technological constraints and the need to create cost -effective 138 
solutions. It will also reference management considerations covered in more depth elsewhere.  139 
Two O-RAN management documents will be referenced (see Section 5.6): 140 
 OAM architecture specification  141 
 OAM interface specification (O1) 142 
The details of implementing each identified scenario will be covered in separate Scenario documents, shown in green in 143 
Figure 1.   144 
 145 
Figure 1:  Relationship of this Document to Scenario Documents and O-RAN Management Documents 146 
This document also draws on some other work from other O-RAN working groups, as well as sources from other 147 
industry bodies.   148 
1.2. Objectives  149 
The O-RAN Alliance seeks to improve RAN flexibility and deployment velocity, while at the same time reducing the 150 
capital and operating costs through the adoption of cloud architectures. The structure of the Orchestration and 151 
Cloudification work is shown graphically below.  This document focuses on the Cloudification deployment aspects as 152 
indicated.   153 


                                                                                                           O-RAN WG6 Technical Report 
 
9 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 154 
Figure 2:  Major Components Related to the Orchestration and Cloudification Effort 155 
A key principle is the decoupling of RAN hardware and software for all components including near-RT RIC, O-CU (O-156 
CU-CP and O-CU-UP), O-DU, and O-RU, and the deployment of software components on commodity server 157 
architectures supplemented with programmable accelerators where necessary. 158 
Key characteristics of cloud architectures which we will reference in this document are:  159 
a) Decoupling of hardware from software.  This aims to improve flexibility and choice for operators by 160 
decoupling selection and deployment of hardware infrastructure from software selection, 161 
b) Standardization of hardware specifications across software implementations, to simplify physical deployment 162 
and maintenance.  This aims to promote the availability of a multitude of software implementation choices for 163 
a given hardware configuration.   164 
c) Sharing of hardware.  This aims to promote the availability of a multitude of hardware implementation choices 165 
for a given software implementation. 166 
d) Flexible instantiation and lifecycle management through orchestration automation.  This aims to reduce 167 
deployment and ongoing maintenance costs by promoting simplification and automation throughout the 168 
hardware and software lifecycle through common chassis specifications and standardized orchestration 169 
interfaces.   170 
This document will define various deployment scenarios that can be supported by the O-RAN specifications and are of 171 
either current or relatively near-term interest.  Each scenario is identified by a specific grouping of functionality at 172 
different key locations (Cell Site, Edge Cloud, and Regional Cloud, which will be defined shortly), and an identification 173 
of whether functionality at a given location is provided by a proprietary solution with software coupled with hardware, 174 
or by a cloud architecture that meets the above requirements. 175 
The scope of this work clearly includes supporting all 5G technologies, i.e. E -UTRA and NR with both EPC-based 176 
Non-Standalone (NSA) and 5GC architectures. This implies that cloud/orchestration aspects of NSA (E -UTRA) are also 177 
supported. However, Version 1 primarily addresses 5G SA deployments. 178 
This technical report examines the constraints that drive a specific solution, and discuss the hierarchical properties of 179 
each solution, including a rough scale of the size of each cloud and a sense of the number of sub clouds expected to be 180 
served by a higher cloud.  Figure 3 shows as example of how multiple cell sites feed into a smaller number of Edge 181 
Clouds, and how in turn multiple Edge Clouds feed into a Regional Cloud.  For a given scenario, the Logical Functions 182 
are distributed in a certain way among each type of cloud, and the “cardinality” of the different functions will be 183 
discussed.   184 


                                                                                                           O-RAN WG6 Technical Report 
 
10 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 185 
Figure 3:  Different Clouds/ Sites 186 
This has implications on the processing power needed in each type of cloud, as well as implications on the 187 
environmental requirements.  This document will also discuss considerations of hardware chassis and components that 188 
are reasonable in each scenario, and the implications of managing such a cloud.   189 
Additional major areas for this document are listed below:   190 
 Mapping of logical functions to physical elements and locations, and implications of that mapping. 191 
 High-level assessment of critical performance requirements, and how that influences architecture. 192 
 Processor and accelerator options (e.g., x86, FPGA, GPU).  In order to determine whether a Network Function 193 
is a candidate for openness, there needs to be the possibility to have multiple suppliers of software for given 194 
hardware, and multiple sources of required chip/accelerators.   195 
 The Hardware Abstraction Layer, aka “Acceleration Abstraction Layer” needs to be addressed in light of 196 
various hardware options that could be used. 197 
 Cloud infrastructure makeup.  This includes considerations such as: 198 
 Deployments are allowed to use VMs, Containers in VMs, or just Containers.  199 
 Multiple Operating Systems are expected to be supported; e.g., open source Ubuntu, CentOS Linux, or 200 
Yocto Linux-based distributions, or selected proprietary OSs.   201 
 Management of a cloudified RAN introduces some new management considerations, because the mapping 202 
between Network Functionality and cloud platforms can be done in multiple ways, depending on the scenario 203 
that is chosen.  Thus, management of aspects that are related to platform aspects rather than RAN functional 204 
aspects need to be designed with flexibility in mind from the start.  For example, logging of physical functions, 205 
scale out actions, and survivability considerations are affected.   206 
 These management considerations are introduced in this document, but management documents will 207 
address the solutions. 208 
 The transport layer will be discussed, but only to the extent that it affects the architecture and design of the 209 
network.  For example, the chosen L1 technology may affect the performance of transport.  As another 210 
example, the use of a Fronthaul Gateway will affect economics as well as the placement options of certain 211 
Network Functions.  And of course, the existence of L2 switches in a cloud platform deployment will be 212 
required for efficient use of server resources. 213 
Additional areas could be considered in the future.   214 
2. References 215 
The following documents contain provisions which, through reference in this text, constitute provisions of this report. 216 
[1] 3GPP TS 38.470, NG-RAN; F1 general aspects and principles (Release 15), July 2019. 217 


                                                                                                           O-RAN WG6 Technical Report 
 
11 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
[2] 3GPP TR 21.905: Vocabulary for 3GPP Specifications. 218 
[3] eCPRI Interface Specification V1.2 (2018-06-25): Common Public Radio Interface:  eCPRI Interface 219 
Specification 220 
[4] eCPRI Transport Network V1.2 (2018-06-25) Requirements Specification: Common Public Radio Interface:  221 
Requirements for the eCPRI Transport Network  222 
[5] IEEE Std 802.1CM-2018 - Time-Sensitive Networking for Fronthaul  223 
[6] ITU-T Technical Report, GSTR-TN5G - Transport network support of IMT-2020/5G, October 2018.  224 
[7] O-RAN Fronthaul Control, User and Synchronization Plane Specification , Technical Specification O-RAN-225 
WG4.CUS.0-v02.00, August 2019.  See https://www.o-ran.org/specifications. 226 
[8] O-RAN Operations and Maintenance Architecture – v01.00, O-RAN Alliance Technical Specification, 227 
August 2019.  See https://www.o-ran.org/specifications. 228 
[9] O-RAN Operations and Maintenance Interface Specification – v1.0, O-RAN Alliance Technical 229 
Specification, August 2019.  See https://www.o-ran.org/specifications .  230 
3. Definitions and Abbreviations 231 
3.1. Definitions 232 
For the purposes of the present document, the terms and definitions given in 3GPP TR 21.905 [2] and the following 233 
apply. A term defined in the present document takes precedence over the definition of the same term, if any, in 3GPP 234 
TR 21.905 [2].  235 
Cell Site This refers to the location of Radio Units (RUs); e.g., placed on same structure as the Radio 236 
Unit or at the base.  The Cell Site in general will support multiple sectors and hence multiple 237 
O-RUs. 238 
Edge Cloud This is a location that supports virtualized RAN functions for multiple Cell Sites, and 239 
provides centralization of functions for those sites and associated economies of scale.  An 240 
Edge Cloud might serve a large physical area or a relatively small one close to its cell sites, 241 
depending on the Operator’s use case.  However, the sites served by the Edge Cloud must be 242 
near enough to the O-RUs to meet the delay requirements of the O-DU functions. 243 
F1 Interface  The open interface between O-CU and O-DU in this document is the same as that defined by 244 
the CU and DU split in 3GPP TS 38.473.  It consists of an F1-u part and an F1-c part. 245 
Managed Element  Term used in OAM to refer to a single entity managed as a whole by the Network 246 
Management System (NMS).  The Managed Element may contain multiple Managed or 247 
Network Functions and be physically deployed over one or more cloud platforms depending 248 
on the requirements of the Managed Functions.  249 
Managed Function  Term used in OAM to refer to a distinct logical function that is managed.  Examples include 250 
near-RT RIC, O-CU-CP, O-CU-UP, O-DU, and O-RU.     251 
From the OAM Framework document:  3GPP TS 28.622 states that a Managed Function 252 
(MF) can represent a telecommunication function either realized by software running on 253 
dedicated hardware or realized by software running on NFVI. Each managed function 254 
instance communicates with a manager (directly or indirectly) over one or more management 255 
interfaces exposed via its containing managed element instance. 256 
Network Function The near-RT RIC, O-CU-CP, O-CU-UP, O-DU, and O-RU logical functions that can be 257 
provided either by virtualized or non-virtualized methods.  258 
Regional Cloud This is a location that supports virtualized RAN functions for many Cell Sites in multiple 259 
Edge Clouds, and provides high centralization of functionality.   260 

                                                                                                           O-RAN WG6 Technical Report 
 
12 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
O-Cloud An O-RAN compliant cloud platform that is based on a server compute style architecture, 261 
and uses hardware accelerator add-ons where needed and a software stack that is decoupled 262 
from the hardware. It supports O-RAN-specified management interfaces. 263 
3.2. Abbreviations 264 
For the purposes of this document, the abbreviations given in 3GPP TR 21.905 [2] and the following apply.  265 
An abbreviation defined in the present document takes precedence over the definition  of the same abbreviation, if any, 266 
in 3GPP TR 21.905 [2]. 267 
3GPP Third Generation Partnership Project 268 
5G Fifth-Generation Mobile Communications 269 
API Application Programming Interface 270 
ASIC Application-Specific Integrated Circuit  271 
BBU BaseBand Unit 272 
BS Base Station 273 
CI Cloud Infrastructure 274 
CoMP   Co-Ordinated Multi-Point transmission/reception 275 
CNF Cloud-Native Network Function  276 
CNI Container Networking Interface 277 
CPU Central Processing Unit 278 
CR Cell Radius 279 
CU Centralized Unit as defined by 3GPP 280 
DFT Discrete Fourier Transform 281 
DL Downlink 282 
DPDK Data Plan Development Kit  283 
DU Distributed Unit as defined by 3GPP 284 
eMBB enhanced Mobile BroadBand 285 
EPC Evolved Packet Core 286 
E-UTRA Evolved UMTS Terrestrial Radio Access 287 
FCAPS Fault Configuration Accounting Performance Security  288 
FEC  Forward Error Correction 289 
FFT Fast Fourier Transform 290 
FH Fronthaul 291 
FH GW Fronthaul Gateway 292 
FPGA Field Programmable Gate Array 293 
GPP General Purpose Processor 294 
GPU Graphics Processing Unit  295 
HARQ Hybrid Automatic Repeat reQuest 296 
HW Hardware 297 
IEEE Institute of Electrical and Electronics Engineers 298 
IM Information Modelling, or Information Model 299 
IRQ Interrupt ReQuest  300 
ISA Instruction Set Architecture 301 
ISD Inter-Site Distance 302 
ITU International Telecommunications Union 303 
KPI Key Performance Indicator 304 
LCM Life Cycle Management 305 
LDPC  Low-Density Parity-Check  306 
LTE Long Term Evolution 307 
LVM Logic Volume Manager 308 
MEC Mobile Edge Computing 309 
mMTC massive Machine Type Communications 310 
MNO Mobile Network Operator 311 
NF Network Function 312 
NFD Node Feature Discovery 313 
NFVI Network Function Virtualization Infrastructure 314 
NIC Network Interface Card 315 
NMS Network Management System  316 
NR  New Radio 317 
NSA Non-Standalone 318 

                                                                                                           O-RAN WG6 Technical Report 
 
13 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
NUMA Non-Uniform Memory Access  319 
NVMe Non-Volatile Memory Express 320 
O-Cloud O-RAN Cloud Platform 321 
OCP  Open Compute Project 322 
O-CU O-RAN Central Unit  323 
O-CU-CP O-CU Control Plane 324 
O-CU-UP O-CU User Plane 325 
O-DU O-RAN Distributed Unit (uses Lower-level Split) 326 
O-RU O-RAN Radio Unit 327 
OTII Open Telecom IT Infrastructure 328 
OWD One-Way Delay 329 
PCI Peripheral Component Interconnect 330 
PNF Physical Network Function 331 
PoE Power over Ethernet 332 
PoP Point of Presence 333 
PTP Precision Time Protocol 334 
QoS  Quality of Service  335 
RAN Radio Access Network 336 
RAT Radio Access Technology 337 
RIC RAN Intelligent Controller  338 
RT Real Time 339 
RTT Round Trip Time 340 
RU Radio Unit  341 
SA Standalone 342 
SFC Service Function Chaining  343 
SMO Service Management and Orchestration 344 
SMP Symmetric MultiProcessing 345 
SoC System on Chip 346 
SR-IOV Single Root Input/ Output Virtualization 347 
SW Software 348 
TCO Total Cost of Ownership 349 
TNE Transport Network Element 350 
TR Technical Report 351 
TRP Transmission Reception Point 352 
TS Technical Specification 353 
Tx Transmitter 354 
UE User Equipment 355 
UL Uplink 356 
UMTS Universal Mobile Telecommunications System 357 
UP User Plane 358 
UPF User Plane Function 359 
URLLC Ultra-Reliable Low-Latency Communications 360 
vCPU virtual CPU 361 
VIM Virtualized Infrastructure Manager 362 
VNF Virtualized Network Function 363 
vO-CU Virtualized O-RAN Central Unit  364 
vO-CU-CP Virtualized O-CU Control Plane 365 
vO-CU-UP Virtualized O-CU User Plane 366 
vO-DU Virtualized O-RAN Distributed Unit 367 
4.  Overall Architecture  368 
This section addresses the overall architecture in terms of the Network Functions and infrastructure (PNFs, servers, and 369 
clouds) that are in scope. 370 
4.1. RAN Component Definitions 371 
This section reviews key RAN component definitions in O-RAN.  372 
 The O-DU/ O-RU split is defined as using Option 7-2x.  See [7].  373 

                                                                                                           O-RAN WG6 Technical Report 
 
14 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 The O-CU/ O-DU split is defined as using the CU/ DU split F1 as defined in 3GPP TS 38.470 [1].    374 
This document assumes these two splits.  375 
Below is a depiction of RAN functionality (inside the gray dashed line), structured to be consistent with the discussion 376 
in this document.  For example, note that the Platform is shown at the bottom, and a given function could be supported 377 
by a proprietary platform or by an O-Cloud, depending on the deployment scenario.  The dashed line in the figure 378 
indicates a case in which the O-RU is implemented in a proprietary way, and the other functions are supported by an O-379 
Cloud.   380 
 381 
Figure 4:  Architecture Overview 382 
4.2. Degree of Openness 383 
In theory, every architecture component could be open in every sense imaginable, but in practice it is likely that 384 
different components will have varying degrees of openness due to economic and other implementation considerations.  385 
Some factors are significantly affected by the deployment scenario; for example, what might be viable in an indoor 386 
deployment might not be viable in an outdoor deployment.   387 
Increasing degrees of openness for a Physical Network Function (PNF) or cloud supporting RAN function(s) are: 388 
A. Interfaces among Network Functions are open; e.g., E2, F1, and Open Fronthaul are used. Therefore, Network 389 
Functions in different PNFs/clouds from different vendors can interconnect. 390 
B. In addition to having open connections as described above, the chassis of servers in a cloud are open and can 391 
accept blades/sleds from multiple vendors.  However, the blades/sleds have RAN software that is not decoupled 392 
from the hardware. 393 


                                                                                                           O-RAN WG6 Technical Report 
 
15 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
C. In addition to having open connections and an open chassis, a specific blade/sled uses software that is decoupled 394 
from the hardware.  In this scenario, the software could be from one supplier, the blade/sled could be from 395 
another, and the chassis could be from another.   396 
Categories A and B have PNFs/clouds with proprietary internal designs.  Category C is an open solution that we are 397 
calling an O-Cloud, and is subject to the cloudification discussion and requirements. 398 
In this document, the degree of openness for each PNF/cloud can vary by scenario. The question of which Network 399 
Functions should be split vs. combined, and the degree of openness in each one, is addressed in the discussion of 400 
scenarios.  401 
4.3. Decoupling of Hardware and Software  402 
There are three layers that we must consider when we discuss decoupling of hardware and software:  403 
 The hardware layer, shown at the bottom in Figure 5.  (In the case of a VM deployment, this maps basically to 404 
the ETSI “NFVI HW” layer.) 405 
 A middle layer that includes Cloud Stack functions as well as hardware abstraction functions.  (In the case of a 406 
VM deployment, these seem to map to the ETSI “NFVI SW” + VIM.) 407 
 A top layer that supports the virtual RAN functions.   408 
Each layer can come from a different supplier.  The first aspect of decoupling has to d o with ensuring that a Cloud 409 
Stack can work on multiple suppliers’ hardware; i.e., it does not require vendor -specific hardware.   410 
The second aspect of decoupling has to do with ensuring that a Cloud Platform can support RAN virtualized functions 411 
from multiple RAN software suppliers.  If this is possible, then we say that the Cloud Platform (which includes the 412 
hardware that it runs on) is an O-RAN Cloud Platform, or “O-Cloud”.  See Figure 5 below.   413 
 414 
Figure 5:  Decoupling, and Illustration of the O-Cloud Concept 415 
The general definition of the O-Cloud Cloud Platform includes the following characteristics: 416 
1. The Cloud Platform is a set of hardware and software components that provide cloud computing capabilities to 417 
execute RAN network functions. 418 
2. The Cloud Platform hardware includes compute, networking and storage components, and may also include 419 
various acceleration technologies required by the RAN network functions to meet their performance 420 
objectives. 421 
3. The Cloud Platform software exposes open and well-defined APIs that enable the management of the entire 422 
life cycle for network functions.  423 
4. The Cloud Platform software is decoupled from the Cloud Platform hardware (i.e., it can typically be sourced 424 
from different vendors). 425 
The scope of this document includes listing specific requirements of the Cloud Platform to support execution of the 426 
various O-RAN Network Functions. 427 
An example of a Cloud Platform is an OpenStack and/or a Kubernetes deployment on a set of COTS serve rs (including 428 
FPGA and GPU cards), interconnected by a spine/leaf networking fabric.  429 


                                                                                                           O-RAN WG6 Technical Report 
 
16 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
There is an important interplay between specific virtualized RAN functions and the hardware that is needed to meet 430 
performance requirements and to support the functionality economically.  Therefore, a hardware/ cloud platform 431 
combination that can support, say, a vO-CU function might not be appropriate to adequately support a vO-DU function.  432 
When RAN functions are combined in different ways in each specific deployment scena rio, these aspects must be 433 
considered.   434 
Below is a high-level conceptual example of how different accelerators, along with their associated cloud capabilities, 435 
can be required for different RAN functions.  Although we do not specify any particular hardwar e requirement or cloud 436 
capability here, we can note some general themes.  For example, any RAN function that involves real -time movement 437 
of user traffic will require the cloud platform to control for delay and jitter, which may in turn require features suc h as 438 
real-time OSs, avoidance of frequent interrupts, CPU pinning, etc.   439 
 440 
Figure 6:  Relationship Between RAN Functions and Demands on Cloud Infrastructure and Hardware  441 
Please note that any cloud that has features required for a given function (e.g., for O-DU) can also support functions that 442 
do not require such features.  For example, a cloud that can support O-DU can also support functions such as O-CU-CP.    443 
5.  Deployment Scenarios:  Common Considerations 444 
In any implementation of logical network functionality, decisions need to be made regarding which logical functions are 445 
mapped to which Cloud Platforms, and therefore which functions are to be co-located with other logical functions.  In 446 
this document we do not prescribe one specific implementation, but we do understand that in order to establish 447 
agreements and requirements, the manner in which the Network Functions are mapped to the same or different Cloud 448 
Platforms must be considered.   449 
We refer to each specific mapping as a “deployment scenario”.  In this section, we examine the deployment scenarios 450 
that are receiving the most consideration.  Then we will select the one or ones that should be the focus of initial scenario 451 
reference design efforts. 452 
5.1. Mapping Logical Functionality to Physical Implementations 453 
There are many aspects that need to be considered when deciding to implement logical functions in distinct O-Clouds.  454 
Some aspects have to do with fundamental technical constraints and economic considerations, while others have to do 455 
with the nature of the services that are being offered.   456 
 Technical Constraints that Affect Hardware Implementations   457 
Below are some factors that will affect the cost of implementations, and can drive a carrier to require separation of or 458 
combining of different logical functions.   459 
 Environment:  Equipment may be deployed in indoor controlled environments (e.g., Central Offices), semi -460 
controlled environments (e.g., cabinets with fans and heaters), and exposed environments (e.g., Radio Units on 461 


                                                                                                           O-RAN WG6 Technical Report 
 
17 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
a tower).  In general, the less controlled the environment, the more difficult and expensive the equipment will 462 
be.  The required temperature range is a key design factor, and can drive higher power requirements.   463 
 Dimensions:  The physical dimensions can also drive deployment constraints – e.g., the need to fit into a tight 464 
cabinet, or to be placed safely on a tower or pole.   465 
 Transport technology:  The transport technology used for Fronthaul, Midhaul, and Backhaul is often fiber, 466 
which has an extremely low and acceptable loss rate.  However, there are options other than fiber, in particular 467 
wireless/ microwave, where the potential for data loss must be considered.  This will be discussed further  in the 468 
next section. 469 
 Acceleration Hardware:  The need for acceleration hardware can be driven by the need to meet basic 470 
performance requirements, but can also be tied to some of the above considerations.  For example, a hardware 471 
acceleration chip (COTS or proprietary) can result in lower power use, less generated heat, and smaller 472 
physical dimensions than if acceleration is not used.  On the other hand, some types of hardware acceleration 473 
chips might not be “hardened” (i.e., they might only operate properly in a restricted environment), and could 474 
require a more controlled environment such as in a central office. 475 
The acceleration hardware most often referred to includes: 476 
 Field Programmable Gate Arrays (FPGAs) 477 
 Graphical Processing Units (GPUs) 478 
 System on Chip (SoC) 479 
 Standardized Hardware:  Use of standardized hardware designs and standardized form factors can have 480 
advantages such as helping to reduce operations complexity, e.g., when an operator makes periodic technology 481 
upgrades of selected components.  An example would be to use an Open Compute Project (OCP) or Open 482 
Telecom IT Infrastructure (OTII) –based design.   483 
 Service Requirements that Affect Implementation Design  484 
RANs can serve a wide range of services and customer requirements, and each market can drive some unique 485 
requirements.  Some examples are below. 486 
 Indoor or outdoor deployment:  Indoor deployments (e.g., in a public venue like a sports stadium, train 487 
station, shopping mall, etc.) often enjoy a controlled environment for all elements, including the Radio Units.  488 
This can improve the economics of some indoor deployment scenarios.  The distance between Network 489 
Functions tends to be much lower, and the devices that support O-RU functionality may be much easier and 490 
cheaper to install and maintain. This can affect the density of certain deployments, and the frequency that 491 
certain scenarios are deployed.   492 
 Bands supported, and Macro cell vs. Small cell:  The choice of bands (e.g., Sub-6 GHz vs. mmWave) might 493 
be driven by whether the target customers are mobile vs. fixed, and whether a clear line of sight to the 494 
customer is available or is needed. The bands to be supported will of course affect O-RU design.  In addition, 495 
because mmWave carriers can support much higher channel width (e.g., 400 MHz vs. 20 MHz), mmWave 496 
deployments can require a great deal more O-DU and O-CU processing power.  And of course the operations 497 
costs of deploying Macro cells vs. Small cells differ in other ways.   498 
 Performance requirements of the Application / Network Slice:  Ultimately, user applications drive 499 
performance requirements, and RANs are expected to support a very wide range of applications.  For example, 500 
the delay requirements to support a Connected Car application using Ultra Reliable Low Latency 501 
Communications (URLLC) will be more demanding than the delay requirements for other types of 502 
applications.  In our discussion of 5G, we can start by considering requirements separately for URLLC, 503 
enhanced Mobile Broadband (eMBB), and massive Machine Type Communications (mMTC). 504 
The consideration of performance requirements is a primary one, and is the subject of Section 5.2.  505 
 Rationalization of Centralizing O-DU Functionality 506 
Almost all Scenarios to be discussed in this document involve a degree of centralization of O -DU.  In this section it is 507 
assumed that O-DU resources for a set of O-RUs are centralized at the same location.   508 
Editor’s Note:  While most Scenarios also centralize O-CU-CP, O-CU-UP, and RIC in one form or another, the 509 
benefits of centralizing them are not discussed in this section.  510 

                                                                                                           O-RAN WG6 Technical Report 
 
18 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
Managing O-DU in equipment at individual cell sites (via on-site BBUs today) has multiple challenges, including: 511 
 If changes are needed at a site (e.g., adding radio carriers), then adding equipment is a coarse -grained activity – 512 
i.e., one cannot generally just add “another 1/5 of a box”, if that is all that is needed.  Adding the minimum 513 
increment of additional capacity might result in poor utilization and thereby prevent expansion at that site.   514 
 Cell sites are in many separate locations, and each requires establishment and maintenanc e of an acceptable 515 
environment for the equipment.  In turn this requires separate visits for any physical operations.  516 
 Micro sites tend to have much lower average utilization than macro sites, but each can experience considerable 517 
peaks. 518 
 “Planned obsolescence” occurs, due to ongoing evolution of smartphone capabilities and throughput 519 
improvements, as well as introduction of new features and services.  It is common practice today to upgrade 520 
(“forklift replace”) BBUs every 36-60 months. 521 
These factors motivate the centralization of resources where possible.  For the O-DU function, we can think of two 522 
types of centralization: simple centralization and pooled centralization.   523 
If the equipment uses O-DU centralization in an Edge Cloud, at any given hour an O-RU will be using a single specific 524 
O-DU resource that is assigned to it (e.g. via Kubernetes).  On a broad time scale, traffic from any cell site can be 525 
rehomed, without any physical work, to use other/additional resources that are available at that Edge Cloud location.  526 
This would likely be done infrequently; e.g., about as often as cell sites are expanded.   527 
Centralization can have some additional benefits, such as only having to maintain a single large controlled environment 528 
for many cell sites rather than creating and maintaining many distributed locations that might be less controlled (e.g., 529 
outside cabinets or huts).  Capacity can be added at the central site and assigned to cell sites as needed.  Note that simple 530 
centralization still assigns each O-RU to a single O-DU resource1, as shown below, and that traffic from one O-RU is 531 
not split into subsets that could be assigned to different O-DUs.  Also note that a Fronthaul (FH) Gateway (GW) may 532 
exist between the cell site and the centralized resources, not only to improve economics but also to enable traffic re-533 
routing when desired.  534 
 535 
Figure 7:  Simple Centralization of O-DU Resources 536 
By comparison, with pooled centralization, traffic from an O-RU (or subsets of the O-RU’s traffic) can be assigned 537 
more dynamically to any of several shared O-DU resources.  So if one cell site is mostly idle and another experiences 538 
high traffic demand, the traffic can be routed to the appropriate O-DU resources in the shared pool.  The total resources 539 
of this shared pool can be smaller than resources of distributed locations, because the peak of the sum of the traffic will 540 
be markedly lower than the sum of the individual cell site traffic peaks.   541 
                                                           
1 In this figure, each O-DU block can be thought of as a unit of server resources that includes a hardware accelerator, a GPP, memory and any other 
associated hardware. 


                                                                                                           O-RAN WG6 Technical Report 
 
19 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 542 
Figure 8:  Pooling of Centralized O-DU Resources 543 
We note that being able to share O-DU resources somewhat dynamically is expected to be a solvable problem, although 544 
we understand that it is by no means a trivial problem.  There are management considerations, among others.  There 545 
may be incremental steps toward true shared pooling, where rehoming of O-RUs to different O-DUs can be performed 546 
more dynamically, based on traffic conditions. 547 
It is noted that O-DU centralization benefits the most dense networks where several cell sites are within  the O-RU to O-548 
DU latency limits.  Sparsely populated areas most probably will be addressed by vO -CU centralization only.   549 
Figure 9 shows the results of an analysis of a simulated greenfield deployment as an attempt to visualize the relative 550 
merit of simple centralization of O-DU (“oDU”) vs. pooled centralization of O-DU (“poDU”) vs. legacy DU (“BBU”), 551 
plotted against the realizable Cell Site pool size.  552 
 553 
Figure 9:  Comparison of Merit of Centralization Options vs. Number of Cell Sites in a Pool 554 
An often-used measure is related to the power required to support a given number of carrier MHz.  The lower the power 555 
used per carrier, the more efficient is the implementation.  In Figure 9, the values of each curve are normalized to the 556 
metric of Watts/MHz for distributed legacy BBUs, normalized to equal 1.  Please note that in this diagram, a lower 557 
value is better.  The following assumptions apply to the figure:   558 
 A legacy BBU processes X MHz (for carriers) and consumes Y watts.  For example, a specific BBU might 559 
process 1600 MHz and consume 160 watts.  560 
 N legacy BBUs will process N x X MHz and consume N x Y watts and have a merit figure of 1, per 561 
normalization.  If a given site requires less than X MHz, it will still be necessary to deploy an X MHz BBU.  562 
For example, we may need only 480 MHz but still deploy a 1600 MHz BBU.  563 


                                                                                                           O-RAN WG6 Technical Report 
 
20 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 Simple Centralization (the “oDU” line):  In this case, active TRPs are statically mapped to specific VMs and 564 
vO-DU tiles2.  Fewer vO-DU tiles are required to support the same number of TRPs, because MHz per site is 565 
not a constant. 566 
 Independent of resources to support active user traffic, a fixed power level is required to power Ethernet 567 
“frontplane” switches and hardware to support management and orchestration processes. 568 
 In a pool, processing capacity will be added over time as required. 569 
 Due to mobility traffic behavior, tiles will not be fully utilized, although centralization of resources will 570 
improve utilization when compared with a legacy BBU approach.   571 
 Centralization with more dynamic pooling (the “poDU” line): In addition to active load balancing,  individual 572 
traffic flows (which can last from a few hundreds of msecs to several seconds) will be routed to the least used 573 
tile, further optimizing (reducing) vO-DU tile requirements.   574 
 As in the simple centralization approach above, there is a fixed power level required for hardware that 575 
supports switching, management and orchestration processes. 576 
As a final note, any form of centralization requires efficient transport between the O-RU and the O-DU resources.  577 
When O-RU functionality is distributed over a relatively large area (e.g., not concentrated in a single large building), 578 
the existence of a Fronthaul Gateway is a key enabler.   579 
5.2. Performance Aspects 580 
Performance requirements drive architectural and design considerations.  Performance can include attributes such as 581 
delay, packet loss, transmission loss, and delay variation (aka “jitter”).   582 
Editor’s Note:  While all aspects are of interest, delay has the largest impact on network design and will be the 583 
focus of the current version of this document.  Future versions can address other performance aspects if 584 
desired and is FFS.   585 
 User Plane Delay 586 
This section discusses the framework for discussing delay of user-plane packets3, and also general delay numbers that it 587 
can be agreed that apply across all scenarios.  Details relevant to a specific Scenario will be discussed  in each 588 
Scenario’s subsection, as applicable. The purpose of these high-level targets is to act as a baseline for allocating the 589 
total latency budget to subsystems that are on the path of each constraint, as required for system engineering and 590 
dimensioning calculations, and to assess the impact on the function place ment within the specific network site tiers.   591 
The goal is to establish reasonable maximum delay targets, as well as to identify and document the major infrastructure 592 
as well as O-RAN NF-specific delay contributing components. For each service or element, minimum delay should be 593 
considered to be zero. The implication of this is that any of the elements can be moved towards the Cell Site (e.g. in a 594 
fully distributed Cloud RAN configuration, all of O-CU-UP, O-DU and O-RU would be distributed to Cell Site).  595 
In real network deployments, the expectation is that, depending on the operator -specific implementation constraints 596 
such as location and fiber availability, deployment area density, etc., deployments result in anything between the fully 597 
distributed and maximally centralized configuration. Even on one operator’s network, it is common that there are many 598 
different sizes of Edge Cloud instances, and combinations of Centralized and Distributed architectures in same network 599 
are also common (e.g. network operator may choose to centralize the deployments on dense Metro areas to the extent 600 
possible and distribute the configurations on suburban/rural areas with larger cell sizes / cell density that do not translat e 601 
to pooling benefits from more centralized architecture). However, the maximum centralization within the constraints of 602 
latencies that can be tolerable is useful for establishing the basis for dimensioning of the maximum sizes, especially for 603 
the Edge and Regional cloud PoPs. 604 
Figure 10 below illustrates the relationship among some key delay parameters.   605 
                                                           
2 A “vO-DU tile” refers to a chip or System on Chip (SoC) that provides hardware acceleration for math-intensive functionality such as that required 
for Digital Signal Processing.  With the Option 7.2x split, acceleration of Forward Error Correction (FEC) functionality is required, and other 
functionality could be considered for acceleration if desired. 
3 Delay of control plane or OAM traffic is not considered in this section.  

                                                                                                           O-RAN WG6 Technical Report 
 
21 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 606 
Figure 10:  Major User Plane Latency Components, by 5G Service Slice and Function Placement  607 
Please note the following: 608 
 NOTE 1: If the T2 or/and T3 transport network(s) is/are Packet Transport Network(s), then time allocation for 609 
the transport network elements processing and queuing delays will require some portion of maximum latency 610 
allocation, and will require reduction of the maximum area accordingly. 611 
 NOTE 2: Site Internal / fabric networks are not shown for clarity, but need some latency allocation (effectively 612 
extensions or part of transport delays; per PoP tier designations TE1, TE2, TE3 and TC). 613 
 NOTE 3: To maximize the potential for resource pooling benefits, minimize network function redundancy 614 
cost, and minimize the amount of hardware / power in progressively more distributed sites (towards UEs), 615 
target design should attempt to maximize the distances and therefore latencies available for transport network s 616 
within the service- and RAN-specific time constraints, especially for TT1. 617 
 NOTE 4: UPF, like EC/MEC, is outside of the scope of O-RAN, so UPF shown as a “black box” to illustrate 618 
where it needs to be placed in context of specific services to be able to take advantage of the RAN service-619 
specific latency improvements. 620 
Figure 10 represents User Equipment locations on the right, and network tiers towards the left, with increasing latency 621 
and increasing maximum area covered per tier towards the left. These Mobile Network Operator’s (MNO’s) Edge tiers 622 
are nominated as Cell Site, Edge Cloud, and Regional Cloud, with one additional tier nominated as Core Cloud in the 623 
figure. 624 
The summary of the associated latency constraints as well as major latency contributing components as depicted in 625 
Figure 10 above is given in Table 1, below. 626 
Table 1:  Service Delay Constraints and Major Delay Contributors 627 
RAN Service-Specific User Plane Delay Constraints 
Identifier Brief Description 
Max. OWD 
(ms) 
Max. RTT 
(ms) 
URLLC Ultra-Reliable Low Latency Communications (3GPP) 0.5 1 
URLLC Ultra-Reliable Low Latency Communications (ITU) 1 2 
eMBB enhanced Mobile Broadband 4 8 


                                                                                                           O-RAN WG6 Technical Report 
 
22 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
mMTC massive Machine Type Communications 15 30 
Transport Specific Delay Components 
TAIR Transport propagation delay over air interface     
TE1 Cell Site Switch/Router delay     
TT1 Transport delay between Cell Site and Edge Cloud 0.1 0.2 
TE2 Edge Cloud Site Fabric delay     
TT2 Transport delay between Edge and Regional Cloud 1 2 
TE3 Regional Cloud Site Fabric delay     
TT3 Transport delay between Regional  and Core Cloud 10 20 
TC Core Cloud Site Fabric delay     
Network Function Specific Delay Components 
TUE Delay Through the UE SW and HW stack     
TRU Delay Through the O-RU User Plane     
TDU Delay Through the O-DU User Plane     
TCU-UP Delay Through the O-CU User Plane     
 628 
The transport network delays are specified as maximums, and link speeds are con sidered to be symmetric for all 629 
components with exception of the air interface (TAIR).  For the S-Plane services utilizing PTP protocol, it is a 630 
requirement that the link lengths, link speeds and forward-reverse path routing for PTP are all symmetric. 631 
Radios (O-RUs) are always located in the Cell Site tier, while O-DU can be located “up to” Edge Cloud tier. It is 632 
possible to move any of the user plane NF instances closer towards the cell site, as implicitly they would be inside the 633 
target maximum delay, but it is not necessarily possible to move them further away from the Cell Sites while remaining 634 
within the RAN internal and/or RAN service-specific timing constraints.  A common expected deployment case is one 635 
where O-DU instances are moved towards or even to the Cell Site and O-RUs (e.g. in Distributed Cloud-RAN 636 
configurations), or in situations where the Edge Cloud needs to be located closer to the Cell Site due to fiber and/or 637 
location availability, or other constraints. While this is expected to work well from the delay constraints perspective, the 638 
centralization and pooling-related benefits will be potentially reduced or even eliminated in the context of such 639 
deployment scenarios.  640 
The maximum transport network latency between the site hosting O-DU(s) and sites hosting associated O-RU(s) is 641 
primarily determined by the RAN internal processes time constraints (such as HARQ loop, scheduling, etc., time-642 
sensitive operations). For the purposes of this document, we use 100us latency, which is commonly used as a target 643 
maximum latency for this transport segment in related industry specifications for user-plane, specifically “High100” on 644 
E-CPRI transport requirements [4] section 4.1.1, as well as “Fronthaul” latency requirement in ITU technical report 645 
GSTR-TN5G [6], section 7-2, and IEEE Std 802.1CM-2018 [5], section 6.3.3.1.  Based on the 5us/km fiber propagation 646 
delay, this implies that in a 2D Manhattan tessellation model, which is a common simple topology model for dense 647 
urban area fiber routing, the maximum area that can be covered from a single Edge Cloud tier site hosting O-DUs is up 648 
to a 400km2  area of Cell Sites and associated RUs.  Based on the radio inter-site distances, number of bands and other 649 
radio network dimensioning specific parameters, this can be used to estimate the maximum number of Cell Sites and 650 
cell sectors that can be covered from single Edge Cloud tier location, as well as maximum number of UEs in this 651 
coverage area. 652 
The maximum transport network latencies towards the entities located at higher tiers are constrained by the lower of F1 653 
interface latency (max 10 ms as per GSTR-TN5G [6], section 7.2), or alternatively service-specific latency constraints, 654 
for the edge-located services that are positioned to take advantage of improved latencies.   For eMBB, UE-CU latency 655 
target is 4ms one-way delay, while for the URLLC it is 0.5ms as per 3GPP (or 1ms as per ITU requirements). The 656 
placement of the O-CU-UP as well as associated UPF, to be able to provide URLLC services would have to be at most 657 
at the Edge Cloud tier to satisfy the service latency constraint. For the eMBB services with 4ms OWD target, it is 658 
possible to locate O-CU-UP and UPF on next higher latency location tier, i.e. Regional Cloud tier. Note that while not 659 
shown in the picture, Edge compute / Multi-Access Edge Compute (MEC) services for a given RAN service type are 660 
expected to be collocated with the associated UPF function to take advantage of the associated service latency reduction 661 
potential.  662 

                                                                                                           O-RAN WG6 Technical Report 
 
23 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
For the services that do not have specific low-latency targets, the associated O-CU-UP and UPF can be located on 663 
higher tier, similar to deployments in typical LTE network designs. This is designated as Core Cloud tier in the example 664 
in Figure 10 above.  For eMBB services, if there are no local service instances in the Edge or Regional clouds to take 665 
advantage of the 4ms OWD enabled by eMBB service definition, but the associated services are provided from either 666 
core clouds, external networks or from other Edge Cloud / RAN instances (in case of user -to-user traffic), the associated 667 
non-constrained (i.e. over 4ms from subscriber) eMBB O-CU-UP and UPF instances can be located in Core Cloud sites 668 
without perceivable impact to the service user, as in such cases the transport and/or service -specific latencies are 669 
dominant latency components.  670 
The intent of this section is not to micromanage the latency budget, but to rather establish a reasonable baseline for 671 
dimensioning purposes, particularly to provide basic assessment to enable sizing of the cloud tiers wi thin the context of 672 
the service-specific constraints and transport allocations. As such, we get the following “allowances” for the aggregate 673 
unspecified elements: 674 
 URLLC3GPP: 0.5ms - 0.1ms (TT1) = 0.4ms  ≥ TUE + TAIR + TE1 + TRU + 2(TE2) + TDU + TCU-UP 675 
 URLLCITU: 1ms - 0.1ms (TT1) = 0.9ms  ≥ TUE + TAIR + TE1 + TRU + 2(TE2) + TDU + TCU-UP 676 
 eMBB: 4ms - 0.1ms (TT1) - 1ms (TT2) = 2.9ms ≥ TUE + TAIR + TE1 + TRU + 2(TE2) + TDU + TE3 + TCU-UP 677 
 mMTC15: 15ms - 0.1ms (TT1) - 1ms (TT2) - 10ms (TT3) = 3.9ms ≥ TUE + TAIR + TE1 + TRU + 2(TE2) + TDU + TE3 + 678 
TCU-UP + TC 679 
 680 
If required, we may provide more specific allocations in later versions of the document, as we gain more 681 
implementation experience and associated test data, but at this stage it is considered to be premature to do so. It should 682 
also be noted that the URLLC specification is still work in progress at this stage in 3GPP, so likely first 683 
implementations will focus on eMBB service, which leaves 2.9ms for combined O -RAN NFs, air interface, UE and 684 
cloud fabric latencies. 685 
It is possible that network queuing delays may be the dominant delay contributor for some service classes. However, 686 
these delay components should be understood to be in context of the most latency-sensitive services, particularly on 687 
RU-DU interfaces, and relevant to the system level dimensioning. It is expected that if we will have multiple QoS 688 
classes, then the delay and loss parameters are specified on per-class basis, but such specification is outside of scope of 689 
this section.  690 
The delay components in this section are based on presently supported O -RAN splits, i.e. 3GPP reference split 691 
configurations 7-2 & 8 for the RU-DU split (as defined in O-RAN), and 3GPP split 2 for F1 (as defined in O-RAN) and 692 
associated transport allocations, and constraints are based on the 5G service requirements from ITU & 3GPP.  693 
Other extensions have been approved and included in version 2.0 of the O-RAN Fronthaul specification [7], which 694 
allow for so called “non-ideal” Fronthaul. It should be noted that while they allow substantially larger delays (e.g. 10 695 
ms FH splits have been described and implemented outside of O-RAN), they cannot be considered for all possible 5G 696 
use cases, as for example it is clearly impossible to meet the 5G service-specification requirements over such large 697 
delay values over the FH for URLLC or even 4 ms eMBB services. In addition, in specific scenarios (e.g. high -speed 698 
users), adding latency to the fronthaul interface can result in reduced performance, and lower potential benefits, e.g. in 699 
Co-Ordinated Multi-Point (CoMP) mechanisms. 700 
5.3. Hardware Acceleration Options 701 
Cloud platforms consist of GPP CPUs, Memory, Networking I/O, and may also provide HW accelerators to offload 702 
computational-intense functions with the aim of optimizing the performance of the VNF (e.g., O -DU, O-CU-CP, O-CU-703 
UP, RIC).  There are many different types of HW accelerators: FPGA, ASIC, GPU and many different types of 704 
acceleration functions, such as Low-Density Parity-Check (LDPC) Forward Error Correction (FEC) for O-DU, Wireless 705 
Cipher for O-CU, and Artificial Intelligence for RIC.  The combination of HW accelerator and acceleration function, 706 
and indeed the option to use HW acceleration, is the vendor’s choice; however all types of HW acceleration on the 707 
cloud platform should ensure the decoupling of SW from HW. The decoupling of HW and SW implies the following 708 
key objectives:  709 
 Multiple vendors of hardware GPP CPUs and accelerators (e.g., FGPA, DSP, or GPU) can support cloud 710 
platforms (including agreed-upon abstraction layers) from multiple vendors, which in turn can support the 711 
software providing RAN functionality. 712 
 A given hardware and cloud platform shall support RAN software (including RIC, O -CU-CP, O-CU-UP, O-713 
DU, and possibly O-RU functionality in the future) from multiple vendors. 714 

                                                                                                           O-RAN WG6 Technical Report 
 
24 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 HW Acceleration Abstraction  715 
There are different methods of abstraction that should be considered for HW acceleration on the cloud platform; these 716 
are:  717 
 HW Accelerator Deployment model  718 
 HW Accelerator Application APIs  719 
 720 
 721 
Figure 11:  HW Abstraction Considerations 722 
 HW Accelerator Deployment Model  723 
Figure 11 above presents two common HW deployment models, an abstracted implementation utilizing a vhost_user 724 
and virtIO type deployment, and a pass-through model using SR-IOV. While the abstracted model allows a full 725 
decoupling of the Network Function (NF) from the HW accelerator, this model may not suit real-time latency sensitive 726 
NFs such as the O-DU. For low-latency HW acceleration, SR-IOV pass through may be required. The SR-IOV pass 727 
through model is also supported in container environments.  728 
 HW Accelerator Application APIs  729 
To allow multiple NF vendors to utilize the same HW accelerator on the cloud platform, HW Accelerators must provide 730 
an open-sourced API. The API shall allow the NF to discover the HW capabilities assigned to it, and submit and 731 
retrieve acceleration requests/responses. Examples of open APIs include DPDK’s CryptoDev, E thDev, EventDev, and 732 
Base Band Device (BBDEV).  733 
 HW Accelerator Management and Orchestration Considerations 734 
The HW accelerators shall be capable of being managed and orchestrated. In particular, HW accelerators shall support 735 
feature discovery and life cycle management.  Existing Open Source solutions may be leveraged for both VMs and 736 
containers as specified in O1*.  Examples include OpenStack Nova and Cyborg. An example for container deployments 737 
is seen in Kubernetes which provides a device plugin framework for vendors to advertise their device and associated 738 
resources to the Kubelet for management.   739 
5.4. Cloud Considerations 740 
In this section we talk about the list of cloud platform capabilities which is expected to be provided by the cloud 741 
platform to be able to support the deployment of the scenarios which are covered by this document.  742 
It is assumed that some or all deployment scenarios may be using VM orchestrated/managed by OpenStack and / or 743 
Container managed/orchestrated by Kubernetes, and therefore this section will cover both options. 744 


                                                                                                           O-RAN WG6 Technical Report 
 
25 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
The discussion in most sub-sections of this section is structured into (up to) three parts:  (1) Common, (2) Container 745 
only, and (3) VM only.  746 
 Networking requirements 747 
A Cloud Platform should have the ability to support high performance N – S and E – W networking, with high 748 
throughput and low latency.  749 
 Support for Multiple Networking Interfaces 750 
Common:  In the different scenarios, near-RT RIC, vO-CU, and vO-DU all depend on having support for multiple 751 
network interfaces. The Cloud Platform is required to support the ability to assign multiple networking interfaces to a 752 
single container or VM instance, so that the cloud platform could support successful deployment for the different 753 
scenarios.  754 
Container-only:  For example, the cloud platform can achieve this by supporting the implementation of Multus 755 
Container Networking Interface (CNI) Plugin. For more details, please see https://github.com/intel/multus-cni. 756 
 757 
Figure 12:  Illustration of the Network Interfaces Attached to a Pod, as Provisioned by Multus CNI 758 
VM-only:  OpenStack provides the Neutron component for networking. For more details, please see 759 
https://docs.openstack.org/neutron/stein/  760 
 Support for High Performance N-S Data Plane 761 
Common:  The Fronthaul connection between the O-RU/RU and vO-DU requires high performance and low latency. 762 
This means handling packets at high speed and low latency. As per the different scenarios covered in this document, 763 
multiple vO-DUs may be running on the same physical cloud platform, which will result in the need for sharing the 764 
same physical networking interface with multiple functions. Typically, the SR -IOV networking interface is used for 765 
this. 766 
The cloud platform will need to provide support for assigning SR-IOV networking interfaces to a container or VM 767 
instance, so the instance can use the network interface (physical function or virtual function) directly without using a 768 
virtual switch.  769 
If only one container needs to use the networking interface, the PCI pass-through network interface can provide high 770 
performance and low latency without using a virtual switch. 771 
In general, the following two items are needed for high performance N-S data throughput: 772 
 Support for SR-IOV; i.e., the ability to assign SR-IOV NIC interfaces to the containers/ VMs 773 
 Support for PCI pass-through for direct access to the NIC by the container/ VM  774 
Container-only:  When containers are used, the cloud platform can achieve this by supporting the implementati on of 775 
SR-IOV Network device plugin for Kubernetes. For more details, please refer to https://github.com/intel/sriov-network-776 
device-plugin  777 
VM-only: OpenStack provides the Neutron component for networking. For more details, please see 778 
https://docs.openstack.org/neutron/stein/admin/config-sriov.html . 779 


                                                                                                           O-RAN WG6 Technical Report 
 
26 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 Support for High-Performance E-W Data Plane 780 
Common:  High-performance E-W data plane throughput is a requirement for the implementation of the different near -781 
RT RIC, vO-CU, and vO-DU scenarios which are covered in this document.  782 
One of commonly used options for E-W high-performance data plane is the use of a virtual switch which provides basic 783 
communication capability for instances deployed at either the same machine or different machines. It provides L2 and 784 
L3 network functions.  785 
To get the high performance required, one of the options is to use a Data Plan Develop ment Kit (DPDK)-based virtual 786 
switch.  Using this method, the packets will not go into Linux kernel space networking, and instead will implement 787 
userspace networking which will improve the throughput and latency. To support this, the container or VM instan ce 788 
will need to use DPDK to accelerate packet handling.  789 
The cloud platform will need to provide the mechanism to support the implementation of userspace networking for 790 
container(s) / VM(s). 791 
Container-only:  As an example, the cloud platform can achieve this by supporting implementation of Userspace CNI 792 
Plugin. For more details, please refer to https://github.com/intel/userspace-cni-network-plugin. 793 
 794 
Figure 13:  Illustration of the Userspace CNI Plugin 795 
VM-only:  OVS DPDK is an example of a Host userspace virtual switch and could provide high performance L2/L3 796 
packet receive and transmit.   797 
 Support for Service Function Chaining  798 
Common:  Support for a Service Function Chaining (SFC) capability requires the ability to create a service function 799 
chain between multiple VMs or containers. In the virtualization environment, multiple instances will usually be 800 
deployed, and being able to efficiently connect the instances to provide service will be a fundamental requirement.  801 
The ability to dynamically configure traffic flow will provide flexibility to Operators.  When the service requirement or 802 
flow direction needs to be changed, the Service Function Chaining capability can be used to easily implement it instead 803 
of having to restart and reconfigure the services, networking configuration and Containers/VMs.  804 
Container-only: An example of SFC functionality is found at: https://networkservicemesh.io/ 805 
VM only:  The OpenStack Neutron SFC and OpenFlow-based SFC are examples of solutions that can implement the 806 
Service Function Chaining capability. 807 
 Assignment of Acceleration Resources 808 
Common:  For both container and VM solutions, specific devices such as accelerator (e.g., FPGA, GPU) may be 809 
needed. In this case, the cloud platform needs to be able to assign the specified device to container instance or VM 810 
instance.  811 


                                                                                                           O-RAN WG6 Technical Report 
 
27 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
For example, some L1 protocols require a FFT algorithm (to compute the DFT) that could be implemented in an FPGA, 812 
and the vO-DU would need the PCI Pass-Through to assign the FPGA device to the vO-DU instance so that the vO-DU 813 
instance can access and use the FPGA device. 814 
 Real-time / General Performance Feature Requirements 815 
 Host Linux OS 816 
 Support for Pre-emptive Scheduling  817 
Support may be required to support Pre-emptive Scheduling (real time Linux uses the preempt_rt patch). Generally, 818 
without real time features, it is very difficult for an application to get deterministic response ti mes for events, interrupts 819 
and other reasons4. In addition, during the housekeeping processes in Linux system, the application also cannot 820 
guarantee the running time (CPU cycle), so from the wireless application design perspective, it needs the real time 821 
feature. In addition, to support the requirements of high throughput, multiple accesses and low latency, some wireless 822 
applications need the priority-based OS environment.  823 
 Support for Node Feature Discovery 824 
Common:  Automated and dynamic placement of Cloud-Native Network Functions (CNFs) / microservices and VMs is 825 
needed, based on the hardware requirements imposed on the vO-DU, vO-CU and near-RT RIC functions.  This requires 826 
the cloud platform to support the ability to discover the hardware capabilities on each node and advertise it via labels vs. 827 
nodes, and allows VNF/CNF descriptions to have hardware requirements via labels. This mechanism is also known as 828 
Node Feature Discovery (NFD). 829 
Container-only:  For example, the cloud platform can achieve this by supporting implementation of NFD for 830 
Kubernetes. For more details, please see https://github.com/kubernetes-sigs/node-feature-discovery. 831 
VM-only:  VMs can use OpenStack mechanisms.  For example, the OpenStack Nova filter, host aggregates and 832 
availability zones can be used to implement the same function. 833 
 Support for CPU Affinity and Isolation 834 
Common:  The vO-DU, vO-CU and even the near-RT RIC are performance sensitive and require the ability to 835 
consume a large amount of CPU cycles to work correctly.  They depend on the ability of the cloud platform to provide a 836 
mechanism to guarantee performance determinism even when there are noisy neighbors.  837 
Container-only:  This requires the cloud platform to support using affinity and isolation of cores, so high performance 838 
Kubernetes Pod cores also can be dedicated to specified tasks.  For example, the cloud platform can achieve this by 839 
implementing CPU Manager for Kubernetes. For more details, please refer to https://github.com/intel/CPU-Manager-840 
for-Kubernetes . 841 
VM-only:  For example the modern Linux operating system uses the Symmetric MultiProcessing (SMP) mode, so the 842 
system process and application will be located at different CPU cores. To run the VM and guarantee the VM 843 
performance, the capability to assign the specific CPU cores to a VM is the way to do that. And at the same time, CPU 844 
isolation will reduce the inter-core affinity.  Please refer to https://docs.openstack.org/senlin/pike/scenarios/affinity.html 845 
 Support for Dynamic HugePages Allocation 846 
Common:  When an application requires high performance and performance determinism, the reduction of paging is 847 
very helpful. vO-DU, vO-CU and even near-RT RIC can require performance determinism. The cloud platform needs to 848 
be able to support the ability to provide this mechanism to applications that require it. 849 
This requires the cloud platform to support ability to dynamically allocate the necessary amount of the faster memory 850 
(a.k.a. HugePages) to the container or VM as necessary, and also to relinquish this memory allocation in the event of 851 
unexpected termination.  852 
                                                           
4 Other options include things such as Linux signal, softwareirq, and perhaps using a common process. Because the pre-emptive kernel could 
interrupt the low priority process and occupy the CPU, it will get more chance to run the high priority process. Then through proper application 
design, it will have guaranteed time/resource and can have deterministic performance. 

                                                                                                           O-RAN WG6 Technical Report 
 
28 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
Container-only:  For example, the cloud platform can achieve this by supporting implementation of Manage 853 
HugePages in Kubernetes. For more details please refer to https://kubernetes.io/docs/tasks/manage-854 
hugepages/scheduling-hugepages/ . 855 
VM-only:  For example, the OpenStack Nova flavor setting can be used to configure the HugePage size for a VM 856 
instance.  See https://docs.openstack.org/nova/pike/admin/huge-pages.html  857 
 Support for Topology Manager 858 
Common:  Some of the cloud infrastructure which is targeted in the scenarios in this document may have servers which 859 
utilize a multiple-socket configuration which comes with multiple memory regions. Each core5 is connected to a 860 
memory region. While each CPU on one socket can access the memory region of the CPUs on another socket of the 861 
same board, the access time is significantly slower when crossing socket boundaries, and this will affect performance 862 
significantly.  863 
The configuration of hardware with multiple memory regions is also known as Non -Uniform Memory Access (NUMA) 864 
regions. To support automated and dynamic placement of CNFs/microservices or VMs based on cloud infrastructure 865 
that has multiple NUMA regions and guarantee the response time of the application (especially for vO -DU), it is critical 866 
to be able to ensure that all the containers/VMs are associated with  core(s) which are connected to the same NUMA 867 
region. In addition, if the application relies on access to hardware accelerators and/or I/O which uses memory as a way 868 
to interact with the application, it is also critical that those also use the same NUMA region that the application uses.  869 
The cloud platform will need to provide the mechanism to enable managing the NUMA topology to ensure the 870 
placement of specified containers/VMs on cores which are on the same NUMA region, as well as making sure that the 871 
devices which the application uses are also connected to the same NUMA region.  872 
 873 
Figure 14:  Example Illustration of Two NUMA Regions 874 
 Support for Scale In/Out 875 
Common:  The act of scaling in/out of containers/ VMs can be based on triggers such as CPU load, network loa d, and 876 
storage consumption. The network service usually is not just a single container or VM, and in order to leverage the 877 
container/ VM benefit, the network service usually will have multiple containers/ VMs. But if demand is changing 878 
dynamically, especially for the O-CU, the service needs to be scaled in/out according to service requirements such as 879 
subscriber quantity.  880 
For example, when the number of subscribers increases, the system needs to start more container/ VM instances to 881 
ensure the service quality. From the cloud platform perspective, it could monitor the CPU load; if the load reaches a 882 
level such as 80%, it needs to scale out. If the CPU load drops 40%, it could then scale in.  883 
Different services can scale in/out depending on different criteria, such as the CPU load, network load and storage 884 
consumption.  Support for scale in/out can be helpful in implementing on-demand services.  885 
                                                           
5 In this document, we use the terms core and socket in the following way.  A socket, or more precisely the multichip platform that fits into a server 
socket, contains multiple cores, each of which is a separate CPU.  Each core in a socket has some dedicated memory, and also some shared 
memory among other cores of the same socket, which are within the same NUMA zone.  


                                                                                                           O-RAN WG6 Technical Report 
 
29 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
Editor’s Note:  Support for scale up/down is not discussed at this time, but may be revisited in the future.   886 
 Support for Device Plugin 887 
Common:  For vO-DU, vO-CU and near-RT RIC applications, hardware accelerators such as SmartNICs, FPGAs and 888 
GPUs may be required to meet performance objectives that can’t be met by using software only implementations.  In 889 
other cases, such accelerators can be useful as an option to reduce the consumption of CPU cycles to achieve better cost 890 
efficiency. 891 
The cloud platform will need to provide the mechanism to support those accelerators. This in turn requires support the 892 
ability to discover, advertise, schedule and manage devices such as SR-IOV, GPU, and FPGA.   893 
Container-only:  For example, the cloud platform can achieve this by supporting implementation of Device Plugins in 894 
Kubernetes. For more details please check: https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-895 
net/device-plugins/. 896 
VM-only:  The PCI passthrough feature in OpenStack allows full access and direct control of a physical PCI device in 897 
guests. This mechanism is generic for any kind of PCI device, and runs with a Network Interface Card (NIC), Graphics 898 
Processing Unit (GPU), or any other devices that can be attached to a PCI bus.  Correct driver installat ion is the only 899 
requirement for the guest to properly use the devices. 900 
Some PCI devices provide Single Root I/O Virtualization and Sharing (SR -IOV) capabilities. When SR-IOV is used, a 901 
physical device is virtualized and appears as multiple PCI devices. Virtual PCI devices are assigned to the same or 902 
different guests. In the case of PCI passthrough, the full physical device is assigned to only one guest and cannot be 903 
shared. 904 
See https://wiki.openstack.org/wiki/Cyborg 905 
 Support for Direct IRQ Assignment 906 
VM-only:  The general-purpose platform has many devices that will generate the IRQ to the system. To develop a 907 
performance-sensitive application, inclusion of low-latency and deterministic timing features, and assigning the IRQ to 908 
a specific CPU core, will reduce the impact of housekeeping processes and decrease the response time to desired IRQs.  909 
 Support for No Over Commit CPU 910 
VM-only:  The “No Over Commit CPU” VM creation option is able to guarantee VM perfor mance with a “dedicated 911 
CPU” model. 912 
In traditional telecom equipment design, this will maintain the level of CPU utilization to avoid burst and congestion 913 
situations. In a virtualization environment, performance-sensitive applications such as vO-DU, vO-CU, and RIC will 914 
need the platform to provide a mechanism to secure the CPU resource.  915 
 Support for Specifying CPU Model 916 
VM-only:  OpenStack can use the CPU model setting to configure the vCPU for a VM.  For example, QEMU allows 917 
the CPU options to be “Nehalem”, “Westmere”, “SandyBridge” or “IvyBridge”, or alternatively it could be configured 918 
as “host-passthrough”. This allows VMs to leverage advanced features of selected CPU architectures. For the vO -CU 919 
and vO-DU design and implementation, there will be some algorithm and computing functions that can leverage host 920 
CPU instructions to realize some benefits such as performance. The cloud platform needs to provide this capability to 921 
VMs.  922 
 Storage Requirements 923 
The storage requirements are the same for both VM and Container based implementations. 924 
For O-RAN components, the VNF/CNF needs storage for the image and for the VNF/CNF itself.  It should support 925 
different scale, e.g., for a Regional Cloud vs. an Edge Cloud.  The cloud platform needs to support a large-scale storage 926 
solution with redundancy, medium and small scale storage solutions for two or more servers, and a very small scale 927 
solution for a single server.  928 

                                                                                                           O-RAN WG6 Technical Report 
 
30 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
5.5. Sync Architecture 929 
Synchronization mechanisms and options are receiving significant attention in the indu stry.  When requirements are 930 
better understood for various deployment scenarios, we can discuss which are applicable to each.  931 
Editor’s Note:  O-RAN Working Groups 4 and 5 are addressing some aspects of synchronization, and more 932 
discussion of Sync is expected in future versions of this document.   933 
5.6. Operations and Maintenance Considerations 934 
Management of a cloudified RAN introduces some new management considerations, because the mapping between 935 
Network Functionality and physical hardware can be done in multiple ways, depending on the Scenario that is chosen.  936 
Thus, management of aspects that are related to physical aspects rather than logical aspects need to be designed with 937 
flexibility in mind from the start.  For example, logging of physical functions, scale out actions, and survivability 938 
considerations are affected.   939 
The O-RAN Alliance has defined key fundamentals of the OAM framework (see [8] and [9], and refer to Figure 1). 940 
Given the number of deployment scenario options and possible variations of O-RAN Managed Functions (MFs) being 941 
mapped into Managed Elements (MEs) in different ways, it is important for all MEs to support a consistent level of 942 
visibility and control of their contained Managed Functions to the Service Management & Orchestration Framework.  943 
This consistency will be enabled by support of the common OAM Interface Specification [9] for Fault Configuration 944 
Accounting Performance Security (FCAPS) and Life Cycle Management (LCM) functionality, and a common 945 
Information Modelling Framework that will provide underlying information models used for the MEs and MFs in a 946 
particular deployment. 947 
A key motivation for the Managed Element concept is that an ME is a tightly integrated and tested group of MFs that 948 
are deployed together.  This has implications on how software updates are managed, because all software updates need 949 
to retain the property that all MFs in the ME have been tested together.    950 
Depending on the deployment scenario and other considerations, the MFs may be grouped in different ways.  An 951 
interface is required to each ME, which can manage the communications to each MF that is contained within it.  The O-952 
RAN Operations and Maintenance Architecture [8] document presents many examples of how the O1 interface can 953 
connect to either individual MFs, or to an integrated ME that contains multiple MFs.  To introduce the general concept, 954 
Figure 15 below shows an example where there is one O1 interface of each type.  However, again it must be stressed 955 
that there are multiple legitimate options that are being considered, and that reference [8] is the authoritative source of 956 
operations options.   957 
 958 
Figure 15:  RAN OAM Logical Architecture – One Example 959 


                                                                                                           O-RAN WG6 Technical Report 
 
31 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
In Figure 15, the O1 interface to the near-RT RIC is only managing that single function, so we can think of this as 960 
having just one MF in an ME.  However, the other O1 interface is to a ME that contains multiple MFs.  In this case, 961 
how do messages get to the correct MF? 962 
Figure 16 below shows a high-level diagram of how an O1 interface relates to an ME that contains multiple MFs.  The 963 
ME provides the functionality (light blue entity) to link the O1 interface termination in the ME and each MF that lies 964 
within the ME.   965 
 966 
Figure 16:  O1 Termination and MFs in an ME 967 
The Service Management and Orchestration (SMO) framework will need a consistent and standardized view of the 968 
Managed Functions that are contained within any Managed Element, regardless of the grouping of MFs in MEs.  The 969 
figure below shows a separate dashed line for each MF that is presented to the SMO.   970 
 971 
Figure 17:  Three types of O1 Terminations in MEs/MFs 972 
Note that the way in which the O1 termination is related to the MF is different in each case: 973 
 In the first case (shown on the left), the ME contains multiple MFs and a function that terminates the O1 974 
interface to each MF.  That function also provides proprietary communication to each MF. 975 
 In the second case, the ME contains just one MF, but has the same functionality to communicate to the MF. 976 
 In the third case, the MF presents a compliant O1 interface.   977 
It should be noted that in addition to MEs that provide RAN functionality, there are MEs that provide Cloud Platform 978 
functionality.  Both are required for Network Functions provided by a cloud platform, because the Cloud Platform and 979 
the RAN functionality are decoupled.  For example, there may be Cloud Platform resources that are not currently 980 
assigned to RAN functions, but they still need to be monitored and managed.  In this case the SMO would manage those 981 
cloud platform resources via O1*.  This is illustrated below. 982 


                                                                                                           O-RAN WG6 Technical Report 
 
32 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 983 
Figure 18:  O1* Interface to Manage Cloud Platform Resources (in addition to O1 for RAN MEs) 984 
5.7. Transport Network Architecture 985 
While a Transport Network is a necessary foundation upon which to build any O-RAN deployment, a great many of the 986 
aspects of transport do not have to be addressed or specified in O-RAN Alliance documents.  For example, any location 987 
with cloud servers will be connected by layer 2 or layer 3 switches, but we do not need to specify much if anything 988 
about them in this document.   989 
The transport media used, particularly for fronthaul, can have an effect on aspects such as performance.  However, in 990 
the current version of this document we have been assuming that fiber transport is used.   991 
Editor’s Note:  Other transport technologies (e.g., microwave) are also possible, and could be addressed at a 992 
later date.  993 
That said, the use of an (optional) Fronthaul Gateway (FH GW) will have noteworthy effects on any O-RAN 994 
deployment that uses it. 995 
 Fronthaul Gateways 996 
In the deployment scenarios that follow, when the O-DU and O-RU functions are not implemented in the same physical 997 
node, a Fronthaul Gateway is shown as an optional element between them.  A Fronthaul Gateway can be motivated by 998 
different factors depending on a carrier’s deployment, and may perform different functions.   999 
The O-RAN Alliance does not currently have a single definition of a Fronthaul Gateway, and this document does not 1000 
attempt to define one.  However, the Fronthaul Gateway is included in the diagrams as an optional implementation to 1001 
acknowledge the fact that carriers are considering Fronthaul Gateways in their plans. Below are some examples of the 1002 
functionality that could be provided: 1003 
 A FH GW can convert CPRI connections to the node supporting the O-RU function to eCPRI connections to 1004 
the node that provides O-DU functionality.   1005 
 Note that when there is no FH GW, it is assumed that the Open Fronthaul interface between the O-RU 1006 
and O-DU uses Option 7-2, as mentioned earlier in Section 4.1.  When there is a FH GW, it may have an 1007 
Option 7-2 interface to both the O-DU and the O-RU, but it is also possible for the FH GW to have a 1008 
different interface to the O-RU/RU; for example, where CPRI is supported.   1009 
 A FH GW can support the aggregation of fiber pairs. 1010 
 A FH GW must support the following forwarding functions: 1011 
 Downlink:  Broadcast traffic from O-DU to each O-RU (and cascading FH GW, if present) 1012 
 Uplink:  Summation of traffic from O-RUs 1013 


                                                                                                           O-RAN WG6 Technical Report 
 
33 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 A FH GW can provide power to the NEs supporting the O-RU function, e.g. via Power over Ethernet (PoE) or 1014 
hybrid cable/fibers 1015 
5.8. Overview of Deployment Scenarios 1016 
The description of logical functionality in O-RAN includes the definition of key interfaces E2, F1, and Open Fronthaul.  1017 
However, as noted earlier, this does not mean that each Network Function block must be implemented in a separate 1018 
PNF/VNF/CNF.  Multiple logical functions can be implemented in a single PNF/VNF/CNF (for example O-DU and O-1019 
RU may be packaged as a single appliance).  1020 
We assume that when Network Functions are implemented as different PNF/VNF/CNFs, the interfaces between them 1021 
must conform to the O-RAN specifications.  However, when multiple Network Functions are implemented by a single 1022 
PNF/VNF/CNF, it is up to the operator to decide whether to enforce the O-RAN interfaces between the embedded 1023 
Network Functions.  However, note that the OAM requirements for each separate Network Function will still need to be 1024 
met.   1025 
The current deployment scenarios for discussion are summarized in the figure below.  This includes options that are 1026 
deployable in both the short and long term.  Each will be discussed in some detail in the following sections, followed by 1027 
a summary of which one or ones are candidates for initial focus. Please note that, to help ease the high-level depiction 1028 
of functionality, a single O-CU box is shown with an F1 interface, but in detailed discussions of specific scenarios, this 1029 
will need to be discussed properly as composed of an O-CU-CP function with an F1-c interface and an O-CU-UP 1030 
function with an F1-u interface.  Furthermore, there would in general be an unequal number of O-CU-CP and O-CU-UP 1031 
instances.   1032 
Figure 19 below shows the Network Functions at the top, and each identified scenario shows how these Network 1033 
Functions are deployed as proprietary PNFs or as VNFs/CNFs running on an O-RAN compliant O-Cloud.  The term O-1034 
Cloud is defined in Section 4.  Please note that the requirements for an O-Cloud are driven by the Network Functions 1035 
that need to be supported by the hardware, so for instance an O-Cloud that supports an O-RU function would be 1036 
different from an O-Cloud that supports O-CU functionality.   1037 
Finally, note that in the high-level figure below, the User Plane (UP) traffic is shown being delivered to the UPF.  As 1038 
will be discussed, in specific scenarios it is sometimes possible for UP traffic to be delivered to edge applications that 1039 
are supported by Mobile Edge Computing (MEC).  However, note that the specification of MEC itself is out of scope of 1040 
this document. 1041 
Note that vendors are not required to support all scenarios – it is a business decision to be made by each vendor.  1042 
Similarly, each operator will decide which scenarios it wishes to deploy.   1043 
 1044 
Figure 19:  High-Level Comparison of Scenarios 1045 


                                                                                                           O-RAN WG6 Technical Report 
 
34 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
Each scenario is discussed in the next section.   1046 
6. Deployment Scenarios and Implementation 1047 
Considerations 1048 
This section reviews each of the deployment scenarios in turn.  For a given scenario, the requirements that apply to the 1049 
proprietary or O-Cloud platforms may become more specific and unique, while many of the logical Network Function 1050 
requirements will remain the same.   1051 
Please note that in all of the scenario figures of this section, the interfaces are logical interfaces (e.g., F1, E2, etc.) .  This 1052 
has a couple of implications.  First, the two functions on each side of an interface could be on different devices 1053 
separated by physical transport connections (e.g., fiber or Ethernet transport connections), could be on different devices 1054 
within the same cloud platform, or could even exist within the same server.  Second, the functions on each side of an 1055 
interface could be from the same vendor or different vendors.  1056 
In addition, please note that all User Plane interfaces are shown with a solid lines, and all Control Plane interfaces use 1057 
dashed lines.  1058 
6.1. Scenario A  1059 
In this scenario, the near-RT RIC, O-CU, and O-DU functions are all virtualized on the same cloud platform, and 1060 
interfaces between those functions are within the same cloud platform.    1061 
This scenario supports deployments in dense urban areas with an abundance of fronthaul capacity that allows BBU  1062 
functionality to be pooled in a central location with sufficiently low latency to meet the O-DU latency requirements. 1063 
Therefore it does not attempt to centralize the near-RT RIC more than the limit that O-DU functionality can be 1064 
centralized.  1065 
 1066 
Figure 20:  Scenario A 1067 
Also please note that if the optional FH GW is present, the interface between it and the Radio Unit might not meet the 1068 
O-RAN Fronthaul requirements (e.g., it might be an Option 8 interface), in which case the Ra dio Unit could be referred 1069 
to as an “RU”, not an “O-RU”.  However, if FH GWs are defined to support an interface such as Option 8, it could be 1070 
argued that the O-RU definition at that time will support Option 8.   1071 
 Key Use Cases and Drivers 1072 
Editor’s Note:  This section is FFS.  1073 
6.2. Scenario B 1074 
In this scenario, the near-RT RIC Network Function is virtualized on a Regional Cloud Platform, and the O-CU and O-1075 
DU functions are virtualized on an Edge Cloud hardware platform that in general will be at a different location.  The 1076 
interface between the Regional Cloud and the Edge cloud is E2.  Interfaces between the O-CU and O-DU Network 1077 
Functions are within the same Cloud Platform.  1078 


                                                                                                           O-RAN WG6 Technical Report 
 
35 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 1079 
Figure 21:  Scenario B 1080 
This scenario is to support deployments in locations with limited remote front haul capacity and O-RUs spread out in an 1081 
area that limits the number of O-RUs that can be supported by pooled vO-CU/vO-DU functionality while still meeting 1082 
the O-DU latency requirements.  The use of a FH GW in the architecture allows significant savings in  providing 1083 
transport between the O-RU and vO-DU functionality.  1084 
As discussed earlier in Section 5.1.3, the O-CU and O-DU functions can be virtualized using either simple 1085 
centralization or pooled centralization.  The desire is to have support for pooled centralization, although we need to 1086 
understand what needs to be developed to enable such sharing.  Perhaps pooling will be a later feature, but any initial 1087 
solution should not preclude a future path to a pooled solution.    1088 
 Key Use Cases and Drivers 1089 
In this case, there are multiple O-RUs distributed in an area served by a centralized vO-DU functionality that can meet 1090 
the latency requirements.  Depending on the concentration of the O-RUs, N could vary, but in general is expected to be 1091 
engineered to support < 64 TRPs per O-DU.6  The near-RT RIC is centralized further to allow for optimization based on 1092 
a more global view (e.g., a single large metropolitan area), and to reduce the number of separate near -RT RIC instances 1093 
that need to be managed.   1094 
The driving use case for this is to support an outdoor deployment of a mix of Small Cells and Macro cells in a relatively 1095 
dense urban setting.  This can support mmWave as well as Sub-6 deployments. 1096 
In this scenario, a given “virtual BBU” supports both vO-CU and vO-DU functions, and can connect many O-RUs.  1097 
Current studies show that savings from pooling are significant but level off once more than 64 Transmission Reception 1098 
Points (TRPs) are pooled.  This would imply N would be around 32-64. This deployment should support tens of 1099 
thousands of O-RUs per near-RT RIC, so L could easily exceed 100.   1100 
Below is a summary of the cardinality requirements assumed for this scenario.  1101 
  Table 2:  Cardinality and Delay Performance for Scenario B 1102 
 
Attribute RIC – O-CU O-CU – O-DU O-DU – O-RU/RU 
 
Example Cardinality L = 100+ M=1 N = 1-64  
 1103 
6.3. Scenario C 1104 
In this scenario, the near-RT RIC and O-CU Network Functions are virtualized on a Regional Cloud Platform with a 1105 
general server hardware platform, and the O-DU Network Functions are virtualized on an Edge Cloud hardware 1106 
platform that is expected to include significant hardware accelerator capabilities.  Interfaces between the near-RT RIC 1107 
                                                           
6 It is assumed that one O-RU is associated with one TRP.  For example, if a cell site has three sectors, then each sector would have at least one TRP 
and hence at least three O-RUs.  


                                                                                                           O-RAN WG6 Technical Report 
 
36 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
and the O-CU network functions are within the same Cloud Platform.  The interface between the Regional Cloud and 1108 
the Edge cloud is F1, and an E2 interface from the near-RT RIC to the O-DU must also be supported.  1109 
 1110 
Figure 22:  Scenario C 1111 
This scenario is to support deployments in locations with limited remote Fronthaul capacity and O-RUs spread out in an 1112 
area that limits the number of O-RUs that can be pooled while still meeting the O-DU latency requirements.  The O-CU 1113 
Network Function is further pooled to increase the efficiency of the hardware platform which it shares with the near-RT 1114 
RIC Network Function.   1115 
However, note that if a service type has tighter O-CU delay requirements than other services, then that may either 1116 
severely limit the number of O-RUs supported by the Regional cloud, or a method will be needed to separate the 1117 
processing of such services.  This will be discussed further in the following C.1 and C.2 Scenarios.   1118 
The use of a FH GW in the architecture allows significant savings in providing transport between the O -RU and vO-DU 1119 
functionality.   1120 
 Key Use Cases and Drivers 1121 
In this case, there are multiple O-RUs distributed in an area where each O-RU can meet the latency requirement for the 1122 
pooled vO-DU function.  The near-RT RIC and O-CU Network Functions are further centralized to realize additional 1123 
efficiencies.   1124 
A use case for this is to support an outdoor deployment of a mix of Small Cells and Macro cells in a relatively dense 1125 
urban setting.  This can support mmWave as well as Sub-6 deployments. 1126 
In this scenario, as in Scenario B, the Edge Cloud is expected to support roughly 32-64 O-RUs. This deployment should 1127 
support tens of thousands of O-RUs per near-RT RIC.  1128 
Below is a summary of the cardinality and the distance/delay requirements assumed for this scenario.   1129 
Table 3:  Cardinality and Delay Performance for Scenario C   1130 
 
Attribute RIC – O-CU O-CU – O-DU O-DU – O-RU/RU 
 
Example Cardinality L= 1 M=100+  N=Roughly 32-64 
 1131 
 Scenario C.1, and Use Case and Drivers 1132 
This is a variation of Scenario C, driven by the fact that different types of traffic (network slices) have different latency 1133 
requirements.  In particular, URLLC has more demanding user-plane latency requirements, and Figure 23 below shows 1134 
how the vO-CU User Part (vO-CU-UP) could be terminated in different places for different network slices.  Below, 1135 
network slice 3 is terminated in the Edge Cloud.  This scenario is also suitable in case there isn’t enough space or power 1136 
supply to install all vO-CUs and vO-DUs in one Edge Cloud site.  1137 


                                                                                                           O-RAN WG6 Technical Report 
 
37 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 1138 
Figure 23:  Treatment of Network Slices:  MEC for URLLC at Edge Cloud, Centralized Control, Single vO-DU 1139 
In Scenario C.1, all O-CU control is placed in the Regional Cloud, and there is a single vO-DU for all Network Slices.  1140 
Only the placement of the vO-CU-CP differs, depending on the network slice.  Below is the diagram of this scenario, 1141 
using the common diagram conventions of all scenarios.  1142 
 1143 
Figure 24:  Scenario C.1 1144 
Below is a summary of the cardinality and the distance/delay requirements assumed for this scenario.  The URLLC user 1145 
plane requirements are what drive the placement of the vO-CU-UP function to be in the Edge cloud.   1146 
Table 4:  Cardinality and Delay Performance for Scenario C.1 1147 
 
Attribute RIC – O-CU O-CU – O-DU O-DU – O-RU/RU 
 
Example Cardinality L= 1 M=320 N=100 
Delay Max  
1-way (distance) 
   mMTC NA 625 μs (125 km) 100 μs (20 km)  
   eMBB NA 625 μs (125 km) 100 μs (20 km) 
   URLLC (user/control) NA 100 μs (20 km)/625 μs (125 km) 100 μs (20 km) 
 1148 
 Scenario C.2, and Use Case and Drivers 1149 
This is a second variation of Scenario C, which utilizes the same method of placing some vO-CU user plane 1150 
functionality in the Edge Cloud, and some in the Regional Cloud.  However, instead of having one vO-DU for all 1151 
network slices, there are different vO-DU instances in the Edge Cloud.  1152 
It is driven by factors including the following two use cases: 1153 
 One driver is RAN (O-RU) sharing among operators. In this use case, any operator can flexibly launch vO-CU 1154 
and vO-DU instances at Edge or Regional Cloud site.  For example, as shown in Figure 25, Operator #1 wants 1155 
to launch the vO-CU1 instance in the Regional Cloud, and the vO-DU1 instance at subtending Edge Cloud 1156 
sites. On the other hand, Operator #2 wants to install both the vO-CU2 and vO-DU2 instances at the same 1157 
Regional Cloud site.  Note that both operators will share the O-RU).  1158 


                                                                                                           O-RAN WG6 Technical Report 
 
38 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 Another driver is that, even within a single operator, that operator can customize scheduler functions 1159 
depending on the network slice types, and can place the vO-CU and vO-DU instances depending on the 1160 
network slice types. For example, an operator may launch both vO-CU and vO-DU at the edge cloud site (see 1161 
Operator #2 below) to provide a URLLC service.   1162 
 1163 
Figure 25:  Treatment of Network Slice: MEC for URLLC at Edge Cloud, Separate vO-DUs 1164 
The multi-Operator use case has the following pros and cons: 1165 
Pros: 1166 
 O-RU sharing can reduce TCO 1167 
 Flexible CU/DU location allows deployments to consider not only service requirements but also limitations of 1168 
space or power in each site 1169 
Cons: 1170 
 Allowing multiple operators to share O-RU resources is expected to require changes to the Open Fronthaul 1171 
interface (especially the handshake among more than one vO-DU and a given O-RU).   1172 
 This change seems likely to have M-plane specification impact.  Therefore, this approach would need O-RAN 1173 
buy-in and approval.   1174 
Figure 26 below illustrates how different Component Carriers can be allocated to different operators, at the same O-RU 1175 
at the same time.  Note that some updates of not only M-plane but also CUS-plane specifications will be required when 1176 
considering frequency resource sharing among DUs. 1177 
 1178 
Figure 26:  Single O-RU Being Shared by More than One Operator 1179 
The diagram of how Network Functions map to Networks Elements for Scenario C.2 is shown below .  1180 


                                                                                                           O-RAN WG6 Technical Report 
 
39 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 1181 
Figure 27:  Scenario C.2 1182 
The performance requirements are the same as those discussed earlier for Scenario C.1 in Section 6.3.2. 1183 
6.4. Scenario D  1184 
This scenario is a variation on Scenario C, but in this case the O-DU functionality is supported by a proprietary 1185 
hardware platform rather than an O-Cloud.  1186 
The general assumption is that Scenario D has the same use cases and performance requirements as Scenario C, and the 1187 
primary difference is in the business decision of how the proprietary solution compares with the O -RAN compliant O-1188 
Cloud solution.  Implementation considerations (discussed in Section 5.1) could lead a carrier to decide that an 1189 
acceptable O-Cloud solution is not available in a deployment’s timeframe.   1190 
 1191 
Figure 28:  Scenario D 1192 
6.5. Scenario E  1193 
In contrast to Scenario D, this scenario assumes that not only can the O-DU be virtualized as in Scenario C, but that the 1194 
O-RU can also be successfully virtualized.  Furthermore, the O-RU and O-DU would be implemented in the same O-1195 
Cloud, which has acceleration hardware required by both the O-RU and O-DU.   1196 
Note, this seems to be a future scenario, and is not part of our initial focus.   1197 


                                                                                                           O-RAN WG6 Technical Report 
 
40 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 1198 
Figure 29:  Scenario E 1199 
 Key Use Cases and Drivers 1200 
Because the O-DU and O-RU are implemented in the same O-Cloud in this Scenario, it seems that the O-DU 1201 
implementation must meet the environmental and accessibility requirements typically associated with an O -RU.  1202 
Therefore, an indoor use case seems most appropriate.  1203 
6.6. Scenario F  1204 
This is a variation on Scenario E in which the O-DU and O-RU are both virtualized, but in different O-Clouds. This 1205 
means that: 1206 
 The O-DU function can be placed in a more convenient location in terms of accessibility for maintenance and 1207 
upgrades. 1208 
 The O-DU function can be placed in an environment that is semi-controlled or controlled, which reduces some 1209 
of the implementation complexity.  1210 
 1211 
Figure 30:  Scenario F 1212 
 Key Use Cases and Drivers 1213 
Because this assumes that the O-RU is virtualized, this is a future use case. 1214 
This use case seems to be better suited for outdoor deployments (e.g., pole mounted) than Scenario E. 1215 
6.7. Scenarios of Initial Interest 1216 
More scenarios have been identified than can be addressed in the initial release of this document.  Scenario B has been 1217 
selected as the one to address initially, and to be the subject of detailed treatment in a Scenario document (refer back to 1218 
Figure 1).  Other scenarios are expected to be addressed in later work.   1219 
 1220 


                                                                                                           O-RAN WG6 Technical Report 
 
41 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
7. Appendix A (informative):  Extensions to Current 1221 
Deployment Scenarios to Include NSA 1222 
In this appendix, some extensions to (some of) the current deployment scenarios are proposed with the aim of 1223 
introducing Non-Standalone (NSA) in the pictures, consistently with the scope O-RAN cloud architecture. These 1224 
extensions will be the basis of the discussion for next version of the present document. In the following charts the 1225 
subscript ‘N’ is indicating blocks related to NR, while the subscript ‘E’ is indicating bloc ks related to E-UTRA.7  For E-1226 
UTRA, the W1 interface is indicated. Its definition is ongoing in a 3GPP work item.  1227 
7.1. Scenario A 1228 
 1229 
Figure 31:  Scenario A, Including NSA 1230 
7.2. Scenario B 1231 
 1232 
Figure 32:  Scenario B, Including NSA 1233 
                                                           
7 No UPF or MEC blocks are explicitly indicated in the figures of this appendix, as the focus of this appendix is on the radio part. 


                                                                                                           O-RAN WG6 Technical Report 
 
42 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
7.3. Scenario C 1234 
 1235 
Figure 33:  Scenario C, Including NSA 1236 
7.4. Scenario C.2 1237 
The scenario addresses both the single and multi-operator cases. To reduce the complexity in the figure the multi 1238 
operator case is considered, so no X2/Xn interface is present between CUN1 and CUE2 or between CUE1 and CUN2. 1239 
 1240 
Figure 34:  Scenario C.2, Including NSA 1241 
7.5. Scenario D 1242 
 1243 
Figure 35:  Scenario D, Including NSA  1244 


                                                                                                           O-RAN WG6 Technical Report 
 
43 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
Annex ZZZ:  O-RAN Adopter License Agreement 1245 
BY DOWNLOADING, USING OR OTHERWISE ACCESSING ANY O-RAN SPECIFICATION, ADOPTER AGREES TO 1246 
THE TERMS OF THIS AGREEMENT.   1247 
This O-RAN Adopter License Agreement (the “Agreement”) is made by and between the O-RAN Alliance and 1248 
the entity that downloads, uses or otherwise accesses any O-RAN Specification, including its Affiliates (the 1249 
“Adopter”). 1250 
This is a license agreement for entities who wish to adopt any O-RAN Specification. 1251 
SECTION 1:  DEFINITIONS 1252 
 1253 
1.1 “Affiliate” means an entity that directly or indirectly controls, is controlled by, or is under common 1254 
control with another entity, so long as such control exists.  For the purpose of this Section, “Control” 1255 
means beneficial ownership of fifty (50%) percent or more of the voting stock or equity in an entity. 1256 
 1257 
1.2 “Compliant Portio n” means only those specific portions of products (hardware, software or 1258 
combinations thereof) that implement any O-RAN Specification. 1259 
 1260 
1.3  “Adopter(s)” means all entities, who are not Members, Contributors or Academic Contributors, 1261 
including their Affiliates, who wish to download, use or otherwise access O-RAN Specifications. 1262 
 1263 
1.4 “Minor Update” means an update or revision to an O-RAN Specification published by O-RAN Alliance 1264 
that does not add any significant new features or functionality and remains interoperabl e with the prior 1265 
version of an O-RAN Specification.  The term “O-RAN Specifications” includes Minor Updates. 1266 
 1267 
1.5 “Necessary Claims” means those claims of all present and future patents and patent applications, 1268 
other than design patents and design registration s, throughout the world, which (i) are owned or 1269 
otherwise licensable by a Member, Contributor or Academic Contributor during the term of its Member, 1270 
Contributor or Academic Contributorship; (ii) such Member, Contributor or Academic Contributor has the 1271 
right to grant a license without the payment of consideration to a third party; and (iii) are necessarily 1272 
infringed by implementation of a Final Specification (without considering any Contributions not included 1273 
in the Final Specification). A claim is necessarily infringed only when it is not possible on technical (but not 1274 
commercial) grounds, taking into account normal technical practice and the state of the art generally 1275 
available at the date any Final Specification was published by the O -RAN Alliance or the date the patent 1276 
claim first came into existence, whichever last occurred, to make, sell, lease, otherwise dispose of, repair, 1277 
use or operate an implementation which complies with a Final Specification without infringing that claim. 1278 
For the avoidance of doubt in exceptional cases where a Final Specification can only be implemented by 1279 
technical solutions, all of which infringe patent claims, all such patent claims shall be considered Necessary 1280 
Claims. 1281 
 1282 
1.6 “Defensive Suspension” means for the purposes of any license grant pursuant to Section 3, Member, 1283 
Contributor, Academic Contributor, Adopter, or any of their Affiliates, may have the discretion to include 1284 
in their license a term allowing the licensor to suspend the license against a licensee who brings a patent 1285 
infringement suit against the licensing Member, Contributor, Academic Contributor, Adopter, or any of 1286 
their Affiliates. 1287 
 1288 
SECTION 2: COPYRIGHT LICENSE 1289 
 1290 
2.1 Subject to the terms and conditions of this Agreement, O-RAN Alliance hereby grants to Adopter a 1291 
nonexclusive, nontransferable, irrevocable, non-sublicensable, worldwide copyright license to obtain, use 1292 

                                                                                                           O-RAN WG6 Technical Report 
 
44 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
and modify O-RAN Specifications, but not to further distribute such O-RAN Specification in any modified 1293 
or unmodified way, solely in furtherance of implementations of an O-RAN Specification. 1294 
 1295 
2.2 Adopter shall not use O -RAN Specifications except as expressly set forth in this Agreement or in a 1296 
separate written agreement with O-RAN Alliance. 1297 
 1298 
SECTION 3: FRAND LICENSE 1299 
 1300 
3.1 Members, Contributors and Academic Contributors and their Affiliates are prepared to grant based 1301 
on a separate Patent License Agreement to each Adopter under Fair, Reasonable And Non-Discriminatory 1302 
(FRAND) terms and conditions with or without compensation (royalties) a nonexclusive, non-transferable, 1303 
irrevocable (but subj ect to Defensive S uspension), non -sublicensable, worldwide license under their 1304 
Necessary Claims to make, have made, use, import, offer to sell, lease, sell and otherwise distribute 1305 
Compliant Portions; provided, however, that such license shall not extend: (a) to any part or function of a 1306 
product in which a Compliant Portion is incorporated that is not itself part of the Compliant Portion; or 1307 
(b) to any Adopter if that Adopter is not making a reciprocal grant to Members, Contributors and Academic 1308 
Contributors, as set forth in Section 3.3.  For the avoidance of doubt, the foregoing license includes the 1309 
distribution by the Adopter’s distributors and the use by the Adopter’s customers of such licensed 1310 
Compliant Portions. 1311 
 1312 
3.2  Notwithstanding the above, if any Member, Contributor or Academic Contributor, Adopter or their 1313 
Affiliates has reserved the right to charge a FRAND royalty or other fee for its license of Necessary Claims 1314 
to Adopter, then Adopter is entitled to charge a FRAND royalty or other fee to such Member, Contributor 1315 
or Academic Contributor, Adopter and its Affiliates for its license of Necessary Claims to its licensees. 1316 
 1317 
3.3 Adopter, on behalf of itself and its Affiliates, shall be prepared to grant based on a separate Patent 1318 
License Agreement to each Members, Contributors, Academic Contributors, Adopters and their Affiliates 1319 
under FRAND terms and conditions with or without compensation (royalties) a nonexclusive, non -1320 
transferable, irrevocable (but subject to Defensive Suspension), non -sublicensable,  worldwide license 1321 
under their Necessary Claims to make, have made, use, import, offer to sell, lease, sell and otherwise 1322 
distribute Compliant Portions; provided, however, that such license will not extend: (a) to any part or 1323 
function of a product in which a Compliant Portion is incorporated that is not itself part of the Compliant 1324 
Portion; or (b) to any Members, Contributors, Academic Contributors, Adopters and their Affiliates that is 1325 
not making a reciprocal grant to Adopter, as set forth in Section 3.1.   For the avoidance of doubt, the 1326 
foregoing license includes the distribution by the Members’, Contributors’, Academic Contributors’, 1327 
Adopters’ and their Affiliates’ distributors and the use by the Members’, Contributors’, Academic 1328 
Contributors’, Adopters’ and their Affiliates’ customers of such licensed Compliant Portions. 1329 
 1330 
SECTION 4:  TERM AND TERMINATION 1331 
 1332 
4.1 This Agreement shall remain in force, unless early terminated according to this Section 4. 1333 
 1334 
4.2 O-RAN Alliance on behalf of its Members, Contributors and Academic C ontributors may terminate 1335 
this Agreement if Adopter materially breaches this Agreement and does not cure or is not capable of 1336 
curing such breach within thirty (30) days after being given notice specifying the breach. 1337 
 1338 
4.3 Sections 1, 3, 5 - 11 of this Agreemen t shall survive any termination of this Agreement.  Under 1339 
surviving Section 3, after termination of this Agreement, Adopter will continue to grant licenses (a) to 1340 
entities who become Adopters after the date of termination; and (b) for future versions of O -RAN 1341 

                                                                                                           O-RAN WG6 Technical Report 
 
45 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
Specifications that are backwards compatible with the version that was current as of the date of 1342 
termination. 1343 
 1344 
SECTION 5: CONFIDENTIALITY 1345 
 1346 
Adopter will use the same care and discretion to avoid disclosure, publication, and dissemination of O -1347 
RAN Specifications to third parties, as Adopter employs with its own confidential information, but no less 1348 
than reasonable care.  Any disclosure by Adopter to its Affiliates, contractors and consultants should be 1349 
subject to an obligation of confidentiality at least as restri ctive as those contained in this Section.  The 1350 
foregoing obligation shall not apply to any information which is: (1) rightfully known by Adopter without 1351 
any limitation on use or disclosure prior to disclosure; (2) publicly available through no fault of Ado pter; 1352 
(3) rightfully received without a duty of confidentiality; (4) disclosed by O -RAN Alliance or a Member, 1353 
Contributor or Academic Contributor to a third party without a duty of confidentiality on such third party; 1354 
(5) independently developed by Adopter; (6) disclosed pursuant to the order of a court or other authorized 1355 
governmental body, or as required by law, provided that Adopter provides reasonable prior written notice 1356 
to O-RAN Alliance, and cooperates with O -RAN Alliance and/or the applicable Member , Contributor or 1357 
Academic Contributor to have the opportunity to oppose any such order; or (7) disclosed by Adopter with 1358 
O-RAN Alliance’s prior written approval.  1359 
 1360 
SECTION 6:  INDEMNIFICATION 1361 
 1362 
Adopter shall indemnify, defend, and hold harmless the O -RAN Alliance, i ts Members, Contributors or 1363 
Academic Contributors, and their employees, and agents and their respective successors, heirs and 1364 
assigns (the “ Indemnitees”), against any liability, damage, loss, or expense (including reasonable 1365 
attorneys’ fees and expenses) i ncurred by or imposed upon any of the Indemnitees in connection with 1366 
any claims, suits, investigations, actions, demands or judgments arising out of Adopter’s use of the 1367 
licensed O -RAN Specifications or Adopter’s commercialization of products that comply w ith O -RAN 1368 
Specifications. 1369 
 1370 
SECTION 7:  LIMITATIONS ON LIABILITY; NO WARRANTY 1371 
 1372 
EXCEPT FOR BREACH OF CONFIDENTIALITY, ADOPTER’S BREACH OF SECTION 3, AND ADOPTER’S 1373 
INDEMNIFICATION OBLIGATIONS, IN NO EVENT SHALL ANY PARTY BE LIABLE TO ANY OTHER PARTY OR 1374 
THIRD PARTY FOR  ANY INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE OR CONSEQUENTIAL DAMAGES 1375 
RESULTING FROM ITS PERFORMANCE OR NON-PERFORMANCE UNDER THIS AGREEMENT, IN EACH CASE 1376 
WHETHER UNDER CONTRACT, TORT, WARRANTY, OR OTHERWISE, AND WHETHER OR NOT SUCH PARTY 1377 
HAD ADVANCE NOTICE OF THE POSSIBILITY OF SUCH DAMAGES. 1378 
 1379 
O-RAN SPECIFICATIONS ARE PROVIDED “AS IS” WITH NO WARRANTIES OR CONDITIONS WHATSOEVER, 1380 
WHETHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE.  THE O -RAN ALLIANCE AND THE MEMBERS, 1381 
CONTRIBUTORS OR ACADEMIC CONTRIBUTORS EXPRESSLY DISCLAIM ANY WARRANTY OR CONDITION OF 1382 
MERCHANTABILITY, SECURITY, SATISFACTORY QUALITY, NONINFRINGEMENT, FITNESS FOR ANY 1383 
PARTICULAR PURPOSE, ERROR -FREE OPERATION, OR ANY WARRANTY OR CONDITION FOR O -RAN 1384 
SPECIFICATIONS. 1385 
 1386 
SECTION 8:  ASSIGNMENT 1387 
 1388 
Adopter may not assign the Agreement or any of its rights or obligations under this Agreement or make 1389 
any grants or other sublicenses to this Agreement, except as expressly authorized hereunder , without 1390 
having first received the prior, written consent of the O-RAN Alliance, which consent may be withheld in 1391 
O-RAN Alliance’s sole discretion.  O-RAN Alliance may freely assign this Agreement. 1392 
 1393 

                                                                                                           O-RAN WG6 Technical Report 
 
46 
Copyright © 2019 by the O-RAN Alliance e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ. 
 1394 
 1395 
SECTION 9:  THIRD-PARTY BENEFICIARY RIGHTS 1396 
 1397 
Adopter acknowledges and agrees that Members, Contributors and Academic Contributors  (including 1398 
future Members, Contributors and Academic Contributors ) are entitled to rights as a third -party 1399 
beneficiary under this Agreement, including as licensees under Section 3. 1400 
 1401 
SECTION 10:  BINDING ON AFFILIATES 1402 
 1403 
Execution of this Agreement by Adopter in its capacity as a legal entity or association constitutes that legal 1404 
entity’s or association’s agreement that its Affiliates are likewise bound to the obligations that are 1405 
applicable to Adopter hereunder and are also entitled to the benefits of the rights of Adopter hereunder. 1406 
 1407 
SECTION 11:  GENERAL 1408 
 1409 
This Agreement is governed by the laws of Germany without regard to its conflict or choice of law 1410 
provisions.   1411 
 1412 
This Agreement constitutes the entire agreement between the parties as to its express subject matter and 1413 
expressly supersedes an d replaces any prior or contemporaneous agreements between the parties,  1414 
whether written or oral, relating to the subject matter of this Agreement. 1415 
 1416 
Adopter, on behalf of itself and its Affiliates, agrees to comply at all times with all applicable laws, rul es 1417 
and regulations with respect to its and its Affiliates’ performance under this Agreement, including without 1418 
limitation, export control and antitrust laws.  Without limiting the generality of the foregoing, Adopter 1419 
acknowledges that this Agreement prohibits any communication that would violate the antitrust laws. 1420 
 1421 
By execution hereof, no form of any partnership, joint venture or other special relationship is created 1422 
between Adopter, or O-RAN Alliance or its Members, Contributors or Academic Contributors.  Except as 1423 
expressly set forth in this Agreement, no party is authorized to make any commitment on behalf of 1424 
Adopter, or O-RAN Alliance or its Members, Contributors or Academic Contributors. 1425 
 1426 
In the event that any provision of this Agreement conflicts with  governing law or if any provision is held 1427 
to be null, void or otherwise ineffective or invalid by a court of competent jurisdiction, (i) such provisions 1428 
will be deemed stricken from the contract, and (ii) the remaining terms, provisions, covenants and 1429 
restrictions of this Agreement will remain in full force and effect. 1430 
 1431 
Any failure by a party or third party beneficiary to insist upon or enforce performance by another party of 1432 
any of the provisions of this Agreement or to exercise any rights or remedies und er this Agreement or 1433 
otherwise by law shall not be construed as a waiver or relinquishment to any extent of the other parties’ 1434 
or third party beneficiary’s right to assert or rely upon any such provision, right or remedy in that or any 1435 
other instance; rather the same shall be and remain in full force and effect. 1436 
 1437 