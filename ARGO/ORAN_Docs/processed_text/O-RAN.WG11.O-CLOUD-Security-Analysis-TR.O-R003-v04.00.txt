O-RAN.WG11.O-CLOUD-Security-TR.0-R003-v04.00
Technical Report
O-RAN Work Group 11 (Security Work Group)
Study on Security for O-Cloud
Copyright © 2023 by the O-RAN ALLIANCE e.V.
The copying or incorporation into any other work of part or all of the material available in this specification in any form without the prior written permission of O-RAN ALLIANCE e.V.  is prohibited, save that you may print or download extracts of the material of this specification for your personal use, or copy the material of this specification for the purpose of sending to individual third parties for their information provided that you acknowledge O-RAN ALLIANCE as the source of the material and that you inform the third party that these conditions apply to them and that they must comply with them.
O-RAN ALLIANCE e.V., Buschkauler Weg 27, 53347 Alfter, Germany
Register of Associations, Bonn VR 11238, VAT ID DE321720189
Contents
List of figures	4
List of tables	4
Foreword	5
Modal verbs terminology	5
Introduction	5
1	Scope	6
2	References	6
2.1	Informative references	6
3	Definition of terms, symbols and abbreviations	9
3.1	Terms	9
3.2	Abbreviations	10
4	O-Cloud Architecture	13
4.1	Components	13
4.1.1	SMO	13
4.1.2	O-Cloud management components	14
4.1.3	Hardware resources	14
4.1.4	Operating System (OS)	14
4.1.5	Virtualization Layer	14
4.1.6	NFs Layer	14
4.1.7	O-Cloud images repository	14
4.2	Interfaces	15
4.3	Critical services	17
4.3.1	SERV#01 SMO-O-Cloud O2 services	17
4.3.2	SERV#02 O-Cloud images service	18
4.3.3	SERV#03 O-Cloud monitoring service	18
4.3.4	SERV#04 O-Cloud provisioning service	19
4.3.5	SERV#05 O-Cloud software management service	19
4.3.6	SERV#06 O-Cloud fault management service	19
4.3.7	SERV#07 O-Cloud performance service	20
5	Cloud deployment scenarios	20
5.1	Main actors	20
5.2	Cloud service models	21
5.2.1	Infrastructure as a Service (IaaS)	21
5.2.2	Platform as a Service (PaaS)	21
5.2.3	Software as a Service (SaaS)	22
5.3	Cloud deployment types	22
5.3.1	Private cloud	22
5.3.2	Community Cloud	24
5.3.3	Public Cloud	25
5.3.4	Hybrid Cloud	26
5.4	High-Level risk assessment	28
6	Roles and responsibilities	33
7	Security Problem Definition	34
7.1	Assets	34
7.2	Threats	41
7.2.1	Threat and impact types	41
7.2.2	Attack surface	42
7.2.3	Vulnerabilities	43
7.2.4	Threat events	44
8	Recommendations and best practices	67
8.1	REC-CM Certificate management	68
8.2	REC-NS Network Segmentation & Filter Network Traffic	69
8.3	REC-IAM Identity, Authentication and Access Management	70
8.4	REC-VHPM Vulnerability Handling and Patch Management	73
8.5	REC-SCONF Security Configuration	74
8.6	REC-SDLC Secure Development Lifecycle	75
8.7	REC-SNFLC Security App/VNF/CNF lifecycle	75
8.8	REC-IMGP Image Protection	76
8.9	REC-LOG Logging, Monitoring and Alerting	78
8.10	REC-SB Secure Boot	79
8.11	REC-ISO Strong Isolation	80
8.12	REC-AUD Security Audit	81
8.13	REC-SS Secure Storage	82
8.14	REC-PHY Physical Security Protection	82
8.15	REC-RA Remote Attestation	84
8.16	REC-SDD Secure data deletion	85
9	Risk Assessment	85
Annex A (informative):  Best practices from some of existing main security guidance	86
A.1	CISA/NSA Kubernetes security hardening best practices	86
A.2	CIS Docker security best practices	90
A.3	ONAP VNFs security best practices	92
Revision History	99
History	99
List of figures
Figure 4-1 : O-CLOUD architecture	13
Figure 4-2 : AAL Architecture and interfaces (Source [52])	17
Figure 4-3 : Parallel reporting & Alarm correlation [1]	20
Figure 5-1 : IaaS cloud service model	21
Figure 5-2 : PaaS cloud service model	21
Figure 5-3 : SaaS cloud service model	22
Figure 5-4 : On site private cloud	23
Figure 5-5 : Outsourced private cloud	23
Figure 5-6 : On site community cloud	24
Figure 5-7 : Outsourced community cloud	25
Figure 5-8 : Public cloud	26
Figure 5-9 : Hybrid cloud	27
Figure 7-1 : Cartography of assets	35
Figure 7-2 : Attack vectors	43
Figure 7-3 : Vulnerabilities within O-Cloud	43
Figure 7-4 : Illustration of the VM/Container escape attack	50
Figure 7-5 : Illustration of the migration flooding attack	52
Figure 7-6 : Illustration of the false resource advertising attack	53
Figure 7-7 : Illustration of the migration MITM attack	53
Figure 7-8 : Illustration of the Theft-of-Service/DoS Attack	54
Figure 7-9 : Illustration of the VM/Container hyperjacking attack	59
Figure 7-10 : Illustration of a cross VM/Container side channel attack	62
List of tables
Table 4-1 : O-Cloud interfaces	16
Table 5-1 : High level security risk assessment of cloud deployment models	32
Table 6-1 : List of Users	34
Table 7-1 : List of Assets	40
Foreword
This Technical Report (TR) has been produced by the O-RAN Alliance.
Modal verbs terminology
In the present document "shall", "shall not", "should", "should not", "may", "need not", "will", "will not", "can" and "cannot" are to be interpreted as described in clause 3.2 of the O-RAN Drafting Rules (Verbal forms for the expression of provisions).
"must" and "must not" are NOT allowed in O-RAN deliverables except when used in direct citation.
Introduction
This technical report provides the threat model for the O-Cloud. The report identifies threats, security requirements and recommends potential security controls to protect against identified threats.
Scope
The steps of the threat modelling process are as follows:
Identify assets: Identify the valuable assets that the O-Cloud must protect.
Identify the threats: Identify the threats that could affect O-Cloud
Document the threats: Document each threat using a common threat template that defines a core set of attributes to capture for each threat.
Rate the threats: Rate the threats to prioritize and address the most significant threats first. The rating process weighs the probability of the threat against damage that could result should an attack occur.
Define potential mitigations to counter the identified threats and reduce their risks.
The structure of the document is divided into seven chapters and one annex:
Chapter 4 describes the Cloud architecture in terms of components, interfaces, and critical services.
Chapter 5 explores the different deployment types and models. In addition, it provides a high-level risk assessment of those cloud deployment models.
Chapter 6 highlights the main actors of the O-Cloud in terms of roles and responsibilities.
Chapter 7 outlines the security problem definition in terms of assets, attack vectors, vulnerabilities and threats.
Chapter 8 defines recommendations and best practices to mitigate the identified threats.
Chapter 9 figures out the risk assessment of the identified threats in terms of impact and likelihood.
Annex A provides best practices from some of the main security guidance on cloud, virtualization and containerization.
References
Informative references
References are either specific (identified by date of publication and/or edition number or version number) or non-specific. For specific references, only the cited version applies. For non-specific references, the latest version of the referenced document (including any amendments) applies.
NOTE:	While any hyperlinks included in this clause were valid at the time of publication, O-RAN cannot guarantee their long-term validity.
The following referenced documents are not necessary for the application of the present document, but they assist the user with regard to a particular subject area.
O-RAN ALLIANCE TS: “O-RAN.WG6.O2-GA&P” (O-RAN O2 Interface - General Aspects and Principles)
O-RAN ALLIANCE TS: “O-RAN.WG6.ORCH-USE-CASES” (Cloudification and Orchestration Use Cases and Requirements for O-RAN Virtualized RAN)
CISA/NSA - Kubernetes Hardening Guidance – August 2021
https://media.defense.gov/2021/Aug/03/2002820425/-1/-1/1/CTR_KUBERNETESHARDENINGGUIDANCE.PDF
MITRE ATT&CK containers matrix
https://attack.mitre.org/matrices/enterprise/containers/
ENISA Threat Landscape for 5G Networks: Threat assessment for the fifth generation of mobile telecommunications networks (5G); November 2019
ETSI GS NFV-SEC 025 "work in progress." Network Functions Virtualisation (NFV) Release 4; Security; Secure End-to-End VNF and NS management specification
O-RAN ALLIANCE TS: “O-RAN.WG6.O2DMS-INTERFACE-ETSI-NFV-PROFILE” O2dms Interface Specification)
O-RAN ALLIANCE TS: “O-RAN.WG6.O2IMS-INTERFACE” (O2ims Interface Specification)
IETF RFC 3647: "Internet X.509 Public Key Infrastructure Certificate Policy and Certification Practices Framework"
ETSI GR NFV-SEC 005 Network Functions Virtualisation (NFV); Trust; Report on Certificate Management
IETF RFC 6749: "The OAuth 2.0 Authorization Framework".
ETSI GS NFV-SEC 022 Network Functions Virtualisation (NFV) Release 2; Security; Access Token Specification for API Access
ETSI GR NFV-SEC 018 Network Functions Virtualisation (NFV); Security; Report on NFV Remote Attestation Architecture
Ludovic Jacquin, Antonio Lioy, Diego R. Lopez, Adrian L. Shaw, and Tao Su “The trust problem in modern network infrastructures”
https://security.polito.it/doc/public/trust_modern_network_2015.pdf
ETSI GR NFV-SEC 007 Network Functions Virtualisation (NFV); Trust; Report on Attestation Technologies and Practices for Secure Deployments
3GPP TR 33.848 “Study on security impacts of virtualisation”
OWASP Container Security Verification Standard
https://owasp.org/www-project-container-security-verification-standard/migrated_content
CIS Docker Benchmark Securing Docker
https://www.cisecurity.org/benchmark/docker/
NIST Special Publication 800-190 Application Container Security Guide
OWASP Docker Security Cheat Sheet
https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html
OWASP Kubernetes Security Cheat Sheet
https://cheatsheetseries.owasp.org/cheatsheets/Kubernetes_Security_Cheat_Sheet.html
NIST SP 800-145: The NIST Definition of Cloud Computing
NIST SP 500-322: Evaluation of Cloud Computing Services Based on NIST 800-145
ONAP VNF Development Requirements – VNF Security
https://docs.onap.org/projects/onap-vnfrqts-requirements/en/latest/Chapter4/Security.html
GSMA NG.126 - Cloud Infrastructure Reference Model
https://www.gsma.com/newsroom/wp-content/uploads//NG.126-v1.0-2.pdf
Aqua Top 20 Docker Security Best Practices: Ultimate Guide
https://blog.aquasec.com/docker-security-best-practices
“Security Impacts of Virtualization on a Network Testbed”, Software Security and Reliability (SERE), 2012 IEEE Sixth International Conference
https://www.researchgate.net/publication/261059755_Security_Impacts_of_Virtualization_on_a_Network_Testbed
Beniel Dennyson W, Dr. S. Prabakaran “Detecting Hyperjacking in cloud based virtual environment”
https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwi92Zeznun0AhULHxoKHXO6AkgQFnoECAgQAQ&url=http%3A%2F%2Fsersc.org%2Fjournals%2Findex.php%2FIJAST%2Farticle%2Fdownload%2F15440%2F7789%2F&usg=AOvVaw3THwuT_S_WyKr6miOiJ7GS
Muhammad Kazim “Security Aspects of Virtualization in Cloud Computing”
https://www.researchgate.net/publication/273950406_Security_Aspects_of_Virtualization_in_Cloud_Computing
Anita Choudhary “ A critical survey of live virtual machine migration techniques “
https://journalofcloudcomputing.springeropen.com/track/pdf/10.1186/s13677-017-0092-1.pdf
Svetlana Kolesnikova, Roman Kulikov, Yuriy Gatchin, Daniil Melnik “Hypervisor Security Analyses Based on Ishikawa Methodology”
https://www.researchgate.net/publication/323838205_Hypervisor_Security_Analyses_Based_on_Ishikawa_Methodology?enrichId=rgreq-bf37d66002988b1ad9ef24fb09198313-XXX&enrichSource=Y292ZXJQYWdlOzMyMzgzODIwNTtBUzo2MDg0MzY3NTI0ODIzMDVAMTUyMjA3NDAzNDM4NQ%3D%3D&el=1_x_3&_esc=publicationCoverPdf
Mehiar Dabbagh, Ammar Rayes “Internet of Things Security and Privacy”
https://www.researchgate.net/publication/309375790_Internet_of_Things_Security_and_Privacy?enrichId=rgreq-69e8c33eb9fc28fccec64fbdad0a91d0-XXX&enrichSource=Y292ZXJQYWdlOzMwOTM3NTc5MDtBUzo1NTYxNDY2NDI2OTAwNDhAMTUwOTYwNzEwMDM0OA%3D%3D&el=1_x_3&_esc=publicationCoverPdf
Changwei Liu, Anoop Singhal, Duminda Wijesekera “A Layered Graphical Model for Cloud Forensic Mission Attack Impact Analysis”
https://www.researchgate.net/publication/327314423_A_Layered_Graphical_Model_for_Cloud_Forensic_Mission_Attack_Impact_Analysis_14th_IFIP_WG_119_International_Conference_New_Delhi_India_January_3-5_2018_Revised_Selected_Papers?enrichId=rgreq-b9ce99f09429d1a36256442e04006d0f-XXX&enrichSource=Y292ZXJQYWdlOzMyNzMxNDQyMztBUzo3NTM0MDM2Mzg1ODMzMDFAMTU1NjYzNjgzMzI4Nw%3D%3D&el=1_x_3&_esc=publicationCoverPdf
Docker Best practices for scanning images
https://docs.docker.com/develop/scan-images/
Shankar Lal, Tarik Taleb, and Ashutosh Dutta “NFV: Security Threats and Best Practices”
http://anastacia-h2020.eu/publications/NFV_Security_Threats_and_Best_Practices.pdf
NSA/CISA Mitigating Cloud Vulnerabilities
https://media.defense.gov/2020/Jan/22/2002237484/-1/-1/0/CSI-MITIGATING-CLOUD-VULNERABILITIES_20200121.PDF
Fraunhofer AISEC report: Threat analysis of container-as-a-service for network function virtualization
https://www.aisec.fraunhofer.de/content/dam/aisec/Dokumente/Publikationen/Studien_TechReports/englisch/caas_threat_analysis_wp.pdf
CSA. The treacherous 12: Cloud computing top threats in 2016. Technical report, Cloud Security Alliance, 2016. 12, 14, 15, 16, 17, 27
ETSI GS NFV-SOL 013 Network Functions Virtualisation (NFV) Release 2; Protocols and Data Models; Specification of common aspects for RESTful NFV MANO APIs
CISA Cloud Security Technical Reference Architecture
https://www.cisa.gov/sites/default/files/publications/CISA%20Cloud%20Security%20Technical%20Reference%20Architecture_Version%201.pdf
Guide ANSSI - Hardware security requirements for x86 platforms
https://www.ssi.gouv.fr/uploads/2019/11/anssi-guide-hardware_security_requirements.pdf
O-RAN ALLIANCE TS: “O-RAN.WG11.Security-Requirements-Specification” (O-RAN Security Requirements Specification)
O-RAN ALLIANCE TS: “O-RAN.WG10.OAM-Architecture” (O-RAN Operations and Maintenance Architecture)
NSA/CISA Security Guidance for 5G Cloud Infrastructures
https://www.cisa.gov/news/2021/10/28/nsa-and-cisa-provide-cybersecurity-guidance-5g-cloud-infrastructures
O-RAN ALLIANCE TS: “O-RAN.WG11.Security-Protocols-Specification” (O-RAN Security Protocols Specifications)
ETSI GS NFV-SOL 004 Network Functions Virtualisation (NFV) Release 3; Protocols and Data Models; VNF Package and PNFD Archive specification
ETSI GS NFV-SEC 021 Network Functions Virtualisation (NFV) Release 2; Security; VNF Package Security Specification
NIST SP 800-88 ”Guidelines for Media Sanitization”
https://csrc.nist.gov/publications/detail/sp/800-88/rev-1/final
DoD 5220.22-M wiping standard
O-RAN ALLIANCE TR: “O-RAN.WG11.Threat-Model” (O-RAN Security Threat Modeling and Risk Assessment)
O-RAN ALLIANCE TS: “O-RAN.WG6.AAL-Common-API” (O-RAN Acceleration Abstraction Layer Common API)
O-RAN ALLIANCE TS: “O-RAN.WG6.AAL-GAnP” (O-RAN Acceleration Abstraction Layer General Aspects and Principles)
Definition of terms, symbols and abbreviations
Terms
For the purposes of the present document, the terms and definitions given in 3GPP TR 21.905 and the following apply:
A1: Interface between non-RT RIC and Near-RT RIC to enable policy-driven guidance of Near-RT RIC applications/functions, and support AI/ML workflow.
A1 Enrichment information: Information utilized by near-RT RIC that is collected or derived at SMO/non-RT RIC either from non-network data sources or from network functions themselves.
A1 policy: Type of declarative policies expressed using formal statements that enable the non-RT RIC function in the SMO to guide the near-RT RIC function, and hence the RAN, towards better fulfilment of the RAN intent.
Deployment ID:	Correlation Identity created by the O-Cloud for the SMO to relate to its inventory and manage.
E2:  Interface connecting the Near-RT RIC and one or more O-CU-CPs, one or more O-CU-UPs, and one or more O-DUs.
E2 Node: a logical node terminating E2 interface. In this version of the specification, O-RAN nodes terminating E2 interface are:
-	for NR access: O-CU-CP, O-CU-UP, O-DU or any combination;
-	for E-UTRA access: O-eNB.
FCAPS: Fault, Configuration, Accounting, Performance, Security.
Intents: A declarative policy to steer or guide the behavior of RAN functions, allowing the RAN function to calculate the optimal result to achieve stated objective.
Near-RT RIC: O-RAN near-real-time RAN Intelligent Controller: a logical function that enables real-time control and optimization of RAN elements and resources via fine-grained data collection and actions over E2 interface.
Non-RT RIC: O-RAN non-real-time RAN Intelligent Controller: a logical function that enables non-real-time control and optimization of RAN elements and resources, AI/ML workflow including model training and updates, and policy-based guidance of applications/features in Near-RT RIC.
O-CU: O-RAN Central Unit: a logical node hosting O-CU-CP and O-CU-UP
O-CU-CP: O-RAN Central Unit – Control Plane: a logical node hosting the RRC and the control plane part of the PDCP protocol.
O-CU-UP: O-RAN Central Unit – User Plane: a logical node hosting the user plane part of the PDCP protocol and the SDAP protocol.
O-Cloud Node: An O-Cloud Node is exposed over O2ims as an Abstracted Resource. It is a collection of CPUs, Mem, Storage, NICs, Accelerators, BIOSes, BMCs, etc., and can be thought of as a server.
O-DU: O-RAN Distributed Unit: a logical node hosting RLC/MAC/High-PHY layers based on a lower layer functional split.
O-RU: O-RAN Radio Unit: a logical node hosting Low-PHY layer and RF processing based on a lower layer functional split.  This is similar to 3GPP’s “TRP” or “RRH” but more specific in including the Low-PHY layer (FFT/iFFT, PRACH extraction).
O1: Interface between management entities (NMS/EMS/MANO) and O-RAN managed elements, for operation and management, by which FCAPS management, Software management, File management shall be achieved.
NF Deployment:	An O-Cloud NF Deployment is a deployment of a cloud native Network Function (all or partial), resources shared within a NF Function, or resources shared across network functions. The NF Deployment configures and assembles user-plane resources required for the cloud native construct used to establish the NF Deployment and manage its life cycle from creation to deletion.
NF Deployment Descriptor:	A completed data model which provides an O-Cloud the necessary information to create a deployment.
RAN: Generally referred as Radio Access Network. In terms of this document, any component below Near-RT RIC per O-RAN architecture, including O-CU/O-DU/O-RU.
Service Provider: A network provider who is planning to deploy applications into their network.
Solution Provider: An application developer who delivers applications to Service Providers.
NOTE: For the purposes of the present document, the AAL terms and definitions given in [52] apply.
Symbols
void
Abbreviations
For the purposes of the present document, the abbreviations given in 3GPP TR 21.905 and the following apply:
AAL	Acceleration Abstraction Layer
AALI	Acceleration Abstraction Layer Interface
AALI-C	Acceleration Abstraction Layer Interface-Common
AALI-C-App	Acceleration Abstraction Layer Interface-Common-Application
AALI-C-Mgmt	Acceleration Abstraction Layer Interface-Common-Management
AALI-P	Acceleration Abstraction Layer Interface-Profile
AI/ML	Artificial Intelligence/Machine Learning
CNF 	Cloud-native Network Function (NOTE: same as Containerized Network Function or Cloudified Network Function)
DMS 	O-Cloud Deployment Management Services
eNB	eNodeB (applies to LTE)
FOCOM 	Federated O-Cloud Orchestration & Management
FTP	File Transfer Protocol
FTPS	File Transfer Protocol Secure
gNB	gNodeB (applies to NR)
IMS 	O-Cloud Infrastructure Management Services
IPSEC	Internet Protocol Security
KPI	Key Performance Indicator
KQI	Key Quality Indicator
LBT	Listen Before Talk
LCM 	Life Cycle Management
LLS	Lower Layer Split
MIMO	Multiple Input, Multiple Output
MNO	Mobile Network Operator
NETCONF	Network Configuration Protocol
NF 	Network Function
NFO 	Network Function Orchestration
NFV	Network Function Virtualisation
NFVI 	Network Function Virtualization Infrastructure
O-CU 	O-RAN Central Unit as defined by O-RAN ALLIANCE
O-CU-CP 	O-CU Control Plane
O-CU-UP 	O-CU User Plane
O-DU 	O-RAN Distributed Unit (uses Lower-level Split)
O-RU 	O-RAN Radio Unit
PDCP	Packet Data Convergence Protocol
PNF 	Physical Network Function
PRB	Physical Resource Block
PTP	Precision Timing Protocol
QoE	Quality of Experience
RAN 	Radio Access Network
RBAC	Role-based Access Control
RIC	O-RAN RAN Intelligent Controller
SDN	Software Defined Network
SINR	Signal-to-Interference-plus-Noise Ratio
SMO	Service Management and Orchestration
SSH	Secure Shell
TLS	Transport Layer Security
UAV	Unmanned Aerial Vehicle
V2X	Vehicle to Everything
VM 	Virtual machine
VNF	Virtualised Network Function
O-Cloud Architecture
The O-Cloud architecture is depicted in the following figure:
Figure 4-1 : O-CLOUD architecture
Components
The following are a description of the functional blocks identified in the figure here above [1], [2]:
SMO
The SMO components managing and orchestrating the O-Cloud software are:
Federated O-Cloud Orchestration and Management (FOCOM): The FOCOM is responsible for accounting and asset management of the resources in the cloud. The FOCOM is the primary consumer of services provided by the IMS. The FOCOM has information about the O-Cloud resources management. Specifically, the FOCOM needs to know whether the services are within the operator domain or external.
Network Function Orchestrator (NFO): The NFO is responsible for orchestrating the assembly of the network functions as a composition of NF Deployments in the O-Cloud. It may also utilize OAM Functions in order to access the O1 interface to the NF once it is deployed. Its use of the O1 is not germane to the O2 and is only mentioned here for completeness. The NFO is the primary consumer of the DMS.
O-Cloud management components
The O-Cloud components providing management services for consumption of SMO are:
Infrastructure Management Services (IMS): The IMS is responsible for management of the O-Cloud resources and the software which is used to manage those resources. The IMS generally provides services for consumption by the FOCOM.
Deployment Management Services (DMS): The DMS is responsible for management of NF Deployments into the O-Cloud. It provides the ability to instantiate, monitor, and terminate NF Deployments. The DMS generally provides services for consumption by the NFO.
Hardware resources
A Computer System is defined to be a physical or composed system capable of perform computations that is Underlay-Network connected. A Computer System can run any major Operating System with or without Virtualization and/or Container support functionality.
NOTE: A computer system in the context of a cloudified network and usage in cluster requires a network connectivity. For example, a server in a data center connected to an underlay network.
An underlay network is a physically connected network enabling Computer Systems to communicate with each other and with the gateway(s) connected to networks outside of the data center.
Hardware accelerator manager: It is an acceleration management function, that provides management capabilities for the HW Accelerator(s) in the O-Cloud Node. Management capabilities include but not limited to lifecycle management, configuration, updates/upgrades and failure handling. Hardware Accelerators include ASIC, FPGA, DSP and GPU.
Operating System (OS)
An Operating system is a software platform that manages and abstracts the Computer System hardware and software resources as well as provides common services for NFs such as scheduling and network connectivity.
Virtualization Layer
Hypervisor (VMs): An hypervisor is an OS that includes the ability to offer multiple Virtual Machines, each acting as a well-separated Computer System.
Container Engine: A Container Engine is an OS that include the ability to offer multiple separated name spaces, quotas, and management for Containers.
NFs Layer
The following are three deployment scenarios of a NF:
Bare Metal Container Cluster – A Bare Metal Container Cluster is a set of network-connected computer systems with their individual operating system instances that supports containers in a cluster configuration.
VM-based Container Cluster – A VM-based Container Cluster is a set of network-connected Virtual Machines with their individual guest operating system instance that supports containers in a clustered configuration.
VM Cluster – A VM Cluster is a set of network-connected Computer Systems with their individual operating instance that supports virtual machines in a cluster configuration.
O-Cloud images repository
The repository containing the Software Images of O-RAN Network Functions.
Interfaces
O-Cloud interfaces are illustrated in the following table:
Table 4-1 : O-Cloud interfaces
AAL interfaces are as follows [51]:
AALI-C-Mgmt: between the O-Cloud IMS/DMS and the Hardware Accelerator Manager. It is consistent with O2 interface.
AALI-C-App: common APIs between O-RAN NFs and the hardware accelerator device for initial discovery, AAL initialization, runtime operations, real time telemetry and status, etc.
AALI-P: Profile specific API – fine grain control over selection and off-loading, etc. The AALI configuration and management APIs are the APIs that an application (O-DU) executes to configure and manage the AAL-LPU(s) that have been allocated to the application by the O-Cloud.
Vendor specific interface between the hardware accelerator manager and hardware accelerator device.
The transport between a NF and an AAL implementation can be of different types (e.g., based on shared memory, PCIe interconnect, over Ethernet). AALI-C-App shall support abstraction of these various transport mechanisms between a NF and an AAL implementation through a set of common transport APIs, constituting a transport abstraction framework.
Figure 4-2 : AAL Architecture and interfaces (Source [52])
Critical services
The following main critical services are provided by the O-Cloud components [1], [2] which need to be highly protected and securely maintained. They are identified in this report to assess the negative impact that the identified threats may cause on them.
SERV#01 SMO-O-Cloud O2 services
Using the O2 interface, the SMO can perform the following operations:
Provide a boot image for a remote node
Send a cloud descriptor, (i.e., cloud deployment and configuration files) for initial cloud startup
Query the O-Cloud for attributes such as SW inventory
Send a request to O-Cloud to download software sent to it from the SMO, such as request for download of an xAPP deployment.
Subscribe to notification of configuration events, fault events and performance measurements from the O-Cloud
Query the O-Cloud for the DMS end points it supports
Query the O-Cloud for attributes such as capabilities and capacities
Request the O-Cloud to create a Network Function deployment using cloud resources
Request the O-Cloud to terminate Network Function deployment(s)
Request the O-Cloud to reset NF Deployment(s)
Request the O-Cloud to reset O-Cloud Node(s)
Issue a subscription to the O-Cloud to receive alarm event notifications
Query the O-Cloud for alarms on O-Cloud resources with query criteria which define the alarm characteristics that the SMO is interested in
Query the O-Cloud for state and status information
Perform an Alarm Subscription query towards the IMS
Perform a NF deployment (that releases an NF instance) reset through the O2dms interface
Perform an O-Cloud Node (recovery of the resources within the O-Cloud) reset through the O2ims interface
Using the O2 interface, the O-Cloud can perform the following operations:
Send asynchronous events to the SMO when available capabilities or capacities are changed, including when new hardware is added
Asynchronously notify the SMO when a software upgrade completes
Asynchronously notify the endpoint specified by the SMO (alarm subscriber) of alarm notifications related to O-Cloud resources
Return alarms in response to the SMO alarm query that match the alarm query criteria
SERV#02 O-Cloud images service
It provides Add/Delete/Update/Query functions of O-RAN Cloudified Network Functions images with their related information (e.g. SoftwareImageId, Vendor, and Version) from O-Cloud repository.
SERV#03 O-Cloud monitoring service
When the O-Cloud Infrastructure or the ORAN cloudified NFs fails, it needs to be fixed immediately, and preferably automatically, to prevent end users from experiencing service disruptions. To avoid this service disruption Network Operations must consider the telemetry information of O-Cloud deployments in the network. The telemetry information serves as a vital resource for analysing the O-Cloud’s state and health, and for delivering on service monitoring goals. The O-Cloud Monitoring Service uses telemetry data to provide monitoring of O-Cloud infrastructures. O-Cloud telemetry shall minimally consist of Fault, Performance, and Configuration Data. There are different types of telemetry:
Managed Element Telemetry to monitor the NFs behavior through O1.
Deployment Telemetry to monitor the number of deployment instances an O-Cloud has at that moment and how many were expected, how the on-progress deployment is going, and health checks. Additional Deployment Telemetry metrics like CPU, network, and memory usage can also be collected. This will be performed through the O2dms interface.
Infrastructure Telemetry to monitor the health of the O-Cloud Infrastructure components. Network Operations are interested in discovering if all the components in the O-Cloud Infrastructure are working properly and at what capacity, how many deployments are running on each node, and the resource utilization of the O-Cloud Infrastructure. This will be performed through the O2ims interface.
The SMO shall be able to collect and correlate telemetries to aggregate problems to a root cause.
The O-Cloud shall be able to report telemetries and make all Configuration Data and any external changes to it available to the SMO.
SERV#04 O-Cloud provisioning service
O-Cloud Provisioning is the allocation of O-Cloud’s resources and services to an O-RAN Cloudified Network Function. This is one of the key functionalities of the O-Cloud, relating to how an O-RAN Cloudified Network Function procures O-Cloud services and resources. O-Cloud Provisioning shall provide:
Create/Read/Update/Delete rules for Affinity, Anti-Affinity, and Quorum Diversity
Query of O-Cloud Capacity & Availability
SERV#05 O-Cloud software management service
Software management will manage the O-Cloud software. The software management should be a priority as without proper management unnecessary risks may be taken. The software management ensures security, cost management and software support. There are many benefits to software management, of which the main benefits are:
Prevents unauthorized software from being installed
Maintains a catalog of authorized software and its versions
Provides visibility into what software and version is being used
Provides a better view of which software products and versions are vendor supported
In O-RAN from an O-Cloud perspective there are two types of software which needs to be managed on the O2 Interface:
The O-Cloud Infrastructure Software
The O-RAN Cloudified Network Function Software is the software implementation of O-RAN NFs which can run over the O-Cloud
SERV#06 O-Cloud fault management service
It is related to IMS & DMS fault management. There are three types of fault notifications originate from O-Cloud and collected by SMO through O1, O2dms and O2ims.
Fault notifications originate from the O-cloud infrastructure when a condition occurs within IMS on an O-Cloud resource. An event is issued towards the SMO for analysis. The SMO can issue a specific fault query towards to IMS related to the O-Cloud resources and resource pool through O2ims.
DMS resource faults are associated with a workload. The SMO can issue a specific query related to xApp/NF deployments (e.g. workloads) from the DMS through O2dms.
NFs faults are reported through O1 (application level).
Figure 4-3 : Parallel reporting & Alarm correlation [1]
It is possible that multiple resources can associate with a workload, whereby one fault might trigger a fault notification on IMS, DMS and O1.
For example, it is conceivable that a O-DU application might report a fault on O1, and a related physical server allocated to a workload of a O-DU would raise a DMS fault which might also have an infrastructure fault raised on IMS.
For example, the SMO may receive a fault with a root cause of a network issue among the O-Cloud resources which manifests itself in a deployment and application fault as well. Thus, the SMO might receive three notifications over O2ims, O2dms and O1. It might correlate these faults to a common root cause.
NOTE: Faults issued by the O-Cloud could be stored at the O-Cloud resource for a configured amount of time.
SERV#07 O-Cloud performance service
The purpose of performance is to report operational information related to O-cloud resources. Typically, performance information allows an operator or administrator of the O-cloud a sense of how well the system is operating. It is distinct from faults in that it is not about failures in the system but about how well the overall system is performing. Though, faults or alarms may negatively impact performance of the O-Cloud which might be observable in performance measurements.
Performance measurements are typically captured periodically through time. They are collected and stored at regular intervals by the system in order to gauge the performance of a system over a period of time. This allows for analytical operations to be performed on the collected data and statistics to be built over time. This can tell an operator or system analyst whether they have sufficient capacity in a network based on the demands of the network. This can be vital for making business operational decisions such as scaling a network.
Cloud deployment scenarios
Main actors
Operator
CSP (Cloud Service Provider)
Vendors
Third parties (e.g. service providers, verticals)
Cloud service models
Different levels of abstraction constitute the platform of the cloud architecture. These abstractions are grouped into the different service levels, depending on what resources are offered as a service for a given abstraction level. According to NIST SP 800-145 the three standard cloud service models are Platform as a Service (PaaS), Software as a Service (SaaS), and Infrastructure as a Service (IaaS).
Infrastructure as a Service (IaaS)
The capability provided to the Operator is to provision processing, storage, networks, and other fundamental computing resources where the Operators or Vendors are able to deploy and run arbitrary software, which can include Host OS (Operator or Vendor), virtualization platforms (Operator or Vendor) and VNFs/CNFs (Operator). Operators and Vendors do not manage or control the underlying cloud infrastructure but have control over Host OS (Operator or Vendor), virtualization platforms (Operator or Vendor) and VNFs/CNFs (Operator).
Figure 5-1 : IaaS cloud service model
Platform as a Service (PaaS)
The capability provided to the Operator is to deploy onto the cloud infrastructure VNFs/CNFs created using programming created using programming languages, libraries, services, and tools supported by the cloud provider. The Operator does not manage or control the underlying cloud infrastructure including network, servers, operating systems, or storage, but has control over the deployed VNFs/CNFs and possibly configuration settings for the application-hosting environment. PaaS provides this platform from the cloud, hence allowing operators to develop and run VNFs/CNFs without the overhead cost (CAPEX) of building and maintaining separate platforms.
Figure 5-2 : PaaS cloud service model
Software as a Service (SaaS)
SaaS as defined by NIST is the capability provided to the Cloud Consumer to use the Cloud Provider’s applications running on a cloud infrastructure. Applications are accessible through web browser or program interface. The operator does not manage or control the underlying cloud infrastructure including network, servers, operating systems, storage, or even Applications.
Figure 5-3 : SaaS cloud service model
This service model introduces risk for the Operator, acting as the Cloud Consumer, due to shared resources, reduced control, lack of transparency for data security, secure access configuration, limited visibility to logging at lower layers, malicious insiders, regulatory drift, and lack of due diligence. It is not expected that 5G operators will deploy with the SaaS service model.
The Cloud Provider may offer 5G private networks directly to its customers in a SaaS service model.  In this service model the Cloud Provider’s customer is the Cloud Consumer, which has responsibility for securing its data and access to the applications.
Cloud deployment types
In NIST SP 800-145, cloud deployment models describe how the cloud is operated and who has access to the cloud service resources. The four deployment models are defined in NIST SP 800-145 as follows:
Private cloud
A private cloud gives a single Operator the exclusive access to and usage of the cloud service and related infrastructure and computational resources. It may be managed either by the Operator or by a third party Cloud Provider and may be hosted on the Operator’s premises (i.e., on-site private clouds) or outsourced to a hosting company (i.e., outsourced private clouds). The following figures present an on-site private cloud and an outsourced private cloud, respectively.
Figure 5-4 : On site private cloud
Figure 5-5 : Outsourced private cloud
On site private cloud
Outsourced private cloud
Community Cloud
A community cloud serves a group of Operators that have shared concerns such as mission objectives, security, privacy and compliance policy, rather than serving a single Operator (e.g., a private cloud). Similar to private clouds, a community cloud may be managed by the Operators or by a third party Cloud Provider and may be implemented on the Operator’s premise (i.e., on-site community cloud) or outsourced to a hosting company (i.e., outsourced community cloud). Figure 5-6 depicts an on-site community cloud comprised of a number of participant Operators. An Operator can access the local cloud resources, and also the resources of other Operators through the connections between the associated Operators. Figure 5-7 shows an outsourced community cloud, where the server side is outsourced to a hosting company. In this case, an outsourced community cloud builds its infrastructure off premise, and serves a set of Operators that request and consume cloud services.
Figure 5-6 : On site community cloud
Figure 5-7 : Outsourced community cloud
On-site Community Cloud
Outsourced community cloud
Public Cloud
A public cloud is one in which the cloud infrastructure and computing resources are made available to the general public over a public network. A public cloud is owned by an organization providing cloud services, and serves a diverse pool of clients (e.g. Operators, Third parties service providers).
Figure 5-8 presents a simple view of a public cloud and its actors.
Figure 5-8 : Public cloud
Hybrid Cloud
A hybrid cloud is a composition of two or more clouds (on-site private, on-site community, off-site private, off-site community or public) that remain as distinct entities but are bound together by standardized or proprietary technology that enables data and application portability. Figure 5-9 presents a simple view of a hybrid cloud that could be built with a set of clouds in the five deployment model variants.
The idea is to combine the benefits of multiple deployment models. The deployment in hybrid clouds could be either a combination of private, public or community clouds. One of the popular use cases of hybrid clouds is in enhancing security and privacy on the cloud without incurring the overhead costs (CAPEX) of building a private cloud. In this case, non-critical resources like test workloads can be hosted in the public cloud, while critical resources like user data and workloads are hosted internally.
Figure 5-9 : Hybrid cloud
High-Level risk assessment
The following table illustrates the high-level security risk assessment of the cloud deployment models (Private, Community, Public and Hybrid). Colors red, yellow, and green are used on the security risk assessment to indicate high, medium, and low levels of risk. The Cloud Consumer (operator) is accountable for the security posture of the deployment for all cloud deployment models [44].
Risk levels: High, Moderate, Low
Table 5-1 : High level security risk assessment of cloud deployment models
Roles and responsibilities
The list of users likely to interact with the O-CLOUD as well as their roles are presented in the table below. The last column distinguishes between stakeholders acting:
Under the direct responsibility of an operator or
Under the responsibility of a third party (e.g. equipment supplier) acting through a contract with the operator.
The table shows many roles in which the operator and cloud service provider are both stakeholders.  This highlights the need to have:
Clearly defined and agreed upon roles and responsibilities in the cloud service agreement.
Separation of Duties enforced by the cloud service provider to limit insider threats.
Principles of least privilege access control configured and enforced by the operator.
Table 6-1 : List of Users
Security Problem Definition
Before analyzing the way to protect the O-Cloud, it is important to identify the threats affecting the different O-Cloud components and data.
For this identification of threats, it is essential first to know the critical assets, consisting of anything that has value for within O-Cloud and needs to be protected, and secondly to identify the threat agents, the entities that can adversely act on the asset.
Assets
The services offered by the O-Cloud must be available and non-corrupted and must protect the assets highlighted in the following cartography.
Figure 7-1 : Cartography of assets
For each asset, a description, the owner, and its security protection properties are given in the table below.
Table 7-1 : List of Assets
Threats
The complexity and the extension of attack surface of a virtualized environment increase the difficulty to list, in an exhaustive manner, the security threats to which the O-Cloud assets are exposed. For this activity of threat analysis, some reports have been used to help the identification of a largest number of these threats:
CISA/NSA - Kubernetes Hardening Guidance [3]
MITRE ATT&CK containers matrix [4]
ENISA Threat Landscape for 5G Networks: Threat assessment for the fifth generation of mobile telecommunications networks (5G) [5]
ETSI Secure End-to-End VNF and NS management specification Secure End-to-End VNF and NS management specification [6]
The following table is the template used to present the threat characteristics:
Threat and impact types
For identifying threats, we are using STRIDE:
S - Spoofing identity. An application or program can masquerade as another to gain advantages not typically allowed for that program.
T - Tampering with data. This involves the malicious modification of data, including making unauthorized changes to a database and alteration of data as it flows between computers.
R - Repudiation. A user or program refuses the authenticity of a good or reasonable command or action.
I - Information disclosure. This involves the exposure of information to individuals with unauthorized access to it. For example, users gain the ability to read a file that they normally would not have been granted access to, or an intruder can read data in transit between computers.
D - Denial of service. These attacks deny service to valid users, such as making a website unavailable or unusable by flooding it with illegitimate requests to keep legitimate users without access.
E - Elevation of privileges. An unauthorized user gains privileged rights to access previously no granted to compromise or destroy the system, such as a change in membership.
Attack surface
An attack surface of a system refers to the set of various entry points that can be exploited. The various components that compose the attack surface of the O-Cloud are depicted in the following figure. They are as follows:
VNFs/CNFs: O-DU, O-CU, O-RU, Near RT-RIC/xApps
Images repository with its interface to O-Cloud
Virtualization layer: Hypervisor and/or Container Engine, Host OS
Hardware resources including compute, storage, network, and hardware accelerator manager
O-Cloud API
O2dms and O2ims interfaces
NFO and Federated O-Cloud O&M (FOCOM) within the SMO
AAL
The attack vectors for exploiting vulnerabilities of these components are discussed in detail in the threat events sections.
Figure 7-2 : Attack vectors
Vulnerabilities
The following figure highlights the main vulnerabilities that may emerge within the attack vectors (see Figure 7-2).
Figure 7-3 : Vulnerabilities within O-Cloud
Threat events
The following threats to O-Cloud have been identified and are analyzed in this chapter. Other threats within the attack vectors ④, ⑥, ⑦ ⑧ will be added in future versions of the report.
Generic Threats
Threats concerning VMs/Containers
Figure 7-4 : Illustration of the VM/Container escape attack
Figure 7-5 : Illustration of the migration flooding attack
Figure 7-6 : Illustration of the false resource advertising attack
Figure 7-7 : Illustration of the migration MITM attack
Figure 7-8 : Illustration of the Theft-of-Service/DoS Attack
Threats concerning VM/Container images
Threats concerning the virtualization layer (Host OS-Hypervisor/Container engine)
Figure 7-9 : Illustration of the VM/Container hyperjacking attack
Threats concerning O-Cloud interfaces
O2 interface
Two main interfaces are defined in O-RAN WG6 specification and identified as critical assets of O-Cloud, i.e. interfaces O2 between O-Cloud and SMO. The threats on these interfaces are as follows.
O-Cloud API
Threats concerning hardware resources
Figure 7-10 : Illustration of a cross VM/Container side channel attack
Threats concerning O-Cloud management (SMO, NFO, FOCOM)
Threats concerning Acceleration Abstraction Layer (AAL)
Recommendations and best practices
A large number of useful and relevant security guidelines and best practices exist that are relevant for VM/Container-based virtualization and cloud computing in general. Therefore, we provide them here collectively and highly recommend that readers consult them for additional references:
3GPP TS 33.848 Study on security impacts of virtualisation
MITRE containers matrix
CIS Critical Security Controls
CIS Docker Benchmark
CIS VMWARE Benchmark
ONAP master documentation
CISA/NSA Kubernetes security guidance
OpenStack security guide
Cloud native wiki by aqua
ANSSI “Recommandations de sécurité relatives aux déploiements de conteneur docker “
CISA/NSA “Security guidance for 5g cloud infrastructures” parts 1 to 4
GSMA NG.126 “Cloud Infrastructure Reference Model”
Docker best practices
FFTelecoms “Référentiel d'objectifs de sécurité en matière de fonctions réseau virtualisées “
Fraunhofer AISEC Threat analysis of container-as-a-service for network function virtualization
REC-CM Certificate management
Recommendation: O-Cloud should support Public Key Cryptography for the purpose of distributing Public Key Certificates (PKC) for authenticating, authorizing, and encrypting links between components in O-Cloud. Each operator should develop Certificate Policy in accordance with their regional and national requirements. In addition, Operators should setup a renewal procedure (preferably automatically) of certificates prior to their expiration.
In O-Cloud, the components to be issued certificates include:
O-Cloud and VNFs/CNFs should employ certificates which can be used for images signing and verification during onboarding and registration.
SMO and VNF/CNF should employ certificates which can be used in order to establish secure connections between them.
SMO employs certificates in order to establish secure management connections with NFO and FOCOM.
O-Cloud infrastructure employs certificate(s) in order to establish secure connections with NFO and FOCOM through O2 interface.
Best practices to fulfill this recommendation:
For more details see ETSI GR NFV-SEC 005 [10].
This recommendation can help to mitigate: T-GEN-02, T-GEN-04, T-VM-C-01, T-VM-C-02, T-VM-C-03, T-VM-C-04, T-VM-C-05, T-IMG-01, T-IMG-02, T-IMG-04, T-VL-01, T-O2-01, T-OCAPI-01, T-HW-01, T-HW-02, T-ADMIN-01, T-ADMIN-02
REC-NS Network Segmentation & Filter Network Traffic
Recommendation: Physical and logical segmentation to prevent access to potentially sensitive O-Cloud components and information should be implemented. In addition, network policies should be defined to ensure a secure communication between VMs/Containers and limit the communication between VMs/Containers as much as possible to limit potential damage if a VM/Container is compromised.
Best practices to fulfill this recommendation:
This recommendation can help to mitigate: T-GEN-02, T-VM-C-01, T-VM-C-02, T-VM-C-03, T-VM-C-04, T-IMG-02, T-VL-01, T-O2-01, T-OCAPI-01, T-HW-02, T-ADMIN-02
REC-IAM Identity, Authentication and Access Management
Recommendation: A strong IAM framework should be in place to ensure authentication, authorization, accounting and access control. It should be used to initiate, capture, record and manage user identities and their related access permissions to O-Cloud assets in an automated way.
The IAM framework should be set up to manage and secure access to NFO/FOCOM, VNFs/CNFs, Host OS, Hypervisor, Container Engine, data and services that reside in the O-Cloud. This framework should protect credentials and accounts associated with O-Cloud platforms. Specifically, IAM protects root accounts for servers in the O-Cloud, limits privileged access to the O-Cloud control panel and governs ongoing access to privileged resources in the O-Cloud.
IAM should be based on Authentication, Authorization, and Accounting (AAA) systems and least privileges approach to limit actions administrators, perform and provide a history of user actions to detect unauthorized use and abuse.
Protocols without encryption/authentication mechanisms should not be used. Access to administrative and management interfaces from untrusted network sources should be limited.
Best practices to fulfill this recommendation:
This recommendation can help to mitigate: T-GEN-02, T-GEN-04, T-VM-C-01, T-VM-C-02, T-VM-C-03, T-VM-C-04, T-VM-C-05, T-IMG-01, T-IMG-02, T-IMG-04, T-VL-01, T-O2-01, T-OCAPI-01, T-HW-01, T-HW-02, T-ADMIN-01, T-ADMIN-02
REC-VHPM Vulnerability Handling and Patch Management
Recommendation: A vulnerability handling process should be in place to find potentially exploitable software vulnerabilities, to determine what types and levels of threat may use software exploits and 0-days against O-Cloud and to remediate them.
A patch management process should be implemented to check unused dependencies, unmaintained and/or previously vulnerable dependencies, unnecessary features, components, files, and documentation.
Best practices to fulfill this recommendation:
This recommendation can help to mitigate: T-GEN-01, T-IMG-01, T-VL-01, T-ADMIN-02, T-O2-01, T-OCAPI-01
REC-SCONF Security Configuration
Recommendation: The O-Cloud virtualization layer shall
Be configured to ensure conformance to industry standard benchmarks and requirements with respect to security configurations.
Use automated or manual mechanisms to detect configuration drifts/misconfigurations from industry standard benchmarks and requirements.
Best practices to fulfill this recommendation:
This recommendation can help to mitigate: T-VM-C-01, T-VM-C-02, T-VM-C-06, T-VL-01, T-ADMIN-01, T-ADMIN-02
REC-SDLC Secure Development Lifecycle
Recommendation: Software providers should assume that their VNFs/CNFs and VL software contain flaws and have appropriate processes in place to mitigate these cases. To this end, they should adopt a secure software development life-cycle (S-SDLC). A S-SDLC integrates security considerations into the normal software development life-cycle. This ensures that risks, threats, and security mechanisms are formalized alongside the development of VNFs/CNFs and VL.
Best practices to fulfill this recommendation:
This recommendation can help to mitigate: T-GEN-01, T-IMG-01, T-IMG-03
REC-SNFLC Security App/VNF/CNF lifecycle
Recommendation: Security into the life cycle management of Apps/VNFs/CNFs should be integrated during development, onboarding, instantiation, scaling, migration, and termination. Security lifecycle management must be adapted to work in a more dynamic environment with fast-changing network topology, data flow paths, and network addresses.
Best practices to fulfill this recommendation:
This recommendation can help to mitigate: T-GEN-03, T-VM-C-01, T-VM-C-02, T-VM-C-04, T-VM-C-06
REC-IMGP Image Protection
Recommendation: Only images from secure and trusted sources should be used. These images should be scanned to ensure that images not contain vulnerable software.
The App/VNF/CNF images shall not be packaged with embedded secrets such as passwords or credentials, or any other critical configuration data.
During the onboarding, the authenticity and integrity of the VNF Package should be verified against the signature provided by the VNF provider. Furthermore, Operators should undertake additional security validation of the VNF Package during the onboarding process and operator's signing should be used to certify the VNF as authorized to onboard into the operator's network [46], [47].
Best practices to fulfill this recommendation:
Image Scanning
Scanning process needs to identify when Apps/VNFs/CNFs are running with out-of-date packages that need to be updated for security patches. Further, it should identify malware that has been built into an image. The O-Cloud VL should enforce image scans to check and flag the presence of secrets in images.
VM/Container images need to be frequently scanned throughout the lifecycle of the App/VNF/CNF
As part of O-RAN App/VNF/CNF development, O-RAN Solution Provider/Vendor is recommended to choose base images and packages used in the O-RAN App/VNF/CNF from a trusted publisher or create in-house base images. Choosing trusted signed images helps to mitigate MITM attacks that can introduce vulnerabilities into the base images available in public registries.
As part of O-RAN App/VNF/CNF development, O-RAN Solution Provider/Vendor is recommended to choose minimal base images that bundle only the necessary system tools and libraries required by O-RAN App/VNF/CNF, to limit the attack surface of O-RAN App/VNF/CNF.
App/VNF/CNF Image Signing
Responsible: App/VNF/CNF Solution Provider/Vendor and/or Operator
App/VNF/CNF Solution Provider creates public/private key pair for code signing.
CA verifies public key belong to the owner and issue CA certificate with attached public key.
Hash function on App/VNF/CNF Image returns image Digest.
Image Digest is encrypted with private key.
Signed App/VNF/CNF Image contains CA Certificate, Digest, Hash function.
App/VNF/CNF Image Verification
Responsible: Operator or O-Cloud Operator
Verify CA certificate for authenticity.
Decrypt digest with public key
Compare Decrypted digest with Digest computed as result of Hash function on App/VNF/CNF Image.
Verification is successful if both digest matches.
Proposed Signing/Verification Models
Software signing by Solution Provider/Vendor, Software verification at runtime by O-Cloud Operator
Software signing by Solution Provider/Vendor, Software verification by operator before onboarding vendor images to the operator registry
Software signing by Operator, Software verification at runtime by O-Cloud Operator
Software signing by Solution Provider/Vendor, Software verification at runtime by O-Cloud Operator and/or Operator.
Software signing by solution provider/vendor, Software verification by Operator and software re-signing by Operator before onboarding vendor images to the operator registry, software verification at instantiation.
Other best practices:
Least privilege access that limits and controls access to the images’ repository and the repository of source code, secure storage, and automation for verifying the configuration of images before loading.
Utilize a trust model such as Docker Content Trust with digital signatures to ensure runtime verification of the integrity and publisher of specific image tags.
Implement only digitally signed host images to validate the integrity of the software used on the O-Cloud platform. Make use of this feature where possible in order to prevent and/or detect attempts by adversaries to compromise the host image.
The registry is most sensitive part of the system and should be run in an HSM.
The App/VNF/CNF images shall not be packaged with embedded secrets such as passwords or credentials, or any other critical configuration data. Secrets have to be stored outside the App/VNF/CNF images. For example, K8s secrets or external secrets managers (like Vault) can be used to store secret and sensitive information hence preventing the threat of packaging secrets in CNF container images. Secrets required by containerized applications can be injected into the CNF container from the External Secrets Manager as required.
Encrypting VNF volume / swap areas: Virtual volume disks associated with VNFs may contain sensitive data. Therefore, they need to be protected. The best practice to secure the VNF volume is by encrypting them and storing the cryptographic keys at safe locations. The TPM module can also be used to securely store these keys. In addition, the hypervisor should be configured to securely wipe out the virtual volume disks in the event a VNF is crashed or intentionally destroyed to prevent it from unauthorized access. VM swapping is a memory management technique used to move memory segments from the main memory to disk, which is used as a secondary memory in order to increase system performance in case the system runs out of memory. These transferred memory segments can contain sensitive information such as passwords and certificates. They can be stored on the disk and remain persistent even after system reboot. This enables an attack scenario whereby a VM swap is copied and investigated to retrieve any useful information. One way to avoid this kind of attack is to encrypt VM swap areas. Linux based tools such as dm-crypt can be used for this purpose.
This recommendation can help to mitigate: T-GEN-01, T-GEN-03, T-IMG-01, T-IMG-02, T-IMG-03, T-IMG-04
REC-LOG Logging, Monitoring and Alerting
Recommendation: The continuous security of O-Cloud software (running VNFs/CNFs, Hypervisor/Container Engine, Host OS, NFO/FOCOM) depends on the ability to identify attacks and reconstruct them. The earlier operators identify suspicious behavior and possible attacks, the faster they can initiate countermeasures. In the best case, they are able to stop attacks severe before harm is done. In the worst case, they at least are able to reduce the attacks severity and collect important details on the attack itself. This knowledge is important to find and fix flaws and vulnerabilities to prevent similar attacks in the future. Therefore, a key activity is to perform logging, monitoring and alerting.
The importance increases because of the dynamic VM/Container deployment. VMs/Containers may be started when demand arises and then quickly be terminated once they are no longer required. Furthermore, the focused nature of VMs/Containers and the application of a microservice architecture means that there are more systems to monitor. As a result, monitoring and logging must handle this dynamic landscape and be able to collect data from many systems at once.
Best practices to fulfill this recommendation:
This recommendation can help to mitigate: T-GEN-01, T-GEN-02, T-GEN-03, T-VM-C-02, T-VM-C-03, T-VM-C-04, T-VM-C-05, T-VM-C-06, T-IMG-01, T-IMG-02, T-IMG-04, T-VL-02
REC-SB Secure Boot
Recommendation: All servers part of O-Cloud Infrastructure should support a root of trust and secure boot to trust that the running host OS, Hypervisor/Container Engine and VNFs/CNFs code were loaded.
Best practices to fulfill this recommendation:
For more details see ETSI GR NFV-SEC 007 [15].
This recommendation can help to mitigate: T-GEN-03, T-IMG-04, T-VL-01, T-VL-02
REC-ISO Strong Isolation
Recommendation: Strengthen VNFs/CNFs sandboxing, isolation and segmentation should be enforced to make difficult for adversaries to advance their operation through exploitation of undiscovered or unpatched vulnerabilities. O-Cloud virtualization layer shall adopt measures to ensure strong VM/Container isolation among VM/Container workloads to limit the impact of rogue/misbehaving VM/Container on other co-hosted VMs/Containers.
Best practices to fulfill this recommendation:
This recommendation can help to mitigate: T-GEN-01, T-VM-C-01, T-VM-C-02, T-VM-C-05, T-VL-01, T-HW-01
REC-AUD Security Audit
Recommendation: Periodic audit and scan should be conducted of:
The integrity of images and VM/Containers used in O-Cloud deployments to ensure they have not been modified to include malicious software.
Accounts and privileges for images repositories.
All accounts, access lists and the privileges they have been granted to access O-Cloud components and images repositories.
Best practices to fulfill this recommendation:
This recommendation can help to mitigate: T-GEN-01, T-GEN-02, T-VM-C-05, T-IMG-01, T-IMG-03, T-IMG-04, T-ADMIN-01, T-ADMIN-02
REC-SS Secure Storage
Recommendation: The O-Cloud infrastructure should support encrypted storage, for example, block, object, file storage, credentials, and secrets with access to encryption keys restricted based on a need to know.
Best practices to fulfill this recommendation:
This recommendation can help to mitigate: T-VM-C-01, T-VM-C-02, T-VM-C-03, T-VM-C-06, T-IMG-03, T-IMG-03, T-VL-01, T-OCAPI-01, T-HW-01
REC-PHY Physical Security Protection
Recommendation: Physical security protection measures should be considered to deny unauthorized access to O-Cloud facilities, equipment and resources.
Best practices to fulfill this recommendation:
This recommendation can help to mitigate: T-VM-C-03, T-VM-C-05, T-VL-02, T-OCAPI-01, T-HW-01, T-HW-02
REC-RA Remote Attestation
Recommendation: A remote attestation (RA) approach should be used to determine the trustworthiness of O-Cloud components. It is a defensive measure that address the malicious software execution. In O-Cloud it is obvious the need for applying procedures to verify the integrity of the whole O-Cloud service deployment by the appropriate attestation of the O-Cloud architectural elements, including software and firmware images and associated supporting security sub-systems that will run to instantiate individual VNFs/CNFs and their composition into a O-Cloud service.
The remote attestation requires identifying the root(s) of trust, establishing a chain of trust for the O-Cloud infrastructure, the VL, VNFs/CNFs, and the NFO/FOCOM sub-systems, and verification of the trust chain, so the NFO/FOCOM components can verifiably establish a sufficient level of assurance in the different software elements constituting the VNFs/CNFs and the service(s) that use them.
The remote attestation (RA) capabilities should be used to realize trust establishment in the following use case scenarios:
Measurement of VM/Container during launch.
Protected VM/Container launch on a trusted O-Cloud platform
Measurement of VM/Container during launch and while in use
Remote attestation of secret storage
Secure VM/Container migration between two trusted O-Cloud platform
Best practices to fulfill this recommendation:
Example: Operators might use RA to assess if the overall O-Cloud infrastructure is trustworthy, datacenters might use RA to assess trustworthiness of subsystems they use, and management entities might use RA to assess the trustworthiness of individual infrastructural components. Hence, there are numerous use-cases and scenarios that might be considered where attestation is a fundamental step of creating an overall trustworthy system.
A trustworthy element is the entity which has a component that provides a unique identifier, certification (e.g. through cryptographic signing) and which is able to store measurements and data about the state of that element (including related sub-elements or dependent elements if necessary) in a tamperproof and verifiable form. For example, the TPM2.0 quoting mechanism using the TPMS_ATTEST data structure is an example of this.
This recommendation can help to mitigate: T-GEN-03, T-VM-C-04, T-IMG-04, T-VL-02
REC-SDD Secure data deletion
Recommendation: The O-Cloud platform should delete cryptographic keys and sensitive data using a secure deletion method from both active and backup storage systems.
Best practices to fulfill this recommendation:
This recommendation can help to mitigate: T-GEN-05, T-VM-C-03
Risk Assessment
The risk assessment of the identified threats against O-Cloud is provided in the O-RAN Security Threat Modeling and Risk Assessment [50].
Annex A (informative):  Best practices from some of existing main security guidance
Operators, O-Cloud vendors and Cloud providers should ensure that O-Cloud components and interfaces are secure under very strict set of security best practices. Such security guidance include:
A.1 	CISA/NSA Kubernetes security hardening best practices
A.2 	CIS Docker security best practices
A.3 	ONAP VNFs security best practices
CISA/NSA Kubernetes security hardening best practices
This section summarizes recommendations and best practices from the CISA/NSA Kubernetes Hardening Guidance. The report details recommendations to harden Kubernetes systems. Primary actions include the scanning of containers and Pods for vulnerabilities or misconfigurations, running containers and Pods with the least privileges possible, and using network separation, firewalls, strong authentication, and log auditing. To ensure the security of applications, system administrators should keep up to date with patches, updates, and upgrades to minimize risk. NSA and CISA also recommend periodic reviews of Kubernetes settings and vulnerability scans to ensure appropriate risks are accounted for and security patches are applied.
Download the full Kubernetes Hardening Guidance: https://media.defense.gov/2021/Aug/03/2002820425/-1/-1/1/CTR_KUBERNETES%20HARDENING%20GUIDANCE.PDF
Kubernetes Pod security
Pods are the smallest deployable Kubernetes unit and consist of one or more containers. Pods are often a cyber actor’s initial execution environment upon exploiting a container. For this reason, Pods should be hardened to make exploitation more difficult and to limit the impact of a successful compromise.
“Non-root” containers: Preventing root execution by using non-root containers limits the impact of a container compromise. container engines allow containers to run applications as a non-root user with non-root group membership. Typically, this non-default setting is configured when the container image is built. Having non-root execution integrated at build time provides better assurance that applications will function correctly without root privileges. (See Appendix A in [3]).
Immutable container file systems: By default, containers are permitted mostly unrestricted execution within their own context. A cyber actor who has gained execution in a container can create files, download scripts, and modify the application within the container. Kubernetes can lock down a container’s file system, thereby preventing many post-exploitation activities. However, these limitations also affect legitimate container applications and can potentially result in crashes or anomalous behavior. To prevent damaging legitimate applications, Kubernetes administrators can mount secondary read/write file systems for specific directories where applications require write access. (See Appendix B in [3]).
Building secure container images: Container images are usually created by either building a container from scratch or by building on top of an existing image pulled from a repository. In addition to using trusted repositories to build containers, image scanning is key to ensuring deployed containers are secure. Throughout the container build workflow, images should be scanned to identify outdated libraries, known vulnerabilities, or misconfigurations, such as insecure ports or permissions. One approach to implementing image scanning is by using an admission controller [3]. This admission controller could block deployments if the image doesn’t comply with the organization’s security policies defined in the webhook configuration.
Hardening container engines: Some platforms and container engines provide additional options to harden the containerized environments. A powerful example is the use of hypervisors to provide container isolation. Hypervisors rely on hardware to enforce the virtualization boundary rather than the operating system. Hypervisor isolation is more secure than traditional container isolation. Container engines running on the Windows operating system can be configured to use the built-in Windows hypervisor, Hyper-V, to enhance security. Additionally, some security focused container engines natively deploy each container within a lightweight hypervisor for defense-in-depth. Hypervisor-backed containers mitigate container breakouts.
Network separation and hardening
Cluster networking is a central concept of Kubernetes. Communication between containers, Pods, services, and external services must be taken into consideration. Resource separation and encryption can be an effective way to limit a cyber actor’s movement and escalation within a cluster.
Namespaces: Kubernetes namespaces are one way to partition cluster resources among multiple individuals, teams, or applications within the same cluster (See [3] for more details).
Network policies: Network policies control traffic flow between Pods, namespaces, and external IP addresses. By default, no network policies are applied to Pods or namespaces, resulting in unrestricted ingress and egress traffic within the Pod network. Pods become isolated through a network policy that applies to the Pod or the Pod’s namespace. Once a Pod is selected in a network policy, it rejects any connections that are not specifically allowed by any applicable policy object. (See Appendix E in [3] for more details).
Resource policies: In addition to network policies, LimitRange and ResourceQuota are two policies that can limit resource usage for namespaces or nodes. A LimitRange policy constrains individual resources per Pod or container within a particular namespace, e.g., by enforcing maximum compute and storage resources. ResourceQuotas are restrictions placed on the aggregate resource usage for an entire namespace, such as limits placed on total CPU and memory usage. If a user tries to create a Pod that violates a LimitRange or ResourceQuota policy, the Pod creation fails. (See Appendixes F and G in [3] for more details).
Control plane hardening: The control plane is the core of Kubernetes and gives users the ability to view containers, schedule new Pods, read Secrets, and execute commands in the cluster. Because of these sensitive capabilities, the control plane should be highly protected. In addition to secure configurations such as TLS encryption, RBAC, and a strong authentication method, network separation can help prevent unauthorized users from accessing the control plane. The Kubernetes API server should be protected (e.g. by firewall, TLS encryption) and not be exposed to the Internet or an untrusted network.
Worker node segmentation: A worker node can be a virtual or physical machine, depending on the cluster’s implementation. Because nodes run the microservices and host the web applications for the cluster, they are often the target of exploits. If a node becomes compromised, an administrator should proactively limit the attack surface by separating the worker nodes from other network segments that do not need to communicate with the worker nodes or Kubernetes services. A firewall can be used to separate internal network segments from the external facing worker nodes or the entire Kubernetes service depending on the network. Examples of services that may need to be separated from the possible attack surface of the worker nodes are confidential databases or internal services that would not need to be internet accessible.
Encryption: Administrators should configure all traffic in the Kubernetes cluster—including between components, nodes, and the control plane—to use TLS 1.2 or 1.3 encryption. Encryption can be set up during installation or afterward using TLS bootstrapping, detailed in the Kubernetes documentation, to create and distribute certificates to nodes. For all methods, certificates must be distributed amongst nodes to communicate securely.
Secrets: Kubernetes Secrets maintain sensitive information, such as passwords, OAuth tokens, SSH and TLS keys. Secrets can be encrypted by configuring data-at-rest encryption on the API server or by using an external Key Management Service (KMS), which may be available through a cloud provider. (See Appendixes H and I in [3] for more details).
Protecting sensitive cloud infrastructure: Kubernetes is often deployed on virtual machines in a cloud environment. As such, administrators should carefully consider the attack surface of the virtual machines on which the Kubernetes worker nodes are running. In many cases, Pods running on these virtual machines have access to sensitive cloud metadata services on a non-routable address. These metadata services provide cyber actors with information about the cloud infrastructure and possibly even short-lived credentials for cloud resources. Cyber actors abuse these metadata services for privilege escalation. Kubernetes administrators should prevent Pods from accessing cloud metadata services by using network policies or through the cloud configuration policy. Because these services vary based on the cloud provider, administrators should follow vendor guidance to harden these access vectors.
Authentication and authorization
Authentication and authorization are the primary mechanisms to restrict access to cluster resources. Cyber actors can scan for well-known Kubernetes ports and access the cluster’s database or make API calls without being authenticated if the cluster is misconfigured. User authentication is not a built-in feature of Kubernetes. However, several methods exist for administrators to add authentication to a cluster.
Authentication: Kubernetes clusters have two types of users: service accounts and normal user accounts. Service accounts handle API requests on behalf of Pods. Authentication is typically managed automatically by Kubernetes through the ServiceAccount Admission Controller using bearer tokens. The bearer tokens are mounted into Pods at well-known locations and can be used from outside the cluster if the tokens are left unsecured. Because of this, access to Pod Secrets should be restricted to those with a need to view them using Kubernetes RBAC. For normal users and admin accounts, there is no automatic authentication method for users. Administrators must add an authentication method to the cluster to implement authentication and authorization mechanisms. The Kubernetes documentation lists several ways to implement user authentication including client certificates, bearer tokens, authentication plugins, and other authentication protocols. At least one user authentication method should be implemented. Administrators should not use weak methods such as static password files. Weak authentication methods could allow cyber actors to authenticate as legitimate users.
Role-based access control: RBAC is one method to control access to cluster resources based on the roles of individuals within an organization. RBAC is enabled by default in Kubernetes version 1.6 and newer. Privileges assigned to users, groups, and service accounts should follow the principle of least privilege, giving only required permissions to resources. Users or user groups can be limited to particular namespaces where required resources reside. By default, a service account is created for each namespace for Pods to access the Kubernetes API. RBAC policies can be used to specify allowed actions from the service accounts in each namespace. Access to the Kubernetes API is limited by creating an RBAC Role or ClusterRole with the appropriate API request verb and desired resource on which the action can be applied. Tools exist that can help audit RBAC policies by printing users, groups, and service accounts with their associated assigned Roles and ClusterRoles. (See Appendixes J and K in [3] for more details).
Log auditing
Logs capture activity in the cluster. Auditing logs is necessary, not only for ensuring that services are operating and configured as intended, but also for ensuring the security of the system. Systematic audit requirements mandate consistent and thorough checks of security settings to help identify compromises. Kubernetes is capable of capturing audit logs for cluster actions and monitoring basic CPU and memory usage information; however, it does not natively provide in-depth monitoring or alerting services.
Logging: System administrators running applications within Kubernetes should establish an effective logging, monitoring, and alerting system for their environment. Logging Kubernetes events alone is not enough to provide a full picture of the actions occurring on the system. Logging should also be performed at the host level, application level, and on the cloud if applicable. These logs can then be correlated with any external authentication and system logs as applicable to provide a full view of the actions taken throughout the environment for use by security auditors and incident responders. Within the Kubernetes environment, administrators should monitor/log the following:
API request history
Performance metrics
Deployments
Resource consumption
Operating system calls
Protocols, permission changes
Network traffic
Pod scaling
When a Pod is created or updated, administrators should capture detailed logs of the network communications, response times, requests, resource consumption, and any other relevant metrics to establish a baseline.
RBAC policy configurations should be audited periodically and whenever changes occur to the organization’s system administrators.
Audits of internal and external traffic logs should be conducted to ensure all intended security constraints on connections have been configured properly and are working as intended.
Logs can be streamed to an external logging service to ensure availability to security professionals outside of the cluster, identify abnormalities as close to real time as possible, and protect logs from being deleted if a compromise occurs. If using this method, logs should be encrypted during transit with TLS 1.2 or 1.3 to ensure cyber actors cannot access the logs in transit and gain valuable information about the environment. (See Appendixes L and M in [3] for more details).
SIEM platforms: Security Information and Event Management (SIEM) software collects logs from across an organization’s network. SIEM software brings together firewall logs, application logs, and more; parsing them out to provide a centralized platform from which analysts can monitor system security. SIEM tools have variations in their capabilities. Generally, these platforms provide log collection, threat detection, and alerting capabilities. Some include machine learning capabilities, which can better predict system behavior and help to reduce false alerts. Organizations using these platforms in their environment can integrate them with Kubernetes to better monitor and secure clusters. Open-source platforms for managing logs from a Kubernetes environment exist as an alternative to SIEM platforms.
Alerting: Kubernetes does not natively support alerting; however, several monitoring tools with alerting capabilities are compatible with Kubernetes. If Kubernetes administrators choose to configure an alerting tool to work within a Kubernetes environment, there are several metrics for which administrators should monitor and configure alerts. Examples of cases that could trigger alerts include but are not limited to:
Low disk space on any of the machines in the environment,
Available storage space on a logging volume running low,
External logging service going offline,
A Pod or application running with root permissions,
An anonymous account being used or gaining privileges,
Unusual system calls or failed API calls,
user/admin behavior that is abnormal (i.e. at unusual times or from an unusual location), and
Significant deviations from the standard operation metrics baseline.
Service meshes: Service meshes are platforms that streamline microservice communications within an application by allowing for the logic of these communications to be coded into the service mesh rather than within each microservice. Coding this communication logic into individual microservices is difficult to scale, difficult to debug as failures occur, and difficult to secure. Using a service mesh can simplify this for developers. The mesh can:
Redirect traffic when a service is down,
Gather performance metrics for optimizing communications,
Allow management of service-to-service communication encryption,
Collect logs for service-to-service communication,
Collect logs from each service, and
Help developers diagnose problems and failures of microservices or communication mechanisms.
Fault tolerance: Fault tolerance policies should be put in place to ensure logging service availability. One policy that can be put in place is to allow new logs to overwrite the oldest log files if absolutely necessary in the event of storage capacity being exceeded. If logs are being sent to an external service, a mechanism should be in place for logs to be stored locally if a communication loss or external service failure occurs. Once communication to the external service is restored, a policy should be in place for the locally stored logs to be pushed up to the external server.
Tools: Kubernetes does not include extensive auditing capabilities. However, the system is built to be extensible, allowing users the freedom to develop their own custom solution or to choose an existing add-on that suits their needs. One of the most common solutions is to add additional audit backend services, which can use the information logged by Kubernetes and perform additional functions for users, such as extended search parameters, data mapping features, and alerting functionality. Organizations that already use SIEM platforms can integrate Kubernetes with these existing capabilities. Open-source monitoring tools—such as the Cloud Native Computing Foundation’s Prometheus, Grafana Labs’ Grafana, and Elasticsearch’s Elastic Stack (ELK) —are available to conduct event monitoring, run threat analytics, manage alerting, and collect resource isolation parameters, historical usage, and network statistics on running containers. Scanning tools can be useful when auditing the access control and permission configurations by assisting in identifying risky permission configurations in RBAC. NSA and CISA encourage organizations utilizing Intrusion Detection Systems (IDSs) on their existing environment to consider integrating that service into their Kubernetes environment as well. This integration would allow an organization to monitor for—and potentially kill containers showing signs of—unusual behavior so the containers can be restarted from the initial clean image. Many cloud service providers also provide container monitoring services for those wanting more managed and scalable solutions.
Upgrading and application security practices
Security of applications running on Kubernetes orchestrated containers is an ongoing process, and it is vital to keep up with patches, updates, and upgrades. The specific software components vary depending on the individual configuration, but each piece of the overall system should be kept as secure as possible. This includes updating: Kubernetes, hypervisors, virtualization software, plugins, operating systems on which the environment is running, applications running on the servers, and any other software hosted in the Kubernetes environment.
The Center for Internet Security (CIS) publishes benchmarks for securing software. Administrators should adhere to the CIS benchmarks for Kubernetes and any other relevant system components. Administrators should check periodically to ensure their system's security is compliant with the current security experts’ consensus on best practices. Periodic vulnerability scans and penetration tests should be performed on the various system components to proactively look for insecure configurations and zero-day vulnerabilities. Any discoveries should be promptly remediated before potential cyber actors can discover and exploit them.
As updates are deployed, administrators should also keep up with removing any old components that are no longer needed from the environment. Using a managed Kubernetes service can help to automate upgrades and patches for Kubernetes, operating systems, and networking protocols. However, administrators must still patch and upgrade their containerized applications.
CIS Docker security best practices
CIS Benchmarks are universal security best practices developed by cybersecurity professionals and experts. Each CIS Benchmark provides guidelines for creating a secure system configuration. The following table summarizes recommendations from the CIS Docker Community Edition Benchmark, specifying how to set up a safe docker configuration.
Download the full CIS Docker Benchmark: https://www.cisecurity.org/benchmark/docker/
Host Configuration
Create a separate partition for containers
Harden the container host
Update your Docker software on a regular basis
Manage Docker daemon access authorization wisely
Configure your Docker files directories, and
Audit all Docker daemon activity.
Docker Daemon Configuration
Restrict network traffic between default bridge containers and access to new privileges from containers.
Enable user namespace support to provide additional, Docker client commands authorization, live restore, and default cgroup usage
Disable legacy registry operations and Userland Proxy
Avoid networking misconfiguration by allowing Docker to make changes to iptables, and avoid experimental features during production.
Configure TLS authentication for Docker daemon and centralized and remote logging.
Set the logging level to 'info', and set an appropriate default ulimit
Don’t use insecure registries and aufs storage drivers
Apply base device size for containers and a daemon-wide custom SECCOMP profile to limit calls.
Container Images and Build File
Create a user for the container
Ensure containers use only trusted images
Ensure unnecessary packages are not installed in the container
Include security patches during scans and rebuilding processes
Enable content trust for Docker
Add HEALTHCHECK instructions to the container image
Remove setuid and setgid permissions from the images
Use COPY is instead of ADD in Dockerfile
Install only verified packages
Don’t use update instructions in a single line or alone in the Dockerfile
Don’t store secrets in Dockerfiles
Container Runtime
Restrict containers from acquiring additional privileges and restrict Linux Kernel Capabilities.
Enable AppArmor Profile.
Avoid use of privileged containers during runtime, running ssh within containers, mapping privileged ports within containers.
Ensure sensitive host system directories aren’t mounted on containers, the container's root filesystem is mounted as read-only, the Docker socket is not mounted inside any containers.
Set appropriate CPU priority for the container, set 'on-failure' container restart policy to '5', and open only necessary ports on the container.
Apply per need SELinux security options, and overwrite the default ulimit at runtime.
Don’t share the host's network namespace and the host's process namespace, the host's IPC namespace, mount propagation mode, the host's UTS namespace, the host's user namespaces.
Limit memory usage for container and bind incoming container traffic to a specific host interface.
Don’t expose host devices directly to containers, don’t disable the default SECCOMP profile, don’t use docker exec commands with privileged and user option, and don’t use Docker's default bridge docker0.
Confirm cgroup usage and use PIDs cgroup limit, check container health at runtime, and always update docker commands with the latest version of the image.
Docker Security Operations
Avoid image sprawl and container sprawl.
Docker Swarm Configuration
Enable swarm mode only if needed
Create a minimum number of manager nodes in a swarm
Bind swarm services are bound to a specific host interface
Encrypt containers data exchange on different overlay network nodes
Manage secrets in a Swarm cluster with Docker's secret management commands
Run swarm manager in auto-lock mode
Rotate swarm manager auto-lock key periodically
Rotate node and CA certificates as needed
Separate management plane traffic from data plane traffic
ONAP VNFs security best practices
ONAP provides details on the VNF general security requirements on various security areas such as user access control, network security, ACLs, infrastructure security, and vulnerability management. These requirements cover topics associated with compliance, security patching, logging/accounting, authentication, encryption, role-based access control, least privilege access/authorization. This section summarizes requirements from ONAP, specifying how to integrate and operateVNFs within a robust security environment.
For more details see the full documentation: https://docs.onap.org/projects/onap-vnfrqts-requirements/en/latest/index.html
VNF General Security Requirements
This section provides details on the VNF general security requirements on various security areas such as user access control, network security, ACLs, infrastructure security, and vulnerability management. These requirements cover topics associated with compliance, security patching, logging/accounting, authentication, encryption, role-based access control, least privilege access/authorization. The following security requirements need to be met by the O-RAN NFs in a virtual environment:
The VNF must implement and enforce the principle of least privilege on all protected interfaces.
The VNF must provide a mechanism (e.g., access control list) to permit and/or restrict access to services on the VNF by source, destination, protocol, and/or port.
The VNF should provide a mechanism that enables the operators to perform automated system configuration auditing at configurable time intervals.
The VNF provider must follow GSMA vendor practices and SEI CERT Coding Standards when developing the VNF in order to minimize the risk of vulnerabilities. See GSMA NESAS Network Equipment Security Assurance Scheme – Development and Lifecycle Security Requirements Version 1.0 (https://www.gsma.com/ security/wp-content/uploads/2019/11/FS.16-NESAS-Development-and-Lifecycle-Security- Requirements-v1.0.pdf) and SEI CERT Coding Standards (https://wiki.sei.cmu.edu/ confluence/display/seccode/SEI+CERT+Coding+Standards).
The VNF must have all code (e.g., QCOW2) and configuration files (e.g., HEAT template, Ansible playbook, script) hardened, or with documented recommended configurations for hardening and interfaces that allow the Operator to harden the VNF. Actions taken to harden a system include disabling all unnecessary services, and changing default values such as default credentials and community strings.
The VNF should support the separation of (1) signaling and payload traffic (i.e., customer facing traffic), (2) operations, administration and management traffic, and (3) internal VNF traffic (i.e., east-west traffic such as storage access) using technologies such as VPN and VLAN.
The VNF Provider must have patches available for vulnerabilities in the VNF as soon as possible. Patching shall be controlled via change control process with vulnerabilities disclosed along with mitigation recommendations.
The VNF must support only encrypted access protocols, e.g., TLS 1.2/1.3.
The VNF must store authentication credentials used to authenticate to other systems encrypted except where there is a technical need to store the password unencrypted in which case it must be protected using other security techniques that include the use of file and directory permissions. Ideally, credentials should rely on a HW Root of Trust, such as a TPM or HSM.
VNFs that are subject to regulatory requirements must provide functionality that enables the Operator to comply with ETSI TC LI requirements, and, optionally, other relevant national equivalents.
The VNF must be able to authenticate and authorize all remote access.
The VNF must log any security event required by the VNF Requirements to Syslog using LOG_AUTHPRIV for any event that would contain sensitive information and LOG_AUTH for all other relevant events.
If SNMP is utilized, the VNF must support at least SNMPv3 with message authentication.
The VNF application processes should not run as root. If a VNF application process must run as root, the technical reason must be documented.
Login access (e.g., shell access) to the virtualization layer, whether interactive or as part of an automated process, must be through an encrypted protocol such as TLS 1.2/1.3.
The VNF must include a configuration (e.g. a template) that specifies the targeted parameters (e.g. a limited set of ports) over which the VNF will communicate; including internal, external and management communication.
Containerized components of VNFs should follow the recommendations for Container Base Images and Build File Configuration in the latest available version of the CIS Docker Community Edition Benchmarks to ensure that containerized VNFs are secure. All non-compliances with the benchmarks must be documented.
Containerized components of VNFs should execute in a Docker run-time environment that follows the Container Runtime Configuration in the latest available version of the CIS Docker Community Edition Benchmarks to ensure that containerized VNFs are secure. All non-compliances with the benchmarks must be documented.
VNF Identity and Access Management Requirements
The following security requirements for logging, identity, and access management need to be met by the O-RAN NFs in a virtual environment:
The VNF must, if not integrated with the Operator’s Identity and Access Management system, support the creation of multiple IDs so that individual accountability can be supported.
The VNF must, if not integrated with the operator’s IAM system, provide a mechanism for assigning roles and/or permissions to an identity.
The VNF must, if not integrated with the Operator’s Identity and Access Management system, support multifactor authentication on all protected interfaces exposed by the VNF for use by human users.
The VNF must support account names that contain at least A-Z, a-z, and 0–9-character sets and be at least 6 characters in length.
The VNF must, if not integrated with the Operator’s Identity and Access Management system, comply with “password complexity” policy and support configurable password expiration. When passwords are used, they shall be complex and shall at least meet the following password construction requirements: (1) be a minimum configurable number of characters in length, (2) include 3 of the 4 following types of characters: upper-case alphabetic, lower-case alphabetic, numeric, and special, (3) not be the same as the UserID with which they are associated or other common strings as specified by the environment, (4) not contain repeating or sequential characters or numbers, (5) not to use special characters that may have command functions, and (6) new passwords must not contain sequences of three or more characters from the previous password.
The VNF must allow the Operator to restrict access to protected resources based on the assigned permissions associated with an ID in order to support Least Privilege (no more privilege than required to perform job functions).
The VNF must set the default settings for user access to deny authorization, except for a super user type of account.
The VNF must not store authentication credentials to itself in clear text or any reversible form and must use salting.
The VNF must, if not integrated with the Operator’s Identity and Access Management system, support the ability to lock out the userID after a configurable number of consecutive unsuccessful authentication attempts using the same userID. The locking mechanism must be reversible by an administrator and should be reversible after a configurable time period.
The VNF must, if not integrated with the Operator’s identity and access management system, authenticate all access to protected resources.
The VNF must support LDAP in order to integrate with an external identity and access manage system. It MAY support other identity and access management protocols.
The VNF must not identify the reason for a failed authentication, only that the authentication failed.
The VNF must provide a means to explicitly logout, thus ending that session.
The VNF must provide explicit confirmation of a session termination such as a message, new page, or rerouting to a login page.
The VNF must, if not integrated with the Operator’s Identity and Access Management system, enforce a configurable “terminate idle sessions” policy by terminating the session after a configurable period of inactivity.
VNF API Security Requirements
This section covers API security requirements when these are used by the VNFs. Key security areas covered in API security are Access Control, Authentication, Passwords, PKI Authentication Alarming, Anomaly Detection, Lawful Intercept, Monitoring and Logging, Input Validation, Cryptography, Business continuity, Biometric Authentication, Identification, Confidentiality and Integrity, and Denial of Service.
The O-RAN NFs in a virtual environment needs to meet the following API security requirements:
The VNF should integrate with the Operator’s authentication and authorization services (e.g., IDAM).
The VNF must implement the following input validation control: Check the size (length) of all input. Do not permit an amount of input so great that it would cause the VNF to fail. Where the input may be a file, the VNF API must enforce a size limit.
The VNF must implement the following input validation controls: Do not permit input that contains content or characters inappropriate to the input expected by the design. Inappropriate input, such as SQL expressions, may cause the system to execute undesirable and unauthorized transactions against the database or allow other inappropriate access to the internal network (injection attacks).
The VNF must implement the following input validation control on APIs: Validate that any input file has a correct and valid Multipurpose Internet Mail Extensions (MIME) type. Input files should be tested for spoofed MIME types.
VNF Security Analytics Requirements
This section covers VNF security analytics requirements that are mostly applicable to security monitoring. The VNF Security Analytics cover the collection and analysis of data following key areas of security monitoring:
Anti-virus software
Logging
Data capture
Tasking
DPI
API based monitoring
Detection and notification
Resource exhaustion detection
Proactive and scalable monitoring
Mobility and guest VNF monitoring
Closed loop monitoring
Interfaces to management and orchestration
Malformed packet detections
Service chaining
Dynamic security control
Dynamic load balancing
Connection attempts to inactive ports (malicious port scanning)
The following requirements of security monitoring need to be met by the O-RAN NFs in a virtual environment.
The VNF must support Real-time detection and notification of security events.
The VNF must support API-based monitoring to take care of the scenarios where the control interfaces are not exposed or are optimized and proprietary in nature.
The VNF must support detection of malformed packets due to software misconfiguration or software vulnerability and generate an error to the syslog console facility.
The VNF must support proactive monitoring to detect and report the attacks on resources so that the VNFs and associated VMs can be isolated, such as detection techniques for resource exhaustion, namely OS resource attacks, CPU attacks, consumption of kernel memory, local storage attacks.
The VNF should operate with anti-virus software which produces alarms every time a virus is detected.
The VNF must protect all security audit logs (including API, OS and application-generated logs), security audit software, data, and associated documentation from modification, or unauthorized viewing, by standard OS access control mechanisms, by sending to a remote system, or by encryption.
The VNF must log successful and unsuccessful authentication attempts, e.g., authentication associated with a transaction, authentication to create a session, authentication to assume elevated privilege.
The VNF must log logoffs.
The VNF must log starting and stopping of security logging.
The VNF must log success and unsuccessful creation, removal, or change to the inherent privilege level of users.
The VNF must log connections to the network listeners of the resource.
he VNF must log the field “event type” in the security audit logs.
The VNF must log the field “date/time” in the security audit logs.
The VNF must log the field “protocol” in the security audit logs.
The VNF must log the field “service or program used for access” in the security audit logs.
The VNF must log the field “success/failure” in the security audit logs.
The VNF must log the field “Login ID” in the security audit logs.
The VNF must not include an authentication credential, e.g., password, in the security audit logs, even if encrypted.
The VNF must detect when its security audit log storage medium is approaching capacity (configurable) and issue an alarm.
The VNF must support the capability of online storage of security audit logs.
The VNF must activate security alarms automatically when a configurable number of consecutive unsuccessful login attempts is reached.
The VNF must activate security alarms automatically when it detects the successful modification of a critical system or application file.
The VNF must activate security alarms automatically when it detects an unsuccessful attempt to gain permissions or assume the identity of another user.
The VNF must include the field “date” in the Security alarms (where applicable and technically feasible).
The VNF must include the field “time” in the Security alarms (where applicable and technically feasible).
The VNF must include the field “service or program used for access” in the Security alarms (where applicable and technically feasible).
The VNF must include the field “success/failure” in the Security alarms (where applicable and technically feasible).
The VNF must include the field “Login ID” in the Security alarms (where applicable and technically feasible).
The VNF must restrict changing the criticality level of a system security alarm to users with administrative privileges.
The VNF must monitor API invocation patterns to detect anomalous access patterns that may represent fraudulent access or other types of attacks or integrate with tools that implement anomaly and abuse detection.
The VNF must generate security audit logs that can be sent to Security Analytics Tools for analysis.
The VNF must log successful and unsuccessful access to VNF resources, including data.
The VNF must support the storage of security audit logs for a configurable period of time.
The VNF must have security logging for VNFs and their OSs be active from initialization. Audit logging includes automatic routines to maintain activity records and cleanup programs to ensure the integrity of the audit/logging systems.
The VNF must be implemented so that it is not vulnerable to OWASP Top 10 web application security risks.
The VNF must protect against all denial-of-service attacks, both volumetric and non-volumetric, or integrate with external denial of service protection tools.
The VNF must be capable of automatically synchronizing the system clock daily with the Operator’s trusted time source, to assure accurate time reporting in log files. It is recommended that Coordinated Universal Time (UTC) be used where possible, so as to eliminate ambiguity owing to daylight savings time.
The VNF must log the Source IP address in the security audit logs.
The VNF must have the capability to securely transmit the security logs and security events to a remote system before they are purged from the system.
The VNF should provide the capability of maintaining the integrity of its static files using a cryptographic method.
The VNF must log automated remote activities performed with elevated privileges.
VNF Data Protection Requirements
This section covers VNF data protection requirements that are mostly applicable to security monitoring.
The VNF MUST provide the capability to restrict read and write access to data handled by the VNF.
The VNF MUST Provide the capability to encrypt data in transit on a physical or virtual network.
The VNF MUST provide the capability to encrypt data on non-volatile memory. Non-volative memory is storage that is capable of retaining data without electrical power, e.g. Complementary metal-oxide-semiconductor (CMOS) or hard drives.
The VNF SHOULD disable the paging of the data requiring encryption, if possible, where the encryption of non-transient data is required on a device for which the operating system performs paging to virtual memory. If not possible to disable the paging of the data requiring encryption, the virtual memory should be encrypted.
The VNF MUST use NIST and industry standard cryptographic algorithms and standard modes of operations when implementing cryptography.
The VNF MUST NOT use compromised encryption algorithms. For example, SHA, DSS, MD5, SHA-1 and Skipjack algorithms. Acceptable algorithms can be found in the NIST FIPS publications (https://csrc.nist.gov/publications/fips) and in the NIST Special Publications (https://csrc.nist.gov/publications/sp).
The VNF MUST use, whenever possible, standard implementations of security applications, protocols, and formats, e.g., S/MIME, TLS, SSH, IPSec, X.509 digital certificates for cryptographic implementations. These implementations must be purchased from reputable vendors or obtained from reputable open source communities and must not be developed in-house.
The VNF MUST provide the ability to migrate to newer versions of cryptographic algorithms and protocols with minimal impact.
The VNF MUST support digital certificates that comply with X.509 standards.
The VNF MUST NOT use keys generated or derived from predictable functions or values, e.g., values considered predictable include user identity information, time of day, stored/transmitted data.
The VNF MUST provide the capability of using X.509 certificates issued by an external Certificate Authority.
The VNF MUST be capable of protecting the confidentiality and integrity of data at rest and in transit from unauthorized access and modification.
VNF Cryptography Requirements
This section covers VNF cryptography requirements that are mostly applicable to encryption or protocol methods.
The VNF SHOULD support an automated certificate management protocol such as CMPv2, Simple Certificate Enrollment Protocol (SCEP) or Automated Certificate Management Environment (ACME).
The VNF SHOULD provide the capability to integrate with an external encryption service.
The VNF MUST use symmetric keys of at least 112 bits in length.
The VNF MUST use asymmetric keys of at least 2048 bits in length.
The VNF MUST provide the capability to configure encryption algorithms or devices so that they comply with the laws of the jurisdiction in which there are plans to use data encryption.
The VNF MUST provide the capability of allowing certificate renewal and revocation.
The VNF MUST provide the capability of testing the validity of a digital certificate by validating the CA signature on the certificate.
The VNF MUST provide the capability of testing the validity of a digital certificate by validating the date the certificate is being used is within the validity period for the certificate.
The VNF MUST provide the capability of testing the validity of a digital certificate by checking the Certificate Revocation List (CRL) for the certificates of that type to ensure that the certificate has not been revoked.
The VNF MUST provide the capability of testing the validity of a digital certificate by recognizing the identity represented by the certificate - the “distinguished name”.
The VNF MUST support HTTPS using TLS v1.2 or higher with strong cryptographic ciphers.
The VNF MUST support the use of X.509 certificates issued from any Certificate Authority (CA) that is compliant with RFC5280, e.g., a public CA such as DigiCert or Let’s Encrypt, or an RFC5280 compliant Operator CA.
NOTE: The VNF provider cannot require the use of self-signed certificates in an Operator’s run time environment.
Revision History
History