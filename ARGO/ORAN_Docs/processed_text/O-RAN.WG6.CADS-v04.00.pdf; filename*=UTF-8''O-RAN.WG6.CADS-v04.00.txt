O-RAN.WG6.CADS-v04.00                                                                                                                          Technical Report    O-RAN Working Group 6 (Cloudification and Orchestration) Cloud Architecture and Deployment Scenarios  for O-RAN Virtualized RAN
Copyright © 2022 by the O-RAN ALLIANCE e.V. The copying or incorporation into any other work of part or all of the material available in this document in any form without the prior written permission of O-RAN ALLIANCE e.V.  is prohibited, save that you may print or download extracts of the material of this document for your personal use, or copy the material of this document for the purpose of sending to individual third parties for their information provided that you acknowledge O-RAN ALLIANCE as the source of the material and that you inform the third party that these conditions apply to them and that they must comply with them.  O-RAN ALLIANCE e.V., Buschkauler Weg 27, 53347 Alfter, Germany Register of Associations, Bonn VR 11238, VAT ID DE321720189  1

                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 2 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
 2 Table of Contents 3 Table of Contents ............................................................................................................................................... 2 4 Table of Figures .................................................................................................................................................. 3 5 Table of Tables ................................................................................................................................................... 4 6 1 Scope ........................................................................................................................................................ 5 7 1.1 Context; Relationship to Other O-RAN Work ................................................................................................... 5 8 1.2 Objectives .......................................................................................................................................................... 5 9 2 References ................................................................................................................................................ 7 10 3 Definitions and Abbreviations ................................................................................................................. 8 11 3.1 Definitions ......................................................................................................................................................... 8 12 3.2 Abbreviations ..................................................................................................................................................... 9 13 4 Overall Architecture ............................................................................................................................... 11 14 4.1 O-RAN Functions Definitions ......................................................................................................................... 11 15 4.2 Degree of Openness ......................................................................................................................................... 12 16 4.3 Decoupling of Hardware and Software ............................................................................................................ 12 17
 The O-Cloud............................................................................................................................................... 13 18
 Key O-Cloud Concepts .............................................................................................................................. 14 19 5 Deployment Scenarios:  Common Considerations ................................................................................. 18 20 5.1 Mapping Logical Functionality to Physical Implementations ......................................................................... 18 21
 Technical Constraints that Affect Hardware Implementations................................................................... 18 22
 Service Requirements that Affect Implementation Design ........................................................................ 19 23
 Rationalization of Centralizing O-DU Functionality ................................................................................. 19 24 5.2 Performance Aspects ....................................................................................................................................... 22 25
 User Plane Delay ........................................................................................................................................ 22 26 5.3 Hardware Acceleration and Acceleration Abstraction Layer (AAL) ............................................................... 25 27
 Accelerator Deployment Model ................................................................................................................. 26 28
 Acceleration Abstraction Layer (AAL) Interface ....................................................................................... 26 29
 Accelerator Management and Orchestration Considerations ..................................................................... 27 30 5.4 Cloud Considerations ....................................................................................................................................... 27 31
 Networking requirements ........................................................................................................................... 27 32 5.4.1.1 Support for Multiple Networking Interfaces ................................................................................... 27 33 5.4.1.2 Support for High Performance N-S Data Plane .............................................................................. 27 34 5.4.1.3 Support for High-Performance E-W Data Plane ............................................................................. 28 35 5.4.1.4 Support for Service Function Chaining .......................................................................................... 29 36 5.4.1.5 Support for VLAN based networking ............................................................................................. 29 37 5.4.1.6 Support for O-Cloud Gateways to connect O-Cloud Sites with external Transport Networks ....... 29 38
 Assignment of Acceleration Resources ...................................................................................................... 30 39
 Real-time / General Performance Feature Requirements ........................................................................... 30 40 5.4.3.1 Host Linux OS ................................................................................................................................ 30 41 5.4.3.1.1 Support for Pre-emptive Scheduling .................................................................................. 30 42 5.4.3.2 Support for Node Feature Discovery .............................................................................................. 30 43 5.4.3.3 Support for CPU Affinity and Isolation .......................................................................................... 30 44 5.4.3.4 Support for Dynamic HugePages Allocation .................................................................................. 30 45 5.4.3.5 Support for Topology Manager ...................................................................................................... 31 46 5.4.3.6 Support for Scale In/Out ................................................................................................................. 31 47 5.4.3.7 Support for Device Plugin .............................................................................................................. 32 48 5.4.3.8 Support for Direct IRQ Assignment ............................................................................................... 32 49 5.4.3.9 Support for No Over Commit CPU ................................................................................................ 32 50 5.4.3.10 Support for Specifying CPU Model ................................................................................................ 32 51
 Storage Requirements ................................................................................................................................ 32 52
 Notification Subscription Framework ........................................................................................................ 33 53 5.4.5.1 O-Cloud Notification Subscription Requirements .......................................................................... 33 54 5.5 Sync Architecture ............................................................................................................................................ 34 55
                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 3 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
 Cloud Platform Time Synchronization Architecture .................................................................................. 34 56 5.5.1.1 Edge Cloud Site Level – LLS-C3 Synchronization Topology ........................................................ 35 57 5.5.1.1.1 LLS-C3 Synchronization Topology Edge Site Time Synchronization Architecture .......... 35 58 5.5.1.1.2 LLS-C3 Synchronization Topology Edge Site Requirements ............................................ 36 59 5.5.1.1.2.1 Software .................................................................................................................................... 36 60 5.5.1.1.2.2 Hardware ................................................................................................................................... 36 61 5.5.1.2 Edge Cloud Site Level – LLS-C1 Synchronization Topology ........................................................ 36 62 5.5.1.2.1 LLS-C1 Synchronization Topology Edge Site Time Synchronization Architecture .......... 36 63 5.5.1.2.2 LLS-C1 Synchronization Topology Edge Site Requirements ............................................ 38 64 5.5.1.2.2.1 Software .................................................................................................................................... 38 65 5.5.1.2.2.2 Hardware ................................................................................................................................... 38 66
 Loss of Synchronization Notification ......................................................................................................... 38 67 5.6 Operations and Maintenance Considerations ................................................................................................... 39 68 5.7 Transport Network Architecture ...................................................................................................................... 40 69
 Fronthaul Gateways ................................................................................................................................... 40 70 5.8 Overview of Deployment Scenarios ................................................................................................................ 40 71 6 Deployment Scenarios and Implementation Considerations .................................................................. 42 72 6.1 Scenario A ....................................................................................................................................................... 42 73
 Key Use Cases and Drivers ........................................................................................................................ 42 74 6.2 Scenario B ........................................................................................................................................................ 42 75
 Key Use Cases and Drivers ........................................................................................................................ 44 76 6.3 Scenario C ........................................................................................................................................................ 44 77
 Key Use Cases and Drivers ........................................................................................................................ 45 78
 Scenario C.1, and Use Case and Drivers .................................................................................................... 45 79
 Scenario C.2, and Use Case and Drivers .................................................................................................... 46 80 6.4 Scenario D ....................................................................................................................................................... 48 81 6.5 Scenario E ........................................................................................................................................................ 48 82
 Key Use Cases and Drivers ........................................................................................................................ 49 83
 Scenario E.1 vO-DU with O-RU ................................................................................................................ 49 84 6.6 Scenario F ........................................................................................................................................................ 49 85
 Key Use Cases and Drivers ........................................................................................................................ 50 86 6.7 Scenarios of Initial Interest .............................................................................................................................. 50 87 7 Appendix A (informative):  Extensions to Current Deployment Scenarios to Include NSA ................. 50 88 7.1 Scenario A ....................................................................................................................................................... 51 89 7.2 Scenario B ........................................................................................................................................................ 51 90 7.3 Scenario C ........................................................................................................................................................ 51 91 7.4 Scenario C.2 ..................................................................................................................................................... 51 92 7.5 Scenario D ....................................................................................................................................................... 52 93 Revision History ............................................................................................................................................... 52 94  95 Table of Figures  96 Figure 1:  Relationship of this Document to Scenario Documents and O-RAN Management Documents ........................ 5 97 Figure 2:  Major Components Related to the Orchestration and Cloudification Effort ...................................................... 6 98 Figure 3:  Different Clouds/ Sites ....................................................................................................................................... 7 99 Figure 4: High Level Architecture of O-RAN .................................................................................................................. 11 100 Figure 5:  Logical Architecture of O-RAN ....................................................................................................................... 12 101 Figure 6:  Decoupling, and Illustration of the O-Cloud Concept ...................................................................................... 13 102 Figure 7:  Relationship between RAN Functions and Demands on Cloud Infrastructure and Hardware ......................... 14 103 Figure 8: Key Components Involved in/with an O-Cloud ................................................................................................ 14 104 Figure 9: Example of O-Cloud Resources mapped to O-Cloud Nodes and O-Cloud Networks in an O-Cloud Node 105 Cluster ............................................................................................................................................................................... 16 106 Figure 10: Edge O-Cloud Site deployment example with an O-Cloud Site Network Fabric (Hub) ................................. 17 107 Figure 11: Edge O-Cloud Site deployment example without an O-Cloud Site Network Fabric (Single servers) ............ 18 108 Figure 12:  Simple Centralization of O-DU Resources .................................................................................................... 20 109 Figure 13:  Pooling of Centralized O-DU Resources........................................................................................................ 21 110 Figure 14:  Comparison of Merit of Centralization Options vs. Number of Cell Sites in a Pool ...................................... 21 111
                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 4 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
Figure 15:  Major User Plane Latency Components, by 5G Service Slice and Function Placement ................................ 23 112 Figure 16: Hardware Abstraction Considerations ............................................................................................................. 26 113 Figure 17: Accelerator APIs/Libraries in Container and Virtual Machine Implementations ............................................ 26 114 Figure 18:  Illustration of the Network Interfaces Attached to a Pod, as Provisioned by Multus CNI ............................. 27 115 Figure 19:  Illustration of the Userspace CNI Plugin ........................................................................................................ 28 116 Figure 20:  Example Illustration of Two NUMA Regions ............................................................................................... 31 117 Figure 21: O-Cloud Notification Framework Architecture .............................................................................................. 34 118 Figure 22: Edge Cloud Site Time Sync Architecture for LLS-C3 .................................................................................... 35 119 Figure 23: Hyperconverged Edge Cloud Time Sync Architecture for LLS-C3 ................................................................ 36 120 Figure 24: Edge Cloud Site Time Sync Architecture for LLS-C1 .................................................................................... 37 121 Figure 25: Hyperconverged Edge Cloud Time Sync Architecture for LLS-C1 ................................................................ 38 122 Figure 26: vO-DU Subscribes to PTP Notification .......................................................................................................... 39 123 Figure 27:  High-Level Comparison of Scenarios ............................................................................................................ 41 124 Figure 28:  Scenario A ...................................................................................................................................................... 42 125 Figure 29:  Scenario B – NR Stand-alone ......................................................................................................................... 43 126 Figure 30: Scenario B – MR-DC (inter-RAT NR and E-UTRA) regardless of the Core Network type (either EPC or 127 5GC) ................................................................................................................................................................................. 43 128 Figure 31:  Scenario C ...................................................................................................................................................... 44 129 Figure 32:  Treatment of Network Slices:  MEC for URLLC at Edge Cloud, Centralized Control, Single vO-DU ........ 45 130 Figure 33:  Scenario C.1 ................................................................................................................................................... 46 131 Figure 34:  Treatment of Network Slice: MEC for URLLC at Edge Cloud, Separate vO-DUs ....................................... 47 132 Figure 35:  Single O-RU Being Shared by More than One Operator ............................................................................... 47 133 Figure 36:  Scenario C.2 ................................................................................................................................................... 48 134 Figure 37:  Scenario D ...................................................................................................................................................... 48 135 Figure 38:  Scenario E ...................................................................................................................................................... 49 136 Figure 39: Scenario E.1 .................................................................................................................................................... 49 137 Figure 40:  Scenario F ....................................................................................................................................................... 50 138 Figure 41:  Scenario A, Including NSA ............................................................................................................................ 51 139 Figure 42:  Scenario C, Including NSA ............................................................................................................................ 51 140 Figure 43:  Scenario C.2, Including NSA ......................................................................................................................... 52 141 Figure 44:  Scenario D, Including NSA ............................................................................................................................ 52 142 Table of Tables 143 Table 1:  Service Delay Constraints and Major Delay Contributors ................................................................................. 23 144 Table 2:  Cardinality and Delay Performance for Scenario B........................................................................................... 44 145 Table 3:  Cardinality and Delay Performance for Scenario C........................................................................................... 45 146 Table 4:  Cardinality and Delay Performance for Scenario C.1........................................................................................ 46 147  148 149
                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 5 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
1 Scope  150 This Technical Report has been produced by the O-RAN Alliance. 151 The contents of the present document are subject to continuing work within O-RAN and may change following formal 152 O-RAN approval. Should O-RAN modify the contents of the present document, it will be re-released by O-RAN with an 153 identifying change of release date and an increase in version number as follows: 154 Version x.y.z 155 where: 156 x the first digit is incremented for all changes of substance, i.e. technical enhancements, corrections, updates, 157 etc. (the initial approved document will have x=01). 158 y the second digit is incremented when editorial only changes have been incorporated in the document. 159 z the third digit included only in working versions of the document indicating incremental changes during the 160 editing process. 161 1.1 Context; Relationship to Other O-RAN Work 162 This document introduces and examines different scenarios and use cases for O-RAN deployments of Network 163 Functionality into Cloud Platforms, O-RAN Cloudified NFs and O-RAN Physical NFs.  Deployment scenarios are 164 associated with meeting customer and service requirements, while considering technological constraints and the need to 165 create cost-effective solutions. It will also reference management considerations covered in more depth elsewhere. 166 The following O-RAN documents will be referenced (see Section 5.6): 167  OAM architecture specification [8] 168  OAM interface specification (O1) [9] 169  O-RAN Architecture Description [10] 170 The details of implementing each identified scenario will be covered in separate Scenario documents, shown in green in 171 Figure 1.   172  173 Figure 1:  Relationship of this Document to Scenario Documents and O-RAN Management Documents 174 This document also draws on some other work from other O-RAN working groups, as well as sources from other industry 175 bodies.   176 1.2 Objectives  177 The O-RAN Alliance seeks to improve RAN flexibility and deployment velocity, while at the same time reducing the 178 capital and operating costs through the adoption of cloud architectures. The structure of the Orchestration and 179 Cloudification work is shown graphically below.  This document focuses on the Cloudification deployment aspects as 180 indicated.  181 Scenario Reference Design…Cloud Architecture and Deployment ScenariosOAM ArchitectureOAM Interface Specification Management documentsScenario  Reference DesignScenario  Reference DesignO-RAN Architecture DescriptionArchitecture documents
                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 6 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
Editor’s note: O-RU cloudification and O-RU AAL are future study items.  182   183 Figure 2:  Major Components Related to the Orchestration and Cloudification Effort 184 A key principle is the decoupling of RAN hardware and software for all components including near-RT RIC, O-CU (O-185 CU-CP and O-CU-UP), O-DU, and O-RU, and the deployment of software components on commodity server 186 architectures supplemented with programmable accelerators where necessary.  187 Key characteristics of cloud architectures which we will reference in this document are:  188 a) Decoupling of hardware from software.  This aims to improve flexibility and choice for operators by decoupling 189 selection and deployment of hardware infrastructure from software selection, 190 b) Standardization of hardware specifications across software implementations, to simplify physical deployment 191 and maintenance.  This aims to promote the availability of a multitude of software implementation choices for a 192 given hardware configuration.   193 c) Sharing of hardware.  This aims to promote the availability of a multitude of hardware implementation choices 194 for a given software implementation. 195 d) Flexible instantiation and lifecycle management through orchestration automation.  This aims to reduce 196 deployment and ongoing maintenance costs by promoting simplification and automation throughout the 197 hardware and software lifecycle through common chassis specifications and standardized orchestration 198 interfaces.   199 This document will define various deployment scenarios that can be supported by the O-RAN specifications and are of 200 either current or relatively near-term interest.  Each scenario is identified by a specific grouping of functionality at 201 different key locations (Cell Site, Edge Cloud, and Regional Cloud, which will be defined shortly), and an identification 202 of whether functionality at a given location is provided by an O-RAN Physical NF based solution where software and 203 hardware are tightly integrated and sharing a single identity, or by a cloud architecture that meets the above requirements. 204 The scope of this work clearly includes supporting all 5G technologies, i.e. E-UTRA and NR with both EPC-based Non-205 Standalone (NSA) and 5GC architectures. This implies that cloud/orchestration aspects of NSA (E-UTRA) are also 206 supported. However, this version primarily addresses 5G SA deployments. 207 This technical report examines the constraints that drive a specific solution, and discuss the hierarchical properties of each 208 solution, including a rough scale of the size of each cloud and a sense of the number of sub clouds expected to be served 209 by a higher cloud.  Figure 3 shows as example of how multiple cell sites feed into a smaller number of Edge Clouds, and 210 how in turn multiple Edge Clouds feed into a Regional Cloud.  For a given scenario, the Logical Functions are distributed 211 in a certain way among each type of cloud, and the “cardinality” of the different functions will be discussed.   212 This has implications on the processing power needed in each type of cloud, as well as implications on the environmental 213 requirements.  This document will also discuss considerations of hardware chassis and components that are reasonable in 214 each scenario, and the implications of managing such a cloud.   215  216
Orchestration
S/WH/WCloud stack  ( Containers/VMs, OS, Cloud Mgmt. )O-CUO-DUO-RU
Centralized CU/DU(C-RAN)
CU/DU splitDistributed CU/DU (D-RAN)BlackboxBBUMultitude of deploymentmodels: CloudRAN, CU-DU split, dRAN on whitebox or DCAll RAN modulesFlexibleOrch.Inventory,Discovery, RegistrationPolicy,MetricsSupport 10,000sof distributedcloud sitesMultitude of siliconacceleratorsCommon LCMmechanismsacross VNF &PNFs
ASICCloudificationAALAALAAL
                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 7 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
 217 Figure 3:  Different Clouds/ Sites 218 Additional major areas for this document are listed below:   219  Mapping of logical functions to physical elements and locations, and implications of that mapping. 220  High-level assessment of critical performance requirements, and how that influences architecture. 221  Processor and accelerator options (e.g., x86, FPGA, GPU).  In order to determine whether a Network Function 222 is a candidate for openness, there needs to be the possibility to have multiple suppliers of software for given 223 hardware, and multiple sources of required chip/accelerators.   224  The Hardware Abstraction Layer, aka “Acceleration Abstraction Layer” needs to be addressed in light of 225 various hardware options that could be used. 226  Cloud infrastructure makeup.  This includes considerations such as: 227  Deployments are allowed to use VMs, Containers in VMs, or just Containers.  228  Multiple Operating Systems are expected to be supported; e.g., open source Ubuntu, CentOS Linux, or 229 Yocto Linux-based distributions, or selected proprietary OSs.   230  Management of a cloudified RAN introduces some new management considerations, because the mapping 231 between Network Functionality and cloud platforms can be done in multiple ways, depending on the scenario 232 that is chosen.  Thus, management of aspects that are related to platform aspects rather than RAN functional 233 aspects need to be designed with flexibility in mind from the start.  For example, logging of physical functions, 234 scale out actions, and survivability considerations are affected.   235  These management considerations are introduced in this document, but management documents will 236 address the solutions. 237  The transport layer will be discussed, but only to the extent that it affects the architecture and design of the 238 network.  For example, the chosen L1 technology may affect the performance of transport.  As another example, 239 the use of a Fronthaul Gateway will affect economics as well as the placement options of certain Network 240 Functions.  And of course, the existence of L2 switches in a cloud platform deployment will be required for 241 efficient use of server resources. 242 Additional areas could be considered in the future.   243 2 References 244 The following documents contain provisions which, through reference in this text, constitute provisions of this report. 245 [1] 3GPP TS 38.470, NG-RAN; F1 general aspects and principles. 246 [2] 3GPP TR 21.905, Vocabulary for 3GPP Specifications. 247 [3] eCPRI Interface Specification V1.2, Common Public Radio Interface:  eCPRI Interface Specification. 248

                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 8 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
[4] eCPRI Transport Network V1.2, Requirements Specification, Common Public Radio Interface:  249 Requirements for the eCPRI Transport Network. 250 [5] IEEE Std 802.1CM-2018,  Time-Sensitive Networking for Fronthaul.  251 [6] ITU-T Technical Report, GSTR-TN5G - Transport network support of IMT-2020/5G.  252 [7] O-RAN WG4, Control, User and Synchronization Plane Specification, Technical Specification.  See 253 https://www.o-ran.org/specifications. 254 [8] O-RAN WG1, Operations and Maintenance Architecture, Technical Specification.  See https://www.o-255 ran.org/specifications. 256 [9] O-RAN WG1, Operations and Maintenance Interface Specification, Technical Specification.  See 257 https://www.o-ran.org/specifications.  258 [10] O-RAN WG1, O-RAN Architecture Description, Technical Specification. See https://www.o-259 ran.org/specifications.  260 [11] 3GPP TS 28.622, Telecommunication management; Generic Network Resource Model (NRM) Integration 261 Reference Point (IRP); Information Service (IS). 262 [12] O-RAN WG6, Cloud Platform Reference Design for Deployment Scenario B, Technical Specification.  See 263 https://www.o-ran.org/specifications. 264 [13] O-RAN WG7 OMAC HAR 0-v01.00 O-RAN White Box Hardware Working Group Outdoor Macrocell 265 Hardware Architecture and Requirements (FR1) Specification. 266 [14] O-RAN WG1, Use Cases Detailed Specifications – v05.00, Technical Specification. See https://www.o-267 ran.org/specifications. 268 3 Definitions and Abbreviations 269 3.1 Definitions 270 For the purposes of the present document, the terms and definitions given in 3GPP TR 21.905 [2] and the following apply. 271 A term defined in the present document takes precedence over the definition of the same term, if any, in 3GPP 272 TR 21.905 [2].  273 Cell Site This refers to the location of Radio Units (RUs); e.g., placed on same structure as the Radio 274 Unit or at the base.  The Cell Site in general will support multiple sectors and hence multiple 275 O-RUs. 276 Cloud Infrastructure (CInf) This refers to a set of computation, storage and networking equipment with related software 277 that offers physical and/or virtual cloud resources and services into the O-Cloud as an under-278 cloud from the Cloud Infrastructure provider organization that could be operator internal or 279 external. The Cloud Infrastructure resources are not addressed by the present document and 280 not specified as part of O2ims and O2dms. 281 CInf Management This is a vendor or operator software operated by the Cloud Infrastructure provider 282 organization. It handles discovery, health and maintenance of the Cloud Infrastructure 283 included equipment and its offered physical and logical services that can be distributed over 284 multiple Cloud Sites. The Cloud Infrastructure Management are not addressed by the present 285 document and not specified as part of O2ims and O2dms. 286 Cloud Site This refers to a physical place that has Cloud Infrastructure resources that can be used for O-287 Clouds and potentially other non O-Cloud resources.  288 Edge Cloud This is a location that supports virtualized RAN functions for multiple Cell Sites, and provides 289 centralization of functions for those sites and associated economies of scale.  An Edge Cloud 290 might serve a large physical area or a relatively small one close to its cell sites, depending on 291 the Operator’s use case.  However, the sites served by the Edge Cloud must be near enough to 292 the O-RUs to meet the network latency requirements of the O-DU functions. 293
                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 9 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
F1 Interface  The open interface between O-CU and O-DU in this document is the same as that defined by 294 the CU and DU split in 3GPP TS 38.473.  It consists of an F1-u part and an F1-c part. 295 Managed Element  The definition of a Managed Element (ME) is given in 3GPP TS 28.622 [11] section 296 4.3.3.  The ME supports communication over management interface(s) to the manager for 297 purposes of control and monitoring.  298 Managed Function  The definition of a Managed Function (MF) is given in 3GPP TS 28.622 [11] section 4.3.4.  An 299 MF instance is managed using the management interface(s) exposed by its containing ME 300 instance. 301 Network Function The near-RT RIC, O-CU-CP, O-CU-UP, O-DU, and O-RU logical functions that can be 302 provided either by virtualized or non-virtualized methods.  303 Regional Cloud This is a location that supports virtualized RAN functions for many Cell Sites in multiple Edge 304 Clouds, and provides high centralization of functionality. The sites served by the Regional 305 Cloud must be near enough to the O-DUs to meet the network latency requirements of the O-306 CU and near-RT RIC.  307 O-Cloud This refers to a collection of O-Cloud Resource Pools at one or more location and the software 308 to manage Nodes and Deployments hosted on them.  An O-Cloud will include functionality to 309 support both Deployment-plane and Management services. The O-Cloud provides a single 310 logical reference point for all O-Cloud Resource Pools within the O-Cloud boundary. 311 O-RAN Physical NF  A RAN NF software deployed on tightly integrated hardware sharing a single Managed 312 Element identity. 313 O-RAN Cloudified NF  A RAN NF software deployed on an O-Cloud with its own Managed Element identity, i.e., 314 separate from the identity of the O-Cloud. 315 3.2 Abbreviations 316 For the purposes of this document, the abbreviations given in 3GPP TR 21.905 [2] and the following apply.  317 An abbreviation defined in the present document takes precedence over the definition of the same abbreviation, if any, in 318 3GPP TR 21.905 [2]. 319 3GPP Third Generation Partnership Project 320 5G Fifth-Generation Mobile Communications 321 AAL Acceleration Abstraction Layer 322 API Application Programming Interface 323 ASIC Application-Specific Integrated Circuit  324 BBU BaseBand Unit 325 BS Base Station 326 CI Cloud Infrastructure 327 CoMP   Co-Ordinated Multi-Point transmission/reception 328 CNF Cloud-Native Network Function  329 CNI Container Networking Interface 330 CPU Central Processing Unit 331 CR Cell Radius 332 CU Centralized Unit as defined by 3GPP 333 DFT Discrete Fourier Transform 334 DL Downlink 335 DPDK Data Plan Development Kit  336 DMS Deployment Management Services 337 DU Distributed Unit as defined by 3GPP 338 eMBB enhanced Mobile BroadBand 339 EPC Evolved Packet Core 340 E-UTRA Evolved UMTS Terrestrial Radio Access 341 FCAPS Fault Configuration Accounting Performance Security  342 FEC  Forward Error Correction 343 FFT Fast Fourier Transform 344 FH Fronthaul 345 FH GW Fronthaul Gateway 346 FPGA Field Programmable Gate Array 347 GNSS Global Navigation Satellite System 348
                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 10 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
GPP General Purpose Processor 349 GPS Global Positioning System 350 GPU Graphics Processing Unit  351 HARQ Hybrid Automatic Repeat ReQuest 352 HW Hardware 353 IEEE Institute of Electrical and Electronics Engineers 354 IM Information Modelling, or Information Model 355 IMS Infrastructure Management Services 356 IRQ Interrupt ReQuest  357 ISA Instruction Set Architecture 358 ISD Inter-Site Distance 359 ITU International Telecommunications Union 360 KPI Key Performance Indicator 361 LCM Life Cycle Management 362 LDPC  Low-Density Parity-Check 363 LLS Lower Layer Split   364 LTE Long Term Evolution 365 LVM Logic Volume Manager 366 MEC Mobile Edge Computing 367 mMTC massive Machine Type Communications 368 MNO Mobile Network Operator 369 NF Network Function 370 NFD Node Feature Discovery 371 NFVI Network Function Virtualization Infrastructure 372 NIC Network Interface Card 373 NMS Network Management System  374 NR  New Radio 375 NSA Non-Standalone 376 NTP Network Time Protocol 377 NUMA Non-Uniform Memory Access  378 NVMe Non-Volatile Memory Express 379 O-Cloud O-RAN Cloud Platform 380 OCP  Open Compute Project 381 O-CU O-RAN Central Unit  382 O-CU-CP O-CU Control Plane 383 O-CU-UP O-CU User Plane 384 O-DU O-RAN Distributed Unit (uses Lower-level Split) 385 O-RU O-RAN Radio Unit 386 OTII Open Telecom IT Infrastructure 387 OWD One-Way Delay 388 PCI Peripheral Component Interconnect 389 PNF Physical Network Function 390 PoE Power over Ethernet 391 PoP Point of Presence 392 PRTC Primary Reference Time Clock 393 PTP Precision Time Protocol 394 QoS  Quality of Service  395 RAN Radio Access Network 396 RAT Radio Access Technology 397 RIC RAN Intelligent Controller  398 RT Real Time 399 RTT Round Trip Time 400 RU Radio Unit  401 SA Standalone 402 SFC Service Function Chaining  403 SMO Service Management and Orchestration 404 SMP Symmetric MultiProcessing 405 SoC System on Chip 406 SR-IOV Single Root Input/ Output Virtualization 407 SW Software 408 TCO Total Cost of Ownership 409 TNE Transport Network Element 410
                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 11 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
TR Technical Report 411 TRP Transmission Reception Point 412 TS Technical Specification 413 TSC (T-TSC) Telecom Slave Clock 414 Tx Transmitter 415 UE User Equipment 416 UL Uplink 417 UMTS Universal Mobile Telecommunications System 418 UP User Plane 419 UPF User Plane Function 420 URLLC Ultra-Reliable Low-Latency Communications 421 vCPU virtual CPU 422 VIM Virtualized Infrastructure Manager 423 VM Virtual Machine  424 VNF Virtualized Network Function 425 vO-CU Virtualized O-RAN Central Unit  426 vO-CU-CP Virtualized O-CU Control Plane 427 vO-CU-UP Virtualized O-CU User Plane 428 vO-DU Virtualized O-RAN Distributed Unit 429 4  Overall Architecture  430 This section addresses the overall architecture in terms of the Network Functions and infrastructure (O-RAN Physical 431 NFs, servers, and clouds) that are in scope. Figure 4 provides a high-level view of the O-RAN architecture as depicted in 432 [10].  433  434  435 Figure 4: High Level Architecture of O-RAN 436 4.1 O-RAN Functions Definitions 437 This section reviews key O-RAN functions definitions in O-RAN.  438  The O-DU/ O-RU split is defined as using Option 7-2x.  See [7].  439  The O-CU/ O-DU split is defined as using the CU/ DU split F1 as defined in 3GPP TS 38.470 [1].    440 This document assumes these two splits.  441 Figure 5 shows the logical architecture of O-RAN (as depicted in [10]) with O-Cloud platform at the bottom, where any 442 given O-RAN function could be supported by O-Cloud, depending on the deployment scenario.  For example, the figure 443 here illustrates a case where the O-RU is implemented as an O-RAN Physical NF, and the other functions within the 444 dashed line are supported by O-Cloud.   445

                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 12 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
 446 Figure 5:  Logical Architecture of O-RAN 447 4.2 Degree of Openness 448 In theory, every architecture component could be open in every sense imaginable, but in practice it is likely that different 449 components will have varying degrees of openness due to economic and other implementation considerations.  Some 450 factors are significantly affected by the deployment scenario; for example, what might be viable in an indoor deployment 451 might not be viable in an outdoor deployment.   452 Increasing degrees of openness for an O-RAN Physical Network Function or O-RAN Cloudified Network Function(s) 453 are: 454 A. Interfaces among Network Functions are open; e.g., E2, F1, and Open Fronthaul are used. Therefore, Network 455 Functions in different O-RAN Physical NFs/clouds from different vendors can interconnect. 456 B. In addition to having open connections as described above, the chassis of servers in a cloud are open and can 457 accept blades/sleds from multiple vendors.  However, the blades/sleds have RAN software that is not decoupled 458 from the hardware. 459 C. In addition to having open connections and an open chassis, a specific blade/sled uses software that is decoupled 460 from the hardware.  In this scenario, the software could be from one supplier, the blade/sled could be from another, 461 and the chassis could be from another.   462 Categories A and B have O-RAN Physical NFs/clouds, while Category C is an open solution that we are calling an O-463 Cloud, and is subject to the cloudification discussion and requirements. 464 In this document, the degree of openness for each O-RAN Physical NF/cloud can vary by scenario. The question of which 465 Network Functions should be split vs. combined, and the degree of openness in each one, is addressed in the discussion 466 of scenarios.  467 4.3 Decoupling of Hardware and Software  468 Editor’s note: O-RU AAL is a future study item.  469 There are three layers that we must consider when we discuss decoupling of hardware and software:  470  The hardware layer, shown at the bottom in Figure 6.  (In the case of a VM deployment, this maps basically to 471 the ETSI NFVI hardware sub-layer.) 472  A middle layer that includes Cloud Stack functions as well as Acceleration Abstraction Layer functions.  (In the 473 case of a VM deployment, these map to the ETSI NFVI virtualization sub-layer + VIM.) 474
OFH CUS-Plane F1-cA1F1-uE1E2Service Management and Orchestration FrameworkNon-Real Time RIC Near-Real Time RAN Intelligent Controller (RIC)O-DUNG-cNG-uX2-cXn-uO-CloudO2E2E2E2X2-uXn-cO-CU-CPO-CU-UPO-eNBO1O-RUO1OFH M-PlaneOpen Fronthaul M-Plane
                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 13 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
 A top layer that supports the virtual RAN functions.  475 Each layer can come from a different supplier.  The first aspect of decoupling has to do with ensuring that a Cloud Stack 476 can work on multiple suppliers’ hardware; i.e., it does not require vendor-specific hardware.   477 The second aspect of decoupling has to do with ensuring that a Cloud Platform can support RAN virtualized functions 478 from multiple RAN software suppliers.  If this is possible, then we say that the Cloud Platform (which includes the 479 hardware that it runs on) is an O-RAN Cloud Platform, or “O-Cloud”.  See Figure 6 below.   480  481 Figure 6:  Decoupling, and Illustration of the O-Cloud Concept 482
 The O-Cloud  483 The general definition of the O-Cloud Cloud Platform includes the following characteristics: 484 1. The Cloud Platform is a set of hardware and software components that provide cloud computing capabilities to 485 execute RAN network functions. 486 2. The Cloud Platform hardware includes compute, networking and storage components, and may also include 487 various acceleration technologies required by the RAN network functions to meet their performance objectives. 488 3. The Cloud Platform software exposes open and well-defined APIs that enable the management of the entire life 489 cycle for network functions.  490 4. The Cloud Platform software is decoupled from the Cloud Platform hardware (i.e., it can typically be sourced 491 from different vendors). 492 The management aspects of the O-Cloud platform are discussed in 5.6. The scope of this document includes listing 493 specific requirements of the Cloud Platform to support execution of the various O-RAN Network Functions. 494 An example of a Cloud Platform is an OpenStack and/or a Kubernetes deployment on a set of COTS servers (including 495 FPGA and GPU cards), interconnected by a spine/leaf networking fabric. 496 There is an important interplay between specific virtualized RAN functions and the hardware that is needed to meet 497 performance requirements and to support the functionality economically.  Therefore, a hardware/ cloud platform 498 combination that can support, say, a vO-CU function might not be appropriate to adequately support a vO-DU function.  499 When RAN functions are combined in different ways in each specific deployment scenario, these aspects must be 500 considered. 501 Below is a high-level conceptual example of how different accelerators, along with their associated cloud capabilities, 502 can be required for different RAN functions.  Although we do not specify any particular hardware requirement or cloud 503 capability here, we can note some general themes.  For example, any RAN function that involves real-time movement of 504 user traffic will require the cloud platform to control for delay and jitter, which may in turn require features such as real-505 time OSs, avoidance of frequent interrupts, CPU pinning, etc.   506
Cloud stack  ( Containers/VMs, OS, Cloud Mgmt. )O-CUO-DUO-RU
ASICHardwareO-CloudAALAALAAL
                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 14 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
 507 Figure 7:  Relationship between RAN Functions and Demands on Cloud Infrastructure and Hardware 508 Please note that any cloud that has features required for a given function (e.g., for O-DU) can also support functions that 509 do not require such features.  For example, a cloud that can support O-DU can also support functions such as O-CU-CP.  510  511  512
 Key O-Cloud Concepts  513 Figure 8 illustrates key components of an O-Cloud and its management. 514  515  516
 517 Figure 8: Key Components Involved in/with an O-Cloud 518 Key terms in this figure are defined below: 519

                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 15 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
 An O-Cloud refers to a collection of O-Cloud Resources, Resource Pools and O-Cloud Services at one or more 520 O-Cloud Sites including the software to manage O-Cloud Resource provisioning, Nodes, Clusters and 521 Deployments hosted on them.  An O-Cloud will include functionality to support both Deployment-plane (aka. 522 user-plane) and Management services. The O-Cloud provides a single logical reference point for all O-Cloud 523 Resources, Resource Pools and Services within the O-Cloud boundary, i.e. for the distributed O-Cloud. 524  An O-Cloud Site refers to a set of O-Cloud Resources at a Cloud Site with a geographical location. The size of 525 the O-Cloud Site can be from a single to thousands of O-Cloud Resources. O-Cloud Resources are generally 526 interconnected through one or more O-Cloud Site Network Fabrics that are the demarcation for direct O-Cloud 527 Site internal L2 switching. Multiple O-Cloud Sites can be interconnected into a distributed O-Cloud which 528 generally would require bridging, routing or stitching on any other networking layer in between each O-Cloud 529 Site and its respective external transport network attachment point. Note: Very small O-Cloud Sites with just a 530 few O-Cloud Resources can be directly connected to external networks e.g. fronthaul and backhaul networks, 531 without an O-Cloud Site Network Fabric. 532  The O2 Interfaces are the interfaces associated with a collection of O-Cloud Services that are provided by the 533 O-Cloud platform to the SMO. The services are categorized into two logical groups: (i) Infrastructure 534 Management Services (IMS), which include the subset of O2 functions that are responsible for deploying and 535 managing cloud infrastructure. (ii) Deployment Management Services (DMS), which include the subset of O2 536 functions that are responsible for managing the lifecycle of virtualized/containerized deployments on the cloud 537 infrastructure. The O2 interfaces associated with the O-Cloud Infrastructure and Deployment Management 538 Services will be specified in the upcoming O2 specification. Any definitions of SMO functional elements needed 539 to consume these services shall be described in OAM architecture. Further details of these key concepts and how 540 they relate to each other can be found in the O-RAN O2 Interface General Aspects and Principles (GAnP). 541  O-Cloud IMS related concepts and views of the Cloud Infrastructure 542 o An O-Cloud Resource represent a unit of defined capabilities and characteristics within an O-Cloud 543 Cloud Site that can be provisioned and used for the O-Cloud Deployment Plane. There are some 544 different sorts of O-Cloud Resources e.g. Compute, HW-Accelerator, Storage, Gateway and Site 545 Network Fabric. Note: Exact classes of O-Cloud Resources are FFS and needs alignment to existing 546 other specifications e.g. GAnP and IMS Interface Specification. 547 o An O-Cloud Resource Pool is a collection of O-Cloud Resources with homogeneous capabilities and 548 characteristics as defined by the operator within an O-Cloud Site.  549 o The Unspecified O-Cloud Resource Pool is the collection of O-Cloud Resources that are exposed in 550 the O-Cloud IMS inventory without a classification or being placed in any O-Cloud Resource Pool. 551 Note: Exact classes of O-Cloud Resources are FFS. 552 o An O-Cloud Site Network Fabric is an O-Cloud Resource that connects the O-Cloud Resources that 553 can connect to other O-Cloud Resources in an O-Cloud Site. 554 o An O-Cloud Site Network is a provisioned Network Resource with its configured defined capabilities 555 and characteristics out of an O-Cloud Site Network Fabric. 556  O-Cloud DMS related concepts and views of the O-Cloud Resources that are created/updated through IMS 557 provisioning 558 o O-Cloud Deployment Plane is a logical construct representing the O-Cloud Nodes, O-Cloud Networks 559 and O-Cloud Node Clusters which are used to create O-Cloud NF Deployments. The O-Cloud 560 Deployment Plane is created using IMS provisioned O-Cloud Resources from O-Cloud Resource Pools 561 and O-Cloud Site Network Fabrics. 562 o An O-Cloud NF Deployment is a deployment of a cloud native Network Function (all or partial), 563 resources shared within a Network Function, or resources shared across network functions. The O-564 Cloud NF Deployment configures and assembles user-plane resources required for the cloud native 565 construct used to establish the O-Cloud NF Deployment and manage its life cycle from creation to 566 deletion.  567 o An O-Cloud Node is a network connected (physical and/or logical) computer or a network connection 568 terminating function. An O-Cloud Node can be provisioned by the IMS into the O-Cloud Node Cluster. 569 O-Cloud Nodes are typically comprised of physical and/or logical CPUs, Memories, Storages, NICs, 570 HW Accelerators, etc. and a loaded Operating System with relevant Cluster SW. The O-Cloud Node 571 software discovers, abstracts and exposes the IMS-assigned O-Cloud Resources or partitions of them 572 as O-Cloud Deployment Plane constructs.  Note that an O-Cloud Node could also exist as a stand-alone 573 O-Cloud Node. 574 o An O-Cloud Node Cluster is a collection of O-Cloud Nodes that work in concert with each other, 575 through a set of interconnecting O-Cloud Node Cluster Networks. The O-Cloud Nodes Operating 576 System and Cluster SW discover its capabilities, characteristics and initial parameters with additional 577 configuration done through the IMS. The cluster concepts will be further specified in the GAnP 578 document. 579 o An O-Cloud Node Cluster Network is an O-Cloud Site Network assigned to an O-Cloud Node Cluster. 580
                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 16 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
o An O-Cloud Node Group is a set of O-Cloud Nodes within an O-Cloud Node Cluster that are to be 581 treated as equal by some aspects e.g. the O-Cloud Node Cluster scheduler. These O-Cloud Nodes are 582 interconnected through the set of O-Cloud Node Cluster Networks and an optional set of O-Cloud Node 583 Group Networks. These O-Cloud Nodes would commonly have similar capabilities and characteristics 584 exposed from their set of used computational, storage and networking Resources. 585 o An O-Cloud Node Group Network is an O-Cloud Site Network assigned to an O-Cloud Node Group 586 in a O-Cloud Node Cluster. 587  588 Figure 9 illustrates an example of how O-Cloud Resources and parts of O-Cloud Resources are mapped into O-Cloud 589 Nodes and O-Cloud Node Clusters which is done through the O-Cloud IMS Provisioning services. The depicted O-Cloud 590 Node Groups and their related O-Cloud Node Group Networks are dashed to indicate that this grouping level is optional. 591   592
 593 Figure 9: Example of O-Cloud Resources mapped to O-Cloud Nodes and O-Cloud Networks in an O-Cloud 594 Node Cluster 595 An O-Cloud Resource Pool comprises one or more O-Cloud Resources, each with one or more network connections and 596 optionally, one or more internal HW accelerators and storage devices. The O-Cloud Site Network Fabric Resources may 597 provide connectivity between the pooled O-Cloud Resources of O-Cloud Compute, O-Cloud HW-Accelerator, O-Cloud 598 Storage and O-Cloud Gateway Resources. and to the O-RU through an O-RAN 7.2x compliant Fronthaul transport. The 599 O-Cloud Gateways may bridge or stitch O-Cloud Site Networks across multiple O-Cloud Resource Pools in different 600 Cloud Sites inside a distributed O-Cloud. The O-Cloud Site Network Fabrics are managed by the Infrastructure 601 Management Services (IMS) described earlier. Interconnection of the different O-Cloud Sites in a distributed O-Cloud is 602 typically done through an externally provisioned and managed WAN Transport, but could also be done through Cloud 603 Infrastructure internally managed WAN Transport.  Figure 10 shows an example of the architecture and usage of one or 604 more O-Cloud Compute Resource Pools, comprising multiple servers interconnected over an O-Cloud Site Network 605 Fabric.  606

                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 17 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
  607  608  609  610  611 Figure 10: Edge O-Cloud Site deployment example with an O-Cloud Site Network Fabric (Hub) 612 An O-Cloud Resource Pool may also comprise one or a set of single servers without any associated O-Cloud Site 613 Network Fabric, e.g., infrastructure deployed at a cell site. In such a scenario where an O-Cloud Site Network Fabric is 614 not present, the O-Cloud Compute Resources may be directly connected to the O-RU through an O-RAN compliant 615 front haul connection and to an externally provisioned backhaul or midhaul Transport. Figure 11 shows an example of 616 the architecture and usage of an O-Cloud Compute Resource Pool in such a configuration without the O-Cloud Site 617 Network Fabric. 618

                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 18 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
 619  620 Figure 11: Edge O-Cloud Site deployment example without an O-Cloud Site Network Fabric (Single servers) 621 The requirements on the O-Cloud Site Network Fabric such as clock/sync requirements, latency and jitter 622 recommendations shall be described in a future version of this document. We note that the architecture of a regional cloud 623 may be similar to that of an edge cloud but may not include some requirements such as time source.  624  625 5  Deployment Scenarios:  Common Considerations 626 In any implementation of logical network functionality, decisions need to be made regarding which logical functions are 627 mapped to which Cloud Platforms, and therefore which functions are to be co-located with other logical functions.  In 628 this document we do not prescribe one specific implementation, but we do understand that in order to establish agreements 629 and requirements, the manner in which the Network Functions are mapped to the same or different Cloud Platforms must 630 be considered.   631 We refer to each specific mapping as a “deployment scenario”.  In this section, we examine the deployment scenarios that 632 are receiving the most consideration.  Then we will select the one or ones that should be the focus of initial scenario 633 reference design efforts. 634 5.1 Mapping Logical Functionality to Physical Implementations 635 There are many aspects that need to be considered when deciding to implement logical functions in distinct O-Clouds.  636 Some aspects have to do with fundamental technical constraints and economic considerations, while others have to do 637 with the nature of the services that are being offered.   638
 Technical Constraints that Affect Hardware Implementations   639 Below are some factors that will affect the cost of implementations, and can drive a carrier to require separation of or 640 combining of different logical functions.   641  Environment:  Equipment may be deployed in indoor controlled environments (e.g., Central Offices), semi-642 controlled environments (e.g., cabinets with fans and heaters), and exposed environments (e.g., Radio Units on 643 a tower).  In general, the less controlled the environment, the more difficult and expensive the equipment will 644 be.  The required temperature range is a key design factor, and can drive higher power requirements.   645

                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 19 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
 Dimensions:  The physical dimensions can also drive deployment constraints – e.g., the need to fit into a tight 646 cabinet, or to be placed safely on a tower or pole.   647  Transport technology:  The transport technology used for Fronthaul, Midhaul, and Backhaul is often fiber, 648 which has an extremely low and acceptable loss rate.  However, there are options other than fiber, in particular 649 wireless/ microwave, where the potential for data loss must be considered.  This will be discussed further in the 650 next section. 651  Acceleration Hardware:  The need for acceleration hardware can be driven by the need to meet basic 652 performance requirements, but can also be tied to some of the above considerations.  For example, a hardware 653 acceleration chip (COTS or proprietary) can result in lower power use, less generated heat, and smaller physical 654 dimensions than if acceleration is not used.  On the other hand, some types of hardware acceleration chips might 655 not be “hardened” (i.e., they might only operate properly in a restricted environment), and could require a more 656 controlled environment such as in a central office. 657 The acceleration hardware most often referred to includes: 658  Field Programmable Gate Arrays (FPGAs) 659  Graphical Processing Units (GPUs) 660  System on Chip (SoC) 661  Standardized Hardware:  Use of standardized hardware designs and standardized form factors can have 662 advantages such as helping to reduce operations complexity, e.g., when an operator makes periodic technology 663 upgrades of selected components.  An example would be to use an Open Compute Project (OCP) or Open 664 Telecom IT Infrastructure (OTII) –based design.   665
 Service Requirements that Affect Implementation Design  666 RANs can serve a wide range of services and customer requirements, and each market can drive some unique 667 requirements.  Some examples are below. 668  Indoor or outdoor deployment:  Indoor deployments (e.g., in a public venue like a sports stadium, train station, 669 shopping mall, etc.) often enjoy a controlled environment for all elements, including the Radio Units.  This can 670 improve the economics of some indoor deployment scenarios.  The distance between Network Functions tends 671 to be much lower, and the devices that support O-RU functionality may be much easier and cheaper to install 672 and maintain. This can affect the density of certain deployments, and the frequency that certain scenarios are 673 deployed.   674  Bands supported, and Macro cell vs. Small cell:  The choice of bands (e.g., Sub-6 GHz vs. mmWave) might 675 be driven by whether the target customers are mobile vs. fixed, and whether a clear line of sight to the customer 676 is available or is needed. The bands to be supported will of course affect O-RU design.  In addition, because 677 mmWave carriers can support much higher channel width (e.g., 400 MHz vs. 20 MHz), mmWave deployments 678 can require a great deal more O-DU and O-CU processing power.  And of course the operations costs of 679 deploying Macro cells vs. Small cells differ in other ways.   680  Performance requirements of the Application / Network Slice:  Ultimately, user applications drive 681 performance requirements, and RANs are expected to support a very wide range of applications.  For example, 682 the delay requirements to support a Connected Car application using Ultra Reliable Low Latency 683 Communications (URLLC) will be more demanding than the delay requirements for other types of applications.  684 In our discussion of 5G, we can start by considering requirements separately for URLLC, enhanced Mobile 685 Broadband (eMBB), and massive Machine Type Communications (mMTC). 686 The consideration of performance requirements is a primary one, and is the subject of Section 5.2.  687
 Rationalization of Centralizing O-DU Functionality 688 Almost all Scenarios to be discussed in this document involve a degree of centralization of O-DU.  In this section it is 689 assumed that O-DU resources for a set of O-RUs are centralized at the same location.   690 Editor’s Note:  While most Scenarios also centralize O-CU-CP, O-CU-UP, and near-RT RIC in one form or another, 691 the benefits of centralizing them are not discussed in this section.  692 Managing O-DU in equipment at individual cell sites (via on-site BBUs today) has multiple challenges, including: 693
                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 20 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
 If changes are needed at a site (e.g., adding radio carriers), then adding equipment is a coarse-grained activity – 694 i.e., one cannot generally just add “another 1/5 of a box”, if that is all that is needed.  Adding the minimum 695 increment of additional capacity might result in poor utilization and thereby prevent expansion at that site.   696  Cell sites are in many separate locations, and each requires establishment and maintenance of an acceptable 697 environment for the equipment.  In turn this requires separate visits for any physical operations. 698  Micro sites tend to have much lower average utilization than macro sites, but each can experience considerable 699 peaks. 700  “Planned obsolescence” occurs, due to ongoing evolution of smartphone capabilities and throughput 701 improvements, as well as introduction of new features and services.  It is common practice today to upgrade 702 (“forklift replace”) BBUs every 36-60 months. 703 These factors motivate the centralization of resources where possible.  For the O-DU function, we can think of two types 704 of centralization: simple centralization and pooled centralization.   705 If the equipment uses O-DU centralization in an Edge Cloud, at any given hour an O-RU will be using a single specific 706 O-DU resource that is assigned to it (e.g. via Kubernetes).  On a broad time scale, traffic from any cell site can be rehomed, 707 without any physical work, to use other/additional resources that are available at that Edge Cloud location.  This would 708 likely be done infrequently; e.g., about as often as cell sites are expanded.   709 Centralization can have some additional benefits, such as only having to maintain a single large controlled environment 710 for many cell sites rather than creating and maintaining many distributed locations that might be less controlled (e.g., 711 outside cabinets or huts).  Capacity can be added at the central site and assigned to cell sites as needed.  Note that simple 712 centralization still assigns each O-RU to a single O-DU resource1, as shown below, and that traffic from one O-RU is not 713 split into subsets that could be assigned to different O-DUs.  Also note that a Fronthaul (FH) Gateway (GW) may exist 714 between the cell site and the centralized resources, not only to improve economics but also to enable traffic re-routing 715 when desired.  716  717 Figure 12:  Simple Centralization of O-DU Resources 718 By comparison, with pooled centralization, traffic from an O-RU (or subsets of the O-RU’s traffic) can be assigned more 719 dynamically to any of several shared O-DU resources.  So if one cell site is mostly idle and another experiences high 720 traffic demand, the traffic can be routed to the appropriate O-DU resources in the shared pool.  The total resources of this 721 shared pool can be smaller than resources of distributed locations, because the peak of the sum of the traffic will be 722 markedly lower than the sum of the individual cell site traffic peaks.   723  1 In this figure, each O-DU block can be thought of as a unit of server resources that includes a hardware accelerator, a GPP, memory and any other associated hardware.

                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 21 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
 724 Figure 13:  Pooling of Centralized O-DU Resources 725 We note that being able to share O-DU resources somewhat dynamically is expected to be a solvable problem, although 726 we understand that it is by no means a trivial problem.  There are management considerations, among others.  There may 727 be incremental steps toward true shared pooling, where rehoming of O-RUs to different O-DUs can be performed more 728 dynamically, based on traffic conditions. 729 It is noted that O-DU centralization benefits the most dense networks where several cell sites are within the O-RU to O-730 DU latency limits.  Sparsely populated areas most probably will be addressed by vO-CU centralization only.   731 Figure 14 shows the results of an analysis of a simulated greenfield deployment as an attempt to visualize the relative 732 merit of simple centralization of O-DU (“oDU”) vs. pooled centralization of O-DU (“poDU”) vs. legacy DU (“BBU”), 733 plotted against the realizable Cell Site pool size.  734  735 Figure 14:  Comparison of Merit of Centralization Options vs. Number of Cell Sites in a Pool 736 An often-used measure is related to the power required to support a given number of carrier MHz.  The lower the power 737 used per carrier, the more efficient is the implementation.  In Figure 14, the values of each curve are normalized to the 738 metric of Watts/MHz for distributed legacy BBUs, normalized to equal 1.  Please note that in this diagram, a lower value 739 is better.  The following assumptions apply to the figure:   740  A legacy BBU processes X MHz (for carriers) and consumes Y watts.  For example, a specific BBU might 741 process 1600 MHz and consume 160 watts.  742  N legacy BBUs will process N x X MHz and consume N x Y watts and have a merit figure of 1, per 743 normalization.  If a given site requires less than X MHz, it will still be necessary to deploy an X MHz BBU.  For 744 example, we may need only 480 MHz but still deploy a 1600 MHz BBU.  745

                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 22 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
 Simple Centralization (the “oDU” line):  In this case, active TRPs are statically mapped to specific VMs and 746 vO-DU tiles2.  Fewer vO-DU tiles are required to support the same number of TRPs, because MHz per site is 747 not a constant. 748  Independent of resources to support active user traffic, a fixed power level is required to power Ethernet 749 “frontplane” switches and hardware to support management and orchestration processes. 750  In a pool, processing capacity will be added over time as required. 751  Due to mobility traffic behavior, tiles will not be fully utilized, although centralization of resources will 752 improve utilization when compared with a legacy BBU approach.   753  Centralization with more dynamic pooling (the “poDU” line): In addition to active load balancing, individual 754 traffic flows (which can last from a few hundreds of msecs to several seconds) will be routed to the least used 755 tile, further optimizing (reducing) vO-DU tile requirements.   756  As in the simple centralization approach above, there is a fixed power level required for hardware that 757 supports switching, management and orchestration processes. 758 As a final note, any form of centralization requires efficient transport between the O-RU and the O-DU resources.  When 759 O-RU functionality is distributed over a relatively large area (e.g., not concentrated in a single large building), the 760 existence of a Fronthaul Gateway is a key enabler.   761 5.2 Performance Aspects 762 Performance requirements drive architectural and design considerations.  Performance can include attributes such as 763 delay, packet loss, transmission loss, and delay variation (aka “jitter”).   764 Editor’s Note:  While all aspects are of interest, delay has the largest impact on network design and will be the 765 focus of this version.  Future versions can address other performance aspects if desired and is FFS.   766
 User Plane Delay 767 This section discusses the framework for discussing delay of user-plane packets3, and also general delay numbers that it 768 can be agreed that apply across all scenarios.  Details relevant to a specific Scenario will be discussed in each Scenario’s 769 subsection, as applicable. The purpose of these high-level targets is to act as a baseline for allocating the total latency 770 budget to subsystems that are on the path of each constraint, as required for system engineering and dimensioning 771 calculations, and to assess the impact on the function placement within the specific network site tiers.   772 The goal is to establish reasonable maximum delay targets, as well as to identify and document the major infrastructure 773 as well as O-RAN NF-specific delay contributing components. For each service or element, minimum delay should be 774 considered to be zero. The implication of this is that any of the elements can be moved towards the Cell Site (e.g. in a 775 fully distributed Cloud RAN configuration, all of O-CU-UP, O-DU and O-RU would be distributed to Cell Site).  776 In real network deployments, the expectation is that, depending on the operator-specific implementation constraints such 777 as location and fiber availability, deployment area density, etc., deployments result in anything between the fully 778 distributed and maximally centralized configuration. Even on one operator’s network, it is common that there are many 779 different sizes of Edge Cloud instances, and combinations of Centralized and Distributed architectures in same network 780 are also common (e.g. network operator may choose to centralize the deployments on dense Metro areas to the extent 781 possible and distribute the configurations on suburban/rural areas with larger cell sizes / cell density that do not translate 782 to pooling benefits from more centralized architecture). However, the maximum centralization within the constraints of 783 latencies that can be tolerable is useful for establishing the basis for dimensioning of the maximum sizes, especially for 784 the Edge and Regional cloud PoPs. Figure 15 below illustrates the relationship among some key delay parameters.   785  2 A “vO-DU tile” refers to a chip or System on Chip (SoC) that provides hardware acceleration for math-intensive functionality such as that required for Digital Signal Processing.  With the Option 7.2x split, acceleration of Forward Error Correction (FEC) functionality is required (FEC is optional for e.g. low band.), and other functionality could be considered for acceleration if desired.  3 Delay of control plane or OAM traffic is not considered in this section.
                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 23 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
 786 Figure 15:  Major User Plane Latency Components, by 5G Service Slice and Function Placement 787 Please note the following: 788  NOTE 1: If the T2 or/and T3 transport network(s) is/are Packet Transport Network(s), then time allocation for 789 the transport network elements processing and queuing delays will require some portion of maximum latency 790 allocation, and will require reduction of the maximum area accordingly. 791  NOTE 2: Site Internal / fabric networks are not shown for clarity, but need some latency allocation (effectively 792 extensions or part of transport delays; per PoP tier designations TE1, TE2, TE3 and TC). 793  NOTE 3: To maximize the potential for resource pooling benefits, minimize network function redundancy cost, 794 and minimize the amount of hardware / power in progressively more distributed sites (towards UEs), target 795 design should attempt to maximize the distances and therefore latencies available for transport networks within 796 the service- and RAN-specific time constraints, especially for TT1. 797  NOTE 4: UPF, like EC/MEC, is outside of the scope of O-RAN, so UPF shown as a “black box” to illustrate 798 where it needs to be placed in context of specific services to be able to take advantage of the RAN service-799 specific latency improvements. 800 Figure 15 represents User Equipment locations on the right, and network tiers towards the left, with increasing latency 801 and increasing maximum area covered per tier towards the left. These Mobile Network Operator’s (MNO’s) Edge tiers 802 are nominated as Cell Site, Edge Cloud, and Regional Cloud, with one additional tier nominated as Core Cloud in the 803 figure. 804 The summary of the associated latency constraints as well as major latency contributing components as depicted in Figure 805 15 above is given in Table 1, below. 806 Table 1:  Service Delay Constraints and Major Delay Contributors 807 RAN Service-Specific User Plane Delay Constraints Identifier Brief Description Max. OWD (ms) Max. RTT (ms) URLLC Ultra-Reliable Low Latency Communications (3GPP) 0.5 1 URLLC Ultra-Reliable Low Latency Communications (ITU) 1 2

                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 24 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
eMBB enhanced Mobile Broadband 4 8 mMTC massive Machine Type Communications 15 30 Transport Specific Delay Components TAIR Transport propagation delay over air interface     TE1 Cell Site Switch/Router delay     TT1 Transport delay between Cell Site and Edge Cloud 0.1 0.2 TE2 Edge Cloud Site Fabric delay     TT2 Transport delay between Edge and Regional Cloud 1 2 TE3 Regional Cloud Site Fabric delay     TT3 Transport delay between Regional  and Core Cloud 10 20 TC Core Cloud Site Fabric delay     Network Function Specific Delay Components TUE Delay Through the UE SW and HW stack     TRU Delay Through the O-RU User Plane     TDU Delay Through the O-DU User Plane     TCU-UP Delay Through the O-CU User Plane      808 The transport network delays are specified as maximums, and link speeds are considered to be symmetric for all 809 components with exception of the air interface (TAIR).  For the S-Plane services utilizing PTP protocol, it is a requirement 810 that the link lengths, link speeds and forward-reverse path routing for PTP are all symmetric. 811 Radios (O-RUs) are always located in the Cell Site tier, while O-DU can be located “up to” Edge Cloud tier. It is possible 812 to move any of the user plane NF instances closer towards the cell site, as implicitly they would be inside the target 813 maximum delay, but it is not necessarily possible to move them further away from the Cell Sites while remaining within 814 the RAN internal and/or RAN service-specific timing constraints.  A common expected deployment case is one where O-815 DU instances are moved towards or even to the Cell Site and O-RUs (e.g. in Distributed Cloud-RAN configurations), or 816 in situations where the Edge Cloud needs to be located closer to the Cell Site due to fiber and/or location availability, or 817 other constraints. While this is expected to work well from the delay constraints perspective, the centralization and 818 pooling-related benefits will be potentially reduced or even eliminated in the context of such deployment scenarios.  819 The maximum transport network latency between the site hosting O-DU(s) and sites hosting associated O-RU(s) is 820 primarily determined by the RAN internal processes time constraints (such as HARQ loop, scheduling, etc., time-sensitive 821 operations). For the purposes of this document, we use 100us latency, which is commonly used as a target maximum 822 latency for this transport segment in related industry specifications for user-plane, specifically “High100” on E-CPRI 823 transport requirements [4] section 4.1.1, as well as “Fronthaul” latency requirement in ITU technical report GSTR-TN5G 824 [6], section 7-2, and IEEE Std 802.1CM-2018 [5], section 6.3.3.1.  Based on the 5us/km fiber propagation delay, this 825 implies that in a 2D Manhattan tessellation model, which is a common simple topology model for dense urban area fiber 826 routing, the maximum area that can be covered from a single Edge Cloud tier site hosting O-DUs is up to a 400km2  area 827 of Cell Sites and associated RUs.  Based on the radio inter-site distances, number of bands and other radio network 828 dimensioning specific parameters, this can be used to estimate the maximum number of Cell Sites and cell sectors that 829 can be covered from single Edge Cloud tier location, as well as maximum number of UEs in this coverage area. 830 The maximum transport network latencies towards the entities located at higher tiers are constrained by the lower of F1 831 interface latency (max 10 ms as per GSTR-TN5G [6], section 7.2), or alternatively service-specific latency constraints, 832 for the edge-located services that are positioned to take advantage of improved latencies.  For eMBB, UE-CU latency 833 target is 4ms one-way delay, while for the URLLC it is 0.5ms as per 3GPP (or 1ms as per ITU requirements). The 834 placement of the O-CU-UP as well as associated UPF, to be able to provide URLLC services would have to be at most at 835 the Edge Cloud tier to satisfy the service latency constraint. For the eMBB services with 4ms OWD target, it is possible 836 to locate O-CU-UP and UPF on next higher latency location tier, i.e. Regional Cloud tier. Note that while not shown in 837 the picture, Edge compute / Multi-Access Edge Compute (MEC) services for a given RAN service type are expected to 838 be collocated with the associated UPF function to take advantage of the associated service latency reduction potential.  839 For the services that do not have specific low-latency targets, the associated O-CU-UP and UPF can be located on higher 840 tier, similar to deployments in typical LTE network designs. This is designated as Core Cloud tier in the example in Figure 841
                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 25 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
15 above.  For eMBB services, if there are no local service instances in the Edge or Regional clouds to take advantage of 842 the 4ms OWD enabled by eMBB service definition, but the associated services are provided from either core clouds, 843 external networks or from other Edge Cloud / RAN instances (in case of user-to-user traffic), the associated non-844 constrained (i.e. over 4ms from subscriber) eMBB O-CU-UP and UPF instances can be located in Core Cloud sites 845 without perceivable impact to the service user, as in such cases the transport and/or service-specific latencies are dominant 846 latency components.  847 The intent of this section is not to micromanage the latency budget, but to rather establish a reasonable baseline for 848 dimensioning purposes, particularly to provide basic assessment to enable sizing of the cloud tiers within the context of 849 the service-specific constraints and transport allocations. As such, we get the following “allowances” for the aggregate 850 unspecified elements: 851  URLLC3GPP: 0.5ms - 0.1ms (TT1) = 0.4ms  ≥ TUE + TAIR + TE1 + TRU + 2(TE2) + TDU + TCU-UP 852  URLLCITU: 1ms - 0.1ms (TT1) = 0.9ms  ≥ TUE + TAIR + TE1 + TRU + 2(TE2) + TDU + TCU-UP 853  eMBB: 4ms - 0.1ms (TT1) - 1ms (TT2) = 2.9ms ≥ TUE + TAIR + TE1 + TRU + 2(TE2) + TDU + TE3 + TCU-UP 854  mMTC15: 15ms - 0.1ms (TT1) - 1ms (TT2) - 10ms (TT3) = 3.9ms ≥ TUE + TAIR + TE1 + TRU + 2(TE2) + TDU + TE3 + 855 TCU-UP + TC 856  857 If required, we may provide more specific allocations in later versions of the document, as we gain more implementation 858 experience and associated test data, but at this stage it is considered to be premature to do so. It should also be noted that 859 the URLLC specification is still work in progress at this stage in 3GPP, so likely first implementations will focus on 860 eMBB service, which leaves 2.9ms for combined O-RAN NFs, air interface, UE and cloud fabric latencies. 861 It is possible that network queuing delays may be the dominant delay contributor for some service classes. However, these 862 delay components should be understood to be in context of the most latency-sensitive services, particularly on RU-DU 863 interfaces, and relevant to the system level dimensioning. It is expected that if we will have multiple QoS classes, then 864 the delay and loss parameters are specified on per-class basis, but such specification is outside of scope of this section.  865 The delay components in this section are based on presently supported O-RAN splits, i.e. 3GPP reference split 866 configurations 7-2 & 8 for the RU-DU split (as defined in O-RAN), and 3GPP split 2 for F1 (as defined in O-RAN) and 867 associated transport allocations, and constraints are based on the 5G service requirements from ITU & 3GPP.  868 Other extensions have been approved and included in version 2.0 of the O-RAN Fronthaul specification [7], which allow 869 for so called “non-ideal” Fronthaul. It should be noted that while they allow substantially larger delays (e.g. 10 ms FH 870 splits have been described and implemented outside of O-RAN), they cannot be considered for all possible 5G use cases, 871 as for example it is clearly impossible to meet the 5G service-specification requirements over such large delay values 872 over the FH for URLLC or even 4 ms eMBB services. In addition, in specific scenarios (e.g. high-speed users), adding 873 latency to the fronthaul interface can result in reduced performance, and lower potential benefits, e.g. in Co-Ordinated 874 Multi-Point (CoMP) mechanisms. 875 5.3 Hardware Acceleration and Acceleration Abstraction Layer 876 (AAL) 877 As stated in Section 4.3.2, an O-Cloud Node is a collection of CPUs, Memory, Storage, NICs, BIOSes, BMCs, etc., and 878 may include hardware accelerators to offload computational-intense functions with the aim of optimizing the performance 879 of the O-RAN Cloudified NF (e.g., O-RU, O-DU, O-CU-CP, O-CU-UP, near-RT RIC).  There are many different types 880 of hardware accelerators, such as FPGA, ASIC, DSP, GPU, and many different types of acceleration functions, such as 881 Low-Density Parity-Check (LDPC), Forward Error Correction (FEC), end-to-end high-PHY for O-DU, security 882 algorithms for O-CU, and Artificial Intelligence for RIC.  The combination of hardware accelerator and acceleration 883 function, and indeed the option to use hardware acceleration, is the vendor’s choice; however, all types of hardware 884 acceleration on the cloud platform should ensure the decoupling of software from hardware. This decoupling implies the 885 following key objectives:  886  Multiple vendors of hardware GPP CPUs and accelerators (e.g., FGPA, ASIC, DSP, or GPU) can be used in O-887 Cloud platforms (including agreed-upon Acceleration Abstraction Layer as defined in an upcoming 888 specification) from multiple vendors, which in turn can support the software providing RAN functionality. 889  A given hardware and cloud platform shall support RAN software (including near-RT RIC, O-CU-CP, O-CU-890 UP, O-DU, and possibly O-RU functionality in the future) from multiple vendors.  891 There are different concepts that should be considered for the hardware acceleration abstraction layer on the cloud 892 platform; these are usually the following:  893
                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 26 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
 Accelerator Deployment Model  894  Acceleration Abstraction Layer (AAL) Interface (i.e., the APIs used by the NFs)  895  896 Figure 16: Hardware Abstraction Considerations 897
 Accelerator Deployment Model  898 Figure 16 above presents two common hardware accelerator deployment models as examples: an abstracted 899 implementation utilizing a vhost_user and virtIO type deployment, and a pass-through model using SR-IOV. While the 900 abstracted model allows a full decoupling of the Network Function (NF) from the hardware accelerator, this model may 901 not suit real-time latency sensitive NFs such as the O-DU. For better acceleration capabilities, SR-IOV pass through may 902 be required, as it is supported in both VM and container environments.  903
 Acceleration Abstraction Layer (AAL) Interface 904 To allow multiple NF vendors to utilize a given accelerator through its Acceleration Abstraction Layer (AAL) interface, 905 the accelerators must provide an open-sourced API. Likewise, this API shall allow NFs applications to discover, 906 configure, select and use (one or more) acceleration functions provided by a given accelerator on the cloud platform. 907 Moreover, this API shall also support different offload architectures including look aside, inline and any combination of 908 both. Examples of open APIs include DPDK’s CryptoDev, EthDev, EventDev, and Base Band Device (BBDEV).  909 When delivering an NF to an Operator, it is assumed that the supplier of that Network Function will provide not only the 910 Network Function, but it will also package the appropriate Accelerator Driver (possibly provided by a 3rd party) and will 911 indicate the corresponding AAL profile needed in the Operator’s O-Cloud. Figure 17 illustrates this for both Container 912 and Virtual Machine (VM) deployments.  913   914  915 Figure 17: Accelerator APIs/Libraries in Container and Virtual Machine Implementations 916

                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 27 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
 Accelerator Management and Orchestration Considerations 917 Note that Figure 17 shows the APIs/Libraries as used by the NF application running in a Container or a VM, but there are 918 several entities that require management. Accordingly, the figure also shows the Accelerator Management and 919 Accelerator Driver in the O-Cloud.  As will be discussed in Section 5.6, these entities (in addition to any hardware 920 accelerator considerations) will be managed via O2, specifically the Infrastructure Management Services.  Figure 17 also 921 shows that the Accelerator Driver (e.g., the PMD driver) needs to be supported both by the O-Cloud Platform, by the 922 Guest OS in case of VMs, and by the NF packaged into a container.   923 In general, the hardware accelerators shall be capable of being managed and orchestrated. In particular, hardware 924 accelerators shall support feature discovery and life cycle management.  Existing Open Source solutions may be leveraged 925 for both VMs and containers as defined in an upcomingO2 specification.  Examples include OpenStack Nova and Cyborg, 926 while in Kubernetes we can leverage the device plugin framework for vendors to advertise their device and associated 927 resources for the accelerator management.   928 5.4 Cloud Considerations 929 In this section we talk about the list of cloud platform capabilities which is expected to be provided by the cloud platform 930 to be able to support the deployment of the scenarios which are covered by this document.  931 It is assumed that some or all deployment scenarios may be using VM orchestrated/managed by OpenStack and / or 932 Container managed/orchestrated by Kubernetes, and therefore this section will cover both options. 933 The discussion in most sub-sections of this section is structured into (up to) three parts:  (1) Common, (2) Container 934 only, and (3) VM only.  935
 Networking requirements 936 A Cloud Platform should have the ability to support high performance N – S and E – W networking, with high throughput 937 and low latency.  938 5.4.1.1 Support for Multiple Networking Interfaces 939 Common:  In the different scenarios, near-RT RIC, vO-CU, and vO-DU all depend on having support for multiple 940 network interfaces. The Cloud Platform is required to support the ability to assign multiple networking interfaces to a 941 single container or VM instance, so that the cloud platform could support successful deployment for the different 942 scenarios.  943 Container-only:  For example, the cloud platform can achieve this by supporting the implementation of Multus Container 944 Networking Interface (CNI) Plugin. For more details, please see https://github.com/intel/multus-cni. 945  946 Figure 18:  Illustration of the Network Interfaces Attached to a Pod, as Provisioned by Multus CNI 947 VM-only:  OpenStack provides the Neutron component for networking. For more details, please see 948 https://docs.openstack.org/neutron/stein/  949 5.4.1.2 Support for High Performance N-S Data Plane 950 Common:  The Fronthaul connection between the O-RU/RU and vO-DU requires high performance and low latency. 951 This means handling packets at high speed and low latency. As per the different scenarios covered in this document, 952

                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 28 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
multiple vO-DUs may be running on the same physical cloud platform, which will result in the need for sharing the same 953 physical networking interface with multiple functions. Typically, the SR-IOV networking interface is used for this. 954 The cloud platform will need to provide support for assigning SR-IOV networking interfaces to a container or VM 955 instance, so the instance can use the network interface (physical function or virtual function) directly without using a 956 virtual switch.  957 If only one container needs to use the networking interface, the PCI pass-through network interface can provide high 958 performance and low latency without using a virtual switch. 959 In general, the following two items are needed for high performance N-S data throughput: 960  Support for SR-IOV; i.e., the ability to assign SR-IOV NIC interfaces to the containers/ VMs 961  Support for PCI pass-through for direct access to the NIC by the container/ VM  962 Container-only:  When containers are used, the cloud platform can achieve this by supporting the implementation of SR-963 IOV Network device plugin for Kubernetes. For more details, please refer to https://github.com/intel/sriov-network-964 device-plugin  965 VM-only: OpenStack provides the Neutron component for networking. For more details, please see 966 https://docs.openstack.org/neutron/stein/admin/config-sriov.html . 967 5.4.1.3 Support for High-Performance E-W Data Plane 968 Common:  High-performance E-W data plane throughput is a requirement for the implementation of the different near-969 RT RIC, vO-CU, and vO-DU scenarios which are covered in this document.  970 One of commonly used options for E-W high-performance data plane is the use of a virtual switch which provides basic 971 communication capability for instances deployed at either the same machine or different machines. It provides L2 and L3 972 network functions.  973 To get the high performance required, one of the options is to use a Data Plan Development Kit (DPDK)-based virtual 974 switch.  Using this method, the packets will not go into Linux kernel space networking, and instead will implement 975 userspace networking which will improve the throughput and latency. To support this, the container or VM instance will 976 need to use DPDK to accelerate packet handling.  977 The cloud platform will need to provide the mechanism to support the implementation of userspace networking for 978 container(s) / VM(s). 979 Container-only:  As an example, the cloud platform can achieve this by supporting implementation of Userspace CNI 980 Plugin. For more details, please refer to https://github.com/intel/userspace-cni-network-plugin. 981  982 Figure 19:  Illustration of the Userspace CNI Plugin 983 VM-only:  OVS DPDK is an example of a Host userspace virtual switch and could provide high performance L2/L3 984 packet receive and transmit.   985

                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 29 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
5.4.1.4 Support for Service Function Chaining  986 Common:  Support for a Service Function Chaining (SFC) capability requires the ability to create a service function 987 chain between multiple VMs or containers. In the virtualization environment, multiple instances will usually be deployed, 988 and being able to efficiently connect the instances to provide service will be a fundamental requirement.  989 The ability to dynamically configure traffic flow will provide flexibility to Operators.  When the service requirement or 990 flow direction needs to be changed, the Service Function Chaining capability can be used to easily implement it instead 991 of having to restart and reconfigure the services, networking configuration and Containers/VMs. 992 Container-only: An example of SFC functionality is found at: https://networkservicemesh.io/ 993 VM only:  The OpenStack Neutron SFC and OpenFlow-based SFC are examples of solutions that can implement the 994 Service Function Chaining capability. 995 5.4.1.5 Support for VLAN based networking 996 Common:  VLAN based networking is the most common and fundamental form of networking. VLANs are typically 997 used to provide the isolation of various types of traffic in cloud environments. Cloud platforms must support the traffic 998 isolation requirements of the application.  999 The O-RAN slicing use cases specified in [14] require the use of VLANs by O-RAN NFs to distinguish traffic belonging 1000 to different slices. To support this requirement, the O-Cloud platform must provide support for trunked VLAN network 1001 interfaces to be made available to Cloudified NFs (VMs and Containers) so that packets tagged with different VLANs 1002 can be transported on the same virtual network interface. 1003  1004 VLANs may also be used to differentiate slices in the transport network once the appropriate VLAN tags are applied by 1005 Cloudified NFs in the Data Center as specified in [14]. Therefore, the O-Cloud must also ensure that any VLAN tags 1006 applied by the O-RAN NFs are carried over to the transport network. 1007  1008 Container-only: For example, the cloud platform can achieve this by supporting the implementation of Multus Container 1009 Networking Interface (CNI) Plugin. For more details, please see https://github.com/k8snetworkplumbingwg/multus-cni 1010 VM only: OpenStack provides the Neutron component for networking. For more details, please see 1011 https://docs.openstack.org/neutron/stein/  1012  1013 5.4.1.6 Support for O-Cloud Gateways to connect O-Cloud Sites with external Transport 1014 Networks 1015 In disaggregated O-RAN deployments, the Network Functions (NFs) may be deployed in multiple O-Clouds or different 1016 locations within a given distributed O-Cloud.  As an example, O-DU and O-CU-CP may be deployed in two different O-1017 Clouds or different distributed O-Cloud sites. For O-CU-CP to communicate with O-DU, the networking needs to span 1018 across the O-Clouds or distributed O-Cloud sites via external transport networks.   1019 For a transport network to interconnect different O-Clouds or O-Cloud sites, it needs an endpoint, herein referred to as 1020 O-Cloud gateway, in each of the O-Clouds. For the sake of this architecture, the O-Cloud gateway is a logical endpoint 1021 inside the O-Cloud that connects the O-Cloud to the outside world. 1022  1023 Beside the interconnection of O-Clouds and distributed O-Cloud sites, there are other external connections that also need 1024 to terminate the O-Clouds and O-Cloud sites domains in an O-Cloud gateway function to ensure a clear demarcation of 1025 the different network domains. It is FFS which other gateway functions are needed as how they are to be named and how 1026 they are to be managed for example seeking inspiration in the ETSI GS NVF-SOL.005. 1027  1028 O-Cloud shall provide support for one or more O-Cloud gateway instances to provide connectivity to one or more external 1029 networks. This does not restrict or impose any networking models within the O-Cloud as long as the O-Cloud provides a 1030 mechanism to connect the NFs to the O-Cloud gateway so that the deployed NFs could reach other NFs deployed in other 1031 O-Clouds or O-Cloud sites, while maintaining the segmentation of the traffic between the NFs. It shall also be noted that 1032 each network domain can have its own networking model and segmentation scheme. 1033  1034 O-Cloud gateway augments the O-Cloud architecture model depicted by figure 10 and 11 in section 4.3.2. 1035
                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 30 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
 Assignment of Acceleration Resources 1036 Common:  For both container and VM solutions, specific devices such as accelerator (e.g., FPGA, GPU) may be needed. 1037 In this case, the cloud platform needs to be able to assign the specified device to container instance or VM instance.  1038 For example, some L1 protocols require an FFT algorithm (to compute the DFT) that could be implemented in an FPGA 1039 or GPU, and the vO-DU would need the PCI Pass-Through to assign the accelerator device to the vO-DU for access and 1040 use. 1041
 Real-time / General Performance Feature Requirements 1042 5.4.3.1 Host Linux OS 1043 5.4.3.1.1 Support for Pre-emptive Scheduling  1044 Support may be required to support Pre-emptive Scheduling (real time Linux uses the preempt_rt patch). Generally, 1045 without real time features, it is very difficult for an application to get deterministic response times for events, interrupts 1046 and other reasons4. In addition, during the housekeeping processes in Linux system, the application also cannot guarantee 1047 the running time (CPU cycle), so from the wireless application design perspective, it needs the real time feature. In 1048 addition, to support the requirements of high throughput, multiple accesses and low latency, some wireless applications 1049 need the priority-based OS environment.  1050 5.4.3.2 Support for Node Feature Discovery 1051 Common:  Automated and dynamic placement of Cloud-Native Network Functions (CNFs) / microservices and VMs is 1052 needed, based on the hardware requirements imposed on the vO-DU, vO-CU and near-RT RIC functions.  This requires 1053 the cloud platform to support the ability to discover the hardware capabilities on each node and advertise it via labels vs. 1054 nodes, and allows O-RAN Cloudified NFs’ descriptions to have hardware requirements via labels. This mechanism is 1055 also known as Node Feature Discovery (NFD). 1056 Container-only:  For example, the cloud platform can achieve this by supporting implementation of NFD for Kubernetes. 1057 For more details, please see https://github.com/kubernetes-sigs/node-feature-discovery. 1058 VM-only:  VMs can use OpenStack mechanisms.  For example, the OpenStack Nova filter, host aggregates and 1059 availability zones can be used to implement the same function. 1060 5.4.3.3 Support for CPU Affinity and Isolation 1061 Common:  The vO-DU, vO-CU and even the near-RT RIC are performance sensitive and require the ability to consume 1062 a large amount of CPU cycles to work correctly.  They depend on the ability of the cloud platform to provide a mechanism 1063 to guarantee performance determinism even when there are noisy neighbors.  1064 Container-only:  This requires the cloud platform to support using affinity and isolation of cores, so high performance 1065 Kubernetes Pod cores also can be dedicated to specified tasks.  For example, the cloud platform can achieve this by 1066 implementing CPU Manager for Kubernetes. For more details, please refer to https://github.com/intel/CPU-Manager-for-1067 Kubernetes . 1068 VM-only:  For example the modern Linux operating system uses the Symmetric MultiProcessing (SMP) mode, so the 1069 system process and application will be located at different CPU cores. To run the VM and guarantee the VM performance, 1070 the capability to assign the specific CPU cores to a VM is the way to do that. And at the same time, CPU isolation will 1071 reduce the inter-core affinity.  Please refer to https://docs.openstack.org/senlin/pike/scenarios/affinity.html 1072 5.4.3.4 Support for Dynamic HugePages Allocation 1073 Common:  When an application requires high performance and performance determinism, the reduction of paging is very 1074 helpful. vO-DU, vO-CU and even near-RT RIC can require performance determinism. The cloud platform needs to be 1075 able to support the ability to provide this mechanism to applications that require it. 1076  4 Other options include things such as Linux signal, softwareirq, and perhaps using a common process. Because the pre-emptive kernel could interrupt the low priority process and occupy the CPU, it will get more chance to run the high priority process. Then through proper application design, it will have guaranteed time/resource and can have deterministic performance.
                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 31 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
This requires the cloud platform to support ability to dynamically allocate the necessary amount of the faster memory 1077 (a.k.a. HugePages) to the container or VM as necessary, and also to relinquish this memory allocation in the event of 1078 unexpected termination.  1079 Container-only:  For example, the cloud platform can achieve this by supporting implementation of Manage HugePages 1080 in Kubernetes. For more details please refer to https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-1081 hugepages/ . 1082 VM-only:  For example, the OpenStack Nova flavor setting can be used to configure the HugePage size for a VM instance.  1083 See https://docs.openstack.org/nova/pike/admin/huge-pages.html  1084 5.4.3.5 Support for Topology Manager 1085 Common:  Some of the cloud infrastructure which is targeted in the scenarios in this document may have servers which 1086 utilize a multiple-socket configuration which comes with multiple memory regions. Each core5 is connected to a memory 1087 region. While each CPU on one socket can access the memory region of the CPUs on another socket of the same board, 1088 the access time is significantly slower when crossing socket boundaries, and this will affect performance significantly.  1089 The configuration of hardware with multiple memory regions is also known as Non-Uniform Memory Access (NUMA) 1090 regions. To support automated and dynamic placement of CNFs/microservices or VMs based on cloud infrastructure that 1091 has multiple NUMA regions and guarantee the response time of the application (especially for vO-DU), it is critical to be 1092 able to ensure that all the containers/VMs are associated with core(s) which are connected to the same NUMA region. In 1093 addition, if the application relies on access to hardware accelerators and/or I/O which uses memory as a way to interact 1094 with the application, it is also critical that those also use the same NUMA region that the application uses. 1095 The cloud platform will need to provide the mechanism to enable managing the NUMA topology to ensure the placement 1096 of specified containers/VMs on cores which are on the same NUMA region, as well as making sure that the devices which 1097 the application uses are also connected to the same NUMA region.  1098  1099 Figure 20:  Example Illustration of Two NUMA Regions 1100 5.4.3.6 Support for Scale In/Out 1101 Common:  The act of scaling in/out of containers/ VMs can be based on triggers such as CPU load, network load, and 1102 storage consumption. The network service usually is not just a single container or VM, and in order to leverage the 1103 container/ VM benefit, the network service usually will have multiple containers/ VMs. But if demand is changing 1104 dynamically, especially for the O-CU, the service needs to be scaled in/out according to service requirements such as 1105 subscriber quantity.  1106 For example, when the number of subscribers increases, the system needs to start more container/ VM instances to ensure 1107 the service quality. From the cloud platform perspective, it could monitor the CPU load; if the load reaches a level such 1108 as 80%, it needs to scale out. If the CPU load drops 40%, it could then scale in. 1109  5 In this document, we use the terms core and socket in the following way.  A socket, or more precisely the multichip platform that fits into a server socket, contains multiple cores, each of which is a separate CPU.  Each core in a socket has some dedicated memory, and also some shared memory among other cores of the same socket, which are within the same NUMA zone.

                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 32 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
Different services can scale in/out depending on different criteria, such as the CPU load, network load and storage 1110 consumption.  Support for scale in/out can be helpful in implementing on-demand services.  1111 Editor’s Note:  Support for scale up/down is not discussed at this time, but may be revisited in the future.   1112 5.4.3.7 Support for Device Plugin 1113 Common:  For vO-DU, vO-CU and near-RT RIC applications, hardware accelerators such as SmartNICs, FPGAs and 1114 GPUs may be required to meet performance objectives that can’t be met by using software only implementations.  In 1115 other cases, such accelerators can be useful as an option to reduce the consumption of CPU cycles to achieve better cost 1116 efficiency. 1117 The cloud platform will need to provide the mechanism to support those accelerators. This in turn requires support the 1118 ability to discover, advertise, schedule and manage devices such as SR-IOV, GPU, and FPGA.   1119 Container-only:  For example, the cloud platform can achieve this by supporting implementation of Device Plugins in 1120 Kubernetes. For more details please check: https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-1121 net/device-plugins/. 1122 VM-only:  The PCI passthrough feature in OpenStack allows full access and direct control of a physical PCI device in 1123 guests. This mechanism is generic for any kind of PCI device, and runs with a Network Interface Card (NIC), Graphics 1124 Processing Unit (GPU), or any other devices that can be attached to a PCI bus.  Correct driver installation is the only 1125 requirement for the guest to properly use the devices. 1126 Some PCI devices provide Single Root I/O Virtualization and Sharing (SR-IOV) capabilities. When SR-IOV is used, a 1127 physical device is virtualized and appears as multiple PCI devices. Virtual PCI devices are assigned to the same or 1128 different guests. In the case of PCI passthrough, the full physical device is assigned to only one guest and cannot be shared. 1129 See https://wiki.openstack.org/wiki/Cyborg 1130 5.4.3.8 Support for Direct IRQ Assignment 1131 VM-only:  The general-purpose platform has many devices that will generate the IRQ to the system. To develop a 1132 performance-sensitive application, inclusion of low-latency and deterministic timing features, and assigning the IRQ to a 1133 specific CPU core, will reduce the impact of housekeeping processes and decrease the response time to desired IRQs. 1134 5.4.3.9 Support for No Over Commit CPU 1135 VM-only:  The “No Over Commit CPU” VM creation option is able to guarantee VM performance with a “dedicated 1136 CPU” model. 1137 In traditional telecom equipment design, this will maintain the level of CPU utilization to avoid burst and congestion 1138 situations. In a virtualization environment, performance-sensitive applications such as vO-DU, vO-CU, and near-RT RIC 1139 will need the platform to provide a mechanism to secure the CPU resource.  1140 5.4.3.10 Support for Specifying CPU Model 1141 VM-only:  OpenStack can use the CPU model setting to configure the vCPU for a VM.  For example, QEMU allows the 1142 CPU options to be “Nehalem”, “Westmere”, “SandyBridge” or “IvyBridge”, or alternatively it could be configured as 1143 “host-passthrough”. This allows VMs to leverage advanced features of selected CPU architectures. For the vO-CU and 1144 vO-DU design and implementation, there will be some algorithm and computing functions that can leverage host CPU 1145 instructions to realize some benefits such as performance. The cloud platform needs to provide this capability to VMs.  1146
 Storage Requirements 1147 The storage requirements are the same for both VM and Container based implementations. 1148 For O-RAN components, the O-RAN Cloudified NF needs storage for the image and for the O-RAN Cloudified NF itself.  1149 It should support different scale, e.g., for a Regional Cloud vs. an Edge Cloud.  The cloud platform needs to support a 1150 large-scale storage solution with redundancy, medium and small-scale storage solutions for two or more servers, and a 1151 very small-scale solution for a single server.  1152
                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 33 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
 Notification Subscription Framework 1153 Applications should have the ability to retrieve notifications that are necessary for their functionality. For example, vO-1154 DU needs to know that the node that it starts on has a PTP clock in sync with the master clock.  1155 Rationale – Application functionality often relies on but is not limited to O-Cloud platform HW resources such as FPGA, 1156 GPU, PHC. Hence, these application(s) should have the ability to select the resources that will provide them notifications 1157 about the status of these resources, initial state and changing state. This requires the applications to use a privilege mode 1158 in order to access the O-Cloud platform drivers and retrieve the status. However, in a Cloud Native environment, 1159 applications should not have a privilege mode for accessing the O-Cloud resources. This framework allows applications 1160 to subscribe for their necessary notifications without claiming a privilege mode and comply with O-Cloud Native 1161 requirements.    1162 5.4.5.1 O-Cloud Notification Subscription Requirements 1163 Tracking function: 1164 • tracks for resource(s) state of relevant data (for example, change in class of a master clock)    1165 • tracking function can be configured with tracking frequency per the resource being tracked (default value will 1166 be defined) 1167 Registration function: 1168 • allows application(s) and/or SMO (or other entities) to query for the resources that provide notifications  1169 • allows application(s) and/or SMO (or other entities) to subscribe to receiving notifications from the selected 1170 resource(s) 1171 • allows application(s) and/or SMO (or other entities) to subscribe to pulling notifications/data from the selected 1172 resource(s) 1173 • allows application(s) and/or SMO (or other entities) to unsubscribe to notification(s) which were previously 1174 subscribed to for either receiving or pulling notifications 1175 • The registration function updates the notification function about the state of the subscription and its request type 1176 (receiving or pulling notifications) 1177 Notification function: 1178 • used by the tracking function to message registered listeners of the resource state and/or its relevant data 1179 • pulls the tracking function per the application and/or SMO request  1180 • as soon as an application and/or SMO registers it receives a notification of the resource(s) status it is subscribed 1181 to 1182
                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 34 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
Figure 21 illustrates the architecture for implementing a framework for notification subscription. This diagram shows the 1183 functionally and interaction from a logical perspective, however, where these functions reside or how they are 1184 implemented is not in scope of this document and will be described by Cloud Platform Reference Design [12].  1185  1186 Figure 21: O-Cloud Notification Framework Architecture 1187 5.5 Sync Architecture 1188 Synchronization mechanisms and options are receiving significant attention in the industry.   1189 Editor’s Note:  O-RAN Working Groups 4 and 5 are addressing some aspects of synchronization, and more 1190 discussion of Sync is expected in future versions of this document.   1191 Version 2 of the Control, User and Synchronization (CUS) Plane Specification [7] discusses, in chapter 9.2.2, “Clock 1192 Model and Synchronization Topology”, four topology configuration options Lower Layer Split Control Plane 1 – 4 (LLS-1193 C1 – LLS-C4) that are required to support different O-RAN deployment scenarios.  Configuration LLS-C3 is seen as the 1194 most likely initial option for deployment and is discussed below.  This section will provide a summary of what is required 1195 to support the LLS-C1 and LLS-C3 synchronization topology from the cloud platform perspective. 1196 Note that in chapter 6 “Deployment Scenarios and Implementation Considerations” of this document, we call the site 1197 which runs the vO-DU the “Edge Cloud”, while the Control, User and Synchronization (CUS) Plane Specification [7] 1198 calls it the “Central Site”.  However, the meaning is the same. 1199
 Cloud Platform Time Synchronization Architecture 1200 The Time Sync deployment architecture which is described below relies on usage of Precision Time Protocol (PTP) IEEE 1201 1588-2008 (a.k.a. IEEE 1588 Version 2) to synchronize clocks throughout the Edge Cloud site.  1202 For LLS-C3 in the CUS specification [7], vO-DU may act as a Telecom Slave Clock (T-TSC) and select the time source 1203 the same SyncE and PTP distribution from fronthaul as O-RU. Please note that the following synchronization topology 1204 for LLS-C3 will address only the case where O-DU and O-RU are synchronized from the same time source connected to 1205 the fronthaul network, other cases are for Further Study. 1206 For LLS-C1, the O-Cloud running the vO-DU acts as synchronization master towards the fronthaul interface to 1207 synchronize the O-RU. Please note that the following synchronization topology for LLS-C1 will address only the case 1208 where O-DU synchronization source is from a local PRTC (GNSS receiver), other cases are for Further Study. 1209  1210

                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 35 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
5.5.1.1 Edge Cloud Site Level – LLS-C3 Synchronization Topology 1211 This section outlines what the time synchronization architecture should be from the cloud platform perspective, and 1212 identifies requirements that the Cloud Platform and Edge Site need to support in order to support the O-RAN deployment 1213 scenarios that use the LLS-C3 synchronization topology described in CUS specification [7]. 1214 5.5.1.1.1 LLS-C3 Synchronization Topology Edge Site Time Synchronization Architecture 1215 The deployment architecture at the Edge Cloud site level includes: 1216  Primary Reference Time Clock (PRTC)-traceable time source (i.e., Grandmaster Clocks):  1217 o External precision time source for the PTP networks, usually based on Global Navigation Satellite 1218 System/Global Positioning System (GNSS/GPS) 1219  Compute Nodes:  1220 o Compute Nodes synchronize their clocks to a Grandmaster Clock via the Fronthaul Network 1221  Controller Nodes: 1222 o Controller Nodes synchronize their clocks to the Network Time Protocol (NTP) via the Management 1223 Network 1224  1225 Figure 22 illustrates the relationship of these entities where the Controller functions are hosted on separate nodes from 1226 the Compute nodes.  Figure 23 illustrates the relationships where each Compute node also includes the Controller 1227 functions (i.e., the hyperconverged case). 1228   1229   1230 Figure 22: Edge Cloud Site Time Sync Architecture for LLS-C3 1231 O-Cloud Compute NodeO-Cloud Management (OCM) compute-0Scompute-1compute-NScontroller-0controller-NManagement NetworkSFronthaul NetworkData NetworkCell Site (O-RU)Cell Site(O-RU)…Cell Site (O-RU)MasterMSlaveSClock SourceM
GrandmasterClock(s)T-BCPTP Switch…
PTPNTPManagement Network…SS S
                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 36 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
  1232 Figure 23: Hyperconverged Edge Cloud Time Sync Architecture for LLS-C3 1233 5.5.1.1.2 LLS-C3 Synchronization Topology Edge Site Requirements 1234 To support time synchronization at the Edge site, the cloud platform (O-Cloud) used at the Edge site needs to support 1235 implementation of the PTP IEEE 1588-2008 (a.k.a. IEEE 1588 Version 2) standard. The compute nodes should meet the 1236 “Time and Frequency Synchronization Requirements” described in CUS specification [7]. The following software and 1237 hardware capabilities are required: 1238 5.5.1.1.2.1 Software 1239 Support for PTP will be needed in all the Edge Site O-Cloud nodes that support compute roles and will run vO-DU service 1240 operating as a Slave Clock. The following PTP configuration options should be provided: 1241 o Network Transport – L2, UDPv4, UDPv6 1242 o Delay Measurement Mechanism – utilize E2E to measure the delay 1243 o Time Stamping – support for hardware time stamping 1244  1245 For example: in the case when an O-Cloud is based on the Linux OS, this will require support for Linux PTP (see 1246 http://linuxptp.sourceforge.net) with the following: 1247 o ptp4l – implementation of PTP (Ordinary Clock, Boundary Clock), HW timestamping, E2E delay measurement 1248 mechanism. 1249 o phc2sys – Synchronization of two clocks, PHC and system clock (Linux clock) when using HW timestamping 1250  1251 5.5.1.1.2.2 Hardware  1252 Use of High speed, low latency Network Interface Card (NIC) with support for PTP Hardware Clock (PHC) subsystem 1253 for the data interface (fronthaul) on all the compute node(s) that will run the vO-DU function. When vO-DU requires 1254 SyncE, the NIC must support it. 1255 5.5.1.2 Edge Cloud Site Level – LLS-C1 Synchronization Topology 1256 This section outlines what the time synchronization architecture should be from the cloud platform perspective, and 1257 identifies requirements that the Cloud Platform and Edge Site need to support in order to support the O-RAN deployment 1258 scenarios that use the LLS-C1 synchronization topology described in CUS specification [7]. 1259 5.5.1.2.1 LLS-C1 Synchronization Topology Edge Site Time Synchronization Architecture 1260 The deployment architecture at the Edge Cloud site level includes: 1261  Primary Reference Time Clock (PRTC)-traceable time source (i.e., Grandmaster Clocks):  1262 o External precision time source for the PTP networks, usually based on Global Navigation Satellite 1263 System/Global Positioning System (GNSS/GPS) 1264
O-Cloud Hyperconvergedcompute-0Scontroller-0Fronthaul NetworkData NetworkCell Site (O-RU)Cell Site (O-RU)…Cell Site (O-RU)MasterMSlaveSClock SourceM
GrandmasterClock(s)T-BCPTP Switch
PTPManagement Networkcompute-1SController-1.SS S
                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 37 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
 Compute Nodes:  1265 o Compute Node as acts synchronization master towards the fronthaul interface  1266  Controller Nodes: 1267 o Controller Nodes synchronize their clocks to the Network Time Protocol (NTP) via the Management 1268 Network 1269  1270 Figure 24 illustrates the relationship of these entities where the Controller functions are hosted on separate nodes from 1271 the Compute nodes. Figure 25 illustrates the relationships where each Compute node also includes the Controller functions 1272 (i.e., the hyperconverged case). 1273
 1274 Figure 24: Edge Cloud Site Time Sync Architecture for LLS-C1 1275

                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 38 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
 1276 Figure 25: Hyperconverged Edge Cloud Time Sync Architecture for LLS-C1 1277 5.5.1.2.2 LLS-C1 Synchronization Topology Edge Site Requirements 1278 To support time synchronization at the Edge site, the cloud platform (O-Cloud) used at the Edge site needs to support 1279 implementation of the PTP IEEE 1588-2008 (a.k.a. IEEE 1588 Version 2) standard. The compute nodes should meet the 1280 “Time and Frequency Synchronization Requirements” described in CUS specification [7]. The following software and 1281 hardware capabilities are required: 1282 5.5.1.2.2.1 Software 1283 Support for PTP will be needed in all the Edge Site O-Cloud node that supports compute role and will run vO-DU service 1284 operating as a Master Clock. The following PTP configuration options should be provided: 1285 o Network Transport – L2, UDPv4, UDPv6 1286 o Delay Measurement Mechanism – utilize E2E to measure the delay 1287 o Time Stamping – support for hardware time stamping 1288  1289 5.5.1.2.2.2 Hardware  1290 Use of High speed, low latency Network Interface Card (NIC) with support for PTP Hardware Clock (PHC) subsystem 1291 for the data interface (fronthaul) on all the compute node(s) that will run the vO-DU function. 1292 When vO-DU requires SyncE, the NIC must support it. 1293
 Loss of Synchronization Notification 1294 Applications that rely on a Precision Time Protocol for synchronization (such as vO-DU but not limited to) should have 1295 the ability to retrieve the relevant data that can indicate the status of the PHC clock related to the worker node that the 1296 application is running on (for example a source clock class). Once an application subscribes to PTP notifications it 1297 receives the initial data which shows the PHC synchronization state and it will receive notifications when there is a state  1298 change to the sync status and/or per request for notification (pull), please refer to the notification subscription framework 1299 (section 5.4.5) how to subscribe for a PTP Notification.   1300 Rationale - The CUS specification [7] section 9.4.2, specifies various behaviours related to the state of the vO-DU and 1301 O-RU time synchronization.  1302

                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 39 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
• For example, if an vO-DU transits to the FREERUN state, because the synchronizing network delivers 1303 unacceptable synchronization quality, the vO-DU shall disable RF transmission on all connected O-RUs, and 1304 keep it turned off until synchronization is reacquired again.  1305 It should be noted that since vO-DU may need to take an action upon the synchronization notification (see example above) 1306 it is required to handle these notifications at the scope of the edge cloud (at the site location where the vO-DU is running) 1307 for two main reasons: ensuring that the vO-DU receives the notifications regardless of the communication state of its 1308 backhaul link and reducing the round trip delay for notifying the vO-DU. 1309 Figure 26 illustrates an vO-DU subscribes to retrieve PTP Notification based on the subscription framework described at 1310 section 5.4.5.  1311  1312 Figure 26: vO-DU Subscribes to PTP Notification 1313  1314 5.6 Operations and Maintenance Considerations 1315 Management of cloudified RAN Network Functions introduces some new management considerations, because the 1316 mapping between Network Functionality and physical hardware can be done in multiple ways, depending on the Scenario 1317 that is chosen.  Thus, management of aspects that are related to physical aspects rather than logical aspects need to be 1318 designed with flexibility in mind from the start.  For example, logging of physical functions, scale out actions, and 1319 survivability considerations are affected.   1320 The O-RAN Alliance has defined key fundamentals of the OAM framework (see [8] and [9], and refer to Figure 1). Given 1321 the number of deployment scenario options and possible variations of O-RAN Managed Functions (MFs) being mapped 1322 into Managed Elements (MEs) in different ways, it is important for all MEs to support a consistent level of visibility and 1323 control of their contained Managed Functions to the Service Management & Orchestration Framework.  This consistency 1324 will be enabled by support of the common OAM Interface Specification [9] for Fault Configuration Accounting 1325 Performance Security (FCAPS) and Life Cycle Management (LCM) functionality, and a common Information Modelling 1326 Framework that will provide underlying information models used for the MEs and MFs in a particular deployment. 1327
 The O1 Interface 1328 As described in [8], the O1 is an interface between management entities in Service Management and Orchestration 1329 Framework and O-RAN managed elements, for operation and management, by which FCAPS management, Software 1330 management, File management shall be achieved.  1331

                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 40 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
 The O2 Interface 1332 The O2 Interface is a collection of services and their associated interfaces that are provided by the O-Cloud platform to 1333 the SMO.  The services are categorized into two logical groups: 1334 • Infrastructure Management Services: which include the subset of O2 functions that are responsible for 1335 deploying and managing cloud infrastructure. 1336 • Deployment Management Services:  which include the subset of O2 functions that are responsible for 1337 managing the lifecycle of virtualized/containerized deployments on the cloud infrastructure. 1338 The O2 services and their associated interfaces shall be specified in the upcoming O2 specification. Any definitions of 1339 SMO functional elements needed to consume these services shall be described in OAM architecture. O2 interface would 1340 also address the management of hardware acceleration and supporting software in the O-Cloud platform. 1341 5.7 Transport Network Architecture 1342 While a Transport Network is a necessary foundation upon which to build any O-RAN deployment, a great many of the 1343 aspects of transport do not have to be addressed or specified in O-RAN Alliance documents.  For example, any location 1344 with cloud servers will be connected by layer 2 or layer 3 switches, but we do not need to specify much if anything about 1345 them in this document.   1346 The transport media used, particularly for fronthaul, can have an effect on aspects such as performance.  However, in the 1347 current version of this document we have been assuming that fiber transport is used.   1348 Editor’s Note:  Other transport technologies (e.g., microwave) are also possible, and could be addressed at a later 1349 date.  1350 That said, the use of an (optional) Fronthaul Gateway (FH GW) will have noteworthy effects on any O-RAN deployment 1351 that uses it. 1352
 Fronthaul Gateways 1353 In the deployment scenarios that follow, when the O-DU and O-RU functions are not implemented in the same physical 1354 node, a Fronthaul Gateway is shown as an optional element between them.  A Fronthaul Gateway can be motivated by 1355 different factors depending on a carrier’s deployment, and may perform different functions.   1356 The O-RAN Alliance does not currently have a single definition of a Fronthaul Gateway, and this document does not 1357 attempt to define one.  However, the Fronthaul Gateway is included in the diagrams as an optional implementation to 1358 acknowledge the fact that carriers are considering Fronthaul Gateways in their plans. Below are some examples of the 1359 functionality that could be provided: 1360  A FH GW can convert CPRI connections to the node supporting the O-RU function to eCPRI connections to the 1361 node that provides O-DU functionality.   1362  Note that when there is no FH GW, it is assumed that the Open Fronthaul interface between the O-RU and 1363 O-DU uses Option 7-2, as mentioned earlier in Section 4.1.  When there is a FH GW, it may have an Option 1364 7-2 interface to both the O-DU and the O-RU, but it is also possible for the FH GW to have a different 1365 interface to the O-RU/RU; for example, where CPRI is supported.   1366  A FH GW can support the aggregation of fiber pairs. 1367  A FH GW must support the following forwarding functions: 1368  Downlink:  Transport the traffic from O-DU to each O-RU (and cascading FH GW, if present) 1369  Uplink:  Summation of traffic from O-RUs 1370  A FH GW can provide power to the NEs supporting the O-RU function, e.g. via Power over Ethernet (PoE) or 1371 hybrid cable/fibers 1372 5.8 Overview of Deployment Scenarios 1373 The description of logical functionality in O-RAN includes the definition of key interfaces E2, F1, and Open Fronthaul.  1374 However, as noted earlier, this does not mean that each Network Function block must be implemented in a separate O-1375
                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 41 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
RAN Physical NF/O-RAN Cloudified NF.  Multiple logical functions can be implemented in a single O-RAN Physical 1376 NF/O-RAN Cloudified NF (for example O-DU and O-RU may be packaged as a single appliance).  1377 We assume that when Network Functions are implemented as different O-RAN Physical NFs/O-RAN Cloudified NFs, 1378 the interfaces between them must conform to the O-RAN specifications.  However, when multiple Network Functions 1379 are implemented by a single O-RAN Physical NF/O-RAN Cloudified NF, it is up to the operator to decide whether to 1380 enforce the O-RAN interfaces between the embedded Network Functions.  However, note that the OAM requirements 1381 for each separate Network Function will still need to be met.   1382 The current deployment scenarios for discussion are summarized in the figure below.  This includes options that are 1383 deployable in both the short and long term.  Each will be discussed in some detail in the following sections, followed by 1384 a summary of which one or ones are candidates for initial focus. Please note that, to help ease the high-level depiction of 1385 functionality, a single O-CU box is shown with an F1 interface, but in detailed discussions of specific scenarios, this will 1386 need to be discussed properly as composed of an O-CU-CP function with an F1-c interface and an O-CU-UP function 1387 with an F1-u interface.  Furthermore, there would in general be an unequal number of O-CU-CP and O-CU-UP instances.   1388 Figure 27 below shows the Network Functions at the top, and each identified scenario shows how these Network 1389 Functions are deployed as O-RAN Physical NFs or as O-RAN Cloudified NFs running on an O-RAN compliant O-Cloud.  1390 The term O-Cloud is defined in Section 4.  Please note that the requirements for an O-Cloud are driven by the Network 1391 Functions that need to be supported by the hardware, so for instance an O-Cloud that supports an O-RU function would 1392 be different from an O-Cloud that supports O-CU functionality.   1393 Finally, note that in the high-level figure below, the User Plane (UP) traffic is shown being delivered to the UPF.  As will 1394 be discussed, in specific scenarios it is sometimes possible for UP traffic to be delivered to edge applications that are 1395 supported by Mobile Edge Computing (MEC).  However, note that the specification of MEC itself is out of scope of this 1396 document. 1397 Note that vendors are not required to support all scenarios – it is a business decision to be made by each vendor.  Similarly, 1398 each operator will decide which scenarios it wishes to deploy.   1399  1400 Figure 27:  High-Level Comparison of Scenarios 1401 Each scenario is discussed in the next section.   1402  1403
KeyO-CloudNetwork Functions (e.g., O-CU + O-DU)O-RAN Physical NFCould be 100% O-RAN Physical NF (potentially in an open chassis, open HW design). Uses Open interfaces.“O-Cloud” indicates that an O-RAN Cloud Platform is used to support the RAN functions. This will optionally use hardware accelerator add-ons as required by each RAN function, and the software stack is decoupled from the hardware. Each O-Cloud uses open interfaces.O-CloudOpen fronthaulO-RUNear-RT RICO-CUO-DUScenario AO-RAN Physical NFO-CloudScenario BO-RAN Physical NFO-CloudScenario CO-RAN Physical NFScenario DO-RAN Physical NFO-CloudScenario C.1 &  C.2O-CloudO-CloudO-CloudO-RAN Physical NFO-CloudScenario E & E.1O-Cloud & optional O-RAN Physical NFO-CloudE2F1, E2Open FHO-RAN Physical NFO-CloudScenario FO-CloudO-CloudE2F1E2UPFEdge CloudCell SiteEdge CloudCell SiteRegional CloudEdge Cloud Cell SiteRegional CloudEdge CloudCell SiteRegional CloudEdge LocationCell SiteRegional CloudCell SiteRegional CloudEdge CloudCell SiteRegional Cloud
                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 42 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
6 Deployment Scenarios and Implementation 1404 Considerations 1405 This section reviews each of the deployment scenarios in turn.  For a given scenario, the requirements that apply to the 1406 O-RAN Physical NFs, O-RAN Cloudified NFs or O-Cloud platforms may become more specific and unique, while many 1407 of the logical Network Function requirements will remain the same.   1408 Please note that in all of the scenario figures of this section, the interfaces are logical interfaces (e.g., F1, E2, etc.).  This 1409 has a couple of implications.  First, the two functions on each side of an interface could be on different devices separated 1410 by physical transport connections (e.g., fiber or Ethernet transport connections), could be on different devices within the 1411 same cloud platform, or could even exist within the same server.  Second, the functions on each side of an interface could 1412 be from the same vendor or different vendors.  1413 In addition, please note that all User Plane interfaces are shown with a solid lines, and all Control Plane interfaces use 1414 dashed lines.  1415 Editor’s note: The terms vO-CU and vO-DU represent virtualized or containerized O-CU and O-DU, and are used 1416 interchangeably with O-CU and O-DU in these scenarios (with the exception when the O-DU is explicitly stated 1417 as a non-virtualized O-DU). 1418  1419 6.1 Scenario A  1420 In this scenario, the near-RT RIC, O-CU, and O-DU functions are all virtualized on the same cloud platform, and 1421 interfaces between those functions are within the same cloud platform.    1422 This scenario supports deployments in dense urban areas with an abundance of fronthaul capacity that allows BBU 1423 functionality to be pooled in a central location with sufficiently low latency to meet the O-DU latency requirements. 1424 Therefore, it does not attempt to centralize the near-RT RIC more than the limit that O-DU functionality can be 1425 centralized.  1426  1427 Figure 28:  Scenario A 1428 Also please note that if the optional FH GW is present, the interface between it and the Radio Unit might not meet the O-1429 RAN Fronthaul requirements (e.g., it might be an Option 8 interface), in which case the Radio Unit could be referred to 1430 as an “RU”, not an “O-RU”.  However, if FH GWs are defined to support an interface such as Option 8, it could be argued 1431 that the O-RU definition at that time will support Option 8.   1432
 Key Use Cases and Drivers 1433 Editor’s Note:  This section is FFS.  1434 6.2 Scenario B 1435 In this scenario, the near-RT RIC Network Function is virtualized on a Regional Cloud Platform, and the O-CU and O-1436 DU functions are virtualized on an Edge Cloud hardware platform that in general will be at a different location.  The 1437

                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 43 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
interface between the Near-RT RIC network function and the O-CU/O-DU network functions is E2.  Interfaces between 1438 the O-CU and O-DU Network Functions are within the same Cloud Platform.  1439  1440 Figure 29:  Scenario B – NR Stand-alone 1441 This scenario addresses deployments in locations with limited remote fronthaul capacity and O-RUs spread out in an area 1442 that limits the number of O-RUs that can be supported by pooled vO-CU/vO-DU functionality while still meeting the O-1443 DU latency requirements.  The use of a FH GW in the architecture allows significant savings in providing transport 1444 between the O-RU and vO-DU functionality. 1445  1446 Figure 30: Scenario B – MR-DC (inter-RAT NR and E-UTRA) regardless of the Core Network type (either EPC 1447 or 5GC) 1448 An Alternative to NR Standalone scenario B is given by the MR-DC (inter-RAT NR/E-UTRA) scenarios which extend 1449 requirements on the cloud platform to additionally support E-UTRA network functions (subscript E) and required 1450 interfaces Xn, open fronthaul and W1. The W1 interface, defined in 3GPP TS 37.470, only applies to E-UTRA nodes 1451 connected to 5G Core Network, i.e. ng-eNB as defined in 3GPP TS 38.300 and TS 38.401. Moreover, the foreseen MR-1452 DC (inter-RAT NR/E-UTRA) scenarios also include the EPC-connected E-UTRA-NR Dual Connectivity (EN-DC) by 1453 properly replacing the Xn interface with the X2 interface interconnecting E-UTRA nodes (eNBs) and NR ones (en-gNBs), 1454 with the possibility to exploit vO-CU/vO-DU functional split only for the en-gNBs6.  1455 As discussed earlier in Section 5.1.3, the O-CU and O-DU functions can be virtualized using either simple centralization 1456 or pooled centralization.  The desire is to have support for pooled centralization, although we need to understand what 1457 needs to be developed to enable such sharing.  Perhaps pooling will be a later feature, but any initial solution should not 1458 preclude a future path to a pooled solution.    1459  6 O-eNB vO-CUE/vO-DUE split (foreseen in 3GPP), is pending O-RAN architecture alignment in wg1.
O-RU(/RU)Near-RT RICOpen chassis and blade specCloud PlatformCell siteRegional cloudCloud PlatformE21:LEdge cloudUPF, MECvO-CUvO-DUOpen chassis and blade specE2F1Open fronthaul1:NGW (Opt)Near-RT RICOpen chassis and blade specCloud PlatformCell siteRegional cloudCloud PlatformE21:LEdge cloudvO-CUNvO-DUNE2F1Open fronthaul1:NGW (Opt)Open chassis and blade specXnE2Open fronthaulGW (Opt)1:N(*) for ng-eNB onlyvO-DUEW1(*)vO-CUEO-eNBO-RUN(/RUN)O-RUE(/RUE)
                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 44 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
 Key Use Cases and Drivers 1460 In this case, there are multiple O-RUs distributed in an area served by a centralized vO-DU functionality that can meet 1461 the latency requirements.  Depending on the concentration of the O-RUs, N could vary, but in general is expected to be 1462 engineered to support < 64 TRPs per O-DU.7  The near-RT RIC is centralized further to allow for optimization based on 1463 a more global view (e.g., a single large metropolitan area), and to reduce the number of separate near-RT RIC instances 1464 that need to be managed.   1465 The driving use case for this is to support an outdoor deployment of a mix of Small Cells and Macro cells in a relatively 1466 dense urban setting.  This can support mmWave as well as Sub-6 deployments. 1467 In this scenario, a given “virtual BBU” supports both vO-CU and vO-DU functions, and can connect many O-RUs.  1468 Current studies show that savings from pooling are significant but level off once more than 64 Transmission Reception 1469 Points (TRPs) are pooled.  This would imply N would be around 32-64. This deployment should support tens of thousands 1470 of O-RUs per near-RT RIC, so L could easily exceed 100.   1471 Below is a summary of the cardinality requirements assumed for this scenario.  1472   Table 2:  Cardinality and Delay Performance for Scenario B 1473  Attribute RIC – O-CU O-CU – O-DU O-DU – O-RU/RU  Example Cardinality L = 100+ M=1 N = 1-64  6.3 Scenario C 1474 In this scenario, the near-RT RIC and O-CU Network Functions are virtualized on a Regional Cloud Platform with a 1475 general server hardware platform, and the O-DU Network Functions are virtualized on an Edge Cloud hardware platform 1476 that is expected to include significant hardware accelerator capabilities.  Interfaces between the near-RT RIC and the O-1477 CU network functions are within the same Cloud Platform.  The interface between the Regional Cloud and the Edge cloud 1478 is F1, and an E2 interface from the near-RT RIC to the O-DU must also be supported.  1479  1480 Figure 31:  Scenario C 1481 This scenario is to support deployments in locations with limited remote Fronthaul capacity and O-RUs spread out in an 1482 area that limits the number of O-RUs that can be pooled while still meeting the O-DU latency requirements. It also applies 1483 to some whitebox macrocell deployments. The O-CU Network Function is further pooled to increase the efficiency of the 1484 hardware platform which it shares with the near-RT RIC Network Function.   1485 However, note that if a service type has tighter O-CU delay requirements than other services, then that may either severely 1486 limit the number of O-RUs supported by the Regional cloud, or a method will be needed to separate the processing of 1487 such services.  This will be discussed further in the following C.1 and C.2 Scenarios.   1488  7 It is assumed that one O-RU is associated with one TRP.  For example, if a cell site has three sectors, then each sector would have at least one TRP and hence at least three O-RUs.

                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 45 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
The use of a FH GW in the architecture allows significant savings in providing transport between the O-RU and vO-DU 1489 functionality.   1490
 Key Use Cases and Drivers 1491 In this case, there are multiple O-RUs distributed in an area where each O-RU can meet the latency requirement for the 1492 pooled vO-DU function.  The near-RT RIC and O-CU Network Functions are further centralized to realize additional 1493 efficiencies.   1494 A use case for this is to support an outdoor deployment of a mix of Small Cells and Macro cells in a relatively dense 1495 urban setting.  This can support mmWave as well as Sub-6 deployments. 1496 In this scenario, as in Scenario B, the Edge Cloud is expected to support roughly 32-64 O-RUs. This deployment should 1497 support tens of thousands of O-RUs per near-RT RIC.  1498 Below is a summary of the cardinality and the distance/delay requirements assumed for this scenario.   1499 Table 3:  Cardinality and Delay Performance for Scenario C   1500  Attribute RIC – O-CU O-CU – O-DU O-DU – O-RU/RU  Example Cardinality L= 1 M=100+  N=Roughly 32-64
 Scenario C.1, and Use Case and Drivers 1501 This is a variation of Scenario C, driven by the fact that different types of traffic (network slices) have different latency 1502 requirements.  In particular, URLLC has more demanding user-plane latency requirements, and Figure 32 below shows 1503 how the vO-CU User Part (vO-CU-UP) could be terminated in different places for different network slices.  Below, 1504 network slice 3 is terminated in the Edge Cloud.  This scenario is also suitable in case there isn’t enough space or power 1505 supply to install all vO-CUs and vO-DUs in one Edge Cloud site.  1506  1507 Figure 32:  Treatment of Network Slices:  MEC for URLLC at Edge Cloud, Centralized Control, Single vO-DU 1508 In Scenario C.1, all O-CU control is placed in the Regional Cloud, and there is a single vO-DU for all Network Slices.  1509 Only the placement of the vO-CU-CP differs, depending on the network slice.  Below is the diagram of this scenario, 1510 using the common diagram conventions of all scenarios.  1511

                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 46 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
 1512 Figure 33:  Scenario C.1 1513 Below is a summary of the cardinality and the distance/delay requirements assumed for this scenario.  The URLLC user 1514 plane requirements are what drive the placement of the vO-CU-UP function to be in the Edge cloud.   1515 Table 4:  Cardinality and Delay Performance for Scenario C.1 1516  Attribute RIC – O-CU O-CU – O-DU O-DU – O-RU/RU  Example Cardinality L= 1 M=320 N=100
Delay Max  1-way (distance)    mMTC NA 625 μs (125 km) 100 μs (20 km)     eMBB NA 625 μs (125 km) 100 μs (20 km)    URLLC (user/control) NA 100 μs (20 km)/625 μs (125 km) 100 μs (20 km)  1517
 Scenario C.2, and Use Case and Drivers 1518 This is a second variation of Scenario C, which utilizes the same method of placing some vO-CU user plane functionality 1519 in the Edge Cloud, and some in the Regional Cloud.  However, instead of having one vO-DU for all network slices, there 1520 are different vO-DU instances in the Edge Cloud.  1521 It is driven by factors including the following two use cases: 1522  One driver is RAN (O-RU) sharing among operators. In this use case, any operator can flexibly launch vO-CU 1523 and vO-DU instances at Edge or Regional Cloud site.  For example, as shown in Figure 34, Operator #1 wants 1524 to launch the vO-CU1 instance in the Regional Cloud, and the vO-DU1 instance at subtending Edge Cloud sites. 1525 On the other hand, Operator #2 wants to install both the vO-CU2 and vO-DU2 instances at the same Regional 1526 Cloud site.  Note that both operators will share the O-RU).  1527  Another driver is that, even within a single operator, that operator can customize scheduler functions depending 1528 on the network slice types, and can place the vO-CU and vO-DU instances depending on the network slice types. 1529 For example, an operator may launch both vO-CU and vO-DU at the edge cloud site (see Operator #2 below) to 1530 provide a URLLC service.   1531

                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 47 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
 1532 Figure 34:  Treatment of Network Slice: MEC for URLLC at Edge Cloud, Separate vO-DUs 1533 The multi-Operator use case has the following pros and cons: 1534 Pros: 1535  O-RU sharing can reduce TCO 1536  Flexible CU/DU location allows deployments to consider not only service requirements but also limitations of 1537 space or power in each site 1538 Cons: 1539  Allowing multiple operators to share O-RU resources is expected to require changes to the Open Fronthaul 1540 interface (especially the handshake among more than one vO-DU and a given O-RU).   1541  This change seems likely to have M-plane specification impact.  Therefore, this approach would need O-RAN 1542 buy-in and approval.   1543 Figure 35 below illustrates how different Component Carriers can be allocated to different operators, at the same O-RU 1544 at the same time.  Note that some updates of not only M-plane but also CUS-plane specifications will be required when 1545 considering frequency resource sharing among DUs. 1546  1547 Figure 35:  Single O-RU Being Shared by More than One Operator 1548 The diagram of how Network Functions map to Networks Elements for Scenario C.2 is shown below.  1549

                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 48 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
 1550 Figure 36:  Scenario C.2 1551 The performance requirements are the same as those discussed earlier for Scenario C.1 in Section 6.3.2. 1552 6.4 Scenario D  1553 This scenario is a variation on Scenario C, but in this case the O-DU functionality is supported by an O-RAN Physical 1554 NF rather than an O-Cloud.  1555 The general assumption is that Scenario D has the same use cases and performance requirements as Scenario C, and the 1556 primary difference is in the business decision of how the O-RAN Physical NF based solution compares with the O-RAN 1557 compliant O-Cloud solution.  Implementation considerations (discussed in Section 5.1) could lead a carrier to decide that 1558 an acceptable O-Cloud solution is not available in a deployment’s timeframe.   1559  1560 Figure 37:  Scenario D 1561 6.5 Scenario E  1562 In contrast to Scenario D, this scenario assumes that not only can the O-DU be virtualized as in Scenario C, but that the 1563 O-RU can also be successfully virtualized.  Furthermore, the O-RU and O-DU would be implemented in the same O-1564 Cloud, which has acceleration hardware if required by either or both the O-RU and O-DU.   1565 Note, this seems to be a future scenario, and is not part of our initial focus.   1566

                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 49 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
 1567 Figure 38:  Scenario E 1568
 Key Use Cases and Drivers 1569 Because the O-DU and O-RU are implemented in the same O-Cloud in this Scenario, it seems that the O-DU 1570 implementation must meet the environmental and accessibility requirements typically associated with an O-RU.  1571 Therefore, an indoor use case seems most appropriate.  1572
 Scenario E.1 vO-DU with O-RU 1573 For Macrocell deployment with the Open Hardware approach that is used in WG7, the O-DU 7-2 of O-RAN WG7 1574 OMAC HAR 0-v01.00 [13] can be a virtual function. In this small-scale scenario, HW acceleration is optional. The 1575 Cloud platform could be physically located near or at the bottom of the tower and be associated with a number of O-1576 RUs implemented with the Open HW design, possibly but not necessarily in the same chassis. 1577  1578 Figure 39: Scenario E.1 1579  1580 6.6 Scenario F  1581 This is a variation on Scenario E in which the O-DU and O-RU are both virtualized, but in different O-Clouds. This means 1582 that: 1583  The O-DU function can be placed in a more convenient location in terms of accessibility for maintenance and 1584 upgrades. 1585  The O-DU function can be placed in an environment that is semi-controlled or controlled, which reduces some 1586 of the implementation complexity.  1587  1588
Near-RT RICOpen chassis and blade specCloud PlatformRegional cloudvO-DUCloud PlatformF11:MvO-CUCell siteE2Open chassis and blade specUPF, MECE2O-RU(/RU)GW (Opt)Open fronthaul1:NO-RU uses Open HW design
                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 50 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
 1589 Figure 40:  Scenario F 1590
 Key Use Cases and Drivers 1591 Because this assumes that the O-RU is virtualized, this is a future use case. 1592 This use case seems to be better suited for outdoor deployments (e.g., pole mounted) than Scenario E. 1593 6.7 Scenarios of Initial Interest 1594 More scenarios have been identified than can be addressed in the initial release of this document.  Scenario B has been 1595 selected as the one to address initially, and to be the subject of detailed treatment in a Scenario document (refer back to 1596 Figure 1).  Other scenarios are expected to be addressed in later work.   1597  1598 7 Appendix A (informative):  Extensions to Current 1599 Deployment Scenarios to Include NSA 1600 In this appendix, some extensions to (some of) the current deployment scenarios are proposed with the aim of introducing 1601 Non-Standalone (NSA) in the pictures, consistently with the scope O-RAN cloud architecture. These extensions will be 1602 the basis of the discussion for next version of the present document. In the following charts the subscript ‘N’ is indicating 1603 blocks related to NR, while the subscript ‘E’ is indicating blocks related to E-UTRA.8  For E-UTRA, the W1 interface is 1604 indicated. Its definition is ongoing in a 3GPP work item. 1605  8 No UPF or MEC blocks are explicitly indicated in the figures of this appendix, as the focus of this appendix is on the radio part.

                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 51 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
7.1 Scenario A 1606  1607 Figure 41:  Scenario A, Including NSA 1608 7.2 Scenario B 1609 Editor’s Note: Scenario B, Including NSA has been incorporated into 6.2.  1610 7.3 Scenario C 1611  1612 Figure 42:  Scenario C, Including NSA 1613 7.4 Scenario C.2 1614 The scenario addresses both the single and multi-operator cases. To reduce the complexity in the figure the multi operator 1615 case is considered, so no X2/Xn interface is present between CUN1 and CUE2 or between CUE1 and CUN2. 1616

                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 52 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
 1617 Figure 43:  Scenario C.2, Including NSA 1618 7.5 Scenario D 1619  1620 Figure 44:  Scenario D, Including NSA  1621 Revision History 1622 Date Revision Company Description 2019.01.18 V0000 AT&T, Orange, Lenovo, … Template with initial scenarios. 2019.01.29 V00.00 Editor (AT&T) Updates to terminology, miscellaneous other updates 2019.02.07 V00.00 Editor (AT&T)  More definitions in 2.1, New Sec 4 on Overall Architecture, expansion/ updates of sec 5 Profiles, added Sec 6 OAM placeholder.  2019.03.18 V00.00 Editor (AT&T) Many additions in content and section structure. 2019.04.01 V00.00 Editor (AT&T) Some restructuring and combining of early sections, and more discussion on scope and context.  Addition of implementation consideration section, including performance.  Added optional Fronthaul GW. Provided framework discussion in each scenario’s subsection.  Other updates.

                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 53 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
2019.04.10 V00.00 Aricent, Red Hat, KDDI, Ciena Updates to include comments before April 11 review.  Comments from RaviKanth (Aricent), Pasi (Red Hat), Shinobu (KDDI), and Lyndon (Ciena).  2019.04.15 V00.00 Editor (AT&T) Updates to include some updates from comments from April 11 review. 2019.04.24 V00.00 Editor (AT&T) Updates of diagrams to address comments, additional figures on scope, and other changes to address April 11 review comments. 2019.05.01 V00.00 KDDI Updates to diagrams for Scenarios A and B.  Modifications per KDDI regarding C.2.  2019.05.12 V00.00 KDDI, Red Hat, Editor (AT&T) Updates based on meeting discussions, subsection additions based on proposals. 2019.05.15 V00.00 Editor (AT&T) Clean-up in preparation of creating a baseline document – marking of many comments as done, adding editor notes where needed, and other clarifications. 2019.05.20 V00.00 Editor (AT&T) Continued clean-up in preparation of a baseline. 2019.05.29 V00.00 Editor (AT&T) Continued clean-up in preparation of a baseline. 2019.06.04 V00.00 Wind River, China Mobile Major additions to the Cloud requirements in section 5.4 and Appendix B by Wind River, plus updates to the Fronthaul section from China Mobile. Various additional minor updates. 2019.06.13 V00.01 Editor (AT&T) This is the same as V00.00.13, but with renumbering to indicate this is the initial baseline for comment, V00.01.00  2019.06.14  V00.01 Wind River, AT&T This includes updates from CRs discussed and agreed to on the June 13 call:    Wind River contributions on adding a figure for NUMA illustration and a major enhancement of Sec 9.1 on cache  AT&T contribution to add material on centralization of O-DU/O-CU resources, to Sections 5.1 and 6.2    Update of figures to address Open Fronthaul comments (discussed June 6)  2019.07.05 V00.01 Editor (AT&T), based on meeting discussion Updates to address several CRs:  Multiple editorial items: o Draft text to address 5G/4G scope in Sec 1.2 – further discussion via separate CR o Statement in 5.2 about performance to focus on delay o Statement in 5.7 about transport o 5.8; update of Figure 13 to indicate cloud locations.  Added MEC text that to address MEC comment during call. o Delay and loss table updates in 6, and statement in 5.2  Former 9.1 and 9.3 sections of Appendix B (on cache and storage details) will be transferred to Tong’s document (Reference Design).    Update the O-DU pooling analysis in Section 5.1.3. 2019.07.18 V00.01 AT&T, Red Hat, TIM, Intel, Ericsson Updates to address multiple CRs, through July 18:  Address NSA aspects in scope  Addition of 5.3 (Acceleration)  Removal of Scale up/down appendix, and note for future study  Update of delay figure in 5.2.  Update of Figure 4
                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 54 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
 Replacement of Zbox concept with O-Cloud, and all related updates. 2019.08.02 V00.01 AT&T, Wind River, Red Hat Updates to address multiple CRs, discussed on Aug 1:  Update Section 5.6, merge in sec 7, explain some fundamental operations concepts.  Update the sync section to point to work in other WGs, and say that text will wait until CAD version 2.  Update the delay section (5.2.1)   Remove notes that refer to items that will not receive contributions in version 1.  Remove comments that are no longer relevant.  Remove Appendix A 2019.08.09 V00.01 Red Hat, TIM, DT, Editor (AT&T) Updates to address multiple CRs and DT review comments, discussed on Aug 8.    Update 5.2.1 to address non-optimal fronthaul, and to correct some equations  Update 5.6 to add a figure showing the O1* interface  Addressed a range of comments by DT, some editorial, some more involved. 2019.08.16 V00.01 Ericsson, Wind River, AT&T Updates to address multiple CRs and DT review comments, discussed on Aug 15.    Updates to address Ericsson’s comments  Update to address DT’s request to define vO-DU tile  Update of the Cloud Considerations section (5.4), mostly for restructuring to remove duplication, but to also add material for VMs or Containers where necessary to provide balanced coverage.  Additional updates:  Many resolved and obsolete Word comments have been removed in anticipation of finalization.  References to documents that are not finalized have been removed. 2019.08.23 V00.01 AT&T Updates to reflect:   Updates of the O-DU pooling section based on Aug 20 discussion  Management section updates are to address comments made on Aug 15 discussion, particularly regarding the use of the term domain manager and its role in an ME, and the location of O1 terminations  Edits to remove references to O-RAN WGs, and make updates of the revision history.  Addition of standard O-RAN Annex ZZZ 2019.08.26 V00.01 Editor (AT&T)  Clean up of references and cross references to them  Removed Word comments  Removed cardinality questions in Scenarios A (removed 6.1.1) and Scenario B 2019.08.26 V00.01 Editor (AT&T) Final minor comments during Aug 27 WG6 call, in preparation for vote. 2019.10.01 V01.00 Editor (AT&T) Update of Annex ZZZ, page footers, and addition of title page disclaimer.  2020.01.17 V01.00 Editor (AT&T) Merged the following CRs, but with   ATT-2019-11-19 CADS-C CR ATT-CAD-010 acceleration 01.00.00  WRS 2019-12-04 CAD-C 01.00.00 rev 1 2020.02.09 V01.00 Editor (AT&T) Simplified 5.6.   Removed 5.6.1, 5.6.2 – replaced it with pointers to O1, and O2 specification.  Incorporated NVD comments on 5.3 and 5.4 addressing inline acceleration as an option
                                                                                                                      O-RAN.WG6.CADS-v4.00 TR
 55 Copyright © 2022 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification.
2020.03.03 V01.00 Editor (AT&T)  Updated 4, 4.1 to reflect the latest O-RAN architecture   Incorporated comments on 5.6 to include O1, O2 references.  Updated 4.3 with O-Cloud description and definitions of key components of O-Cloud  Updated 5.3, Figure 15 to reflect O-Cloud reference figure in 4.3 2020.03.09 V01.00 Orange  Various minor editorial modifications, take them as suggestions for better readability… 2020.03.10 V01.05 Editor (AT&T)  Incorporated Ericsson comments provided on v01.00.02  Updated 1.1 to include O-RAN Architecture description  Added definitions for O-RAN Physical NF, O-RAN Cloudified NF 2020.03.14 V01.06 AT&T, Orange Minor editorial modifications, make this version ready for WG6 internal review and voting 2020.03.20 V02.00 AT&T, Orange Minor editorial, make this version ready for TSC review and voting 2020.07.04 V02.01 AT&T Incorporated the following CRs:  TIM.AO-2020.05.18-WG6-CR-0001-CADS Scenario B Extension-v05   WRS-2020-04-24 CAD-v02.00 CR for PTP Notifications v05 2020.07.06 V02.01 AT&T Added architecture of O-Cloud Resource Pool 2020.07.14 V02.01 AT&T Addressed minor editorial comments received, making it ready for TSC review and voting 2021.07.01 V02.02 Editor (AT&T) Incorporated the following CRS:   DIS-2020-08-03 CAD-v02.01 CR for OMAC v02  WRS-2020-09-15 CAD 2.01 CR for LLS-C1 Time Sync Requirements  JNPR-2021.05.13-WG6-CR-0001-VLAN_Based_Networking-v04 Editorial changes:   Rename references to O-vDU as vO-DU 2021.07.15 V02.02 Editor (AT&T) Fixed broken reference for O-RAN WG4, Control, User and Synchronization Plane Specification. 2022.03.31 V03.00 Editor (AT&T) Merged three CRs 1. Clarification about synchronization requirements of CAD scenarios 2. O-Cloud Gateway 3. Terminology Update  2022.03.31 V03.01 Editor (AT&T) Corrections from comment wiki 2022.04.05 V3.02 Editor (AT&T) Merged CR O-Cloud API 2022.04.05 V3.03 Editor (AT&T) Per discussion on approval call, removed version numbers for references to O-RAN’s own documents. References are just to whatever the latest version is. 2022.07.25 V4.00 Editor (AT&T) Merged two CRs 1. PTP Support over UDP 2. IMS Provisioning CADS Concepts update  1623