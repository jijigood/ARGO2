O-RAN.WG6.CLOUD-REF-B-v01.00
Technical Specification
Cloud Platform Reference Design
for Deployment Scenario B
This is a re-published version of the attached final specification.
For this re-published version, the prior versions of the IPR Policy will apply, except that the previous
requirement for Adopters (as defined in the earlier IPR Policy) to agree to an O-RAN Adopter License
Agreement to access and use Final Specifications shall no longer apply or be required for these Final
Specifications after 1st July 2022.
The copying or incorporation into any other work of part or all of the material available in this
specification in any form without the prior written permission of O-RAN ALLIANCE e.V.  is prohibited,
save that you may print or download extracts of the material on this site for your personal use, or copy
the material on this site for the purpose of sending to individual third parties for their information
provided that you acknowledge O-RAN ALLIANCE as the source of the material and that you inform
the third party that these conditions apply to them and that they must comply with them.

Copyright © 2020 by the O-RAN Alliance e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ.

  O-RAN.WG6.CLOUD-REF-B-v01.00
Technical Specification

Prepared by the O-RAN Alliance. Copyright © 2020 by the O-RAN Alliance.
By using, accessing or downloading any part of this O -RAN specification document, including by copying, saving,
distributing, displaying or preparing derivatives of, you agree to be and are bound to the terms of the O -RAN Adopter License
Agreement contained in Annex ZZZ of this specification. All other rights reserved.

Cloud Platform Reference Design
for Deployment Scenario B

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           2
Revision History 1
Date Revision Author Description
2019.06.02 01.00.00 Tong Li (Lenovo) Initial skeleton.
2019.08.05 01.00.01 Tong Li (Lenovo) Initial content with contributions from Intel, Lenovo,
Mavenir, and Wind River.
2019.11.05 01.00.03 Guy Turgeon (Wind River) Initial Cloud Platform Requirements
2019.11.05 01.00.04 Guy Turgeon (Wind River) Cloud platform requirements additions
2019.11.05 01.00.05 Arjun Nanjundappa
(Mavenir)
vBBU initial requirements
2019.11.06 01.00.06 Guy Turgeon (Wind River) Cloud Platform Requirements architecture diagrams
modifications and text
2019.11.08 01.00.07 Arjun Nanjundappa
(Mavenir)
vBBU Stack Requirements updated Sections 3.3.8
(RAN Nodes Traffic Types and Acceleration) and 3.3.9
(Cloud Transport Protocol).
2019.11.08 01.00.08 Guy Turgeon (Wind River) Merge 01.00.06 and 01.00.007
2019.11.14 01.00.09 Guy Turgeon (Wind River) Genericize Cloud Platform Requirements
2019.11.18 01.00.10 Guy Turgeon (Wind River) Update Scenario B: high-level diagram with LLS -C3
and SMO. Modify Chapter 4 VM and Container based
platform diagrams
2019.11.26 01.00.11 Arjun Nanjundappa
(Mavenir)
1. Made the Traffic Types Generic as AAL instead of
BBDEV.2. Included different types of Acceleration:
Look-aside, In -line and Hybrid -Mode Acceleration.3.
Replaced S1 -MME by S1 -AP4. Updated CU -UP with
S1-U and NG-U traffic type.
2019.12.30 01.00.12 Tong Li (Lenovo) Re-structure doc, merge vendor & operator survey
results, merge INT-004 rev 1 from Niall Power  (Intel),
and various edits
2020.01.15 01.00.13 Guy Turgeon (Wind River) Specify CR -IO 4.2.2, add Wireless cipher 4.2.3, clean
up Kubernetes architecture components figure in 5.2.3
2020.01.24 01.00.14 Niall Power (Intel) Specify AAL Requirements for the Hardware
Acceleration section
2020.01.29 01.00.14
(bis)
Arjun Nanjundappa
(Mavenir)
Added content for Time Synchronization  section and
re-worked sections on Acceleration to capture different
acceleration needs (i.e., traffic and algos)
2020.01.31 Baseline
01.00.01
Lucian Suciu (Orange) Integrated contributions from Intel and Mavenir,
implemented agreed resolution s for several comments
received from RH, ERI, NVD (a few are still open…) ,
generated the Baseline document
2020.02.12 Baseline
01.00.02
Tong Li (Lenovo) Revision based on WG6 group discussion and various
edits
2020.02.12 Baseline
01.00.03
Arjun Nanjundappa
(Mavenir)
Time Synchronization Requirements and Figure
Caption update
2020.02.26 Baseline-
v01.00.05
Lopamudra Kundu (Nvidia) Revised section 4.3.2, appended lookaside and in -line
acceleration dataflows and associated Fig. in section
4.3.1, added/revised figs. in section 5.1.6.2 related to
                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           3
  1
GPU based inline PHY acceleration reference design
and high -level edge cloud platform for inline
acceleration, provided add itional Kubernetes plugins in
section 5.2.3.
2020.03.04 Baseline-
v01.00.07
Arjun Nanjundappa
(Mavenir)
Included both In -line and Look -aside acceleration
diagrams for O -DU and Accelerators to other nodes
have been kept generic for section 4.3.2.
2020.03.12 Baseline-
v01.00.11
Marge Hillis (Nokia),
Lopamudra Kundu (Nvidia),
Tong Li (Lenovo), Arjun
Nanjundappa (Mavenir),
Niall Power (Intel), Lucian
Suciu (Orange), Guy
Turgeon (Wind River),
Xiaogang Yan (China
Mobile), Tao Yang (China
Telecom)
Various edits.
2020.03.17 V00.16 Lenovo, Red Hat, Orange Time Sync modifications, minor editorial, make this
version ready for WG6 internal review and voting
2020.03.19 V00.17 Nokia, Red Hat Time Sync section modifications proposal from Nokia
(mostly editorial), and clarifications from Red Hat.

2020.03.20

V01.00

Version ready for TSC review and voting
                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           4
Contents 1
Revision History ................................................................................................................................................. 2 2
Chapter 1 Introductory Material ......................................................................................................................... 5 3
 Scope 5 1.14
 References .................................................................................................................................................................... 5 1.25
 Abbreviations ................................................................................................................................................................ 6 1.36
Chapter 2 Deployment Scenario ......................................................................................................................... 8 7
Chapter 3 Operator Requirements ...................................................................................................................... 9 8
 Typical Use Cases ......................................................................................................................................................... 9 3.19
 Cloud Deployment Requirements ............................................................................................................................... 10 3.210
 Regional Cloud ........................................................................................................................................................ 10 3.2.111
 Edge Cloud .............................................................................................................................................................. 10 3.2.212
Chapter 4 O-Cloud Requirements .................................................................................................................... 11 13
 O-Cloud Architectural Components and Requirements ............................................................................................. 11 4.114
 Hardware Requirements .......................................................................................................................................... 11 4.1.115
 Operating System Requirements.............................................................................................................................. 12 4.1.216
 Cloud Platform Runtime Requirements ................................................................................................................... 12 4.1.317
 Generic Requirements for Cloud Platform Management ......................................................................................... 12 4.1.418
 VM/Container Management and Orchestration Requirements ................................................................................ 13 4.1.519
 Time Synchronization Requirements .......................................................................................................................... 14 4.220
 G.8275.1 Full Timing Support (FTS) ...................................................................................................................... 14 4.2.121
 G.8275.2 Partial Timing Support (PTS) .................................................................................................................. 16 4.2.222
 Hardware Acceleration Requirements ........................................................................................................................ 16 4.323
 Background on Acceleration Types ......................................................................................................................... 17 4.3.124
 Hardware Acceleration Abstraction Layer Requirements ....................................................................................... 19 4.3.225
Chapter 5 Cloud Platform Reference Design Example .................................................................................... 22 26
 Cloud Platform Hardware Reference Design .............................................................................................................. 22 5.127
 Regional Cloud Server ............................................................................................................................................. 22 5.1.128
 Edge Cloud Server ................................................................................................................................................... 23 5.1.229
 BIOS/UEFI Configurations ..................................................................................................................................... 24 5.1.330
 Firmware Versions ................................................................................................................................................... 25 5.1.431
 Accelerator Hardware .............................................................................................................................................. 26 5.1.532
 Cloud Platform Software Reference Design ............................................................................................................... 28 5.233
 Common Reference Configurations for all Profiles ................................................................................................. 29 5.2.134
 Regional Cloud Reference Configuration Profile .................................................................................................... 30 5.2.235
 Edge Cloud Reference Configuration Profile .......................................................................................................... 30 5.2.336
 Example Real-time Linux Configurations ............................................................................................................... 32 5.2.437
Annex ZZZ: O-RAN Adopter License Agreement .......................................................................................... 33 38
Section 1: DEFINITIONS ................................................................................................................................................ 33 39
Section 2: COPYRIGHT LICENSE ................................................................................................................................. 33 40
Section 3: FRAND LICENSE .......................................................................................................................................... 34 41
Section 4: TERM AND TERMINATION ........................................................................................................................ 34 42
Section 5: CONFIDENTIALITY ..................................................................................................................................... 34 43
Section 6: INDEMNIFICATION ..................................................................................................................................... 35 44
Section 7: LIMITATIONS ON LIABILITY; NO WARRANTY .................................................................................... 35 45
Section 8: ASSIGNMENT ............................................................................................................................................... 35 46
Section 9: THIRD-PARTY BENEFICIARY RIGHTS .................................................................................................... 35 47
Section 10: BINDING ON AFFILIATES ........................................................................................................................ 35 48
Section 11: GENERAL ..................................................................................................................................................... 35 49
 50
 51
52
                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           5
Chapter 1 Introductory Material 1
 Scope 1.12
This Technical Specification has been produced by the O-RAN Alliance. 3
The contents of the present document are subject to continuing work within O-RAN and may change following formal 4
O-RAN approval. Should the O-RAN Alliance modify the contents of the present document, it will be re-released by O-5
RAN with an identifying change of release date and an increase in version number as follows:  6
Release x.y.z 7
where: 8
x the first digit  is incremented for all changes of substance, i.e. technical enhancements, corrections, updates, 9
etc. (the initial approved document will have x=01). 10
y the second digit is incremented when editorial only changes have been incorporated in the document.  11
z the third digit included only in working versions of the document indicating incremental changes during the 12
editing process. 13
The present document focuses on cloud d eployment scenario B , as defined in the O -RAN WG6 Cloud Architecture 14
document [1], and specifies requirements and reference designs for the cloud platform hardware and software . 15
 References 1.216
The following documents contain provisions which, throu gh reference in this text, constitute provisions of this 17
specification. 18
[1] O-RAN WG6, Cloud Architecture and Deployment Scenarios for O-RAN Virtualized RAN. 19
[2] O-RAN WG4, Control, User and Synchronization Plane Specification. 20
[3] GR-63-CORE for Network Switching Systems (NEBS) Seismic Rated Enclosures, Telecordia. 21
[4] GR-1089-CORE Electromagnetic Compatibility and Electrical Safety, Telcordia. 22
[5] OTII Server Technical Specification, 2019. 23
[6] OCP openEDGE Project, https://www.opencompute.org/wiki/Telcos/openEDGE. 24
[7] ETSI EN300 019 -1-3 Class 3.2 Equipment Engineering (EE) Environmental Conditions and Environmental 25
Tests for Telecommunications Equipment Part 1 -3: Classification of Environmental Conditions Stationary Use 26
at Weather-protected Locations. 27
[8] ETSI EN300386 (v1.6.1) Terrestrial Trunked Radio (TETRA); Direct Mode Operation (DMO); Part 6: Security 28
[9] FCC CFR47 15 (class A), FCC, Rules and Regulations for Title 47 Telecommunications. 29
[10] CISPR 22/32 (Class A) International  Electrotechnical Commission (IEC) International Special Committee on 30
Radio Interference, Radiated Emission Limits. 31
[11] CISPR 24, IEC Information Technology Equipment – Immunity Characteristics. 32
[12] TEC/EMI/TEL-001/01/FEB-09 – Information Technology Equipment. 33
[13] TEC/IR/SWN-2MB/07/MAR-10 – Information Technology Equipment. 34
[14] IEC 62368-1:2014, Safety Standard for Information Technology Equipment and Audio Video Equipment.  35
[15] O-RAN WG4 Management Plane Specification. 36
[16] 3GPP TS 23.501: “System architecture for the 5G System (5GS)”. 37
[17] O-RAN WG8, Base Station O-DU and O-CU Software Architecture and APIs. 38
[18] O-RAN Infrastructure Project, https://wiki.o-ran-sc.org/display/IN/Infrastructure+Home. 39
                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           6
 Abbreviations 1.31
For the purposes of the present document, the following abbreviations apply.  2
AAL Acceleration Abstraction Layer 3
API Application Programming Interface 4
ASIC Application-specific Integrated Circuit 5
BIOS Basic Input Output System 6
BBU Base Band Unit 7
CNF Containerized Network Function 8
CPU Central Processing Unit 9
DSP Digital Signal Processing 10
FPGA Field Programmable Gate Array 11
DPDK Data Plane Development Kit 12
eNB eNodeB (applies to LTE)   13
EPA Enhanced Platform Awareness 14
FEC Forward Error Correcting 15
FFS For Further Study 16
FFT Fast Fourier Transform 17
FHGW Front-haul Gateway 18
FTS Full Timing Support 19
gNB gNodeB (applies to NR) 20
GNSS Global Navigation Satellite System 21
GPU Graphics Processing Unit 22
HA High Availability 23
HW Hardware 24
LLS-C3 Lower Layer Split Configuration 3 25
NFV Network Functions Virtualization 26
NFVI Network Functions Virtualization Infrastructure 27
NIC Network Interface Card 28
NSA Non-standalone 29
NUMA Non-uniform Memory Access 30
O-CU O-RAN Centralized Unit 31
O-DU O-RAN Distributed Unit 32
O-RU O-RAN Radio Unit 33
OS Operating System 34
PHC PTP Hardware Clock 35
PIM Physical Infrastructure Manager 36
PRACH Physical Random Access Channel 37
PRTC Primary Reference Time Clock 38
PSU Power Supply Unit 39
PTP Precision Time Protocol 40
                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           7
PTS Partial Timing Support 1
RAID Redundant Array of Inexpensive Disks 2
RIC RAN Intelligent Controller 3
RT Real-time 4
SA Standalone 5
SMO Service Management and Orchestration 6
SR-IOV Single Root I/O Virtualization 7
SW Software 8
T-BC Telecom Boundary Clock 9
T-GM Telecom Grand Master 10
T-TSC Telecom Time Slave Clock 11
UEFI Unified Extensible Firmware Interface 12
vBBU Virtual Baseband Unit 13
VF Virtual Function 14
VIM Virtualized Infrastructure Manager 15
VM Virtual Machine 16
VNF Virtualized Network Function 17
  18
                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           8
Chapter 2 Deployment Scenario 1
This document focuses on cloud d eployment scenario B , as defined in the O -RAN Cloud Architecture document [1]. 2
The current version of this document focuses on SA NR gNBs deployed on the O-Cloud. The extensions of O-Cloud, if 3
required, to support NSA scenarios indicated in the annexes of the O-RAN Cloud Architecture document [1] are FFS. 4
Figure 2-1 illustrates scenario B, where the near-RT RIC is virtualized or containerized on a regional O-Cloud, and the 5
O-CU and O -DU functions are virtualized or containerized on an edge O-Cloud that typically resides at a different 6
location. The terms, vO -CU and vO -DU, represent virtualized or containerized  O-CU and O -DU, and are used 7
interchangeably with O-CU and O-DU in this document. 8
The interface that the near-RT RIC uses between the regional O-Cloud and the edge O-Cloud is E2. Interfaces between 9
the O -CU and O -DU managed functions are either within the same single managed element, or among multiple 10
managed elements within the same edge O-Cloud. 11
 12
Figure 2-1. Cloud deployment scenario B. 13
This scenario is to support deployments in  locations with limited remote front -haul capacity and a set of O -RUs spread 14
out in a limited  area, supported by pooled vO -CU/vO-DU functionality while still meeting the O -DU latency 15
requirements. The optional use of a Front -haul Gateway ( FHGW) in the architecture allows significant savings in 16
providing transport between the O-RU and vO-DU functionalities. 17
Figure 2-2 shows the O -Cloud high -level deployment diagram. O ne vO -CU and one vO -DU together form a virtual 18
BBU (vBBU), which connects to  N O-RUs (typically up to 64 , see [1]). This deployment should support ten s of 19
thousands of O-RUs per near-RT RIC, and therefore L could easily exceed 100.  20
O-Clouds are deployed at both regional and edge locations. At a given location, the edge O -Cloud can scale from a 21
single server to a clustered configuration with possibly hundreds of servers. For the time synchronization aspects, this 22
document focuses on the LLS-C3 configuration [2]. 23
 24
Figure 2-2. Scenario B – O-Cloud high-level deployment diagram. 25

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ           9
Chapter 3 Operator Requirements 1
 Typical Use Cases 3.12
Typical use cases include mobile wireless coverage for large metropolitan areas with operator requirements as shown in 3
Table 3-1 to Table 3-4. 4
Carrier Type Component Carrier
Bandwidth
Number of Layers
(Downlink)
Number of
Aggregated Carriers
Total Aggregated
Bandwidth
LTE 20 MHz 4 layers 5 carriers 400 MHz
NR Sub-6 GHz 20 MHz 16 layers 5 carriers 1600 MHz
mmWave 100 MHz 2 layers 8 carriers 1600 MHz
Table 3-1. AT&T Carrier aggregation requirements. 5
 6
Carrier Type Component Carrier
Bandwidth
Number of Layers Number of
Aggregated Carriers
Total Aggregated
Bandwidth
LTE 5-20 MHz 4 layers 3-5 carriers 240-320 MHz
NR Sub-6 GHz 100 MHz 8-16 layers 1 carrier Up to 1600 MHz
mmWave FFS FFS FFS FFS
Table 3-2. Deutsche Telekom carrier aggregation requirements. 7
 8
Carrier Type Component Carrier
Bandwidth
Number of Layers Number of
Aggregated Carriers
Total Aggregated
Bandwidth
LTE 20 MHz 4 layers 4 carriers 320 MHz
NR Sub-6 GHz 100 MHz 8 or 16 layers 1 carrier Up to 1600 MHz
mmWave FFS FFS FFS FFS
Table 3-3. Orange carrier aggregation requirements. 9
 10
Active UEs (95 th percentile) supported per macro sub -6 GHz
TRP
256 (AT&T, others FFS)
Active UEs (95th percentile) supported per mmWave TRP 64 (AT&T, others FFS)
Carrier capacity (MHz) per O-DU pool 57,600 MHz (AT&T, others FFS)
O-DU pooling model  Pooled centralization [1]
Minimum number of TRPs that an O-DU pool supports 32 (AT&T and Orange, others FFS)
Need for a fronthaul gateway (FHGW) Yes
Fronthaul interface to O-DU O-RAN 7.2x, with low phy in O-RU or FHGW
Physical interface to the cloud platform supporting O -DU (e.g.,
from FHGW)
At least 2x 100 Gbps links to minimize
switching latency
Table 3-4. BBU pooling requirements. 11
                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ
10
 Cloud Deployment Requirements 3.21
As Figure 2-1 shows, deployment of a cloud platform occurs in two locations: regional and edge clouds. The p hysical 2
environment of a regional cloud is typically a modern data center, hosting standard server racks with a 19 -inch wide 3
rack enclosure, 42U height, and external depth between 1 and 1.2 meters. In this environment, the cloud platform 4
hardware can use standard rack-mount servers and switches, and does not impose special requirements on the hardware 5
form factors. 6
The edge cloud often resides in a traditional telecommunications equipment room with limited space, cooling capacity, 7
floor loading, and so fort h. The limitations can impose special requirements on the cloud platform  hardware that may 8
require telecommunications  standard certifications, such as the Network Equipment Building System (NEBS) 9
requirements found in GR -63-CORE [3] and GR-1089-CORE [4]. For example, the Open Telecom IT Infrastructure 10
(OTII) Project specifies a 2U rac k server with depth less than 450 mm  [5][3]. In addition, the Open Compute Project 11
(OCP) operates an openEDGE project  [6], which specifie s a compact cloud platform that  has the extended 12
environmental requirements often needed in edge deployments. 13
 Regional Cloud 3.2.114
A regional cloud hosts the near -RT RIC, which includes Radio Connection Management, Mobility Management, QoS 15
Management, Interference Management, Radio -Network Information Base , and possibly 3rd-party applications . A 16
typical regional cloud has the following characteristics: 17
 Standard Internet Data Center (IDC) environment; 18
 At least one standard 19-inch rack with a top-of-rack (TOR) switch and five servers; 19
 Highly available cloud platform software; 20
 Highly available network topology; 21
 Highly available storage system in the form of tradi tional storage area network (SAN), software -defined 22
storage (SDS), or combination of the two. 23
 24
 Edge Cloud 3.2.225
The edge cloud is often deployed in a relatively rough physical environment with typical characteristics in Table 3-5.  26
Rack Form Factors 19-inch width, 600mm depth
Temperature Range Long-term: -5°C to 45°C (ETSI EN300 019-1-3 Class 3.2 [7])
Short-term: -5°C to 55°C (GR-63-CORE [3])
Humidity Operating humidity: 5% to 95%
Long-term: 5% to 85%
Short-term: 5% to 90%
EMC EN300386 (v1.6.1) [8]
FCC CFR47 15 (class A) [9], CISPR 22/32 (class A) [10], CISPR 24 [11]
TEC/EMI/TEL-001/01/FEB-09 [12] and TEC/IR/SWN-2MB/07/MAR-10 [13]
GR-1089-CORE [4]
Equipment Center Class A (Telecom center); Class B (Non-telecom center)
Seismic GR-63-CORE (Zone 4) [3]
Safety IEC 62368-1:2014 [14]
GR-63-CORE (Electrical safety, grounding and bonding) [3]
Acoustic Noise GR-63-CORE (equipment room criteria) [3]
Fire Resistance GR-63-CORE (shelf level criteria) [3]
Power Supply 208V AC, 220V AC, or -48V DC (-60V DC in Germany)
Table 3-5. Edge cloud environmental characteristics. 27
                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ
11
Chapter 4 O-Cloud Requirements 1
This chapter describes design requirements for the  O-Cloud. While most requirements apply to both the regional and 2
edge O-Clouds, the edge O-Cloud may impose unique requirements, e.g., hardware accelerators, which will be noted 3
specifically. In addition, d epending on the implementation choices for O -CU, O-DU, and near-RT RIC, the O-Cloud 4
needs to provide support for virtual machines or containers. 5
 O-Cloud Architectural Components and Requirements 4.16
 7
Figure 4-1. Architecture component requirements for the cloud platform. 8
Figure 4-1depicts the high -level architecture components for deploying VM/container-based O-RAN functions in the 9
regional and edge clouds. 10
The Virtualized/Containerized O -RAN Functions  include the O -DU, O-CU and other workloads deployed on the O -11
Cloud via the O2 interface. 12
The VM/Container Management and Orchestration is typically implemented with OpenStack for VM deployments and 13
Kubernetes for container deployments. 14
The Cloud Platform Management represents the O -Cloud infrastructure management functions required for life cycle 15
management, high availability, fault management and configuration of the O-Cloud platform. 16
The Cloud Platform Runtime  represents the additional cloud platform run -time capabilities required to host O -RAN 17
functions as compared to 5G core network functions. 18
 Hardware Requirements 4.1.119
Table 4-1 shows requirements for the cloud platform hardware. 20
Components Description
Server  Regional cloud: standard COTS server
 Edge cloud: depth ≤ 450mm, height ≤ 2RU, width ≤ 19in
 Minimum 16-core CPU at 2.2 GHz base frequency and 128 GB DDR4
 At least 4x 10/25GbE ports for front -haul interface, 2 x 10 GbE ports  for backhaul
interface, and 1x 1GbE out-of-band management port

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ
12
 SR-IOV-capable NICs with network link aggregation enabled
 For single-socket, at least 3 PCI -E slots for  3x HHHL or 1x FHFL + 2x HHHL cards;
for dual-socket, at least 4 slots for 4x HHHL or 1x FHFL DW + 3x LP cards
 All hard disk drives require hot-pluggable
 Management interfaces support IPMI, SNMP, and Redfish (preferred)
 UEFI secure boot enabled
Hardware Accelerator  Required for O-DU in the edge cloud in forms such as FPGA, ASIC, and GPU
 At least 2 ports at 10 or 25 Gbps eCPRI/RoE open front-haul interface
 Acceleration for functions such as LDPC encoding and decoding, and end-to-end high
PHY
Switch  Standard COTS TOR switch
Storage  Software-defined storage (e.g., Ceph) based on COTS servers
Table 4-1. Requirements for the cloud platform hardware. 1
 Operating System Requirements 4.1.22
Table 4-2 shows requirements for the cloud platform operating system. 3
Operating System Description
Linux

 Linux kernel with real -time pre emption patches for O-DU workloads  (i.e., SMP
PREEMPT RT). Real-time kernel patch for O-CU is optional.
 Deterministic interrupt handling with  a maximum latency of 20 us  as measured by
cyclictest for system interrupts (e.g., external I/O) and interrupt -based O -CU and O-
DU implementations (e.g., those that reply on OS timers)
 CRI-O support
 QEMU/KVM (or other types of hypervisor) for virtual machines
Table 4-2. Requirements for the cloud platform operating system. 4
 Cloud Platform Runtime Requirements 4.1.35
Table 4-3 shows requirements for the cloud platform runtime. 6
Runtime Components Description
Accelerator Driver

 Edge cloud: driver for loading, configuring, managing and interfacing with accelerator
hardware providing offload functions for O-DU container or VM
Crypto Driver  Crypto offload driver for networking and O-CU wireless cipher (optional)
Network Driver  Network driver(s) for  front-haul, back-haul, mid -haul, inter container or VM
communication, management and storage networks
Board Management  Board management for interfacing with server hardware and sensors
PTP  Precision time protocol for distributing phase , time and synchronization over a packet-
based network
Software-defined
Storage (SDS)
 Software implementation of block storage running on COTS servers, optional for edge
cloud and required for regional cloud
 If used in edge cloud, required to be hyperconverged; in regional cloud, either
hyperconverged or deployed on dedicated storage nodes
Container Runtime  Executes and manages container images on a node
Hypervisor  Allows host to run multiple isolated VMs
 Low performance overhead, compared with non -virtualized execution of specified
benchmarks, e.g., SPECcpu, fio, L2Fwd, etc. (quantitative requirements FFS)
Table 4-3. Requirements for the cloud platform runtime. 7
 Generic Requirements for Cloud Platform Management  4.1.48
Table 4-4 shows the generic requirements for management of the cloud platform.  9
                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ
13
Components Description
Configuration
Management

 Auto-discovery of new servers added to an O-Cloud
 Management of installation parameters such as console, root disks
 Configuration of core, memory, huge-page assignments on each server node
 Network interfaces and storage assignments
 Hardware discovery of CPUs, cores, storage, network ports, hardware accelerators, etc.
 Virtual-to-physical CPU core and NUMA node pinning
 Virtual NUMA topology configuration
 Huge-page memory
 Interrupt and thread pinning to virtual and physical cores
Host Management

 Life-cycle and availability management of the server
 Host failure detection and recovery
 Fault monitoring and reporting
 Cluster connectivity failure detection and recovery
 VM/container failure detection and recovery
 Critical process failure detection and recovery
 Resource utilization thresholds (e.g., CPU, memory, storage limits)
 Network interface states
 Hardware sensors and faults
 Watchdog
 Board management interfaces and out-of-band server management
 Hardware sensor monitoring
Service Management  Monitoring of critical platform infrastructure services and processes
 Automatic recovery of failed platform infrastructure services and processes
HA Management  High-availability services for supporting cloud platform redundancy
Fault management  Cloud platform infrastructure alarm and fault management.
 Ability to set, clear, query, filter, suppress, log and collect alarms
Software Management

 Ability to deploy software updates
 Support for rolling updates across all nodes of the local O-Cloud
 Support for migrating  VMs and/or containers within the local O -Cloud for software
upgrade procedures
User Management  User authentication and authorization
 Isolation of control and resources among different users
Node Feature
Management
 Detection and set ting of  node-level policies to align resource allocation choices (i.e.
NUMA, SR-IOV, CPU, etc.)
HW Accelerator
Management
 Support for m anaging hardware accelerator s, mapping them to O -RAN applications
VMs and/or containers, and updating accelerator firmware
Table 4-4. Cloud platform management requirements. 1
 VM/Container Management and Orchestration Requirements 4.1.52
Table 4-5 shows requirements for VM/container management and orchestration as part of the cloud platform.  3
Components Description
VM/Container
Management

 VM management and scheduling with OpenStack Nova
 Container management and scheduling with Kubernetes kube -apiserver, kube -
scheduler, etcd, kube-controller-manager
VM/Container
Orchestration
 VM orchestration with Heat templates
 Container orchestration with HELM charts
VM/Container Storage  VM storage and image services with OpenStack Glance and Cinder
 Container persistent volume claims (PVCs)
 VM/Container Ceph backends for persistent storage (optional)
VM/Container
Networking
 VM networking with OpenStack Neutron
 Container networking with kube-proxy and CNI
Table 4-5. Requirements for VM/container management and orchestration. 4
                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ
14
 Time Synchronization Requirements 4.21
The synchronization requirements relevant to this document cover only the synchronization aspects of the O -Cloud 2
nodes that are part of the edge cloud. Specifically, we need to comply with the synchronization requirements associated 3
with the O-DU instances when instantiated on the O -Cloud nodes, as in CUS specification [2] LLS-C3 synchronization 4
configuration with G.8275.1 full timing support profile or G.8275.2 parti al timing support profile.  5
The primary synchronization point in an O -RAN system is in the radio units, i.e. O -RUs, and those requirements drive 6
the performance aspects of many associated entities in the synchronization transfer path. The end -to-end performance 7
requirements for absolute and relative timing at the O -RU are use-case and system-design specific, and are specifically 8
outside of the scope of this section. The performance aspects with specific use cases in the context of O -RU 9
requirements are specified in the CUS specification [2]. 10
This section assumes that the O-DU and O-RU requirements are decoupled and independent, which is the case when O -11
DU instances are not in the synchronization transfer path from a synchronization source, i.e. PRTC/T -GM instance and 12
O-RU. The LLS-C3 configuration meets these criteria, as the synchronization source is decoupled from O -DU instances, 13
and placed in the front-haul network.  14
In terms of the synchroniza tion transfer chain, both the O -DU and O -RU act as slaves with respect to the physical 15
network elements (such as switches and/or routers) in the synchronization transfer path, which act as masters in 16
interfaces towards the O -DU (and O -RU) slave instances, and Boundary Clocks, i.e. slave towards the synchronization 17
source elements and masters towards slave and/or downstream switches when they are responsible for timing transfer 18
to/from other physical network elements (i.e. other switches and/or Grandmaster c locks). 19
The performance requirements in the CUS specification  [2] are end-to-end performance requirements. Specifically, in 20
the context of this specif ication and use case, the CUS specification  [2] specifies an absolute end -to-end performance 21
applicable to O-DU instances, the applicable requirements  are the requirements associated with the LLS -C3 case in the 22
CUS specification [2]. There are multiple performance classes for all of the instances i nvolved in the CUS specification 23
[2] and the associated standards specified therein. The end -to-end performance must be met with the maximum required 24
number of the nodes on the path, which is dependent on a combination of the network design, each element’s  25
performance class, and location of the desired synchronization source (PRTC/T -GM) in the topology.  26
Implementers are encouraged to work on improving the performance of all parts of the elements (e.g. through designing 27
individual components to more demandi ng classes), which allows more  flexibility on the synchronization source 28
placement with respect to the clients (e.g. through leveraging redundant PRTC/T -GM sources from surrounding edge 29
cloud sites and/or from regional cloud sites), and more intermediate n odes in the synchronization transfer path and/or 30
more accurate synchronization performance at the T-TSC levels. 31
Since the synchronization functions are largely decoupled from the O -DU instances in the cloud implementation (i.e. 32
most synchronization functions are provided as infrastructure services), the associated synchronization management and 33
monitoring interfaces in the cloud are for further study. In the interim, implementers should target at least semantic 34
equivalence to the synchronization configurati on management, performance monitoring metrics and events (such as 35
synchronization state transitions) as described in the CUS specification [2], M-plane specification [15], and references 36
therein for synchronization management aspects as relevant to the LLS-C3 case. 37
 G.8275.1 Full Timing Support (FTS) 4.2.138
Figure 4-2 illustrates an example of the redundant synchronization network for an edge site, which is aligned with the 39
LLS-C3 synchronization configuration.  40
 41
 42
 43
 44
 45
 46
 47
                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ
15
 1
Figure 4-2. Key synchronization chain elements in an example implementation in O-Cloud for LLS-C3. 2
The salient features of this design example are: 3
(1) The synchronization reference sources (PRTC/T -GM) are redundant and located in the edge cloud. In this 4
example, the references are derived from the GNSS system, but any other implementation compliant with the 5
CUS specification [2] requirements (and references therein), which acts as G.82751 T-GM, is acceptable. 6
(2) The synchronization referenc e sources are connected to the physical fabric in the edge cloud , with redundant 7
Ethernet interconnects. Specifically, each grandmaster connects to at least two leaf switches. 8
(3) Each switch within the synchronization tr ansfer path implements  the G.8275.1-compliant Boundary Clock 9
functionality. 10
(4) For larger site configurations (i.e.  in which the switch capacity and port count requirements exceed the 11
capabilities of the single physical switch), the structure is expanded in a “leaf-spine” fabric configuration, as 12
illustrated in Figure 4-2. When spine switches are used/required, they also need to be T -BC capable, or 13
alternative path for the synchronization transfer that meets the requirements of G.8275.1 profile needs to be 14
provided. For most edge site configurations, it is expected that a two-tier structure is sufficient, which implies 15
that the maximum length of any active synchronization transfer path from co -located sources is three boundary 16
clocks between the grandmaster and any node in the e dge cloud. Therefore, the synchronization path “length” 17
in terms of number of boundary clocks that need s to be supportable in this configuration is typically from one 18
to three. Capability to support more nodes in the path is desirable to allow for configuratio ns where the 19
grandmasters are not co-located at the edge cloud. 20
(5) All edge O-Cloud nodes are attached to at least two physically separated, independent leaf switches to provide 21
network redundancy and tolerance against a single point of failure that can affect more than one node (i.e. 22
switch failures). In addition to normal network fault tolerance, this is also required /useful for maintaining the 23
synchronization network fault tolerance. 24
(6) All leaf switch ports which are facing the nodes are configured as ma sters in synchronization transfer, and all 25
node ports are configured as slaves. It is not a requirement or expectation for any of the node s to assume a role 26
of the master (nor be able to meet the performance specifications of the T-GM). 27
(7) All of the edge O-Cloud nodes implement PTP Hardware Clock (PHC) function on all of the NICs that are 28
associated with timing transfer. Nodes may have other interfaces, such as management interfaces, backhaul 29
network interfaces and so forth, and associated switching infrastru cture, but those are omitted from illustration 30
for clarity. Such interfaces are not required to implement synchronization support features, but if they choose 31
to do so, the synchronization-capable interface requirements apply to those interfaces as well. 32
(8) All of the edge O-Cloud nodes implement T-TSC functionality in full compliance with the G.8275.1 profile. 33
(9) The operating system clock on each node is synchronized from the NIC T-TSC clock(s). The cloud platform 34
software on each node  is responsible for PTP pr otocol implementation, including adjustments to associated 35

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ
16
PHC clock(s). Whether the direct read -only access is required to the PHC counters from the i nstances is for 1
further study. Write access is not allowable, as that would interfere with the PHC manage ment from the cloud 2
platform software, and as there are limited number of PHC instances in typical NICs, usually associated with 3
the number of ports, the number of PHC instances can also in many cases be substantially less than the number 4
of running application instances such as O-DU instances, in the node. 5
(10) Nodes (or switches) are not required to implement the S yncE capabilities, as long as the accuracy requirements 6
associated with O-DU instances as specified in the  CUS specification [2] for O-DU are met. Note that the O -7
RU implementations may require the switches also to implement SyncE, but that is a network and 8
implementation-level detail that is out of scope of this document. 9
  10
Many alternative variations and  physical realizations of the above design are possible (e.g. PRTCs/GMs can be 11
combined with switches or routers, leaf -spine structure can be integrated onto physical box, s izes of elements can vary, 12
etc.), but functionally with respect to the synchronization transfer chain, all of the entities are expected to be present (i.e. 13
PRTC/T-GMs, Telecom Boundary Clock nodes, as well Telecom Slave Clock nodes).  14
All physical network elements that are in the synchronization transfer path (i.e. PRTC/T -GM, switches that act as T-BC, 15
and all O-Cloud node slaves that act as T -TSC) should  actually implement a  1PPS output interface to access the 16
synchronization reference signal to facilitate connection to external performance test equipment for performance 17
validation purposes. 18
The physical switches (leaf –spine) can be considered to be an integr al part or an extension of any f ront-haul transport 19
network, i.e. G.8275.1 synchronization requiremen ts apply to these elements as well, as outlined above. Further, for 20
G.8275.1 profile and CUS specification  [2] compliance, it is required that at least the Synchronization Plane aspects of 21
these networks support untagged L2 Ethernet transport for the S -Plane traffic (even if C - and/or U-planes may util ize 22
VLAN tags and/or IP encapsulations, potentially over the same interfaces with respect to O -DU front-haul traffic). 23
For the purposes of this document, the O -DU synchronization performance requirements as specified in the CUS 24
specification [2] for the O-DU LLS -C3 configuration should be met with the following set of elements in the 25
synchronization transfer path: 26
 a T-GM clock with the least demanding performance class (i.e. G.8272 PRTC/T-GM type PRTC-A);  27
 at least three interim network elements acting as T -BCs in the path, with the least demanding T -BC 28
performance class (i.e. G.8273.2 T-BC Class A); 29
 T-TSC implementation with the least demanding performance class (i.e. G.8273.2, Annex C, T-TSC Class A). 30
 31
The initial requirements as outlined above are conservative and , to some extent , take advantage of the expected 32
proximity of the synchronization sources with respect to the edge O-Cloud nodes. Also, if the O -RUs are synchronized 33
from the same sources, which is expected to be a typical case, O -RU rather than O -DU related synchronization 34
requirements may determine the performance classes of the PRTC/T-GM and T-BCs, which are on the synchronization 35
distribution path towards the O-RUs. 36
 G.8275.2 Partial Timing Support (PTS) 4.2.237
As specified in the CUS  specification [2], DU synchronization requirements a re more relaxed than the requirements 38
associated with the RU, and CUS specification [2] also allows separate PRTC sources to be used for the 39
synchronization of the RUs and DUs. The use of G.8275.2 Partial Timing Support is allowed for the synchronization of 40
O-Cloud nodes (and instantiated O -DU's), provided that the synchronization accuracy requirements for the O -DU as 41
specified in CUS specification [2] are met in such implementation. The detailed implementation of this configuration is 42
subject for the further study. 43
 Hardware Acceleration Requirements 4.344
The O-RAN Cloud Architecture document [1] discusses the hardware abstraction consider ations for the O-Cloud. It 45
lists two levels of abstraction referring to the deployment model and APIs , and  also discusse s the management and 46
orchestration considerations that should be supported by such hardware accelerators deployed on the cloud platform. 47
This section describes the types of acceleration as background infor mation and then specifies requirements applicable to 48
the O-Cloud for acceleration hardware abstraction. 49
                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ
17
 Background on Acceleration Types 4.3.11
Hardware acceleration for the managed functions, such as O-DU and O-CU, can be classified into two types: 2
 Interface acceleration: acceleration of I/O data transfers on any interface  between 3GPP or O -RAN network 3
functions [16]; 4
 Algorithm acceleration: acceleration of any algorithm processing within an O-RAN network function. 5
 6
Interface acceleration mostly exhibits as acceleration of network traffic  transmission and reception , for which SR -IOV 7
NICs, combined with software mechanisms such as DPDK, is commonly used. 8
Algorithm accelera tion includes acceleration of compute -intensive algorithm s in the O -DU and O -CU, utilizing 9
accelerator hardware such as FPGA, GPU, DSP, and ASIC. 10
 Acceleration for the O-DU 4.3.1.111
Figure 4-3 illustrates acceleration of the interfaces between the O -DU and other O -RAN network functions and 12
acceleration of the high-PHY algorithms within the O-DU. Table 4-6 shows in more detail the accelerated functions. 13
 14
Figure 4-3. O-DU interface and algorithm acceleration. 15
 16
Accelerated Function Interface Name Acceleration Type
I/O between O-DU and O-CU-CP F1-C Interface acceleration (optional)
I/O between O-DU and O-CU-UP F1-U Interface acceleration
I/O between O-DU and SMO O1 Interface acceleration (optional)
I/O between O-DU and Near-RT RIC E2 Interface acceleration (optional)
I/O between O-DU and O-RU FH7.2 Interface acceleration
High-PHY algorithms N/A Algorithm acceleration
Table 4-6. O-DU acceleration types. 17

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ
18
 Acceleration for the O-CU-CP 4.3.1.21
Figure 4-4 illustrates acceleration of the interfaces between the O-CU-CP and other O-RAN network functions and 2
acceleration of the security algorithms within the O-CU-CP. Table 4-7 shows in more detail the accelerated functions. 3
 4
Figure 4-4. O-CU-CP interface and algorithm acceleration. 5
 6
Accelerated Function Interface Name Acceleration Type
I/O between O-CU-CP and O-DU F1-C Interface acceleration (optional)
I/O between O-CU-CP and SMO O1 Interface acceleration (optional)
I/O between O-CU-CP and Near-RT RIC E2 Interface acceleration (optional)
I/O between O-CU-CP and O-CU-UP E1 Interface acceleration (optional)
I/O between O-CU-CP and O-CU-CP Xn Interface acceleration (optional)
I/O between O-CU-CP and O-CU X2 Interface acceleration (optional)
I/O between O-CU-CP and MME S1AP Interface acceleration (optional)
I/O between O-CU-CP and AMF NG-C Interface acceleration (optional)
Security algorithms N/A Algorithm acceleration
Table 4-7. O-CU-CP acceleration types. 7
 Acceleration for the O-CU-UP 4.3.1.38
Figure 4-5 illustrates acceleration of the interfaces between the O-CU-UP and other O-RAN network functions and 9
acceleration of the security algorithms within the O-CU-UP. Table 4-8 shows in more detail the accelerated functions. 10
 11

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ
19
 1
Figure 4-5. O-CU-UP interface and algorithm acceleration. 2
 3
Accelerated Function Interface Name Acceleration Type
I/O between O-CU-UP and O-DU F1-U Interface acceleration
I/O between O-CU-UP and SMO O1 Interface acceleration (optional)
I/O between O-CU-UP and Near-RT RIC E2 Interface acceleration (optional)
I/O between O-CU-UP and O-CU-CP E1 Interface acceleration (optional)
I/O between O-CU-UP and SAE-GW S1U Interface acceleration
I/O between O-CU-UP and UPF NG-U Interface acceleration
Security algorithms N/A Algorithm acceleration
Table 4-8. O-CU-UP acceleration types. 4
 Hardware Acceleration Abstraction Layer Requirements  4.3.25
The Acceleration Abstraction Layer (AAL) provides a set of APIs to VNF/CNF software  for offloading certain 6
functions that, for instance, are compute- and power-intensive, to hardware accelerators supported on the O-Cloud 7
platform. The objective of the AAL is to hide the different implementations of hardware accelerators and present APIs 8
to VNF/CNF applications to enable portability and software-and-hardware decoupling. Full listing of the functions that 9
can be offloaded through the AAL is outside the scope of this document. 10
 Generic Requirements  4.3.2.111
Extensibility  12
O-RAN has defined the functions that can be accelerated by the cloud platform based on 3GPP specifications and O-13
RAN deployment scenarios. However, the AAL API should not limit innovation of future implementations and should 14
evolve as the specification requires . In this way , the API sha ll be exten sible to accommodate future revisions of the 15
specification. 16

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ
20
Interrupt and Poll Mode 1
The API shall allow multiple design choices for application vendors and sh all not preclude a vendor from adopting an 2
interrupt-driven design or poll -mode design or any combination of both. As such, the API shall support both interrupt 3
mode and poll mode for the data-path application interface.   4
Discovery and Configuration 5
The API shall support application software to discover and configur e the accelerator hardware. The API shall allow an 6
application to discover what physical resources have been assigned to it from the upper layers and then to configure said 7
resources for offload operations.  8
Multiple Device Support  9
There may be scenarios where multiple hardware accelerators (either implementing the same acceleration function or 10
different) are assigned to a single application , which uses one or more of these accelerators as needed. The API shall 11
support an application using one or more accelerator devices at the same time, as Figure 4-6 illustrates. 12
 13
Figure 4-6. AAL support for multiple devices. 14
 API offload requirements 4.3.2.215
The API in supporting different implementations shall support different offload architectures including look -aside, 16
inline, and any combination of both.  17
Look-aside Acceleration Model  18
The API shall support look-aside acceleration model where the host CPU invokes an accelerator for data processing and 19
receives the result after processing is complete.  A look -aside architecture , illustrated in Figure 4-7, allows the 20
application to offload  work to a hardware accelerator and continue to perform other work in parallel—this could be to 21
continue to execute other software tasks in parallel or to sleep and wait for the accelerator hardware to complete. This 22
model requires the API to support two operations , one for initiating the offload and another for retrievin g the operation 23
once complete. 24

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ
21
 1
Figure 4-7. AAL look-aside acceleration model. 2
Inline Acceleration Model 3
The API shall support inline acceleration model where acceleration by function and I /O-based acceleration are 4
performed on the physical interface as the packet ingresses/egresses the platform. Figure 4-8 shows one possible  5
implementation of an inline acceleration model. 6
 7
Figure 4-8. AAL inline acceleration model. 8
In Figure 4-8, a “Func”  block refers to some algorithmic function that is accelerated in the device before the data 9
egresses or ingresses from the acceleration device . “Tx”  refers to the transmission of the  data from the acceleration 10
device to an Ethernet interface , while ‘Rx’ refers to the reception of data from the Ethernet interface to the acceleration 11
device.  12
While the look-aside architecture shall support dataflow from the CPU to the accelerator and back to the CPU before 13
being sent to the front-haul interface, the inline architecture shall support data flow from the CPU to the accelerator and 14
directly from the accelerator to the front-haul interface, instead of being sent back to the CPU. The typical data flows 15
for accelerating the O-DU high-PHY functions for the look-aside and inline architectures are as follows. 16
Look-aside architecture dataflow 17
CPU ↔ accelerator ↔ CPU ↔ front-haul: for a set of consecutive PHY functions offload (e.g., FEC) 18
CPU ↔ accelerator ↔ CPU ↔ accelerator ↔ …… ↔ CPU ↔ front-haul: for a set of non-consecutive PHY functions 19
offload 20
Inline architecture dataflow 21
CPU ↔ accelerator ↔ front-haul: for a set of consecutive PHY functions offload (up to the end of the PHY pipeline)  22

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ
22
Figure 4-9 illustrates one possible implementation of the look-aside and inline architectures. While a set of PHY-layer 1
functions are offloaded to the accelerator hardware for look-aside acceleration, the entire end-to-end high PHY pipeline 2
is offloaded to the accelerator for inline acceleration.  3
L
2
+
P
H
Y
F
H
O-DU
L2+ on
CPU
Func. 2 on
accelerator
L2+ on
CPU

RRU
RRU
Fronthaul
interface
Func. (n-1) on
accelerator
Look-aside
model
Inline
model
Func.
1
Func.
3
Func.
4
Func.
n
AAL
L1/L2
interface*
High PHY on CPU
High PHY on accelerator
Func.
n-1
Func.
2
Func.
3
Func.
1
Accelerator interface on CPU
Fronthaul
interface
I/O data flow PHY downlink
PHY uplink
L1/L2
interface*
Func.
n
*e.g. FAPI
...
...
 4
Figure 4-9. Dataflow paths in look-aside and inline acceleration architectures. 5
API Concurrency and Parallelism   6
To enable greater flexibility and design choice by application vendors, the API shall support multi -threading 7
environment allowing an application to offload acceleration requests in parallel from several threads.  8
Chapter 5 Cloud Platform Reference Design Example 9
Based on the requirements, this chapter specifies reference designs for the regional and edge clouds for both the cloud 10
platform hardware and software.  These reference designs are example s, not normative , and focus o n sub -6GHz 11
scenarios. Support for mmWave and quanti fication of capacities of these reference designs (such as the number of cells, 12
data rates, and so forth) will be further studied in the next release of this document. 13
 Cloud Platform Hardware Reference Design 5.114
This section specifies the cloud platform hardware  with a focus on server and accelerator hardware. These reference 15
designs are examples, not normative. 16
 Regional Cloud Server 5.1.117
Table 5-1 shows an example reference design for the regional cloud servers. 18
Processor 2x 20-core CPUs at 2.4 GHz, e.g., 2nd-generation Intel Xeon Scalable Processor
Memory 12x 32 GB, 2400 MHz TruDDR4
Expansion Slots 6x PCIe 3.0
                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ
23
Drive Bays 14x 3.5” or 24x 2.5” hot-swap bays or 24x NVMe bays
RAID support RAID 1, 12 Gb/s
Network
Interface
4x 10GbE/25GbE ports, 1 dedicated 1GbE management port , 1x Fortville 40GbE Ethernet PCIe
XL710-QDA2 Dual Port QSFP+ (PCI-E card direct to CPU 0)
Network link
aggregation
Enabled
Power 2x hot swap/redundant; support 220V/380V AC, 480V and -48V DC
Security TPM or TCM
Availability  Hot-swap fans/PSUs; 45℃ continuous operation; light path diagnostic LEDs; front-access
diagnostics via dedicated USB port
Manageability Support for IPMI, SNMP, and Redfish
Table 5-1. Regional cloud server example reference design. 1
 Edge Cloud Server 5.1.22
Table 5-2 shows an example reference design for the edge cloud servers. 3
Form factors Depth ≤ 450mm, height ≤ 2RU, width ≤ 19in
Processor 2nd-generation Intel Xeon Gold (e.g ., 6210U) or Xeon D-2187NT (16-core) or Xeon D -2177NT
(14-core); hyperthreading enabled for O-CU, disabled for O-DU
Note: the NT models have Intel QAT integrated with the CPU and can perform PDCP crypto
acceleration in the vO-CU.
Memory 12x 32 GB, 2400 MHz TruDDR4
Expansion Slots At least 2x half-height half-length (HHHL)  or 1x HHHL and 1x FHFL
Drive Bays 6x hot -swappable 2.5 -inch SATA or SAS drives ; 2x NVMe drives; all drives accessible from
front panel
RAID support RAID1
Network
Interface
4x 10GbE/25GbE ports, 1 dedicated 1GbE management port , 1x Fortville 40GbE Ethernet PCIe
XL710-QDA2 Dual Port QSFP+ (PCI-E card direct to CPU 0), network cabling at front panel
PSU  1+1 redundant
 Hot-swappable
 Support for 220V AC or -48V DC
 PSUs are  connected through the rear panel (AT&T, China Mobile) or front panel ( China
Telecom, DT, KDDI, Orange)
Temperature -5 °C ~ 55 °C (-10 °C to 60 °C preferred)
Security TPM or TCM
Availability  Hot-swap fans/PSUs; 45℃ continuous operation; light path diagnostic LEDs; front -access
diagnostics via dedicated USB port
Manageability Support for IPMI, SNMP, and Redfish
Table 5-2. Edge cloud server example reference design. 4
                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ
24
 BIOS/UEFI Configurations 5.1.31
Table 5-3 shows recommended settings for the various edge cloud deployment types.  2
BIOS Setting Description Regional
Cloud Settings
for Maximum
Performance
(with Turbo
mode enabled)
Regional &
Edge Cloud
with Real
Time
Edge Cloud
with Strict
Real Time
Configuration
for O-DU
System Power
and Performance
Policy
Favor high performance over low power
consumption for the entire system (e.g., CPU
and memory).
Optional Enabled  Enabled
Intel Hyper-
Threading
Technology (HT)
Enabling HT doubles the number of (logical)
CPU cores. But thread s on the same physical
core share  various core data  structures and
caches, causing performance interference. It is
recommended to enable HT to ensure high
system throughput, but for applications such
as O -DU in the edge cloud, carefully pin
latency-sensitive threads to dedicated cores to
ensure real-time performance.
Enabled  Optional   Optional
Prefetching Memory cache prefetching technologies allow
prefetching data and instructions into the
cache before they are used. In general,
prefetching reduces cache misses  and
increases application performance.
Enabled Enabled Enabled
Intel
virtualization
support (VT-x)
CPU hardware support for virtualization. Enabled Enabled Enabled
Intel VT for
direct I/O (VT-d)
Support for PCI-E passthrough and SRIOV. Enabled Enabled Enabled
Intel Turbo Boost
Technology
Enabling this feature allows individual CPU
cores to run at frequencies higher than the
base frequency when the processor has
sufficient thermal headroom. Frequent
transition between normal and turbo modes,
however, can cause non -predictive
performance and impact system real -time
characteristics.
Enabled  Disabled  Enabled
Enhanced Intel
SpeedStep
Technology
Enhanced SpeedStep is a series of dynamic
frequency scaling technologies built into
modern Intel microprocessors that allow the
clock speed of the processor to be
dynamically changed (to different P -states) by
software. This allows the processor to meet
instantaneous performance needs of the
operation being performed, while minimizing
power draw and heat generation. In general,
disabling this feature  avoids CPU frequency
changing dynamically and thus  facilitates
deterministic performance. Enabling this but
setting the configurable TDP level in the next
setting allows finer control on the frequency
that the CPU will run at.
Enabled  Disabled Enabled
Intel
Configurable
TDP
Nominal Nominal Level 2
                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ
25
Energy-efficient
Turbo
Transitions in and out of turbo mode incur
extra power and overly frequent transitions are
energy inefficient. This feature enables more
energy-efficient turbo operation.
Disabled Disabled  Disabled

Hardware P-
States
Disabling hardware-controlled P -states avoids
unpredicted frequency change and helps
achieve deterministic performance.
Disabled C0/C1 state C0/C1 state
Hardware PM
Interrupt
Disabled Disabled Disabled
EPP Enable Disabled Disabled Disabled
APS Rocketing Disabled Disabled Disabled
Scalability  Disabled Disabled Disabled
PP0-Budget Disabled Disabled Disabled
CPU C State
Control & C1E &
Processor C6
Disabling C -state transitions prevents CPU
cores from entering sleep states when there is
no task to execute. C -states that are often seen
in BIOS settings include C1E, C3, and C6
states. As C -state transitions introduce delay,
disabling them helps real-time performance,
though at the cost of higher power
consumption.
Enabled  Disabled Disabled
Direct Cache
Access (DCA) &
Data Direct I/O
(DDIO)
DCA can only reduce packet reception delay.
Another technology, DDIO, reduces delay for
both transmission  and rec eption. It is
recommended to enable DDIO instead of
DCA when both settings are present.
Enabled Enabled Enabled
Uncore
Frequency
Scaling
Allow the voltage and frequency of the uncore
to be programmed independently. Uncore
activities are monitored to optimize the
frequency in real time.
Enabled  Disabled Enabled

Memory RAS
and Performance
Configuration &
NUMA
Optimized
Enable ACPI tables that are required for
NUMA-aware operating systems.

Optional Optional Optional
Table 5-3. BIOS configurations for strong real-time systems. 1
 Firmware Versions 5.1.42
Table 5-4 shows firmware versions for the example reference design based on the Intel Server Board S2600WF family. 3
BIOS/IFWI SE5C620.86B.00.01.0013.030920180427
Baseboard Management Controller (BMC) 1.43.91f76955
Download link  https://downloadcenter.intel.com/download/27632/Intel-
Server-Board-S2600WF-Family-BIOS-and-Firmware-
Update-Package-for-UEFI-?product=89005
Table 5-4. Firmware for example reference design based on Intel server board S2600WF. 4
                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ
26
 Accelerator Hardware 5.1.51
For deployment scenario B  [1], both the O -DU and O -CU are deployed as VNF(s)  or CNF(s)  on the edge c loud 2
platform and the Near -RT RIC is deployed as a VNF  or CNF on the regional c loud platform. This section details the 3
hardware acceleration options for these VNFs/CNFs that the cloud platforms shall provide.  4
 Regional Cloud Platform 5.1.5.15
In deployment scenario B , the region al cloud platform provides  support for the Near -RT RIC. There is  currently no 6
hardware acceleration options required for the Near-RT RIC at this stage of definition but this may evolve in future.  7
 Edge Cloud Platform  5.1.5.28
The O-CU and O-DU may follow the software architecture as defined in the O-RAN WG8 specification [17]. 9
O-CU Acceleration 10
The O -CU, depending on performance r equirements, may choose to use hardware  acceleration (e.g., Intel QAT) for 11
wireless cipher processing, namely 128-EEA3, 128-EIA3, UEA2 and UIA2—depending on configurations. 12
O-DU Acceleration 13
The O-DU is assumed to use the 7.2 split with the front -end processing (FFT, PRACH) implemented in the O -RU. The 14
remaining O -DU functions are implemented as software  running on the edge cloud p latform. Depending on the 15
performance requirements , the compute -intense workload s may be accelerated in hardware  using look -aside 16
acceleration or inline acceleration or any combination of both. 17
Figure 5-1 shows a high -level architecture  of look -aside acceleration with proposed  Forward Error Correcting (FEC)  18
functions to be hardware-accelerated for the O-DU in deployment scenario B.  19

Figure 5-1. Hardware-accelerated FEC functions with look-aside model. 20
Figure 5-2 shows an example of an inline solution for the O -DU that accelerates the front-end compression and 21
decompression (e.g., Mu-Law and BFP) functions in accelerator hardware before sending to the O-RU.  22
 23
 24
Figure 5-2. Hardware-accelerated front-end functions with inline model. 25
Figure 5-3 shows a high -level architecture of inline acceleration with end-to-end high PHY functions to be hardware -26
accelerated for deployment scenario B. 27

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ
27
CRC
Generation +
segmentation
LDPC
Encoder
Rate
Matching Scrambler Mod.
Mapper Precoder RE
Mapper RRU
CRC
check
Downlink PDSCH pipeline
LDPC
Decoder
De-Rate
Matching
De-
Scrambler Demod. Channel
Estimator
MIMO
Equalizer
RE
Demapper RRU
Uplink PUSCH pipeline
GPU
R
L
C
M
A
C
R
L
C
M
A
C
L1/L2
interface
Layer
Mapper
CPU
 1
Figure 5-3. Hardware-accelerated end-to-end high-PHY functions with inline model. 2
 High-level Platform Architecture Examples  5.1.5.33
Based on the hardware acceleration options above, Figure 5-4 to Figure 5-6 illustrate the system -level platform 4
components required for the cloud platform to support  various acceleration architectures for  the O-DU and O -CU. A 5
cloud platform can also support a combination of accelerator functions, such as inline front-end acceleration and FEC 6
look-aside acceleration. Note that hardware acceleration for O -RAN network fu nctions is desired but a software-only 7
implementation is also possible.  8
Example 1: O-DU and O-CU look-aside acceleration  9
Figure 5-4 shows an example cloud  platform and its data  flows with look-aside acceleration for both the O-DU (FEC) 10
and O-CU (wireless cipher).  11

Figure 5-4. Edge cloud platform for look-aside acceleration of O-DU and O-CU.
Example 2: O-DU inline and O-CU look-aside acceleration
Figure 5-5 shows an example cloud  platform with mixed acceleration models, where the O -DU uses the
inline model for front-haul processing and the O-CU adopts look-aside acceleration for wireless ciphering.

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ
28

Figure 5-5. Edge cloud platform for O-DU inline acceleration and O-CU look-aside acceleration.
Example 3: O-DU end-to-end high-PHY inline acceleration and O-CU look-aside acceleration.  1
Figure 5-6 shows an example cloud platform, where the O-DU uses the inline model for the entire high-PHY processing 2
and the O-CU adopts look-aside acceleration for wireless ciphering. 3
 4
Figure 5-6. Edge cloud platform for O-DU inline acceleration of end-to-end high-PHY. 5
 Cloud Platform Software Reference Design 5.26
This section shows an example reference design for the regional and edge cloud platform software. A single installable 7
package supporting various profiles of the platform software runs in both the regional and edge clouds, and can adjust 8
to the different physical environments and scal e requirements. This document focuses on  a container-based cloud 9
platform design and will address VM-based designs in the next release. 10
Kubernetes is a common implementation for container orchestration and management . Platform infrastructure 11
management and additional components, such as Kubernetes plugins, must be added for hosting accelerated O-DU and 12
O-CU network functions. 13
The regional and edge cloud platform requirements also differ based on the network functions they host. Generally, the 14
edge cloud pla tform requirements are a superset of the functional requirements of the regional cloud platform. The 15
primary differences relate to hardware acceleration features, real -time Linux, precision time protocol and server node 16
scalability down to a single node for cell-site deployments. 17
This reference design focuses on deployment scenario B, LLS-C3 clock synchronization, and FPGA and GPU hardware 18
acceleration at the edge cloud. It defines two reference configuration profiles, for the regional cloud and edge cloud. 19

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ
29
 Common Reference Configurations for all Profiles 5.2.11
Table 5-5 lists the primary platform software components that are common across the two cloud platform reference 2
configuration profiles. A complete list of packages, version information and build recipes to create a cloud platform 3
meeting the requirements of Section  4.1.3 for container deployment are available in O -RAN SC INF project release B 4
(Bronze release). See [18] for further details. 5
Platform
Components
Description
Ansible Orchestration engine for automating configuration management and application deployment.
 ansible.com
Ceph Provides object storage, block storage and POSIX-compliant network file system. See ceph.io.
Collectd Collects system and application performance metrics and provides mechanisms to store values.
collectd.org
Dhcp Assigns IP address and other network information to hosts.
Dnsmasq Provides Domain Name System forwarder.
en.wikipedia.org/wiki/Dnsmasq
docker-distribution Docker registry for storing and distributing images.
github.com/docker/distribution
Drbd Replicated storage.
Etcd Distributed key-value store.
etcd.io
Helm Kubernetes package manager.
helm.sh
helm-charts Tool for managing charts. Charts are packages of pre-configured Kubernetes resources.
github.com/helm/charts
ima Detects if files have been accidentally or maliciously altered, both remotely and locally.
linux-ima.sourceforge.net
iptables Configures the tables from the Linux kernel firewall and the chains and rules it sores.
en.wikipedia.org/wiki/Iptables
Kubernetes Automates deployment, scaling and management of containerized applications.
kubernetes.io
mariadb Relational databases.
mariadb.org
nginx High-performance HTTP server, reverse proxy and load balancer.
pxe-network-
installer
Provides PXE (preboot execution environment) install server.
                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ
30
rabbit-mq Highly-available and scalable message broker.
rabbitmq.com
redfishtool Industry standard protocol providing a RESTful interface for the management of servers.
tpm2 Provides trusted platform module support.
Kubernetes
Components
Description
etcd Distributed key-value store used as Kubernetes backing store for cluster data.
etcd.io
coredns Kubernetes cluster DNS service.
armada Manages helm chart dependences.
calico Enables ne tworking and network policy in K ubernetes clusters. Calico uses a pure IP
networking fabric to provide high-performance networking.
nginx-ingress-
controller
Enables Kubernetes to configure nginx for load balancing Kubernetes based services
tiller Service that communicates with the Kubernetes API to manage helm packages.
Table 5-5. Common cloud platform reference configurations for all profiles. 1
 Regional Cloud Reference Configuration Profile 5.2.22
The regional edge cloud profile is a cluster -based implementation with two redundant cloud platform controller nodes 3
and worker node scal ability from one to 200 nodes. In addition to providing a cloud platform for the Near-RT RIC, the 4
regional cloud platform provides zero-touch deployment and provisioning and management of the edge clouds.  5
The SMO interfaces to the regional cloud platform for deployment of new edge clouds as well as updating and 6
upgrading of the edge cloud platform software. The regional controller functionality is provided b y the distributed cloud 7
component. The regional clou d platform requires the common components in Table 5-5, as well as the components  in 8
Table 5-6. 9
Components Description
distributed-cloud Provides centralized controller functions for management (e.g., lifecycle) of edge clouds.
Table 5-6. Regional cloud reference configuration profile. 10
 Edge Cloud Reference Configuration Profile 5.2.311
Based on the hardware accelerati on choice, the L inux kernel can be “standard” for in line GPU -based acceleration or 12
“real-time” for look -aside FPGA -based accelerat ion. See Section 5.2.4 for the real -time L inux kernel configuration 13
details. In addition, the edge cloud platform requires the common components in Table 5-5, as well as the components 14
in Table 5-7. 15
Platform
Components
Description
distributed-cloud-
client
Provides edge cloud client services for distributed cloud function.
e1000e-kmod E1000e driver for Intel® Gigabit NIC  (optional – leveraged only if specific Intel hardware
support is required).
                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ
31
gdrcopy Provides GPU memory copy library (optional – leveraged only if specific Nvidia hardware
support is required).
i40e-kmod I40e driver for Intel® 40G NIC  (optional – leveraged only if specific Intel hardware support
is required).
iavf-kmod Driver for Intel® Ethernet adaptive virtual function  (optional – leveraged only if specific
Intel hardware support is required).
integrity-kmod Kernel module for IMA
ixgbe-kmod Ixgbe driver for Intel® 82598 and 82599 based 10G NIC  (optional – leveraged only if
specific Intel hardware support is required).
ixgbevf-kmod Driver for Intel® Ethernet adaptive virtual function for 10 Gig NIC  (optional – leveraged
only if specific Intel hardware support is required).
libibverbs-31mlnx1-
ofed
Allows user -space processes to use RDMA verbs dire ctly (optional – leveraged only if
specific Mellanox hardware support is required).
mlnx-ofa-kernel Driver for Mellanox NICs  (optional – leveraged only if specific Mellanox hardware support
is required).
opae-intel-fpga-
driver-kmod
Open Programmable Acceleration Engine (OPAE) Kernel module for Intel FPGA.
qat17 Driver for Intel® QuickAssist Technology (optional – leveraged only if specific Intel
hardware support is required).
rdma-core-45mlnx1 Driver for Mellanox RDMA -based interconnect (optional – leveraged only if specific
Mellanox hardware support is required)
Linuxptp Provides Precision Time Protocol according to IEEE standard 1588 and telecom profiles
G.8265.1, G.8275.1 and G.8275.2
linuxptp.sourceforge.net
Kubernetes
Plugin/Operators
Description
Multus Multus is a Multi CNI plugin to support the Multi Networking feature in Kubernetes using
CRD-based network objects in Kubernetes.  The Multus plugin should be used on the master
on all nodes to enable the use of multiple network interfaces inside a K8 pod.
github.com/intel/multus-cni.
FPGA FPGA Device plugin for FPGA passthrough  (optional – leveraged only if specific Intel
hardware support is required).
github.com/intel/intel-device-plugins-for-kubernetes#fpga-device-plugin
QAT QAT Device plugin for Intel® QAT adapters  (optional – leveraged only if specific Intel
hardware support is required).
github.com/intel/intel-device-plugins-for-kubernetes/blob/master/cmd/qat_plugin/
SR-IOV The SR -IOV plugin is required to enable the assignment of VFs to containers. SR -IOV
provides a low-latency interface for both packet I/O and acceleration interfaces.
github.com/hustcat/sriov-cni
CPU Manager The CPU manager for Kubernetes can be used to pr ovide basic core affinity for VNFs and
CNFs. It is required to pin real - time threads to specific CPU cores in order to improve
performance and meet real-time latency.
                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ
32
github.com/intel/CPU-Manager-for-Kubernetes.
Node Feature
Discovery
The Node Feature D iscovery plugin enables discovering server hardware  node features for
Kubernetes. It detects hardware features available on each node in a Kubernetes cluster, and
advertises those features using node labels.
github.com/kubernetes-sigs/node-feature-discovery
Node Topology
Manager
Node Topology Manager can be used to support NUMA awareness and pinning via the
scheduler.
kubernetes.io/docs/tasks/administer-cluster/topology-manager/
GPU Operator The GPU operator manages NVIDIA GPU and automates tasks related to boostrapping GPU
enabled nodes (optional – leveraged only if specific Nvidia hardware support is required).
nvidia.github.io/gpu-operator/
Table 5-7 Edge Cloud Reference Configuration Profile 1
 Example Real-time Linux Configurations 5.2.42
Table 5-8 shows the reference configurations for the real-time Linux OS. 3
Linux Kernel  3.10.0-957.21.3.rt56.935.el7.x86_64
Host or Guest Linux
Kernel Parameters
rcu_nocbs=1-N rcu_nocb_poll
isolcpus=1-N irqaffinity=0 tsc=perfect
nohz_full=1-N idle=poll noswap
N is the total number of logical CPU cores.
Use the “tuna” tool, provided in some Linux
distributions, to configure run-time CPU
isolation and thread priorities, if permanent
isolation with isolcpus kernel option  is not
desired.
selinux=0 enforcing=0 Disable SELinux policies can help improve
latency but should only be done if the security
considerations are fully understood.
default_hugepagesz=1G hugepagesz=1G
hugepages=21
Use huge pages to minimize TLB miss
overhead; use 2MB huge pages when system
memory is limited.
usbcore.autosuspend=-1
nmi_watchdog=0 softlockup_panic=0
cgroup_disable=memory skew_tick=1
nosoftlockup
General configurations for real time.
kthread_cpus=0 Limit host kernel threads to the OS core. Used
also in guest assuming a pure user space
application design.
intel_pstate=disable
intel_idle.max_cstate=0
processor.max_cstate=1
Disabling P-states and l imiting C-states in the
kernel helps improve real time performance.
Host Linux GRUB
Parameters
intel_iommu=on iommu=pt
pci=pcie_bus_perf
Enable SR-IOV and optimal PCI performance.
Host and Guest
Tuning
Turn off graphical desktop and
unnecessary daemons in the system

If ext3 or ext4 file system is used,
consider using noatime in fstab

                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ
33
Host Kernel
Command Line
example
BOOT_IMAGE=/vmlinuz-3.10.0-957.21.3.rt56.935.el7.x86_64 root=/dev/mapper/centos -
root ro crashkernel=auto nomodeset rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet
hugepagesz=1G hugepages=35 default_hugepagesz=1G intel_iommu=on iommu=pt
LANG=en_GB.UTF-8 tboot=false biosdevname=0 usbcore.autosuspend= -1 selinux=0
enforcing=0 nmi_watchdog=0 softlockup_panic=0 cgroup _disable=memory skew_tick=1
isolcpus=1-N rcu_nocbs=1 -N kthread_cpus=0 irqaffinity=0 nohz_full=1 -N nopti clock=pit
no_timer_check pci=pcie_bus_perf clocksource=tsc tsc=perfect idle=poll
intel_pstate=disable intel_idle.max_cstate=0 processor.max_cstate=1 noh z=on mce=off
nosoftlockup
Table 5-8. Real-time Linux configurations. 1
Annex ZZZ: O-RAN Adopter License Agreement 2
BY DOWNLOADING, USING OR OTHERWISE ACCESSING ANY O -RAN SPECIFICATION, ADOPTER 3
AGREES TO THE TERMS OF THIS AGREEMENT. 4
This O -RAN Adopter License Agreement (the “Agreement”) is made by and between the O -RAN Alliance and the 5
entity that downloads, uses or otherwise accesses any O-RAN Specification, including its Affiliates (the “Adopter”). 6
This is a license agreement for entities who wish to adopt any O-RAN Specification. 7
Section 1: DEFINITIONS 8
1.1 “Affiliate” means an entity that directly or indirectly controls, is controlled by, or is under common control with 9
another entity, so long as such control exists. For the purpose of this Section, “Control” means beneficial ownership of 10
fifty (50%) percent or more of the voting stock or equity in an entity. 11
1.2 “Compliant Implementation” means any system, device, method or operation (whether implement ed in hardware, 12
software or combinations thereof) that fully conforms to a Final Specification. 13
1.3 “Adopter(s)” means all entities, who are not Members, Contributors or Academic Contributors, including their 14
Affiliates, who wish to download, use or otherwise access O-RAN Specifications. 15
1.4 “Minor Update” means an update or revision to an O -RAN Specification published by O -RAN Alliance that does 16
not add any significant new features or functionality and remains interoperable with the prior version of an O -RAN 17
Specification. The term “O-RAN Specifications” includes Minor Updates. 18
1.5 “Necessary Claims” means those claims of all present and future patents and patent applications, other than design 19
patents and design registrations, throughout the world, which ( i) are owned or otherwise licensable by a Member, 20
Contributor or Academic Contributor during the term of its Member, Contributor or Academic Contributorship; (ii) 21
such Member, Contributor or Academic Contributor has the right to grant a license without the  payment of 22
consideration to a third party; and (iii) are necessarily infringed by a Compliant Implementation (without considering 23
any Contributions not included in the Final Specification). A claim is necessarily infringed only when it is not possible 24
on technical (but not commercial) grounds, taking into account normal technical practice and the state of the art 25
generally available at the date any Final Specification was published by the O -RAN Alliance or the date the patent 26
claim first came into existence, whichever last occurred, to make, sell, lease, otherwise dispose of, repair, use or operate 27
a Compliant Implementation without infringing that claim. For the avoidance of doubt in exceptional cases where a 28
Final Specification can only be implemented by technical solutions, all of which infringe patent claims, all such patent 29
claims shall be considered Necessary Claims. 30
1.6 “Defensive Suspension” means for the purposes of any license grant pursuant to Section 3, Member, Contributor, 31
Academic Contributor, Adopter, or any of their Affiliates, may have the discretion to include in their license a term 32
allowing the licensor to suspend the license against a licensee who brings a patent infringement suit against the 33
licensing Member, Contributor, Academic Contributor, Adopter, or any of their Affiliates. 34
Section 2: COPYRIGHT LICENSE 35
2.1 Subject to the terms and conditions of this Agreement, O -RAN Alliance hereby grants to Adopter a nonexclusive, 36
nontransferable, irrevocable, non -sublicensable, worldwide copyright  license to obtain, use and modify O -RAN 37
                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ
34
Specifications, but not to further distribute such O -RAN Specification in any modified or unmodified way, solely in 1
furtherance of implementations of an ORAN 2
Specification. 3
2.2 Adopter shall not use O -RAN Specifications except as expressly set forth in this Agreement or in a separate written 4
agreement with O-RAN Alliance. 5
Section 3: FRAND LICENSE 6
3.1 Members, Contributors and Academic Contributors and their Affiliates are prepared to grant based on a separate 7
Patent License Agreement to each Adopter under Fair Reasonable And Non - Discriminatory (FRAND) terms and 8
conditions with or without compensation (royalties) a nonexclusive, non -transferable, irrevocable (but subject to 9
Defensive Suspension), non-sublicensable, worldwide patent license under their Necessary Claims to make, have made, 10
use, import, offer to sell, lease, sell and otherwise distribute Compliant Implementations; provided, however, that such 11
license shall not extend: (a) to any part or function of a prod uct in which a Compliant Implementation is incorporated 12
that is not itself part of the Compliant Implementation; or (b) to any Adopter if that Adopter is not making a reciprocal 13
grant to Members, Contributors and Academic Contributors, as set forth in Sect ion 3.3. For the avoidance of doubt, the 14
foregoing licensing commitment includes the distribution by the Adopter’s distributors and the use by the Adopter’s 15
customers of such licensed Compliant Implementations. 16
3.2 Notwithstanding the above, if any Member,  Contributor or Academic Contributor, Adopter or their Affiliates has 17
reserved the right to charge a FRAND royalty or other fee for its license of Necessary Claims to Adopter, then Adopter 18
is entitled to charge a FRAND royalty or other fee to such Member, Contributor or Academic Contributor, Adopter and 19
its Affiliates for its license of Necessary Claims to its licensees. 20
3.3 Adopter, on behalf of itself and its Affiliates, shall be prepared to grant based on a separate Patent License 21
Agreement to each Membe rs, Contributors, Academic Contributors, Adopters and their Affiliates under Fair 22
Reasonable And Non -Discriminatory (FRAND) terms and conditions with or without compensation (royalties) a 23
nonexclusive, non-transferable, irrevocable (but subject to Defensiv e Suspension), non-sublicensable, worldwide patent 24
license under their Necessary Claims to make, have made, use, import, offer to sell, lease, sell and otherwise distribute 25
Compliant Implementations; provided, however, that such license will not extend: (a ) to any part or function of a 26
product in which a Compliant Implementation is incorporated that is not itself part of the Compliant Implementation; or 27
(b) to any Members, Contributors, Academic Contributors, Adopters and their Affiliates that is not making  a reciprocal 28
grant to Adopter, as set forth in Section 3.1. For the avoidance of doubt, the foregoing licensing commitment includes 29
the distribution by the Members’, Contributors’, Academic Contributors’, Adopters’ and their Affiliates’ distributors 30
and the use by the Members’, Contributors’, Academic Contributors’, Adopters’ and their Affiliates’ customers of such 31
licensed Compliant Implementations. 32
Section 4: TERM AND TERMINATION 33
4.1 This Agreement shall remain in force, unless early terminated according to this Section 4. 34
4.2 O-RAN Alliance on behalf of its Members, Contributors and Academic Contributors may terminate this Agreement 35
if Adopter materially breaches this Agreement and does not cure or is not capable of curing such breach within thirty 36
(30) days after being given notice specifying the breach. 37
4.3 Sections 1, 3, 5 - 11 of this Agreement shall survive any termination of this Agreement. Under surviving Section 3, 38
after termination of this Agreement, Adopter will continue to grant licenses (a) to  entities who become Adopters after 39
the date of termination; and (b) for future versions of ORAN Specifications that are backwards compatible with the 40
version that was current as of the date of termination. 41
Section 5: CONFIDENTIALITY 42
Adopter will use the s ame care and discretion to avoid disclosure, publication, and dissemination of O -RAN 43
Specifications to third parties, as Adopter employs with its own confidential information, but no less than reasonable 44
care. Any disclosure by Adopter to its Affiliates, c ontractors and consultants should be subject to an obligation of 45
confidentiality at least as restrictive as those contained in this Section. The foregoing obligation shall not apply to any 46
information which is: (1) rightfully known by Adopter without any l imitation on use or disclosure prior to disclosure; 47
(2) publicly available through no fault of Adopter; (3) rightfully received without a duty of confidentiality; (4) disclosed 48
by O -RAN Alliance or a Member, Contributor or Academic Contributor to a third p arty without a duty of 49
                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ
35
confidentiality on such third party; (5) independently developed by Adopter; (6) disclosed pursuant to the order of a 1
court or other authorized governmental body, or as required by law, provided that Adopter provides reasonable prior  2
written notice to O -RAN Alliance, and cooperates with O -RAN Alliance and/or the applicable Member, Contributor or 3
Academic Contributor to have the opportunity to oppose any such order; or (7) disclosed by Adopter with O -RAN 4
Alliance’s prior written approval. 5
Section 6: INDEMNIFICATION 6
Adopter shall indemnify, defend, and hold harmless the O -RAN Alliance, its Members, Contributors or Academic 7
Contributors, and their employees, and agents and their respective successors, heirs and assigns (the “Indemnitees”) , 8
against any liability, damage, loss, or expense (including reasonable attorneys’ fees and expenses) incurred by or 9
imposed upon any of the Indemnitees in connection with any claims, suits, investigations, actions, demands or 10
judgments arising out of Adop ter’s use of the licensed O -RAN Specifications or Adopter’s commercialization of 11
products that comply with O-RAN Specifications. 12
Section 7: LIMITATIONS ON LIABILITY; NO WARRANTY 13
EXCEPT FOR BREACH OF CONFIDENTIALITY, ADOPTER’S BREACH OF SECTION 3, AND ADOPT ER’S 14
INDEMNIFICATION OBLIGATIONS, IN NO EVENT SHALL ANY PARTY BE LIABLE TO ANY OTHER 15
PARTY OR THIRD PARTY FOR ANY INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE OR CONSEQUENTIAL 16
DAMAGES RESULTING FROM ITS PERFORMANCE OR NON -PERFORMANCE UNDER THIS AGREEMENT, 17
IN EACH CASE WHETHER UNDER CONTRACT, TORT, WARRANTY, OR OTHERWISE, AND WHETHER OR 18
NOT SUCH PARTY HAD ADVANCE NOTICE OF THE POSSIBILITY OF SUCH DAMAGES. O -RAN 19
SPECIFICATIONS ARE PROVIDED “AS IS” WITH NO WARRANTIES OR CONDITIONS WHATSOEVER, 20
WHETHER EXPRESS, IMPLI ED, STATUTORY, OR OTHERWISE. THE O -RAN ALLIANCE AND THE 21
MEMBERS, CONTRIBUTORS OR ACADEMIC CONTRIBUTORS EXPRESSLY DISCLAIM ANY WARRANTY 22
OR CONDITION OF MERCHANTABILITY, SECURITY, SATISFACTORY QUALITY, NONINFRINGEMENT, 23
FITNESS FOR ANY PARTICULAR PURPOSE, ERR OR-FREE OPERATION, OR ANY WARRANTY OR 24
CONDITION FOR O-RAN SPECIFICATIONS. 25
Section 8: ASSIGNMENT 26
Adopter may not assign the Agreement or any of its rights or obligations under this Agreement or make any grants or 27
other sublicenses to this Agreement, except as expressly authorized hereunder, without having first received the prior, 28
written consent of the O -RAN Alliance, which consent may be withheld in O -RAN Alliance’s sole discretion. O -RAN 29
Alliance may freely assign this Agreement. 30
Section 9: THIRD-PARTY BENEFICIARY RIGHTS 31
Adopter acknowledges and agrees that Members, Contributors and Academic Contributors (including future Members, 32
Contributors and Academic Contributors) are entitled to rights as a third -party beneficiary under this Agreement, 33
including as licensees under Section 3. 34
Section 10: BINDING ON AFFILIATES 35
Execution of this Agreement by Adopter in its capacity as a legal entity or association constitutes that legal entity’s or 36
association’s agreement that its Affiliates are likewise bound to the ob ligations that are applicable to Adopter hereunder 37
and are also entitled to the benefits of the rights of Adopter hereunder. 38
Section 11: GENERAL 39
This Agreement is governed by the laws of Germany without regard to its conflict or choice of law provisions.  40
This Agreement constitutes the entire agreement between the parties as to its express subject matter and expressly 41
supersedes and replaces any prior or contemporaneous agreements between the parties, whether written or oral, relating 42
to the subject matter of this Agreement.  43
                                                                                                 O-RAN.WG6.CLOUD-REF-B-v01.00 TS

________________________________________________________________________________________________
Copyright © 2020 by the O-RAN Alliance.
Your use is subject to the terms of the O-RAN Adopter License Agreement in Annex ZZZ
36
Adopter, on behalf of itself and its Affiliates, agrees to comply at all times with all applicable laws, rules and 1
regulations with respect to its and its Affiliates’ performance under this Agreement, including without limitation, expor t 2
control and antitrust laws. Without limiting the generality of the foregoing, Adopter acknowledges that this Agreement 3
prohibits any communication that would violate the antitrust laws. 4
By execution hereof, no form of any partnership, joint venture or ot her special relationship is created between Adopter, 5
or O -RAN Alliance or its Members, Contributors or Academic Contributors. Except as expressly set forth in this 6
Agreement, no party is authorized to make any commitment on behalf of Adopter, or O -RAN Alliance or its Members, 7
Contributors or Academic Contributors. 8
In the event that any provision of this Agreement conflicts with governing law or if any provision is held to be null, 9
void or otherwise ineffective or invalid by a court of competent jurisdiction , (i) such provisions will be deemed stricken 10
from the contract, and (ii) the remaining terms, provisions, covenants and restrictions of this Agreement will remain in 11
full force and effect. 12
Any failure by a party or third party beneficiary to insist upon or enforce performance by another party of any of the 13
provisions of this Agreement or to exercise any rights or remedies under this  Agreement or otherwise by law shall not 14
be construed as a waiver or relinquishment to any extent of the other parties’ or th ird party beneficiary’s right to assert 15
or rely upon any such provision, right or remedy in that or any other instance; rather the same shall be and remain in full 16
force and effect. 17