O-RAN.TIFG.E2E-Test.0-v01.00
Technical Specification 
O-RAN ALLIANCE Test and Integration Focus Group
End-to-end Test Specification 
This is a re-published version of the attached final specification. 
For this re-published version, the prior versions of the IPR Policy will apply, except that the previous 
requirement for Adopters (as defined in the earlier IPR Policy) to agree to an O-RAN Adopter License 
Agreement to access and use Final Specifications shall no longer apply or be required for these Final 
Specifications after 1st July 2022.
The copying or incorporation into any other work of part or all of the material available in this 
specification in any form without the prior written permission of O-RAN ALLIANCE e.V.  is prohibited, 
save that you may print or download extracts of the material on this site for your personal use, or copy 
the material on this site for the purpose of sending to individual third parties for their information 
provided that you acknowledge O-RAN ALLIANCE as the source of the material and that you inform the 
third party that these conditions apply to them and that they must comply with them.

Page 1 
 
1 
  O-RAN.TIFG.E2E-Test.0-v01.00 
Technical Specification 
 
 
 
O-RAN ALLIANCE Test and Integration Focus Group  
 
End-to-end Test Specification 
  
 
 
  
Copyright © 2021 by O-RAN ALLIANCE e.V.  
 
By using, accessing or downloading any part of this O-RAN specification document, including by copying, saving, distributing, 
displaying or preparing derivatives of, you agree to be and are bound to the terms of the O -RAN Adopter License Agreement 
contained in the Annex ZZZ of this specification. All other rights reserved. 
 
O-RAN ALLIANCE e.V. 
Buschkauler Weg 27, 53347 Alfter, Germany 
Register of Associations, Bonn VR 11238 
VAT ID DE321720189 
Copyright © 2021 by the O-RAN ALLIANCE e.V. 
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ. 
                      


  
Page 2 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
Revision History 1 
Date Revision Description 
2020.03.06 01.00 Initial version 
   
   
   
   
   

  
Page 3 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
Contents 1 
Revision History ................................ ................................ ................................ ................................ ........... 2 2 
 Introductory Material ................................ ................................ ................................ ........... 9 3 
1.1 Scope of the document ................................ ................................ ................................ ................................ . 9 4 
1.2 References ................................ ................................ ................................ ................................ ...................  9 5 
1.3 Definitions and Abbreviations ................................ ................................ ................................ ....................  10 6 
1.3.1 Definitions ................................ ................................ ................................ ................................ ............  10 7 
1.3.2 Abbreviations ................................ ................................ ................................ ................................ ....... 11 8 
1.4 Revision Guideline ................................ ................................ ................................ ................................ ..... 12 9 
 Overview ................................ ................................ ................................ ...........................  13 10 
 Testing methodology and configuration ................................ ................................ ..............  14 11 
3.1 System under test ................................ ................................ ................................ ................................ ....... 15 12 
3.2 Test and measurement equipment and tools ................................ ................................ ................................  16 13 
3.3 Test report ................................ ................................ ................................ ................................ .................  17 14 
3.4 Data traffic ................................ ................................ ................................ ................................ .................  18 15 
3.5 Mobility Classes ................................ ................................ ................................ ................................ ........ 19 16 
3.6 Radio conditions ................................ ................................ ................................ ................................ ........ 20 17 
3.7 Inter-cell interference ................................ ................................ ................................ ................................ . 21 18 
3.8 Spectral efficiency................................ ................................ ................................ ................................ ...... 22 19 
 Functional tests ................................ ................................ ................................ ..................  23 20 
4.1 LTE/5G NSA attach and detach of single UE................................ ................................ ..............................  23 21 
4.1.1 Test description and applicability ................................ ................................ ................................ .......... 23 22 
4.1.2 Test setup and configuration ................................ ................................ ................................ .................  24 23 
4.1.3 Test Procedure ................................ ................................ ................................ ................................ ...... 24 24 
4.1.4 Test requirements (expected results) ................................ ................................ ................................ ...... 25 25 
4.2 LTE/5G NSA attach and detach of multiple UEs ................................ ................................ ........................  26 26 
4.2.1 Test description and applicability ................................ ................................ ................................ .......... 26 27 
4.2.2 Test setup and configuration ................................ ................................ ................................ .................  26 28 
4.2.3 Test Procedure ................................ ................................ ................................ ................................ ...... 27 29 
4.2.4 Test requirements (expected results) ................................ ................................ ................................ ...... 27 30 
4.3 5G SA registration and deregistration of single UE ................................ ................................ .....................  28 31 
4.3.1 Test description and applicability ................................ ................................ ................................ .......... 28 32 
4.3.2 Test setup and configuration ................................ ................................ ................................ .................  29 33 
4.3.3 Test Procedure ................................ ................................ ................................ ................................ ...... 29 34 
4.3.4 Test requirements (expected results) ................................ ................................ ................................ ...... 30 35 
4.4 Intra-O-DU mobility ................................ ................................ ................................ ................................ .. 30 36 
4.4.1 Test description and applicability ................................ ................................ ................................ .......... 30 37 
4.4.2 Test setup and configuration ................................ ................................ ................................ .................  31 38 
4.4.3 Test Procedure ................................ ................................ ................................ ................................ ...... 32 39 
4.4.4 Test requirements (expected results) ................................ ................................ ................................ ...... 32 40 
4.5 Inter-O-DU mobility ................................ ................................ ................................ ................................ .. 33 41 
4.5.1 Test description and applicability ................................ ................................ ................................ .......... 33 42 
4.5.2 Test setup and configuration ................................ ................................ ................................ .................  33 43 
4.5.3 Test Procedure ................................ ................................ ................................ ................................ ...... 35 44 
4.5.4 Test requirements (expected results) ................................ ................................ ................................ ...... 35 45 
4.6 Inter-O-CU mobility................................ ................................ ................................ ................................ ... 36 46 
4.6.1 Test description and applicability ................................ ................................ ................................ .......... 36 47 
4.6.2 Test setup and configuration ................................ ................................ ................................ .................  36 48 

  
Page 4 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
4.6.3 Test Procedure ................................ ................................ ................................ ................................ ...... 37 1 
4.6.4 Test requirements (expected results) ................................ ................................ ................................ ...... 38 2 
 Performance tests ................................ ................................ ................................ ...............  39 3 
5.1 Expected throughput calculation ................................ ................................ ................................ .................  40 4 
5.1.1 4G LTE ................................ ................................ ................................ ................................ ................  40 5 
5.1.2 5G NR ................................ ................................ ................................ ................................ ..................  41 6 
5.2 Downlink peak throughput ................................ ................................ ................................ .........................  43 7 
5.2.1 Test description and applicability ................................ ................................ ................................ .......... 43 8 
5.2.2 Test setup and configuration ................................ ................................ ................................ .................  43 9 
5.2.3 Test Procedure ................................ ................................ ................................ ................................ ...... 44 10 
5.2.4 Test requirements (expected results) ................................ ................................ ................................ ...... 44 11 
5.3 Uplink peak throughput ................................ ................................ ................................ ..............................  46 12 
5.3.1 Test description and applicability ................................ ................................ ................................ .......... 46 13 
5.3.2 Test setup and configuration ................................ ................................ ................................ .................  46 14 
5.3.3 Test Procedure ................................ ................................ ................................ ................................ ...... 46 15 
5.3.4 Test requirements (expected results) ................................ ................................ ................................ ...... 47 16 
5.4 Downlink throughput in different radio conditions ................................ ................................ ......................  48 17 
5.4.1 Test description and applicability ................................ ................................ ................................ .......... 48 18 
5.4.2 Test setup and configuration ................................ ................................ ................................ .................  48 19 
5.4.3 Test Procedure ................................ ................................ ................................ ................................ ...... 49 20 
5.4.4 Test requirements (expected results) ................................ ................................ ................................ ...... 49 21 
5.5 Uplink throughput in different radio conditions................................ ................................ ...........................  50 22 
5.5.1 Test description and applicability ................................ ................................ ................................ .......... 50 23 
5.5.2 Test setup and configuration ................................ ................................ ................................ .................  50 24 
5.5.3 Test Procedure ................................ ................................ ................................ ................................ ...... 51 25 
5.5.4 Test requirements (expected results) ................................ ................................ ................................ ...... 51 26 
5.6 Bidirectional throughput in different radio conditions ................................ ................................ .................  53 27 
5.6.1 Test description and applicability ................................ ................................ ................................ .......... 53 28 
5.6.2 Test setup and configuration ................................ ................................ ................................ .................  53 29 
5.6.3 Test Procedure ................................ ................................ ................................ ................................ ...... 53 30 
5.6.4 Test requirements (expected results) ................................ ................................ ................................ ...... 54 31 
5.7 Downlink coverage throughput (link budget) ................................ ................................ ..............................  56 32 
5.7.1 Test description and applicability ................................ ................................ ................................ .......... 56 33 
5.7.2 Test setup and configuration ................................ ................................ ................................ .................  56 34 
5.7.3 Test Procedure ................................ ................................ ................................ ................................ ...... 57 35 
5.7.4 Test requirements (expected results) ................................ ................................ ................................ ...... 57 36 
5.8 Uplink coverage throughput (link budget) ................................ ................................ ................................ ... 58 37 
5.8.1 Test description and applicability ................................ ................................ ................................ .......... 58 38 
5.8.2 Test setup and configuration ................................ ................................ ................................ .................  59 39 
5.8.3 Test Procedure ................................ ................................ ................................ ................................ ...... 59 40 
5.8.4 Test requirements (expected results) ................................ ................................ ................................ ...... 60 41 
 Services ................................ ................................ ................................ .............................  62 42 
6.1 Data Services ................................ ................................ ................................ ................................ .............  62 43 
6.1.1 Web Browsing ................................ ................................ ................................ ................................ ...... 63 44 
6.1.1.1 Test Description ................................ ................................ ................................ ..............................  63 45 
6.1.1.2 Test Setup ................................ ................................ ................................ ................................ ....... 63 46 
6.1.1.3 Test Methodology/Procedure ................................ ................................ ................................ ...........  64 47 
6.1.1.4 Test Expectation (expected results) ................................ ................................ ................................ .. 64 48 
6.1.2 File upload/download................................ ................................ ................................ ............................  66 49 
6.1.2.1 Test Description ................................ ................................ ................................ ..............................  66 50 
6.1.2.2 Test Setup ................................ ................................ ................................ ................................ ....... 66 51 

  
Page 5 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
6.1.2.3 Test Methodology/Procedure ................................ ................................ ................................ ...........  67 1 
6.1.2.4 Test Expectation (expected results) ................................ ................................ ................................ .. 67 2 
6.2 Video Streaming ................................ ................................ ................................ ................................ ........ 69 3 
6.2.1 Video Streaming – Stationary Test ................................ ................................ ................................ ........ 70 4 
6.2.1.1 Test Description ................................ ................................ ................................ ..............................  70 5 
6.2.1.2 Test Setup ................................ ................................ ................................ ................................ ....... 70 6 
6.2.1.3 Test Methodology/Procedure ................................ ................................ ................................ ...........  71 7 
6.2.1.4 Test Expectation (expected results) ................................ ................................ ................................ .. 71 8 
6.2.2 Video Streaming – Handover between same Master eNB but different O-RUs – Intra O-DU ..................  73 9 
6.2.2.1 Test Description ................................ ................................ ................................ ..............................  73 10 
6.2.2.2 Test Setup ................................ ................................ ................................ ................................ ....... 73 11 
6.2.2.3 Test Methodology/Procedure ................................ ................................ ................................ ...........  74 12 
6.2.2.4 Test Expectation (expected results) ................................ ................................ ................................ .. 74 13 
6.2.3 Video Streaming – Handover between same MeNB but different O-DUs – Inter O-DU Intra O-CU ....... 76 14 
6.2.3.1 Test Description ................................ ................................ ................................ ..............................  76 15 
6.2.3.2 Test Setup ................................ ................................ ................................ ................................ ....... 76 16 
6.2.3.3 Test Methodology/Procedure ................................ ................................ ................................ ...........  77 17 
6.2.3.4 Test Expectation (expected results) ................................ ................................ ................................ .. 77 18 
6.2.4 Video Streaming – Handover between same MeNB but different O-CUs – Inter O-CU ..........................  78 19 
6.2.4.1 Test Description ................................ ................................ ................................ ..............................  79 20 
6.2.4.2 Test Setup ................................ ................................ ................................ ................................ ....... 79 21 
6.2.4.3 Test Methodology/Procedure ................................ ................................ ................................ ...........  79 22 
6.2.4.4 Test Expectation (expected results) ................................ ................................ ................................ .. 80 23 
6.2.5 Video Streaming – Handover between different MeNB while staying connected to same SgNB .............  81 24 
6.2.5.1 Test Description ................................ ................................ ................................ ..............................  81 25 
6.2.5.2 Test Setup ................................ ................................ ................................ ................................ ....... 82 26 
6.2.5.3 Test Methodology/Procedure ................................ ................................ ................................ ...........  82 27 
6.2.5.4 Test Expectation (expected results) ................................ ................................ ................................ .. 83 28 
6.3 Voice Services – Voice over LTE (VoLTE) ................................ ................................ ................................  84 29 
6.3.1 VoLTE Stationary Test ................................ ................................ ................................ .........................  85 30 
6.3.1.1 Test Description ................................ ................................ ................................ ..............................  85 31 
6.3.1.2 Test Setup ................................ ................................ ................................ ................................ ....... 85 32 
6.3.1.3 Test Methodology/Procedure ................................ ................................ ................................ ...........  86 33 
6.3.1.4 Test Expectation (expected results) ................................ ................................ ................................ .. 86 34 
6.3.2 VoLTE Handover Test ................................ ................................ ................................ ..........................  88 35 
6.3.3 Voice Service - LTE and NR handover tests ................................ ................................ ..........................  88 36 
6.3.3.1 Test Description ................................ ................................ ................................ ..............................  88 37 
6.3.3.2 Test Setup ................................ ................................ ................................ ................................ ....... 88 38 
6.3.3.3 Test Methodology/Procedure ................................ ................................ ................................ ...........  89 39 
6.3.3.4 Test Expectation (expected results) ................................ ................................ ................................ .. 90 40 
6.4 Voice Service – EPS Fallback ................................ ................................ ................................ ....................  91 41 
6.4.1 EPS Fallback Test ................................ ................................ ................................ ................................ . 92 42 
6.4.1.1 Test Description ................................ ................................ ................................ ..............................  92 43 
6.4.1.2 Test Setup ................................ ................................ ................................ ................................ ....... 93 44 
6.4.1.3 Test Methodology/Procedure ................................ ................................ ................................ ...........  93 45 
6.4.1.4 Test Expectation (expected results) ................................ ................................ ................................ .. 94 46 
6.5 Voice Service – Voice over NR (VoNR)................................ ................................ ................................ ..... 96 47 
6.5.1 Voice over NR Test ................................ ................................ ................................ ..............................  97 48 
6.5.1.1 Test Description ................................ ................................ ................................ ..............................  97 49 
6.5.1.2 Test Setup ................................ ................................ ................................ ................................ ....... 97 50 
6.5.1.3 Test Methodology/Procedure ................................ ................................ ................................ ...........  98 51 
6.5.1.4 Test Expectation (expected results) ................................ ................................ ................................ .. 98 52 

  
Page 6 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
6.5.2 VoNR – Intra-Distributed Unit (O-DU) handover ................................ ................................ ..................  99 1 
6.5.2.1 Test Description ................................ ................................ ................................ ............................  100 2 
6.5.2.2 Test Setup ................................ ................................ ................................ ................................ ..... 100 3 
6.5.2.3 Test Methodology/Procedure ................................ ................................ ................................ ......... 100 4 
6.5.2.4 Test Expectation (expected results) ................................ ................................ ................................  101 5 
6.5.3 VoNR – Intra-Central Unit (O-CU) Inter-Distributed Unit (O-DU) handover ................................ ....... 102 6 
6.5.3.1 Test Description ................................ ................................ ................................ ............................  102 7 
6.5.3.2 Test Setup ................................ ................................ ................................ ................................ ..... 103 8 
6.5.3.3 Test Methodology/Procedure ................................ ................................ ................................ ......... 103 9 
6.5.3.4 Test Expectation (expected results) ................................ ................................ ................................  104 10 
6.5.4 VoNR – Inter-Central Unit (O-CU) handover ................................ ................................ ......................  105 11 
6.5.4.1 Test Description ................................ ................................ ................................ ............................  105 12 
6.5.4.2 Test Setup ................................ ................................ ................................ ................................ ..... 105 13 
6.5.4.3 Test Methodology/Procedure ................................ ................................ ................................ ......... 106 14 
6.5.4.4 Test Expectation (expected results) ................................ ................................ ................................  107 15 
6.6 Video Service – Video over LTE (ViLTE) ................................ ................................ ................................  108 16 
6.6.1 ViLTE Stationary Test ................................ ................................ ................................ ........................  109 17 
6.6.1.1 Test Description ................................ ................................ ................................ ............................  109 18 
6.6.1.2 Test Setup ................................ ................................ ................................ ................................ ..... 109 19 
6.6.1.3 Test Methodology/Procedure ................................ ................................ ................................ ......... 110 20 
6.6.1.4 Test Expectation (expected results) ................................ ................................ ................................  111 21 
6.6.2 ViLTE Handover Test................................ ................................ ................................ .........................  112 22 
6.6.3 ViLTE - LTE to NR handover test ................................ ................................ ................................ ...... 112 23 
6.6.3.1 Test Description ................................ ................................ ................................ ............................  112 24 
6.6.3.2 Test Setup ................................ ................................ ................................ ................................ ..... 113 25 
6.6.3.3 Test Methodology/Procedure ................................ ................................ ................................ ......... 113 26 
6.6.3.4 Test Expectation (expected results) ................................ ................................ ................................  114 27 
6.7 Video Service – EPS Fallback ................................ ................................ ................................ ..................  116 28 
6.7.1 Video Service – EPS Fallback testing ................................ ................................ ................................ .. 117 29 
6.7.1.1 Test Description ................................ ................................ ................................ ............................  117 30 
6.7.1.2 Test Setup ................................ ................................ ................................ ................................ ..... 117 31 
6.7.1.3 Test Methodology/Procedure ................................ ................................ ................................ ......... 118 32 
6.7.1.4 Test Expectation (expected results) ................................ ................................ ................................  118 33 
6.8 Video Service – Video over NR................................ ................................ ................................ ................  120 34 
6.8.1 Video over NR – Stationary Testing ................................ ................................ ................................ .... 121 35 
6.8.1.1 Test Description ................................ ................................ ................................ ............................  121 36 
6.8.1.2 Test Setup ................................ ................................ ................................ ................................ ..... 122 37 
6.8.1.3 Test Methodology/Procedure ................................ ................................ ................................ ......... 122 38 
6.8.1.4 Test Expectation (expected results) ................................ ................................ ................................  123 39 
6.8.2 Video over NR – Intra-Distributed Unit (DU) handover ................................ ................................ ....... 124 40 
6.8.2.1 Test Description ................................ ................................ ................................ ............................  124 41 
6.8.2.2 Test Setup ................................ ................................ ................................ ................................ ..... 124 42 
6.8.2.3 Test Methodology/Procedure ................................ ................................ ................................ ......... 125 43 
6.8.2.4 Test Expectation (expected results) ................................ ................................ ................................  125 44 
6.8.3 Video over NR – Intra-Central Unit (CU) Inter-Distributed Unit (DU) handover ................................ .. 127 45 
6.8.3.1 Test Description ................................ ................................ ................................ ............................  127 46 
6.8.3.2 Test Setup ................................ ................................ ................................ ................................ ..... 127 47 
6.8.3.3 Test Methodology/Procedure ................................ ................................ ................................ ......... 128 48 
6.8.3.4 Test Expectation (expected results) ................................ ................................ ................................  128 49 
6.8.4 Video over NR – Intra-Central Unit (CU) handover ................................ ................................ .............  130 50 
6.8.4.1 Test Description ................................ ................................ ................................ ............................  130 51 
6.8.4.2 Test Setup ................................ ................................ ................................ ................................ ..... 130 52 

  
Page 7 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
6.8.4.3 Test Methodology/Procedure ................................ ................................ ................................ ......... 131 1 
6.8.4.4 Test Expectation (expected results) ................................ ................................ ................................  131 2 
 Security................................ ................................ ................................ ............................  134 3 
7.1 gNB Security Assurance Specification (SCAS) required by 3GPP SA3 ................................ .....................  134 4 
7.2 Optional: DoS, fuzzing and blind exploitation types of security test ................................ ..........................  135 5 
7.2.1 S-Plane PTP DoS Attack (Network layer) ................................ ................................ ...........................  136 6 
7.2.1.1 Test description & applicability ................................ ................................ ................................ ..... 136 7 
7.2.1.2 Test setup and configuration ................................ ................................ ................................ .......... 136 8 
7.2.1.3 Test procedure ................................ ................................ ................................ ...............................  136 9 
7.2.1.4 Test requirements (expected results) ................................ ................................ ..............................  137 10 
7.2.2 C-Plane eCPRI DoS Attack (Network layer) ................................ ................................ .......................  137 11 
7.2.2.1 Test description & applicability ................................ ................................ ................................ ..... 137 12 
7.2.2.2 Test setup and configuration ................................ ................................ ................................ .......... 137 13 
7.2.2.3 Test procedure ................................ ................................ ................................ ...............................  137 14 
7.2.2.4 Test requirements (expected results) ................................ ................................ ..............................  138 15 
7.2.3 Near-RT RIC A1 Interface DoS Attack (Network layer) ................................ ................................ ...... 138 16 
7.2.3.1 Test description & applicability ................................ ................................ ................................ ..... 138 17 
7.2.3.2 Test setup and configuration ................................ ................................ ................................ .......... 138 18 
7.2.3.3 Test procedure ................................ ................................ ................................ ...............................  138 19 
7.2.3.4 Test requirements (expected results) ................................ ................................ ..............................  139 20 
7.2.4 S-Plane PTP Unexpected Input (Network layer) ................................ ................................ ..................  139 21 
7.2.4.1 Test description & applicability ................................ ................................ ................................ ..... 139 22 
7.2.4.2 Test setup and configuration ................................ ................................ ................................ .......... 139 23 
7.2.4.3 Test procedure ................................ ................................ ................................ ...............................  139 24 
7.2.4.4 Test requirements (expected results) ................................ ................................ ..............................  140 25 
7.2.5 C-Plane eCPRI Unexpected Input (Network layer) ................................ ................................ ..............  140 26 
7.2.5.1 Test description & applicability ................................ ................................ ................................ ..... 140 27 
7.2.5.2 Test setup and configuration ................................ ................................ ................................ .......... 140 28 
7.2.5.3 Test procedure ................................ ................................ ................................ ...............................  140 29 
7.2.5.4 Test requirements (expected results) ................................ ................................ ..............................  141 30 
7.2.6 Near-RT RIC A1 Interface Unexpected Input (Network layer) ................................ .............................  141 31 
7.2.6.1 Test description & applicability ................................ ................................ ................................ ..... 141 32 
7.2.6.2 Test setup and configuration ................................ ................................ ................................ .......... 141 33 
7.2.6.3 Test procedure ................................ ................................ ................................ ...............................  141 34 
7.2.6.4 Test requirements (expected results) ................................ ................................ ..............................  142 35 
7.2.7 Blind exploitation of well-known vulnerabilities over Near-RT RIC A1 interface (Network layer) ....... 142 36 
7.2.7.1 Test description & applicability ................................ ................................ ................................ ..... 142 37 
7.2.7.2 Test setup and configuration ................................ ................................ ................................ .......... 142 38 
7.2.7.3 Test procedure ................................ ................................ ................................ ...............................  142 39 
7.2.7.4 Test requirements (expected results) ................................ ................................ ..............................  143 40 
7.3 O-Cloud resource exhaustion type of security test (Virtualization layer) ................................ ....................  143 41 
7.3.1 O-Cloud side-channel DoS attack ................................ ................................ ................................ ........ 143 42 
7.3.1.1 Test description & applicability ................................ ................................ ................................ ..... 143 43 
7.3.1.2 Test setup and configuration ................................ ................................ ................................ .......... 143 44 
7.3.1.3 Test procedure ................................ ................................ ................................ ...............................  144 45 
7.3.1.4 Test requirements (expected results) ................................ ................................ ..............................  144 46 
Annex A: Template of test report ................................ ................................ ................................ ..............  145 47 
Annex ZZZ: O-RAN Adopter License Agreement ................................ ................................ ....................  148 48 
Section 1: DEFINITIONS ................................ ................................ ................................ ................................ ........ 148 49 
Section 2: COPYRIGHT LICENSE ................................ ................................ ................................ ..........................  148 50 

  
Page 8 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
Section 3: FRAND LICENSE................................ ................................ ................................ ................................ ... 149 1 
Section 4: TERM AND TERMINATION ................................ ................................ ................................ .................  149 2 
Section 5: CONFIDENTIALITY ................................ ................................ ................................ ..............................  149 3 
Section 6: INDEMNIFICATION ................................ ................................ ................................ ..............................  150 4 
Section 7: LIMITATIONS ON LIABILITY; NO WARRANTY ................................ ................................ ...............  150 5 
Section 8: ASSIGNMENT ................................ ................................ ................................ ................................ ....... 150 6 
Section 9: THIRD-PARTY BENEFICIARY RIGHTS ................................ ................................ ..............................  150 7 
Section 10: BINDING ON AFFILIATES ................................ ................................ ................................ .................  150 8 
Section 11: GENERAL ................................ ................................ ................................ ................................ ............  150 9 
 10 

  
Page 9 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
 Introductory Material 1 
1.1 Scope of the document 2 
The present document has been produced by the O-RAN ALLIANCE (O-RAN). 3 
The present document provides description of the End-to-end (E2E) System Tests. 4 
1.2 References 5 
The following documents contain provisions which, through reference in this text, constitute provisions of the present 6 
document. 7 
- References are either specific (identified by date of publication, edition numbe r, version number, e tc.) or 8 
non-specific. 9 
- For a specific reference, subsequent revisions do not apply. 10 
- For a non-specific reference, the latest version applies. In the case of a reference to a 3GPP document (including 11 
a GSM document), a non-specific reference implicitly refers to the latest version of that document in Release 15. 12 
[1] 3GPP TR 21.905: “Vocabulary for 3GPP Specifications” 13 
[2] 3GPP TR 36.104, “Evolved Universal Terrestrial Radio Access (E-UTRA); Base Station (BS) radio 14 
transmission and reception” 15 
[3] NGMN Alliance, “Definition of the testing framework for the NGMN 5G pre-commercial networks trials”,  16 
White paper July 2019, version 3.0. Available: http://www.ngmn.org 17 
[4] NGM Alliance, “NGMN 5G pre-commercial networks trials - major conclusions”, White paper, December 18 
2019, version 1.0. Available: http://www.ngmn.org 19 
[5] 3GPP TR 38.913: “Study on new radio access technology Radio interface protocol aspects”, March 2017. 20 
[6] IETF RFC7323, “TCP Extensions for High Performance”, September 2014 21 
[7] 3GPP TS 38.215, “Physical layer measurements”, September 2020  22 
[8] 3GPP TS 36.133, “Requirements for support of radio resource management” 23 
[9] 3GPP TS 38.133, “Requirements for support of radio resource management” 24 
[10] O-RAN ALLIANCE, “O-RAN Architecture Description v2.0”, July 2020  25 
[11] O-RAN ALLIANCE, “O-RAN End-to-End System Testing Framework Specification 1.0”, July 2020 26 
[12] O-RAN ALLIANCE, “O-RAN Fronthaul Interoperability Test Specification (IOT) 2.0”, April 2020  27 
[13] ITU-R M.2410-0, “Minimum requirements related to technical performance for IMT-2020 radio 28 
interface(s)”, November 2017.  29 
[14] 3GPP TR 36.814, “Further advancements for E-UTRA physical layer aspects”, March 2017  30 
[15] 3GPP TS 38.306, "User Equipment (UE) radio access capabilities", December 2020 31 
[16] O-RAN ALLIANCE, “O-RAN Fronthaul Control, User and Synchronization Plane Specification v5.0”, 32 
November 2020 33 
[17] 3GPP TS 38.300, “NR and NG-RAN Overall Description; Stage 2”, December 2020 34 
[18] 3GPP TS 38.101-1, “User Equipment (UE) radio transmission and reception; Part 1: Range 1 Standalone”, 35 
December 2020 36 

  
Page 10 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
[19] 3GPP TS 38.101-2, “User Equipment (UE) radio transmission and reception; Part 2: Range 2 Standalone”, 1 
December 2020 2 
[20] 3GPP TS 38.211, “Physical channels and modulation”, December 2020 3 
[21] 3GPP TS 38.213, “Physical layer procedures for control”, December 2020 4 
[22] 3GPP TS 38.214, “Physical layer procedures for data”, December 2020 5 
[23] 3GPP TR 38.308, “Physical Layer Aspects”, September 2017 6 
[24] 3GPP TSG RAN WG1 Meeting #92 R1-1801352, “Discussion on NR UE peak data rate”, March 2018 7 
[25] 3GPP TS 36.211, “Physical channels and modulation”, September 2020 8 
[26] 3GPP TR 38.801, “Radio access architecture and interfaces”, March 2017 9 
[27] 3GPP TS 33.511: “Security Assurance Specification (SCAS) for the next generation Node B (gNodeB) 10 
network product class” (Release 16), September 2020 11 
[28] 3GPP TS 23.502, “Procedures for the 5G System; Stage-2”, December 2020 12 
[29] 3GPP TS 23.401 “General Packet Radio Service (GPRS)enhancements for Evolved Universal Terrestrial 13 
Radio Access Network (E-UTRAN) access” 14 
[30] 3GPP TS 37.340 “Overall description; Stage-2” 15 
[31] 3GPP TS 38.401 “5G; NG-RAN; Architecture description 16 
[32] 3GPP TS 38.473 “5G; NG-RAN; F1 Application Protocol (F1AP)” 17 
1.3 Definitions and Abbreviations 18 
1.3.1 Definitions 19 
For the purposes of the present document, the terms and definitions given in 3GPP TR 21.905 [1] and the following 20 
apply. A term defined in the present document takes precedence over the definition of the same term, if any, in 3GPP 21 
TR 21.905 [1]. 22 
C-Plane Control Plane: refers specifically to real-time control between O-DU and O-RU, and should not be 23 
confused with the UE’s control plane 24 
NSA Non-Stand-Alone network mode that supports operation of SgNB attached to MeNB 25 
O-CU O-RAN Central Unit – a logical node hosting PDCP, RRC, SDAP and other control functions.  26 
This can be considered short-hand for the O-CU-CP and O-CU-UP in an O-RAN system 27 
O-DU O-RAN Distributed Unit: a logical node hosting RLC/MAC/High-PHY layers based on a lower 28 
layer functional split. O-DU in addition hosts an M-Plane instance. 29 
O-RU O-RAN Radio Unit : a logical node hosting Low-PHY layer and RF processing based on a lower 30 
layer functional split. This is similar to 3GPP’s “TRP” or “RRH” but more specific in including 31 
the Low-PHY layer (FFT/iFFT, PRACH extraction). O-RU in addition hosts M-Plane instance. 32 
PTP Precision Time Protocol (PTP) is a protocol for distributing precise time and frequency over 33 
packet networks. PTP is defined in the IEEE Standard 1588. 34 
PDCCH Physical Downlink Control Channel applies for LTE and NR air interface 35 
PBCH Physical Broadcast Channel applies for LTE and NR air interface 36 
SA Stand-Alone network mode that supports operation of gNB attached to a 5G Core Network 37 
SCS OFDM Sub Carrier Spacing 38 

  
Page 11 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
SSB Synchronization Signal Block, in 5G PBCH and synchronization signal are packaged as a single 1 
block 2 
S-Plane Synchronization Plane: Data flow for synchronization and timing information between nodes 3 
U-Plane User Plane: refers to IQ sample data transferred between O-DU and O-RU 4 
 5 
1.3.2 Abbreviations 6 
For the purposes of the present document, the abbreviations given in 3GPP TR 21.905 [1] and the following apply. An 7 
abbreviation defined in the present document takes  precedence over the definition of the same abbreviation, if any, in 8 
3GPP TR 21.905 [1].  9 
CFI Control format indicator 10 
DL Downlink: data flows from the core network towards the UE 11 
DoS Denial of service 12 
DUT Device under Test 13 
E2E End-to-End  14 
eNB Evolved Node B (LTE base station) 15 
FFS For further study 16 
gNB Next-generation Node B (5G NR base station) 17 
IOT Interoperability Testing 18 
IUT Interface under Test 19 
KPI Key performance indicator 20 
MCS Modulation and coding scheme  21 
OTA Over the air 22 
PDSCH Physical downlink shared channel 23 
PUSCH Physical uplink shared channel 24 
PRB Physical resource block (12 x Resource Elements per PRB) 25 
QUIC  Quick UDP Internet Connections 26 
SUT System under test 27 
TCP Transmission Control Protocol: connection-oriented IP protocol 28 
TIFG O-RAN Test and Integration Focus Group 29 
UDP User Datagram Protocol: connectionless IP protocol 30 
UE User Equipment: terminology for a mobile device/terminal in LTE and NR 31 
UL Uplink: data flows from the UE towards the core network 32 
VoLTE Voice over LTE 33 
VoNR Voice over NR 34 
ViLTE Video over LTE 35 

  
Page 12 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
1.4 Revision Guideline 1 
The contents of the present document are subject to continuing work within O -RAN and may change following formal 2 
O-RAN approval. Should the O-RAN modify the contents of the present document, it will be re-released by O-RAN with 3 
an identifying change of release date and an increase in version number as follows: 4 
Release x.y.z 5 
Where: 6 
x the first digit is incremented for all changes of substance, i.e. technical enhancements, corrections, updates, 7 
etc. (the initial approved document will have x=01). 8 
y the second digit is incremented when editorial only changes have been incorporated in the document. 9 
z the third digit included only in working versions of the document indicating incremental changes during the 10 
editing process. 11 
 12 
 13 
 14 

  
Page 13 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
  Overview 1 
This test specification is focused on validating the end-to-end system functionality, performance, and key features of the 2 
O-RAN system as a black box.  It is based on the principles outlined in the end-to-end system test framework document 3 
[11].   4 
Chapter 3 specifies the test methodology, tools, and baseline configurations, which apply to the entire specification, 5 
unless otherwise indicated.   6 
Chapter 4 specifies the baseline functional testing of the radio access network from an end-to-end perspective.   7 
Chapter 5 specifies the performance testing of the radio access network from an end-to-end perspective, with a focus 8 
on end-user experienced key performance indicators (KPIs) 9 
Chapter 6 specifies the test cases and KPIs around the application level services of the radio access network, including 10 
data, voice and video as experienced by an end-user  11 
Chapter 7 specifies the test cases around end-to-end radio access network security 12 
Future versions of this specification may add additional chapters for test cases on timing and synchronization, 13 
operations, administration, and maintenance (OAM), and other cases of relevance from an end-to-end system 14 
perspective. 15 
 16 

  
Page 14 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
 Testing methodology and configuration 1 
This chapter describes the common testing methods and configurations which will be used in the subsequent chapters. To 2 
ensure fair and comparable test results among various test campaigns, consistent test setups should be utilized. The test 3 
conditions should reflect the realistic operational environment as much as possible to ensure meaningful and as close to 4 
real-world results as possible. This end-to-end test  specification harmonizes the test conditions, methodologies, and 5 
procedures, but the test configuration (parameters) of DUT/SUT is not specified in this document. However, it is required 6 
to record the complete test configuration used in the test report to enable the test to be reproduced if needed, and for the 7 
test results to potentially be used for other purposes, e.g. benchmarking or comparison.   8 
There are several design areas in RAN, where the vendors can differentiate such as RF performance (e.g. receiver 9 
sensitivity, PA design) , radio link adaptation algorithms (e.g. radio channel estimation, MCS selection, MIMO mode 10 
selection, transmission mode selection, UL power control), scheduling and overhead management (e.g. number of control 11 
channels). These different approaches result in competitive advantages leading to differences in end-to-end performance 12 
which can be assessed with the tests defined in this document. Hence, it is not possible to set pass/fail criteria for all the 13 
tests, but the pass/fail criteria are set whenever possible, e.g. in the functional tests in Chapter 4. The expected performance 14 
values are also indicated for reference network configurations.  15 
Unless otherwise stated in this document, the tests are suitable for both laboratory as well as field environments. All 16 
laboratory tests should be conducted over a cable, or in case of OTA tests, inside a shielded box/room , to ensure 17 
repeatability. In the laboratory, radio signal strength (i.e. attenuation) on the 5G  NR path and/or 4G  LTE path can be 18 
modified by using variable attenuators. The end-user device, if used,  should be placed inside a shielded box to avoid 19 
interference from external signals. The laboratory environment should allow for stable and repeatable testing conditions, 20 
and it is more suitable for benchmarking. On the other hand, the field environment allows for the evaluation of complex 21 
scenarios with realistic radio channel variations and behavior of the network (e.g. inter-cell interference and handovers). 22 
It is assumed that the field tests will be performed over the air (OTA). In the field, radio signal strength is modified by 23 
placing the UE in different positions inside the cell.  24 
Unless otherwise stated in this document , the same  operating system  (e.g. Windows 10)  with default settings and 25 
configuration should be utilized for both ends (i.e. the host applications at end -user device (test UE) and application 26 
(traffic) server) in order to ensure a consistent test environment.  27 
Unless otherwise stated in this document, the tests are suitable for both TDD and FDD. 28 
Unless otherwise stated in this document, the following network architectures  [26] depicted in Figure 3-1 below are 29 
addressed and supported: 30 
• 4G LTE – Option 1 31 
• 5G NR standalone (5G SA) - Option 2 32 
• 5G NR non-standalone (5G NSA)  - Option 3 / Option 3a / Option 3x 33 

  
Page 15 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
Figure 3-1 : The network architectures supported in this document - red dashed lines indicate control plane 1 
while blue solid lines indicate user plane. 2 
3.1 System under test 3 
The whole O-RAN system is the System under Test (SUT) and can be viewed as an integrated black box in the context 4 
of E2E testing [11], i.e. the internal functionality and architecture of SUT is out of scope. It is expected that all involved 5 
O-RAN f unctions and interfaces can properly interoperate together, and an end-to-end communication link can be 6 
established between the end-user device and the application server  or another end -user device . The testing of 7 
interoperability and conformance of the internal functions of the SUT is out of scope for this document. The SUT is 8 
expected to be in service mode and run in their normal operation state. The E2E KPIs are defined across the whole end-9 
to-end communication link between the end-user device and the application (traffic) server or another end-user device – 10 
see Figure 3-2.  11 
 12 
Figure 3-2: The O-RAN system as System under Test (SUT) and E2E KPIs 13 
O-RAN SystemUE Uu
System under Test (SUT)
3GPP CoreS1, NG 3GPP 
Services
Other 
Services
End to end (E2E) KPIs
 
4G LTE – Option 1 
 
5G SA – Option 2 
 
5G NSA – Option 3 
 
5G NSA – Option 3a 
 
5G NSA – Option 3x 
EPC
4G LTE
5GC
5G NR
EPC
4G LTE 5G NR
EPC
4G LTE 5G NR
EPC
4G LTE 5G NR

  
Page 16 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
The end-to-end communication link includes O -RAN as well as non -O-RAN (e.g. core, end -user device) components 1 
which could negatively affect the end-to-end performance, e.g. limited capability of the test UEs and/or the application 2 
servers to generate/receive enough data traffic, or a bottleneck in the transport network. All these unwanted contributions 3 
should be avoided or at least minimized in order to measure unbiased KPIs. In addition, there can be a performance 4 
difference between different vendors and chipsets depending on their level of maturity. Commercial (production grade) 5 
devices (e.g. test UE) are preferred  whenever possible , ensuring the tests are sufficiently documented, stable and 6 
repeatable.  7 
All O-RAN components [10] (such as O -CU-CP and O -CU-UP, O -DU, O -RU) and interfaces [10] (such as Open 8 
Fronthaul, X2 ) included in the System under Test  are recommended to have been tested  against their respective 9 
conformance and interoperability O-RAN specifications.  10 
3.2 Test and measurement equipment and tools 11 
All the tests shall be performed in a non-intrusive manner; that is, in a manner in which the SUT is not required to support 12 
any functionality or mode of operation beyond that required for normal operation in a production network. The SUT is 13 
not expected to be used as test tools when deployed in a production network, and therefore it should not be used as test 14 
tools during end-to-end testing. 15 
All the measurement equipment and tools used in the tests must be properly calibrated and configured in advance in order 16 
to minimize the influence of the test equipment on the measurements results. The parameters (e.g. attenuation) of cables, 17 
attenuators, splitters, combiners, etc. must be also measured in advance  and compensated for in the final measurement 18 
results.   19 
Table 3-1 Test and measurement equipment and tools 20 
Test tool Description 
Real UE and/or 
UE emulator 
The UE ( Real UE or UE emulator) is used to establish stateful end -to-end connection and to 
generate or receive data traffic. 
The real UE used in this context as a test tool is typically a UE which is designed for commercial 
or testing applications with certain test and diagnostic functions enabled for test and measurement 
purposes. Such test and diagnostic functions should not affect the performance.  
The real UE requires a SIM card (real or emulated)  which is pre-provisioned with subscriber 
profiles. A UE emulator or multiple real UEs can be used in multi -UE test scenarios requiring 
multiple UEs sessions. The UE is connected with the SUT either via RF cables or via an over the 
air (OTA) connection. In a lab environment, t he UE should be placed inside an RF shielded 
box/room in order to avoid interference from external signals.  
The logging tool connected to the UE is used to capture measurements and KPI logs for test 
validation and reporting.   
4G/5G Core or 
Core emulator 
The 4G/5G core or core emulator is used to terminate 4G/5G NAS sessions, and to support core 
network procedures required for RAN (SUT) testing. 4G/5G core or core emulator must support 
end-to-end connection and data transfer between Application server and Real UE/UE emulator.  
IMS Core or IMS 
Core emulator 
The IMS Core or IMS core emulator is used to support voice and video calling services like 
VoLTE, ViLTE, VoNR, Video over NR and EPS Fallback using protocols like SIP and RTP. 
IMS core or IMS core emulator should interface with the 4G/5G core to setup dedicated 
bearers/QoS Flows to support voice and video calling services. 
Application 
(traffic) server 
The application (traffic) server is used as an endpoint for generation and/or termination of various 
data traffic streams to/from Real UE(s)/UE emulator. The application server should be capable to 
generate data traffic for the services under test.   
The application server should be placed as close as possible to the core /core emulator, and 
connected to the core/core emulator via a transport link with sufficient capacity. 

  
Page 17 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
Protocol analyzer The protocol analyzer is used for test results verification, and for troubleshooting and root cause 
analysis of failed tests. Note that if IPsec encryption is applied at the network interface, then it 
would not be possible to use the protocol analyzer without decryption of IPsec.     
Network 
impairment 
emulator 
The network impairment emulator is used for tests which require insertion of impairment (packet 
delay and/or jitter) at the network interface (e.g. Open fronthaul).  
RF attenuators 
and/or Fading 
generator 
RF attenuators are used for tests which require radio signal attenuation. Fading generators can be 
used to simulate specific radio channel conditions (e.g. Urban, Rural, High Speed Train). 
RF shielded 
box/room 
The RF shielded box/room is used for over the air (OTA) connectivity between the UE and SUT 
in the lab environment . The RF shielded box/room should support reliable MIMO testing, if 
MIMO is required.  
Packet generation 
tool / DoS 
emulator 
The packet generation tool / Denial of Service (DoS) emulator is used for DoS traffic generation 
of security tests. The tool must support crafting network traffic on network layers from 2 to 7, 
which conform to network protocols such a s: Ethernet, IP, UDP, TCP, PTP, eCPRI, TLS, 
HTTP/HTTPS. The tool is intended to be deployable in various network segments 
(communication planes) according to the testing needs. 
Packet capture 
tool 
The packet capture tool is used to capture samples of data traffic for validation, analysis, and 
troubleshooting. In the case of security test cases it can be used to capture samples of legitimate 
traffic, which then can be used as templates for fuzzing attacks. The tool must support capturing 
network traffic on network layers from 2 to 7, which conform to network protocols such as: 
Ethernet, IP, UDP, TCP, PTP, eCPRI, TLS, QUIC, HTTP/HTTPS. The tool is intended to be 
deployable in various network segments (communication planes) according to the testing needs. 
Network tap A network tap is a hardware or software device which provides access and visibility to the data 
flowing across a computer network. 
Fuzzing tool The protocol fuzzing tool is used for unexpected protocol input generation of security tests. The 
tool must support mutating and replaying of captured network traffic on network layers from 2 to 
7, which conform to network protocols such as: Ethernet, IP, UDP, TCP, PTP, eCPRI, TLS, 
HTTP/HTTPS. The tool is intended to be deployable in various  network segments 
(communication planes) according to the testing needs. 
Vulnerability 
scanning tool 
The vulnerability scanning tool is used for blind exploitation of well-known vulnerabilities during 
security tests. The tool should rely on cyclically updated database of known vulnerabilities based 
on Common Vulnerabilities and Exposures (CVE) and should support scanning network services 
running on TCP/IP stack of protocols. The tool is intended to be deployable in various network 
segments (communication planes) according to the testing needs. 
NFV 
benchmarking 
and resource 
exhaustion tool 
The Network Function Virtualization (NFV) tool is used for O -Cloud system performance 
measurement and resource exhaustion type of DoS attack generation. This tool should be able to 
be deployed on any types of O-Cloud environment (public or private) with testing VNF(s) and/or 
CNF(s) support. 
3.3 Test report 1 
Tests should be described in the test report with sufficient detail to allow the tests to be reproducible by different parties 2 
and to enable benchmarking and comparison. The unified reporting of test results is important for benchmarking and 3 
comparison of results. The following common minimum set of configuration parameters and information about the 4 
test environment needs to be reported in each test report [3]:  5 
• Carrier frequency 6 
• Total transmission (effective) bandwidth and number of total RBs 7 

  
Page 18 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
• Duplex mode (e.g. FDD, TDD) 1 
• Sub-carrier spacing 2 
• Carrier prefix length 3 
• Slot length 4 
• Number of supported MIMO layers at both eNB/gNB and UE sides 5 
• Antenna configuration (number of Tx/Rx antenna elements, e.g. 4T4R) at both eNB/gNB and UE sides 6 
• Transmit power at antenna connectors at both eNB/gNB and UE sides 7 
• Antenna gains at both eNB/gNB and UE sides 8 
• DL/UL ratio (configuration) in case of TDD duplex mode 9 
• Test UE position with respect to O-RU antenna (e.g. LOS/NLOS/nLOS) in case of field test 10 
• Deployment scenario (e.g. indoor hotspot, macro/micro, dense urban, urban, rural) in case of field test 11 
• List of utilized test and measurement equipment and tools (incl. logging tools, test UE(s)) including the type 12 
and version. The Operating Systems (incl. the version) used at end-user device and application server should 13 
be noted as well. If TCP performance has been measured, the setting of TCP configuration parameters should 14 
be also noted.  15 
• Information about the SUTs (e.g. O-RU, O-DU, O-CU-CP, O-CU-UP) including the type, parameters, 16 
configuration, SW and HW versions, Interface profiles (e.g. Open fronthaul IOT profile [12]).  17 
The template for a complete test report can be found in Annex A. Photos should also be taken as part of the test report in 18 
order to illustrate the test environment. Additional parameters and counters are specified in the description of each test in 19 
the subsequent chapters. 20 
3.4 Data traffic  21 
Full buffer and finite buffer traffic models are utilized in the tests. 22 
The full buffer traffic model is characterized by a constant number of users in the cell during the test, wherein the buffers 23 
of the users' data flows always have unlimited amount of data to transmit. The model is preferred due to its simplicity. 24 
In the Finite buffer traffic model , a user is assigned a finite payload to transmit or receive when it arrives. The user 25 
arrival process of this model captures the fact that the users in the network are not simultaneously active at the same time, 26 
but they rather become active when they start a data session that require the download/upload of data. Examples of models 27 
are FTP traffic model 1/2/3 [14] largely used in 3GPP simulations.  28 
Data throughput can be measured at different protocol layers. Each network protocol layer adds extra overhead (header 29 
information), thereby reducing the data throughput available to the layer above.  The highest throughput is provided at 30 
the physical layer including user data and the overhead from higher protocol layers . The data throughput at the RLC 31 
(Radio Link Control) layer is independent from radio -specific overhead and therefore well -suited for comparison with 32 
other access technologies. The application layer throughput is the net throughput seen by user applications operating on 33 
top of either UDP, TCP or Q UIC – for example, the typical FTP overhead is around 3 -5%, typical HTTP overhead is 34 
around 30% compared to RLC throughput. Unless otherwise stated in this document, the reported throughput (user data 35 
rate) shall consider all the overhead (control channels, reference signals).  36 
UDP (User Datagram Protocol), TCP (Transmission Control Protocol) and QUIC (Quick UDP Internet Connections) are 37 
typical transport layer protocols utilized in the tests.  38 

  
Page 19 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
UDP is a simple, connection-less transport layer protocol which does not guarantee error-checking and recovery. UDP 1 
throughput is more suitable for benchmarking as it is not affected by the system configuration parameters. UDP is also 2 
faster, lighter (less overhead) and more efficient than TCP.   3 
TCP is reliable, connection-oriented transport layer protocol which includes error-checking and recovery, and guarantees 4 
data delivery with preserved order of data packets . The performance of a TCP connection can be impacted by various 5 
factors such as end-to-end latency, number of retransmissions, packet loss, and TCP configuration parameters such as 6 
window size, window scale, timestamps, etc. [6]. The default values of TCP configuration parameters can also vary in 7 
different Operation Systems (incl. different versions of the same OS) which are used at the end-user device (test UE) and 8 
application (traffic) server. It is recommended to use the same OS (incl. the version) with default setting at the end-user 9 
device (test UE) and application server. Since the settings and behavior of TCP connections cannot be easily unified and 10 
normalized, the measurement of TCP performance is recommended only as an illustrative indicator , and UDP 11 
performance should be used for the benchmarking and assessment of system performance.  12 
QUIC (Quick UDP Internet Connections) is a general-purpose transport layer protocol built on top of UDP to support 13 
the next generation of application layer protocols. QUIC provides features like connection establishment, congestion 14 
control, stream multiplexing and forward error correction to provide a secure and reliable connection -oriented protocol 15 
over UDP. QUIC is being used as the standard transport mechanism for HTTP/3.  16 
In addition, the following application layer protocols are utilized in the tests. 17 
Hypertext Transfer Protocol (HTTP) is the application layer protocol used in the internet. HTTP is a stateless protocol 18 
which follows the request-response model between client and the server. The client places a request for a resource to the 19 
server, and the server responds back to the client with requested resource and/or the appropriate response code. HTTP’s 20 
support for headers between the client-server makes this protocol simple, extensible, and powerful. 21 
 Session Initiation Protocol (SIP) is an application layer signalling protocol for real-time sessions like IP telephony. This 22 
is a text-based protocol which allows negotiation between two end points to initiate a session, maintain the session and 23 
terminate the session. SIP is the default signalling protocol used in the telecom network for VoLTE, ViLTE, VoNR and 24 
Video over NR. 25 
Real-time Transport Protocol (RTP) is an application layer protocol used to transmit real-time data such as audio and 26 
video over IP network. RTP is the default data plane protocol used in t he telecom network for services like VoLTE, 27 
ViLTE, VoNR and Video over NR. RTP does not guarantee Quality of Service but works in conjunction with Real Time 28 
Control Protocol (RTCP) to detect and convey packet loss and jitter information.  29 
File Transfer Pro tocol (FTP) is an application layer protocol to transfer files on a computer network. FTP follows a 30 
client-server model where the client can upload the file to the server, or download he file from the server. FTP protocol 31 
uses two separate connections between the client and the server – one for control and the other one for data or transfer of 32 
file. FTP along with multiple variants of the protocol have become the de-facto standards to transfer file on the internet. 33 
3.5 Mobility Classes 34 
The following classes of mobility are defined [13]: 35 
• Stationary: 0 kph 36 
• Pedestrian: 0 kph to 10 kph (typical value 4 kph) 37 
• Vehicular: 10 kph to 120 kph (typical value 50 kph) 38 
• High speed vehicular: 120 kph to 500 kph (typical value 150 kph) 39 
High speed vehicular speeds close to 500 kph are mainly used for high speed trains. 40 

  
Page 20 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
3.6 Radio conditions 1 
The radio signal quality is described by the radio parameters such as RSRP and SINR. These radio parameters are defined 2 
differently in LTE and 5G NR. 3 
• LTE RSRP (Reference Signal Received Power) [7]  is defined as the linear average over the power contributions 4 
(in [W]) of the resource elements that carry cell -specific reference signals  (CRS) within the considered 5 
measurement frequency bandwidth. The RSRP is reported from UE back to eNB. The reporting range of RSRP 6 
is defined from -140dBm to -44dBm [8].   7 
• LTE SINR (Signal to Noise and Interference Ratio) has not been formerly defined in the 3GPP specification. 8 
The UE does not send the results back to eNB. The SINR is measured and used only in UE. Specific 9 
implementations may vary, and it is up to the manufacturer to decide, how to implement this measurement. This 10 
is making difficult to compare results of different devices. In [7], SINR is defined as the linear average over the 11 
power contribution (in [W]) of the resource elements carrying cell-specific reference signals divided by the linear 12 
average of the noise and interference power contribution (in [W]) over the resource elements carrying cell -13 
specific reference signals within the same frequency bandwidth. 14 
• 5G SS-RSRP (Synchronization Signal based Reference Signal Received Power)  [7] is defined as the linear 15 
average over the power contributions (in [W]) of the resource  elements that carry secondary synchronization  16 
(SS) signals. SS-RSRP is the equivalent of the RSRP parameter used in LTE systems . The reporting range of 17 
SS-RSRP is defined from -140dBm to -40dBm [9].   18 
• 5G SS-SINR [7] is defined as the linear average over the power contribution (in [W]) of the resource elements 19 
carrying secondary synchronisation signals divided by the linear average of the noise and interfe rence power 20 
contribution (in [W]) over the resource elements carrying secondary synchronisation signals within the same 21 
frequency bandwidth. The SS-SINR is reported from UE back to gNB. The reporting range of SS -SINR is 22 
defined from -23dB to 40dB.  23 
It is worth to note that in 5G, the Channel State Information Reference Signal (CSI-RS) can also be used for RSRP and 24 
SINR measurements. Due to different transmit powers of CSI-RS and SS, CSI-RS-based SINR and RSRP measurement 25 
values are usually greater than SS-based SINR and RSRP.   26 
The minimum coupling loss (MCL) [2] between O-RU (antenna) and UE must be ensured: 27 
• Macro cell deployment scenario (wide area BS): MCL = 70dB  corresponding to minimal O -RU (antenna) to 28 
UE distance along the ground equal to around 35 m 29 
• Small cell (micro cell) deployment scenario (medium range BS): MCL = 53dB corresponding to minimal O-RU 30 
(antenna) to UE distance along the ground equal to around 5 m 31 
• Pico cell deployment scenario (local area BS): MCL = 45dB corresponding to minimal O-RU (antenna) to UE 32 
distance along the ground equal to around 2 m 33 
The radio parameters (RSRP, SINR) should be measured across the entire range covering scenarios from cell centre to 34 
cell edge. Based on the test results, the RSRP and SINR distribution statistics can be calculated and described as a 35 
cumulative distribution function (CDF) curve. According to the CDF curve, the four types of radio conditions can be 36 
defined as: excellent (95%-100%), good (80%-90%), fair (40%-60%) and poor (5%-15%) [3]. Table 3-2 shows the RSRP 37 
and SINR thresholds for various radio conditions.Table 3-2 RSRP and SINR thresholds for various radio conditions 38 
Note that the RSRP values should primarily be used for UL assessments, and the SINR values for DL assessment. 39 
 40 
Table 3-2 RSRP and SINR thresholds for various radio conditions 41 

  
Page 21 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
Radio 
conditions 
RSRP (dBm) 
SS-RSRP (dBm) 
DL SINR (dB) 
DL SS-SINR (dB) 
 
Excellent 
(cell centre) 
> -75 
 
> 25 
 
• Utilization of the highest possible MCS, 
transport block size and MIMO rank  
• Peak performance measurements 
• Negligible interference from neighbor cells  
Good -75 to -90 
(typical value = -85) 
15 to 20 
(typical value = 17) 
 
Fair -90 to -105 
(typical value = -95) 
5 to 10 
(typical value = 7) 
 
Poor 
(cell edge) 
< -105 
(typical value = -110) 
< 5  
(typical value = 3) 
• Minimum performance measurements  
• Strong interference from neighbor cells 
There are many different factors that influence signal strength and quality during the field testing; these factors include, 1 
but are not limited, to the following: 2 
• Proximity to the cellular tower (antenna) 3 
• Load in neighbor cells 4 
• Surrounding physical barriers (mountains, buildings, etc.) 5 
• Weather conditions  6 
3.7 Inter-cell interference 7 
The tests are conducted either in a single cell scenario without any inter-cell interference or in a multi-cell scenario where 8 
the serving cell is surrounded by neighboring cells generating traffic load (interference on the serving cell) in the downlink 9 
or uplink directions.  10 
Generating a realistic traffic load is important for meaningful results. As the number of real UE s are always a limiting 11 
factor, artificial (dummy) traffic load and interference generation could be used.  12 
In the single cell scenario, the serving cell is isolated, and all the surrounding neighbor cells are turned off (neither control 13 
nor data channels are used).  14 
In the multi-cell scenario, all the neighbor cells are turned on. The following load setups are defined: 15 
• Load 0% - all the surrounding neighbor cells are turned on without any data traffic and end-user device attached. 16 
Inter-cell interference is generated only on control channels (broadcasting, synchronization channels) without 17 
any inter-cell interference on data channels.  18 
• Load 30% - all the surrounding neighbor cells are turned on with data traffic. Inter-cell interference is generated 19 
both on control and data channels. The level of interference on data channel is controlled by the amount of data 20 
traffic. The interference level of 30% in downlink means that 30% of downlink PRBs are randomly occupied 21 
with a dummy traffic. In uplink, t his interference level corresponds to 3dB  rise of  IoT (Interference over 22 
Thermal) noise at the receiver side (i.e. eNB/gNB antenna(s)). The received interference noise from the UEs of 23 
neighbor cells uplink transmission should lead to 3dB rise of receiver’s noise power [3]. 24 
• Load 50% - all the surrounding neighbor cells are turned on with data traffic. Inter-cell interference is generated 25 
both on control and data channels. The level of interference on data channel is controlled by the amount of data 26 
traffic. The interference level of 50% in downlink means that 50% of downlink PRBs are randomly occupied 27 
with a dummy traffic. In uplink, this interference level corresponds to 5dB rise of IoT (Interference over 28 
Thermal) noise at the receiver side (i.e. eNB/gNB antenna(s)). 29 
• Load 70% - all the surrounding neighbor cells are turned on with data traffic. Inter-cell interference is generated 30 
both on control and data channels. The level of interference on data channel is controlled by the amount of data 31 

  
Page 22 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
traffic. The interference level of 70% in downlink means that 70% of downl ink PRBs are randomly occupied 1 
with a dummy traffic. In uplink, this interference level corresponds to 7dB rise of IoT (Interference over 2 
Thermal) noise at the receiver side (i.e. eNB/gNB antenna(s)). 3 
• Load 100% - fully loaded multi -cell scenario generating the highest possible inter -cell interference. All the 4 
surrounding neighbor cells are turned on with data traffic. Inter-cell interference is generated both on control and 5 
data channels. The level of interference on data channel is controlled by the amount  of data traffic. The 6 
interference level of 100% in downlink means that 100% of downlink PRBs are occupied with a dummy traffic. 7 
In uplink, this interference level corresponds to 9dB rise of IoT (Interference over Thermal) noise at the receiver 8 
side (i.e. eNB/gNB antenna(s)). 9 
3.8 Spectral efficiency 10 
The spectral (or spectrum) efficiency (SE) is an important criterion for fair performance assessment and benchmarking  11 
of different systems when various transmission bandwidths, duplex modes (FDD/TDD) and TDD DL/UL configurations 12 
are normalized. 13 
The spectral efficiency is calculated by dividing the data throughput by the aggregated channel bandwidth (incl. guard 14 
bands) in DL or UL assuming single user and FDD duplex mode. The corresponding link frame structure is fully (100%) 15 
utilized in frequency and time domains.   16 
𝑆𝐸𝐹𝐷𝐷_𝐷𝐿  [𝑏𝑝𝑠 𝐻𝑧⁄ ] = 𝐷𝐿 𝑑𝑎𝑡𝑎 𝑡ℎ𝑟𝑜𝑢𝑔ℎ𝑝𝑢𝑡 [𝑏𝑝𝑠]
𝐷𝐿 𝑐ℎ𝑎𝑛𝑛𝑒𝑙 𝑏𝑎𝑛𝑑𝑤𝑖𝑑𝑡ℎ [𝐻𝑧] 17 
𝑆𝐸𝐹𝐷𝐷_𝑈𝐿 [𝑏𝑝𝑠 𝐻𝑧⁄ ] = 𝑈𝐿 𝑑𝑎𝑡𝑎 𝑡ℎ𝑟𝑜𝑢𝑔ℎ𝑝𝑢𝑡 [𝑏𝑝𝑠]
𝑈𝐿 𝑐ℎ𝑎𝑛𝑛𝑒𝑙 𝑏𝑎𝑛𝑑𝑤𝑖𝑑𝑡ℎ [𝐻𝑧] 18 
In case of TDD where the same spectrum is used at different times for the uplink and downlink, the spectral efficiency is 19 
in addition multiplied  by the fraction of resources ( slots and symbols, not including switching gap) allocated to the 20 
particular link direction within 10ms radio frame. 21 
𝑆𝐸𝑇𝐷𝐷_𝐷𝐿  [𝑏𝑝𝑠 𝐻𝑧⁄ ] = 𝐷𝐿 𝑑𝑎𝑡𝑎 𝑡ℎ𝑟𝑜𝑢𝑔ℎ𝑝𝑢𝑡 [𝑏𝑝𝑠]
𝑐ℎ𝑎𝑛𝑛𝑒𝑙 𝑏𝑎𝑛𝑑𝑤𝑖𝑑𝑡ℎ [𝐻𝑧] ∗ 𝑛𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑡𝑜𝑡𝑎𝑙 𝑠𝑦𝑚𝑏𝑜𝑙𝑠 𝑝𝑒𝑟 𝑓𝑟𝑎𝑚𝑒
𝑛𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝐷𝐿 𝑠𝑦𝑚𝑏𝑜𝑙𝑠 𝑝𝑒𝑟 𝑓𝑟𝑎𝑚𝑒  22 
 23 
𝑆𝐸𝑇𝐷𝐷_𝑈𝐿  [𝑏𝑝𝑠 𝐻𝑧⁄ ] = 𝑈𝐿 𝑑𝑎𝑡𝑎 𝑡ℎ𝑟𝑜𝑢𝑔ℎ𝑝𝑢𝑡 [𝑏𝑝𝑠]
𝑐ℎ𝑎𝑛𝑛𝑒𝑙 𝑏𝑎𝑛𝑑𝑤𝑖𝑑𝑡ℎ [𝐻𝑧] ∗ 𝑡𝑜𝑡𝑎𝑙 𝑛𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑠𝑦𝑚𝑏𝑜𝑙𝑠 𝑝𝑒𝑟 𝑓𝑟𝑎𝑚𝑒
𝑛𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝑈𝐿 𝑠𝑦𝑚𝑏𝑜𝑙𝑠 𝑝𝑒𝑟 𝑓𝑟𝑎𝑚𝑒  24 
 25 
 26 
 27 

  
Page 23 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
 Functional tests 1 
This chapter describes the tests evaluating and assessing the functionality of the radio access network from a network 2 
end-to-end perspective . The focus of the testing is on the end -user functionality based on 3GPP and O -RAN 3 
specifications.  Pass-fail criteria are defined for the tests wherever possible.  4 
The general test methodologies and configurations are mentioned in Chapter 3.  5 
Unless otherwise stated in the chapter, the tests are suitable and can be conducted in both laboratory as well as field 6 
environments, with pros and cons of both environments as described in Chapter 3.  7 
The following end-to-end functional tests are defined in this chapter as an extension of the NGMN testing framework [3]:  8 
• LTE/5G NSA attach and detach of a single UE  9 
• LTE/5G NSA attach and detach of multiple UEs  10 
• 5G SA registration and deregistration of a single UE  11 
• Intra- O-DU mobility  12 
• Inter- O-DU mobility  13 
• Inter- O-CU mobility  14 
The following test cases are being considered for future inclusion in this specification:  15 
• 5G/4G Inter-RAT mobility   16 
• 5G SA registration and deregistration to multiple network slices  17 
• Intra-O-RU mobility  18 
• Idle Mode Intra- O-DU mobility  19 
• Idle Mode Inter- O-DU mobility  20 
• Idle Mode Inter- O-CU mobility 21 
The test description, setup and procedures are detailed in the following sections for each test case. 22 
4.1 LTE/5G NSA attach and detach of single UE 23 
4.1.1 Test description and applicability 24 
The purpose of this test is to validate E2E O-RAN C-plane functionality with a single UE. These tests are valid for 25 
either LTE or 5G NSA. In this test scenario, the successful attach and detach procedure shall be validated by the “Power 26 
ON” and “Power OFF” of a single UE, as described in the following specifications: 27 
1. LTE Attach as per 3GPP TS 23.401 [29], Section 5.3.2.1 E-UTRAN Initial Attach  28 
2. LTE Detach as per 3GPP TS 23.401 [29], Section  5.3.8.2.1 UE-initiated Detach procedure for E-UTRAN 29 
3. 5G NSA Attach as per 3GPP TS 23.401 [29], Section 5.3.2.1 E-UTRAN Initial Attach and 3GPP TS 37.340 30 
[30] Section 10.2.1 EN-DC (Secondary Node Addition)  31 

  
Page 24 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
4. 5G NSA Detach as per 3GPP 23.401, Section  5.3.8.2.1 UE-initiated Detach procedure for E-UTRAN and 1 
3GPP TS 37.340 [30] Section 10.4.1 EN-DC (Secondary Node Release) [29] 2 
The test procedure shall be performed in excellent radio conditions for 10 iterations. Attach success rate, detach success 3 
rate,  and attach latency shall be measured and captured.  4 
4.1.2 Test setup and configuration 5 
The test setup is a single cell scenario (i.e. isolated cell without any inter-cell interference – see Section 3.7) with a 6 
stationary UE (real or emulated) placed under excellent radio conditions as defined in Section 3.6, using LTE RSRP 7 
(for LTE) or 5G SS-RSRP (for 5G NSA) as the metric.  Within the cell there should be only one active UE. The test is 8 
suitable for lab as well as field environments.   9 
Test configuration: The test configuration is not specified. The utilized test configuration (parameters) should be 10 
recorded in the test report.   11 
Laboratory setup: The radio conditions experienced by the UE can be modified using a variable attenuator/fading 12 
generator inserted between the antenna connectors (if available) of the O-RU and the UE, or appropriately emulated using 13 
a UE emulator. The test environment should be setup to achieve excellent radio conditions (LTE RSRP (for LTE) or 5G 14 
SS-RSRP (for 5G NSA)  as defined in Section 3.6) for the UE, but the minimum coupling loss (see Section 3.6) should 15 
not be exceeded.  The UE should be placed inside an RF shielded box or RF shielded room if the UE is not connected via 16 
cable. 17 
Field setup: The UE is placed in the centre of the cell close to the radiated eNB/gNB antenna(s), where excellent radio 18 
conditions (LTE RSRP (for LTE) or 5G SS -RSRP (for 5G NSA)   as defined in Section 3.6) should be observed. The 19 
minimum coupling loss (see Section 3.6) should not be exceeded. 20 
Please refer to Figure 5-1 for the E2E test setups for LTE and 5G NSA. 21 
4.1.3 Test Procedure 22 
The test steps below are applicable for either LTE or 5G NSA: 23 
1. The test setup is configured according to the test configuration. The test configuration should be recorded in the test 24 
report. The serving cell under test is activated and unloaded. All other cells are powered off. 25 
2. The UE (real or emulated UE) is placed under excellent radio conditions (Cell centre close to radiated eNB/gNB 26 
Antenna) as defined by LTE RSRP (for LTE) or 5G SS-RSRP (for 5G NSA) in Section  3.6.  27 
3. The End-to-end setup shall be operational for LTE or 5G NSA  as applicable for the test scenario, and there should 28 
not be any connectivity issues.  29 
4. Start the logs to capture the call flow and signalling messages  30 
5. “Power ON” the UE to attach to the LTE or 5G NSA cell. Wait for a successful attach.  31 
6. “Power OFF” the connected UE to detach from the network. Wait for a successful detach. 32 
7. Stop and save the test logs. The logs should be captured and kept for test result reference and measurements  33 
8. Repeat steps 4 to 7, for a total of 10 times and record the KPIs mentioned in Section 4.1.4.  34 

  
Page 25 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
4.1.4 Test requirements (expected results) 1 
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 2 
should be captured and reported in the test report for performance assessment 3 
• Radio parameters such as RSRP, RSRQ 4 
• KPIs  mentioned in Table 4-1 and Table 4-2 5 
Validate the successful procedures from the collected logs. Expected success rate for Attach/Detach and Secondary Node 6 
Addition/Release KPI is 100%. The attach-detach procedure should pass 10 consecutive time s to mark the test case  as 7 
passing. The gap analysis should be provided for the measured and the expected target KPIs. 8 
• LTE Attach-Detach test case validation and KPI measurements  9 
• Validate successful attach 10 times with LTE cell (Refer to 3GPP TS 23.401 [29], Section 5.3.2.1 E-10 
UTRAN Initial Attach ). In the UE logs or applications installed on UE, check that UE is attached to 11 
correct cell (example PCI, Global eNB ID, ARFCN as per test configuration) 12 
• Measure the attach success rate by validating attach request and  attach complete for each iteration. 13 
Record the attach success rate in Table 4-1 14 
• Measure the attach latency by calculating the time between attach request to attach complete. Capture 15 
the latency for each iteration and s ort the latency value observed for each iteration in ascending order. 16 
Record the Minimum, Average (Sum of all latency value/ Total Iterations, Total Iterations are 10  in this 17 
case) and Maximum latency value observed in Table 4-2 18 
• Validate successful detach attach with LTE cell (Refer to 3GPP TS 23.401 [29], Section 5.3.8.2.1 UE-19 
initiated Detach procedure for E -UTRAN). Signalling connection release should be validated from 20 
message flow (UE context release and RRC connection release messages). Measure the detach success 21 
rate by validating detach request and detach accept for each iteration. Record the detach success rate in 22 
Table 4-1 23 
• 5G NSA Attach-Detach test case validations and KPI measurements 24 
• Validate successful multiple attach es with 5G NSA cell (3GPP TS 23.401 [29], Section 5.3.2.1 E -25 
UTRAN Initial Attach and 3GPP TS 37.340 [30] Section 10.2.1 EN-DC for Secondary Node Addition)). 26 
In the UE logs or applications installed on UE, check that UE is attached to correct cell (example PCI, 27 
Global eNB ID/Global gNB, ARFCN/NR-ARFCN as per test configuration for LTE /5G cells ) 28 
• Measure the attach success rate by validating attach request and attach complete for each iteration. Also 29 
measure the secondary node addition success rate by validating the SgNB addition request and SgNB 30 
reconfiguration complete as per flow 3GPP TS 37.340 [30] Section 10.2.1 EN-DC for each iteration.  31 
Record the attach success rate  and secondary node addition success rate  in Table 4-1 32 
• Validate successful detach attach (LTE Detach and 5G Secondary Node re lease) with 5G NSA cell 33 
(Refer to 3GPP TS 23.401 [29], Section 5.3.8.2.1 UE-initiated Detach procedure for E -UTRAN and 34 
3GPP TS 37.340 [30] Section 10.4.1 EN -DC for Secondary Node Release) ). Signalling connection 35 
release should be validated  the from message flow (UE context release and RRC connection release 36 
messages). 5G secondary node should also get release d successfully.  Measure the detach success rate 37 
by validating detach request and detach accept for each iteration. Record the detach success rate in Table 38 
4-1 39 
Table 4-1 KPI to be captured for single UE attach-detach test case 40 

  
Page 26 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
LTE KPI 5G NSA KPI 
Attach Success 
Rate 
Detach Success 
Rate 
Attach 
Success Rate 
Detach 
Success Rate 
SgNB addition 
Success rate 
          
 1 
Table 4-2 Latency KPI for attach 2 
LTE Attach Time (millisecond) 
Minimum Maximum Average 
   
4.2 LTE/5G NSA attach and detach of multiple UEs 3 
4.2.1 Test description and applicability 4 
The purpose of this test is to validate E2E O-RAN C-plane functionality with multiple UEs. These tests are valid for 5 
either LTE or 5G NSA. In this test scenario, the successful attach and detach procedure shall be validated by the “Power 6 
ON” and “Power OFF” of multiple (at least 2) UEs, as described in the following specifications: 7 
1. LTE Attach as per 3GPP TS 23.401 [29], Section 5.3.2.1 E-UTRAN Initial Attach  8 
2. LTE Detach as per 3GPP TS 23.401 [29], Section  5.3.8.2.1 UE-initiated Detach procedure for E-UTRAN 9 
3. 5G NSA Attach as per 3GPP TS 23.401 [29], Section 5.3.2.1 E-UTRAN Initial Attach and 3GPP TS 37.340 10 
[30] Section 10.2.1 EN-DC (Secondary Node Addition)  11 
4. 5G NSA Detach as per 3GPP 23.401, Section  5.3.8.2.1 UE-initiated Detach procedure for E-UTRAN and 12 
3GPP TS 37.340 [30] Section 10.4.1 EN-DC (Secondary Node Release) [29] 13 
The test procedure shall be performed in excellent radio conditions for 10 iterations. Attach success rate, detach success 14 
rate,  and attach latency shall be measured and captured. 15 
4.2.2 Test setup and configuration 16 
The network setup is a single cell scenario (i.e. isolated cell without any inter-cell interference – see Section 3.7) with 17 
multiple stationary UEs (real or emulated) placed under excellent radio conditions as defined in Section 3.6, using LTE 18 
RSRP (for LTE) or 5G SS-RSRP (for 5G NSA) as the metric.  The test is suitable for lab as well as field environments.   19 
Test configuration: The test configuration is not specified. The utilized test configuration (parameters) should be 20 
recorded in the test report.   21 
Laboratory setup: The radio conditions experienced by the UEs can be modified using variable attenuators/fading 22 
generator inserted between the antenna connectors (if available) of the O-RU and the UE, or appropriately emulated using 23 
a UE emulator. The test environment should be setup to achieve excellent radio conditions (LTE RSRP (for LTE) or 5G 24 
SS-RSRP (for 5G NSA)  as defined in Section 3.6) for the UEs, but the minimum coupling loss (see Section 3.6) should 25 
not be exceeded.  The UEs should be placed inside an RF shielded box or RF shielded room if the UEs are not connected 26 
via cable. 27 

  
Page 27 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
Field setup: The multiple UEs are placed in the centre of the cell close to the radiated eNB/gNB antenna(s), where 1 
excellent radio conditions (LTE RSRP and 5G SS-RSRP  as defined in Section 3.6) should be observed. The minimum 2 
coupling loss (see 3.6) should not be exceeded. 3 
Please refer to Figure 5-1 for the E2E test setups for LTE and 5G NSA. 4 
4.2.3 Test Procedure 5 
The test steps below are applicable for either LTE or 5G NSA: 6 
1. The test setup is configured according to the test configuration. The test configuration should be recorded in 7 
the test report. The serving cell under test is activated and unloaded. All other cells are powered off. 8 
2. The multiple UEs (real or emulated) are placed under excellent radio conditions (Cell centre close to radiated 9 
eNB/gNB Antenna) as defined by LTE RSRP (for LTE) or 5G SS-RSRP (for 5G NSA) in Section  3.6.  10 
3. The End-to-end setup shall be operational for LTE or 5G NSA as applicable for the test scenario, and there 11 
should not be any connectivity issues.  12 
4. Start the logs to capture the call flow and signalling messages  13 
5. “Power ON” the multiple connected UEs to attach to the LTE or 5G NSA cell.  Wait for the successful attach 14 
of all UEs.  15 
6. “Power OFF” the multiple UEs to detach from the network. Wait for the successful detach of all UEs 16 
7. Stop and save the test logs. The logs should be captured and kept for test result reference and measurements  17 
8. Repeat steps 4 to 7, for a total of 10 times and record the KPIs mentioned in Section 4.2.4 18 
4.2.4  Test requirements (expected results) 19 
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 20 
should be captured and reported in the test report for performance assessment 21 
• Radio parameters such as RSRP, RSRQ 22 
• KPIs  mentioned in Table 4-3 and Table 4-4 23 
Validate the successful procedures for each UE from the collected logs. Expected success rate for Attach/Detach and 24 
Secondary Node Addition/Release KPI is 100%. The attach-detach procedure should pass 10 consecutive time s for all 25 
UEs to mark the test case as passing. The gap analysis should be provided for the measured and the expected target KPIs.   26 
• LTE Attach-Detach test case validations and KPI measurements  27 
• Validate successful attach of each UE 10 times with LTE cell (Refer to 3GPP TS 23.401 [29], Section 28 
5.3.2.1 E -UTRAN Initial Attach ). In the UE logs or applications installed on UE, check that UE is 29 
attached to correct cell (example PCI, Global eNB ID, ARFCN as per test configuration) 30 
• Measure the attach success rate by validating attach request and attach complete for each iteration. 31 
Record the attach success rate in Table 4-3 for each UE. 32 
• Measure the attach latency by calculating the time between Attach Request to attach complete. Capture 33 
the latency for each iteration and s ort the latency value observed for each iteration in ascending order. 34 
Record the Minimum, Average (Sum of all latency value/ Total Iterations, Total Iterations are 10  in this 35 
case) and Maximum latency value observed in Table 4-4 for each UE 36 

  
Page 28 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
• Validate successful detach attach with LTE cell (Refer to 3GPP TS 23.401 [29], Section 5.3.8.2.1 UE-1 
initiated Detach procedure for E -UTRAN). Signalling connection release should be validated from 2 
message flow (UE context release and RRC connection release messages). Measure the detach success 3 
rate by validating detach request and detach accept for each iteration. Record the detach success rate in 4 
Table 4-3 5 
• 5G NSA Attach-Detach test cases validations and KPI measurements 6 
• Validate successful multiple UE attach with 5G NSA cell (3GPP TS 23.401 [29], Section 5.3.2.1 E-7 
UTRAN Initial Attach and 3GPP TS 37.340 [30] Section 10.2.1 EN-DC for Secondary Node Addition)). 8 
In the UE logs or applications installed on UE, check that UE is attached to correct cell (example PCI, 9 
Global eNB ID/Global gNB, ARFCN/NR-ARFCN as per test configuration for LTE /5G cells ) 10 
• Measure the attach success rate by validating attach request and attach complete for each iteration. Also 11 
measure the secondary node addition success rate by validating the SgNB addition request and SgNB 12 
reconfiguration complete as per flow 3GPP 37.340 Section 10.2.1 EN-DC for each iteration.  Record the 13 
attach success rate  and secondary node addition success rate in Table 4-3 for each UE 14 
• Validate successful detach attach (LTE Detach and 5G Secondary Node release) with 5G NSA cell 15 
(Refer to 3GPP TS 23.401 [29] Section, 5.3.8.2.1 UE-initiated Detach procedure for E -UTRAN and 16 
3GPP TS 37.340 [30] Section 10.4.1 EN -DC for Secondary Node Release) ). Signalling connection 17 
release should be validated from the message flow (UE context release and RRC connection release 18 
messages). 5G secondary node should also get release d successfully.  Measure the detach success rate 19 
by validating detach request and detach accept for each iteration. Record the detach success rate in Table 20 
4-3 for each UE 21 
Table 4-3 KPI to be captured for multi-UE attach-detach test case 22 
LTE KPI 5G NSA KPI 
Attach 
Success Rate 
Detach 
Success Rate 
Attach 
Success 
Rate 
Detach 
Success 
Rate 
SgNB 
addition 
Success rate 
          
 23 
Table 4-4 Latency KPI for multi-UE attach 24 
LTE Attach Time (millisecond) 
Minimum Maximum Average 
   
4.3 5G SA registration and deregistration of single UE 25 
4.3.1 Test description and applicability 26 
The purpose of the test is to verify the full registration and de-registration procedure with a single UE. The test also 27 
verifies the PDU session establishment and release procedures. 28 
The test focuses on the procedure of ‘Initial registration’ as defined in 3GPP TS 23.502 [28] Section 4.2.2.2.2. 29 
The test focuses on the procedure of ‘UE-initiated de-registration’ as defined in 3GPP TS 23.502 [28] Section 4.2.2.3.2. 30 
This test also validates PDU session establishment and release procedures.  31 

  
Page 29 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
The test validates the 3GPP standard registration/de-registration procedure and the latency of the procedure. Bi-1 
directional data transmission shall be observed before the de-registration procedure to verify the stability of the network 2 
slice. 3 
4.3.2 Test setup and configuration 4 
The test setup is a single cell scenario (i.e. isolated cell without any inter-cell interference – see Section 3.7) with a 5 
stationary UE (real or emulated) placed under excellent radio conditions as defined in Section 3.6, using SS-RSRP as 6 
the metric. Within the cell there should be only one active UE. The application server should be placed as close as 7 
possible to the core/core emulator and connected to the core/core emulator via a transport link with enough capacity so 8 
as not to limit the expected data throughput. The UE, RAN, and 5G Core shall support the network slicing, at least for 9 
one Single Network Slice Selection Assistance Information (S-NSSAI). The test is suitable for lab as well as field 10 
environments.   11 
Test configuration: The test configuration is not specified. The utilized test configuration (parameters) should be 12 
recorded in the test report.   13 
Laboratory setup: The radio conditions experienced by  the UE can be modified using a variable attenuator/fading 14 
generator inserted between the antenna connectors (if available) of the O-RU and the UE, or appropriately emulated using 15 
a UE emulator. The test environment should be setup to achieve excellent radio conditions (using SS-RSRP as defined in 16 
Section 3.6) for the UE, but t he minimum coupling loss (see  Section 3.6) should not be exceeded.  The UE should be 17 
placed inside an RF shielded box or RF shielded room if the UE is not connected via cable. 18 
Field setup: The UE is placed in the centre of cell close to the radia ted eNB/gNB antenna(s), where excellent radio 19 
conditions (SS-RSRP as defined in Section 3.6) should be observed. The minimum coupling loss (see 3.6) should not be 20 
exceeded. 21 
Please refer Figure 5-1 for E2E test setup for 5G SA. 22 
4.3.3 Test Procedure 23 
Below are the test procedure steps 24 
1. The test setup is configured according to the test configuration. The test configuration should be recorded in 25 
the test report. The serving cell under test is activated and unloaded. All other cells are powered off. 26 
2. Power on the UE and the UE shall send REGISTRATION REQUEST message.  UE shall successfully register 27 
to the 5G SA network.  28 
3. Full-buffer UDP bi-directional data transmission (see Section 3.4) between the application server and UE is 29 
initiated. 30 
4. The registration procedure messages shall be captured, and the latency of the registration procedure shall be 31 
measured and recorded in Table 4-5. The duration of the test should be at least 3 minutes when the throughput 32 
is stable. The PDU session establishment procedure messages shall also be captured and verified. 33 
5. Power off the UE and UE shall send DEREGISTRATION REQUEST message. UE shall successfully de-34 
register from the 5G SA network. 35 
6. The de-registration procedure messages shall be captured, and the latency of de-registration procedure shall be 36 
measured and recorded in Table 4-5. The PDU session release procedure messages shall also be captured and 37 
verified. 38 
7. Repeat steps 2 to 6, for a total of 10 times and record the KPIs mentioned in Table 4-5. 39 

  
Page 30 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
4.3.4 Test requirements (expected results) 1 
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 2 
should be captured and reported in the test report for the performance assessment. 3 
• Radio parameters such as RSRP, RSRQ, CQI, PDSCH SINR (average sample per second) 4 
• Latency KPI mentioned in Table 4-5 5 
Validate from collected logs registration (as per 3GPP TS 23.502 [28] Section 4.2.2.2.2) and deregistration (as per 6 
3GPP TS 23.502 [28] Section 4.2.2.3.2) procedures and also validate ‘UE Requested PDU Session Establishment for 7 
Non-roaming and Roaming with Local Breakout case’ as defined in 3GPP TS 23.502 [28] Section 4.3.2.2.1, and the 8 
procedure of ‘PDU Session Release for UE or network requested PDU Session Release for Non-Roaming and Roaming 9 
with Local Breakout case’ as defined in 3GPP TS 23.502 [28] Section 4.3.4.2. The procedure should pass 10 10 
consecutive times to mark the test case as passing. The gap analysis should be provided for the measured and the 11 
expected target KPIs. 12 
The Registration Time latency is measured by calculating the time between Registration Request to Registration 13 
Complete; The De-registration Time latency is measured by calculating the time between De-registration Request to 14 
Signaling Connection Release. Capture the latency for each iteration and sort the latency value observed for each 15 
iteration in ascending order. Record the Minimum, Average (Sum of all latency value/ Total Iterations, Total Iterations 16 
are 10 in this case) and Maximum latency value observed in Table 4-5.1.  17 
Table 4-5 5G SA registration/de-registration latency KPI record table of single UE 18 
KPI Repeat Times Calculation 
1 2 3 4 5 6 7 8 9 10 Minimum Maximum Average 
Registration 
Time (single 
slice) 
(millisecond) 
             
De-
registration 
Time (single 
slice) 
(millisecond) 
             
4.4 Intra-O-DU mobility 19 
4.4.1 Test description and applicability 20 
The purpose of the test is to verify intra O-CU, intra O-DU handover of a UE.  The test validates the O-CU, O-DU, and 21 
O-RU functionality in handling inter-cell mobility when two O-RUs are connected to an O-DU. The test measures the 22 
DL / UL throughput variations, handover latency, handover interruption and packet loss during the mobility.  Test 23 
scenarios are classified into two groups as Standalone (SA) and Non-Standalone (NSA). 24 
Intra O-DU mobility with SA shall follow 3GPP TS 38.401 [29], Section 8.2.1 and 3GPP TS 38.473 [32], Section 8.3.4 25 
for the call flow.  Intra O-DU mobility with NSA shall follow 3GPP TS 38.401 [29], Section 8.2.1, 3GPP TS 38.473 26 
[32], Section 8.3.4 and 3GPP TS 37.340 [30] for the call flow.  27 

  
Page 31 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
4.4.2 Test setup and configuration 1 
In NSA mode, the test setup consists of one 4G cell (MeNB) and two 5G cells (SgNB). Each 5G cell is associated with 2 
a single O-DU, connected to a single O-CU, refer to Figure 4-1 for the test setup topology.  The test environment shall 3 
have a single UE with active data traffic. The application server should be placed as close as possible to the core and 4 
connected to the core via a transport link with enough capacity. 5 
 6 
Figure 4-1 Intra O-DU mobility test bed for NSA mode of operation. 7 
In SA, the test setup consists of two 5G cells, each one associated with the same O-DU and O-CU connected to a 5G 8 
core network (see Figure 4-2 for the test setup topology).  The test environment shall have a single UE with active data 9 
traffic. The application server should be placed as close as possible to the core and connected to the core via a transport 10 
link with enough capacity. 11 
 12 
Figure 4-2 Intra O-DU mobility test bed for SA mode of operation. 13 
Test configuration : The test configuration is not specified. The utilized test configuration (parameters) should be 14 
recorded in the test report.  15 
Laboratory setup: The radio conditions of UE can be modified by a variable attenuator/fading generator inserted 16 
between the antenna connectors (if available) of the O-RU and the UE, or appropriately emulated using a UE emulator. 17 


  
Page 32 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
The radio conditions of the UE are initially set to excellent using RSRP as the metric. The minimum coupling loss (see 1 
Section 3.6) should not be exceeded. The UE should be placed inside and RF shielded box or RF shielded room if the 2 
UE is not connected via cable. The UE handover between the cells can be achieved by changing the radio signal 3 
strength of the source and target cells using variable attenuators.  4 
 5 
Field setup: The drive route with source and target cells should be defined. The UE is placed in the centre of cell close 6 
to the radiated eNB/gNB antenna(s), where excellent radio conditions ( using RSRP as the metric as defined in Section 7 
3.6) should be observed. The minimum coupling loss (see 3.5) should not be exceeded. The change in radio conditions is 8 
achieved by moving the UE along the drive route from source cell to target cell. 9 
4.4.3 Test Procedure 10 
Below are the NSA mode steps  11 
1. The 4G and 5G cell setups are configured following Section 3.2. 12 
2. All the three cells are configured according to the test configuration. The cells are activated and unloaded.  13 
3. Both 5G cells are configured as neighbors to each other, so that the UE can trigger measurement events for 14 
handover.   15 
4. The source (cell 1) and target  (cell 2 ) 5G cells for intra O-DU mobility shall be depicted as in Figure 4-1. 16 
5. The test UE is under source 5G cell coverage. 17 
6. Power on the UE and UE shall successfully complete the LTE attach followed by successful SgNB addition to 18 
source 5G cell. 19 
7. The full-buffer UDP bi-directional data transmission (see 3.4) from the application server is initiated. 20 
8. The UE shall move from the source 5G cell to the target 5G cell to trigger a handover. 21 
Below are the SA Mode steps  22 
1. The 5G cell setup is configured following Section 3.2. 23 
2. Configure two 5G cells within an O-DU according to the test configuration. The cells are activated and 24 
unloaded.  25 
3. Both 5G cells are configured as neighbors to each other, so that  the UE can trigger measurement events for 26 
handover.   27 
4. The source and target 5G cells for intra O-DU mobility shall be depicted as in Figure 4-2. 28 
5. The test UE is under source cell coverage. 29 
6. Power on the UE and UE shall successfully register to source 5G cell. 30 
7. The full-buffer UDP bi-directional data transmission (see 3.4) from the application server is initiated. 31 
8. The UE shall move from source cell to target cell to perform handover. 32 
4.4.4 Test requirements (expected results) 33 
The intra O-DU handover call flow shall be verified for both NSA and SA use cases. Following functionalities shall 34 
also be validated: 35 

  
Page 33 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
• PDU Session is established when full-buffer bi-directional data transmission is initiated. (Only in SA Mode) 1 
• Handover is successful.  2 
In addition to the common minimum set of configuration parameters defined (see Section 3.3), the following metrics 3 
and counters shall be recorded and reported for the performance assessment. 4 
eNB/gNB/Application server side: 5 
• Transmit downlink throughput measured at application server in time (average per second)  6 
• Received uplink throughput measured at application server in time (average per second)  7 
• Uplink packet loss percentage during handover. 8 
• Uplink BLER, MCS, MIMO rank (RI) on PUSCH in time (average per second)  9 
UE side: 10 
• Radio parameters such as RSRP, RSRQ, SINR on PDSCH in time (average per second) 11 
• Downlink BLER, MCS, MIMO rank (RI) on PDSCH in time (average per second). 12 
• Received Downlink throughput (L1 and L3 PDCP layers) in time (average per second). 13 
• Downlink packet loss percentage during handover 14 
• Uplink throughput (L1 and L3 PDCP layers) in time (average per second) 15 
• Channel utilization, i.e. Number allocated/occupied downlink and uplink RBs in time (per TTI/average per 16 
second) and Number of allocated/occupied slots in time. 17 
• KPIs related to Handover failure, Call drop, Handover latency, Handover interruption time. 18 
4.5 Inter-O-DU mobility 19 
4.5.1 Test description and applicability 20 
The purpose of the test is to verify intra O-CU, inter O-DU handover of a UE.  The test validates the O-CU, O-DU 21 
functionality in handling Inter O-DU handover. The test measures the DL / UL throughput variations, handover latency, 22 
handover interruption and packet loss during the handover procedure.  Test scenarios are classified into two groups as 23 
SA (Standalone) and NSA (Non-Standalone).  24 
Inter O-DU mobility with SA shall follow 3GPP TS 38.401 [31], Section 8.2.1 and Inter O-DU mobility with NSA shall 25 
follow 3GPP TS 38.401[31], Section 8.2.2 for the call flow.  26 
3GPP 38.401 v15.7.0 has introduced a new CR 0104, which has modified the initial part of call flow for Section 8.2.2. 27 
The ORAN system supporting 3GPP specification later than v15.7.0 shall follow Section 8.2.2 of 3GPP TS 38.401 28 
v15.7.0 or later to verify the inter O-DU handover. 29 
4.5.2 Test setup and configuration 30 
In Non-Standalone Mode, the test setup consists of one 4G Cell (MeNB) and two 5G cells (SgNB). Each 5G Cell is 31 
associated with different O-DUs, connected to the same O-CU.  The test environment shall have a single UE with active 32 
data traffic. The application server should be placed as close as possible to the core and connected to the core via a 33 
transport link with enough capacity. 34 
 35 

  
Page 34 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
 1 
Figure 4-3 Inter O-DU mobility test bed for NSA mode of operation. 2 
In standalone Mode, the test setup consists of two 5G cells, each one associated with a different O-DU, connected to the 3 
same O-CU.  The test environment shall have a single UE with active data traffic. The application server should be 4 
placed as close as possible to the core and connected to the core via a transport link with enough capacity. 5 
 6 
Figure 4-4 Inter O-DU mobility test bed for SA mode of operation 7 
 8 
Test configuration : The test configuration is not specified. The utilized test configuration (parameters) should be 9 
recorded in the test report.  10 
Laboratory setup: The radio conditions of UE can be modified by a variable attenuator/fading generator inserted 11 
between the antenna connectors (if available) of the O-RU and the UE, or appropriately emulated using a UE emulator. 12 
The radio conditions of UE are initially set to excellent. The minimum coupling loss (see Section 3.6) should not be 13 
exceeded. The UE should be placed inside RF shielded box or RF shielded room if the UE is not connected via cable. 14 
The UE handover between the cells can be achieved by changing radio signal strength of source and target cells using 15 
variable attenuators.  16 
 17 


  
Page 35 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
Field setup: The drive route with source and target cells should be defined. The UE is placed in the centre of cell close 1 
to the radiated eNB/gNB antenna(s), where excellent radio conditions (RSRP as defined in Section  3.6) should be 2 
observed. The minimum coupling loss (see 3.5) should not be exceeded. The change in radio conditions is achieved by 3 
moving the UE along the drive route from source cell to target cell. 4 
4.5.3 Test Procedure 5 
In Non-Standalone Mode 6 
1. The 4G and 5G cell setups are configured following Section 3.2. 7 
2. All the three cells are configured according to the test configuration. The cells are activated and unloaded.  8 
3. Both 5G cells are configured neighbors to each other, so that UE can trigger measurement events for handover.   9 
4. The test UE is under source O-DU cell coverage. 10 
5. Power on the UE and the UE shall successfully complete the LTE attach followed by successful SgNB 11 
addition to source O-DU. 12 
6. The full-buffer UDP data transmission (see 3.4) from the application server is initiated. 13 
7. The UE shall move from source O-DU to target O-DU to perform handover. 14 
In Standalone Mode 15 
1. The 5G cell setup is configured following Section 3.2. 16 
2. All the 5G cells are configured according to the test configuration. The cells are activated and unloaded.  17 
3. Both 5G cells are configured neighbors to each other, so that UE can trigger measurement events for handover.   18 
4. The test UE is under source O-DU cell coverage. 19 
5. Power on the UE and the UE shall successfully register to source O-DU cell. 20 
6. The full-buffer UDP data transmission (see 3.4) from the application server is initiated. 21 
7. The UE shall move from source O-DU to target O-DU to perform handover. 22 
4.5.4 Test requirements (expected results) 23 
The inter O-DU handover call flow shall be verified for both NSA and SA use cases. Following functionalities shall 24 
also be validated: 25 
• PDU Session is established when full-buffer bi-directional data transmission is initiated. (Only in SA Mode) 26 
• Handover is successful.  27 
In addition to the common minimum set of configuration parameters defined (see Section 3.3), the following metrics 28 
and counters shall be recorded and reported for the performance assessment. 29 
eNB/gNB/Application server side: 30 
• Transmit downlink throughput measured at application server in time (average per second)  31 
• Received uplink throughput measured at application server in time (average per second)  32 
• Uplink packet loss percentage during handover. 33 
• Uplink BLER, MCS, MIMO rank (RI) on PUSCH in time (average per second)  34 
UE side: 35 
• Radio parameters such as RSRP, RSRQ, SINR on PDSCH in time (average per second) 36 
• Downlink BLER, MCS, MIMO rank (RI) on PDSCH in time (average per second). 37 

  
Page 36 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
• Received Downlink throughput (L1 and L3 PDCP layers) in time (average per second). 1 
• Downlink packet loss percentage during handover 2 
• Uplink throughput (L1 and L3 PDCP layers) in time (average per second) 3 
• Channel utilization, i.e. Number allocated/occupied downlink and uplink RBs in time (per TTI/average per 4 
second) and Number of allocated/occupied slots in time. 5 
• KPIs related to Handover failure, Call drop, Handover latency, Handover interruption time. 6 
4.6 Inter-O-CU mobility 7 
4.6.1 Test description and applicability 8 
The purpose of the test is to verify inter O-CU handover of the UE.  The test validates the O-CU, O-DU functionality in 9 
handling inter O-CU mobility connected to same 5G Core Network (in SA) or Master eNB (in NSA) . The test 10 
measures the DL / UL throughput variations, handover latency, handover interruption and packet loss during the 11 
mobility.  Test scenarios are classified into two groups as Standalone (SA) and Non-Standalone (NSA). 12 
Inter O-CU mobility with SA- Xn based Handover call flow shall follow 3GPP TS 38.401 [31], Section 8.9.4 and 13 
Section 8.9.5. Inter O-CU mobility with NSA shall follow 3GPP TS 37.340 [30], Section 10.5.1 for the call flow. 14 
4.6.2 Test setup and configuration 15 
In non-standalone mode, the test setup consists of a 4G cell (MeNB) and two 5G cells (SgNB), each 5G cell is 16 
associated with a different O-DU and O-CU connected to the same 4G core network, refer to Figure 4-5 for the test 17 
setup topology.  The test environment shall have single UE with active data traffic. The application server should be 18 
placed as close as possible to the core and connected to the core via a transport link with enough capacity. 19 
 20 
Figure 4-5 Inter O-CU mobility test bed for NSA mode of operation 21 
In standalone mode, The test setup consists of two 5G cells, each one associated with a different O-DU and O-CU 22 
connected to same 5G Core network (see Figure 4-6 for the test setup topology).  The test environment shall have a 23 
single UE with active data traffic. The application server should be placed as close as possible to the core and connected 24 
to the core via a transport link with enough capacity. 25 


  
Page 37 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
 1 
Figure 4-6  Inter O-CU mobility test bed for SA mode of operation. 2 
 3 
Test configuration : The test configuration is not specified. The utilized test configur ation (parameters) should be 4 
recorded in the test report.  5 
Laboratory setup: The radio conditions of UE can be modified by a variable attenuator/fading generator inserted 6 
between the antenna connectors (if available) of the O-RU and the UE, or appropriately emulated using a UE emulator. 7 
The radio conditions of UE are initially set to excellent. The minimum coupling loss (see Section 3.6) should not be 8 
exceeded. The UE should be placed inside RF shielded box or RF shielded room if the UE is not connected via cable. 9 
The UE handover between the cells can be achieved by changing radio signal strength of source and target cells using 10 
variable attenuators. 11 
 12 
Field setup: The drive route with source and target cells should be defined. The UE is placed in the centre of cell close 13 
to the radiated eNB/gNB antenna(s), where excellent radio conditions (RSRP as defined in Section 3.6) should be 14 
observed. The minimum coupling loss (see 3.5) should not be exceeded. The change in radio conditions is achieved by 15 
moving the UE along the drive route from source cell to target cell. 16 
 17 
4.6.3 Test Procedure 18 
In Non-Standalone Mode 19 
1. The 4G and 5G cell setups are configured following Section 3.2. 20 
2. Configure two 5G cells connected to different O-DU and O-CU according to the test configuration. The cells 21 
are activated and unloaded.  22 
3. 5G cells are configured as neighbors to 4G cell, so that UE can trigger measurement events for mobility.   23 
4. The source cell (source O-DU and source O-CU) is the cell where UE is initially placed as depicted in Figure 24 
4-5. 25 
5. Power on the UE and the UE shall successfully complete the LTE attach followed by successful SgNB 26 
addition to source 5G  cell. 27 


  
Page 38 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
6. The full-buffer UDP bi-directional data transmission (see 3.4) from the application server is initiated. 1 
7. The UE shall move from source 5G cell to target 5G cell. 2 
In Standalone Mode 3 
1. The 5G cell setup is configured following Section 3.2. 4 
2. Configure two 5G cells connected to different O-DU and O-CU according to the test configuration. The cells 5 
are activated and unloaded.  6 
3. Both 5G cells are configured neighbors to each other, so that UE can trigger measurement events for handover.   7 
4. The source cell (source O-DU and source O-CU) is the cell where UE is initially placed as depicted in Figure 8 
4-6. 9 
5. Power on the UE and UE shall successfully register to source 5G cell. 10 
6. The full-buffer UDP bi-directional data transmission (see 3.4) from the application server is initiated. 11 
7. The UE shall move from source cell to target cell to perform handover. 12 
4.6.4 Test requirements (expected results) 13 
The inter O-CU handover call flow shall be verified for NSA use case. Following functionalities shall also be validated: 14 
• PDU Session is established when full-buffer bi-directional data transmission is initiated. (Only in SA Mode) 15 
• Handover is successful.  16 
In addition to the common minimum set of configuration parameters defined (see Section 3.3), the following metrics 17 
and counters should be recorded and reported for the performance assessment 18 
UE side: 19 
• Radio parameters such as RSRP, RSRQ, SINR on PDSCH in time (average per second) 20 
• Downlink BLER, MCS, MIMO rank (RI) on PDSCH in time (average per second). 21 
• Received Downlink throughput (L1 and L3 PDCP layers) in time (average per second). 22 
• Downlink packet loss percentage during handover 23 
• Uplink throughput (L1 and L3 PDCP layers) in time (average per second) 24 
• Channel utilization, i.e. Number allocated/occupied downlink and uplink RBs in time (per TTI/average per 25 
second) and Number of allocated/occupied slots in time. 26 
• KPIs related to Handover failure, Call drop, Handover latency, Handover interruption time. 27 
eNB/gNB/Application server side: 28 
• Transmit downlink throughput measured at application server in time (average per second)  29 
• Received uplink throughput measured at application server in time (average per second)  30 
• Uplink packet loss percentage during handover. 31 
• Uplink BLER, MCS, MIMO rank (RI) on PUSCH in time (average per second)  32 
     33 
 34 

  
Page 39 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
 Performance tests 1 
This chapter describes the tests evaluating and assessing the performance of radio access network from network end-2 
to-end perspective (see Section 3.1). The focus of the testing is on the end-user performance which is compared against 3 
the target and expected performance values. The pass and fail thresholds are defined for the test wherever possible. 4 
The general test methodologies and configurations are mentioned in Chapter 3.   5 
Unless otherwise stated in the chapter, th e tests are suitable and can be performed in both laboratory as well as field 6 
testing environments, with pros and cons for each environment. The specific lab and field test setups are mentioned in 7 
each test. 8 
The following end-to-end performance tests are defined in this chapter as an extension of NGMN testing framework [3] 9 
• Downlink peak throughput (single cell, single UE scenario) 10 
• Uplink peak throughput (single cell, single UE scenario) 11 
• Downlink throughput in different radio conditions (single cell, single UE scenario) 12 
• Uplink throughput in different radio conditions (single cell, single UE scenario) 13 
• Bidirectional throughput in different radio conditions (single cell, single UE scenario) 14 
• Downlink coverage throughput (link budget) (single cell, single UE scenario) 15 
• Uplink coverage throughput (link budget) (single cell, single UE scenario) 16 
Future versions of this specification may add additional end-to-end performance tests not currently addressed in this 17 
version, for example: 18 
• Downlink aggregated cell throughput (cell capacity) (single cell scenario, multi-UE scenario) 19 
• Uplink aggregated cell throughput (cell capacity) (single cell scenario, multi-UE scenario) 20 
• Downlink throughput with inter-cell interferences (multi-cell, single UE scenario) 21 
• Uplink throughput with inter-cell interferences (multi-cell, single UE scenario) 22 
• Downlink drive throughput (multi-cell, single UE scenario) 23 
• Uplink drive throughput (multi-cell, single UE scenario) 24 
• End-to-end latency (single cell scenario, single UE scenario) 25 
• Impact of fronthaul compression scheme on downlink peak throughput (single cell, single UE scenario) 26 
• Impact of fronthaul compression scheme on uplink peak throughput (single cell, single UE scenario) 27 
• Resource scheduling (single cell, multi-UE scenario) 28 
• Impact of fronthaul latency on downlink peak throughout (single-cell, single UE scenario) 29 
• Impact of fronthaul latency on uplink peak throughout (single-cell, single UE scenario) 30 
• Impact of midhaul latency on downlink peak throughout (single-cell, single UE scenario) 31 
• Impact of midhaul latency on uplink peak throughout (single cell, single UE scenario) 32 

  
Page 40 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
5.1 Expected throughput calculation  1 
This chapter provides methodology to calculate expected theoretical downlink and uplink data throughput for 4G LTE 2 
as well 5G NR and for both FDD and TDD duplex modes. 3 
5.1.1 4G LTE  4 
The 4G LTE expected theoretical downlink or uplink throughput at the physical layer can be calculated using the 5 
following formula: 6 
𝐿1 𝑡ℎ𝑟𝑜𝑢𝑔ℎ𝑝𝑢𝑡 [𝑀𝑏𝑝𝑠] = 10−6 · ∑ (𝑣𝐿𝑎𝑦𝑒𝑟
𝑗 · 𝑄𝑚
𝑗 · 𝑅 · 𝑁𝑃𝑅𝐵
𝐵𝑊,𝑗 · 12
𝑇𝑠
· (1 − 𝑂𝐻𝑗 ) · 𝐷𝐿𝑈𝐿𝑟𝑎𝑡𝑖𝑜)
𝐽
𝑗=1
 7 
where 8 
• J is the number of aggregated LTE component carriers. 9 
• 𝑅 is the coding rate corresponding to channel quality (CQI and SINR). The maximum coding rate is 0.9258. 10 
For the j-th component carrier: 11 
• 𝑣𝐿𝑎𝑦𝑒𝑟𝑠 is the number of MIMO layers. The maximum is 4 (8 in LTE-Advanced) in downlink and 1 (4 in LTE 12 
-Advanced) in uplink. 13 
• 𝑄𝑚 is the modulation order, which is equal to 2 for QPSK, 4 for 16QAM, 5 for 32QAM, 6 for 64QAM, 8 for 14 
256QAM. 15 
• 𝑁𝑃𝑅𝐵
𝐵𝑊  is the number of PRBs allocated in bandwidth BW – see Table 5-1. 16 
Table 5-1 The number of PRBs allocated in bandwidth 17 
Bandwidth [MHz] 1.4 3 5 10 15 20 
Number of PRBs 6 13 25 50 75 100 
• OH is overhead for control channels and signalling (i.e. Reference Signal, PSS, SSS, PBCH, PDCCH, etc. in 18 
downlink or SRS, PUCCH, PRACH in uplink) within a period of 1 sec – see Table 5-2 for downlink. 19 
Table 5-2 Overhead as a function of bandwidth, no. of antenna ports and CFI  20 
Bandwidth 1.4 MHz 3 MHz 5 MHz 10 MHz 15 MHz 20 MHz 
CFI 1 2 1 2 1 2 1 2 1 2 1 2 
1 TX (ant. ports) 0.16 0.21 0.14 0.18 0.12 0.17 0.12 0.16 0.11 0.16 0.11 0.16 
2 TX (ant. ports) 0.20 0.24 0.17 0.22 0.16 0.21 0.15 0.20 0.15 0.20 0.15 0.20 
4 TX (ant. ports) 0.25 0.29 0.22 0.26 0.21 0.25 0.20 0.25 0.20 0.25 0.20 0.24 
• 𝑇𝑠 is the average OFDM symbol duration in a subframe, and it is equal to  𝑇𝑠 = 10−3/14 for normal Cyclic 21 
Prefix (14 OFDM symbols per slot) or  𝑇𝑠 = 10−3/12 for extended CP (12 OFDM symbols per slot). 22 
• 𝐷𝐿𝑈𝐿𝑟𝑎𝑡𝑖𝑜 is a ratio of the symbols allocated for DL or UL data to total number of symbols in a frame (10ms) 23 
– predefined patters (uplink-downlink configuration, special subframe configuration) for DL/UL allocation in 24 
[25]. In case of FDD mode, the DLULratio is equal to 1. 25 
Example of calculation of expected downlink throughput for the following system configuration:  26 
• MIMO 2x2 → 𝑣𝐿𝑎𝑦𝑒𝑟𝑠 = 2 27 

  
Page 41 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
• BW = 20 MHz → 𝑁𝑃𝑅𝐵
𝐵𝑊 =100 1 
• Normal CP 2 
• CFI = 1 → OH = 0.15 3 
• no carrier aggregation → J = 1 4 
• MCS = 28 and modulation 64QAM → 𝑄𝑚 = 6 and 𝑅 = 0.9258 5 
• TDD with uplink-downlink configuration = 1 (i.e. DSUUD) and special subframe configuration = 7 (i.e. 6 
10:2:2) → 𝐷𝐿𝑈𝐿𝑟𝑎𝑡𝑖𝑜 = (14 · 2 + 10)/14 · 5) =  0.543 7 
𝐿1 𝑡ℎ𝑟𝑜𝑢𝑔ℎ𝑝𝑢𝑡 [𝑀𝑏𝑝𝑠] = 10−6 · ∑ (2 · 6 · 0.9258 · 100 · 12
10−3
14
· (1 − 0.15) · 0.543)
1
𝑗=1
= 86.1 𝑀𝑏𝑝𝑠 8 
 9 
5.1.2 5G NR 10 
The 5G NR expected theoretical downlink or uplink throughput at physical layer (UE category is not assumed) can be 11 
calculated using the following formula [15]: 12 
𝐿1 𝑡ℎ𝑟𝑜𝑢𝑔ℎ𝑝𝑢𝑡 [𝑀𝑏𝑝𝑠] = 10−6 · ∑ (𝑣𝐿𝑎𝑦𝑒𝑟𝑠
𝑗 · 𝑄𝑚
𝑗 · 𝑓𝑗 · 𝑅 · 𝑁𝑃𝑅𝐵
𝐵𝑊𝑗,𝜇 · 12
𝑇𝑆
𝜇 · (1 − 𝑂𝐻𝑗 ) · 𝐷𝐿𝑈𝐿𝑟𝑎𝑡𝑖𝑜)
𝐽
𝑗=1
 13 
where 14 
• J is the total number of component carriers (CC) in a band or band combination. The maximum number is 16 15 
[23]. 16 
• 𝑅 is the coding rate corresponding to channel quality (CQI, SINR), and it is calculated as Target_code_rate 17 
[22]/1024. For LDPC code the maximum coding rate is 948/1024. 18 
For the j-th component carrier: 19 
• 𝑣𝐿𝑎𝑦𝑒𝑟𝑠 is the number of MIMO layers. The maximum is 8 in downlink and 4 in uplink [23]. 20 
• 𝑄𝑚 is the modulation order, which is equal to 2 for QPSK, 4 for 16QAM, 5 for 32QAM, 6 for 64QAM, 8 for 21 
256QAM. 22 
• 𝑓 is the scaling factor [24] which can take the values 0.4, 0.75, 0.8 or 1. The scaling factor is signalled per 23 
band and per band combination as per UE capability signalling.   24 
• 𝑂𝐻 is the overhead of control channels and signalling within a period of 1 sec, and it is equal to  25 
o 0.14 for frequency range 1 (FR1) for DL 26 
o 0.18 for frequency range 2 (FR2) for DL 27 
o 0.08 for frequency range 1 (FR1) for UL 28 
o 0.10 for frequency range 2 (FR2) for UL 29 
• µ is 5G NR numerology (sub-carrier spacing) which can take the values from 0 to 4 [20]. The number of slots 30 
per frame and sub-carrier spacing vary with numerology – see Table 5-3.  31 
 32 

  
Page 42 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
Table 5-3 5G NR numerology 1 
5G NR 
numerology µ 
Sub-carrier 
spacing (SCS) 
Number of slots per frame Number of symbols 
per slot 
Total number of 
symbols per frame 
0 15 kHz 10 14 (normal CP) 140 
1 30 kHz 20 14 (normal CP) 280 
2 
60 kHz 40 14 (normal CP) 560 
60 kHz 40 12 (extended CP) 480 
3 120 kHz 80 14 (normal CP) 1120 
4 240 kHz 160 14 (normal CP) 2240 
• 𝑇𝑆
𝜇 is the average OFDM symbol duration in a subframe for numerology µ, and it is equal to 𝑇𝑆
𝜇 = 10−3/(14 ·2 
2µ) for normal Cyclic Prefix (14 OFDM symbols per slot) or  𝑇𝑆
𝜇 = 10−3/(12 · 2µ) for extended CP (12 OFDM 3 
symbols per slot).  4 
• 𝑁𝑃𝑅𝐵
𝐵𝑊,µ is the number of PRBs allocated in bandwidth BW with numerology µ - see Table 5-4.  5 
Table 5-4 The max. number of PRBs  for each supported bandwidth and 5G NR numerology µ [18], [19] 6 
 Channel bandwidth BW [MHz] 
µ 5 10 15 20 25 30 40 50 60 80 90 100 200 400 
0 25 52 79 106 133 160 216 270 N/A N/A N/A N/A N/A N/A 
1 11 24 38 51 65 78 106 133 162 217 245 273 N/A N/A 
2 N/A 11 18 24 31 38 51 65 79 107 121 135 N/A N/A 
3 N/A N/A N/A N/A N/A N/A N/A 66 N/A N/A N/A 132 264 N/A 
4 N/A N/A N/A N/A N/A N/A N/A 32 N/A N/A N/A 66 132 264 
• 𝐷𝐿𝑈𝐿𝑟𝑎𝑡𝑖𝑜 is a ratio of the symbols allocated for DL or UL d ata to total number of symbols in a time period 7 
(periodicity) in TDD mode – predefined slot formats in [21]. In case of FDD mode, the DLULratio is equal to 1.   8 
Example of calculation of expected downlink throughput for the following system configuration:  9 
• SCS = 15kHz → µ = 0 10 
• MIMO 2x2 → 𝑣𝐿𝑎𝑦𝑒𝑟𝑠 = 2 11 
• BW = 20 MHz → 𝑁𝑃𝑅𝐵
𝐵𝑊,µ=106 12 
• Normal CP 13 
• no carrier aggregation → J = 1 14 
• MCS = 28 and modulation 64QAM → 𝑄𝑚 = 6 and Target code rate = 948 15 
• scaling factor f = 1 16 
• downlink throughput and FR1 → OH = 0.14 17 
• TDD configuration 4:1 (DDDDSU), i.e. four DL slots (slot format 0 [21]), one Special slot (slot format 32 18 
[21]) and one UL slot (slot format 1 [21]) where slot format 0 means allocation of all 14 symbols for DL, slot 19 
format 1 means allocation of all 14 symbols for UL, slot format 32 means allocation of 10 symbols for DL, 2 20 
symbols for Guard Period, 2 symbols for UL → 𝐷𝐿𝑈𝐿𝑟𝑎𝑡𝑖𝑜 = (14 · 4 + 10)/14 · 6) =  0.7857  21 

  
Page 43 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
𝐿1 𝑡ℎ𝑟𝑜𝑢𝑔ℎ𝑝𝑢𝑡 [𝑀𝑏𝑝𝑠] = 10−6 · ∑ (2 · 6 · 1 · 948/1024 · 106 · 12
10−3
14 · 20
· (1 − 0.14) · 0.7857) =
1
𝑗=1
133.6 𝑀𝑏𝑝𝑠 1 
 2 
5.2 Downlink peak throughput 3 
5.2.1 Test description and applicability 4 
The purpose of the test is to measure the peak (i.e. maximum achievable) user data throughput in the downlink direction 5 
(i.e. data transmitted from application (traffic) server to UE).  A stationary UE is placed under excellent radio conditions 6 
inside an isolated cell.   7 
5.2.2 Test setup and configuration 8 
The test setup is a single cell scenario (i.e. an isolated cell without any inter-cell interference – see Section 3.7) with a 9 
stationary UE (real or emulated UE) placed under excellent radio conditions as defined in Section 3.6 – using SINR as 10 
the metric since this is a downlink test. Note that in this case of a single cell scenario, SINR is in fact SNR as inter-cell 11 
interference is not present. Within the cell there should be only one active UE downloading data from the application 12 
server.  The application server should be placed as close as possible to the core /core emulator and connected to the 13 
core/core emulator via a transport link with sufficient capacity so as not to limit the expected data throughput. The test is 14 
suitable for both lab and field environments. 15 
Test configuration : The test configuration  is not specified. The utilized test configuration (parameters) should be  16 
recorded in the test report.  17 
Laboratory setup: The radio conditions experienced by the UE can be  modified using a variable attenuator /fading 18 
generator inserted between the antenna connectors (if available) of the O-RU and the UE, or appropriately emulated using 19 
a UE emulator. The test environment should be setup to achieve excellent radio conditions (SINR as defined in Section 20 
3.6) for the UE, but the minimum coupling loss (see Section 3.6) should not be exceeded.  The UE should be placed inside 21 
an RF shielded box or RF shielded room if the UE is not connected via cable. 22 
Field setup: The UE is placed in the centre of cell close to the radiated eNB/gNB antenna(s), where excellent radio 23 
conditions (SINR as defined in Section 3.6) should be observed. The minimum coupling loss (see Section 3.6) should not 24 
be exceeded. 25 
 
3GPP Core
4G EPC4G eNB S1 3GPP 
services
Other 
services
Application server
O-RAN System under test
 UE Uu

  
Page 44 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
 
 
Figure 5-1 The test setups of 4G, 5G NSA and 5G SA 1 
5.2.3 Test Procedure 2 
1. The test setup is configured according to the test configuration. The test configuration should be recorded in the test 3 
report. The serving cell under test is activated and unloaded. All other cells are turned off.  4 
2. The UE (real or emulated UE)  is placed under excellent radio conditions (cell centre close to radiated eN B/gNB 5 
antenna) using SINR thresholds as indicated in Section 3.6. The UE is powered on and attached to the network.  6 
3. The downlink full-buffer UDP and TCP data transmission (see Section 3.4) from the application server should be 7 
verified by adjusting the connection settings (cabled environment) or UE position (OTA environment). The UE under 8 
excellent radio conditions that is achieving peak user throughput should see stable utilization of the highest possible 9 
downlink MCS, downlink transport block size and downlink MIMO rank (number of layers). These KPIs should also 10 
be verified.  11 
4. The UE should be turned off or set to airplane mode, if possible, to empty the buffers. The downlink full-buffer UDP 12 
data transmission from the application server to the UE is started. The UE should receive the sent data.  13 
5. All the required performance data (incl. the signalling and control data)  as specified in the “Test requirements” 14 
Section below is measured and captured at the UE and Application server sides using logging/measurement tools. 15 
The duration of the test should be at least 3 minutes when the throughput is stable. The location and position of the 16 
UE should remain unchanged during the entire measurement duration (capture of log data).  17 
6. The capture of log data is stopped. The downlink full-buffer UDP data transmission from the application server is 18 
stopped.  19 
7. The UE should be turned off then on again using, e.g. airplane mode, to empty the buffers. Start the downlink full-20 
buffer TCP data transmission from the application server to the UE, and Step 5 is repeated.  21 
5.2.4 Test requirements (expected results) 22 
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 23 
should be captured and reported in the test report for the performance assessment 24 
UE side (real or emulated UE): 25 
3GPP Core
4G EPC EN-DC NSA
5G en-gNB
4G eNB 
(anchor)
S1
S1
3GPP 
services
Other 
services
Application server
X2
O-RAN System under test
 UE
Uu
Uu
3GPP Core
5GC SA5G gNB NG 3GPP 
services
Other 
services
Application server
O-RAN System under test
 UE Uu

  
Page 45 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
• Radio parameters such as RSRP, RSRQ, CQI, PDSCH SINR (average sample per second) 1 
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second) 2 
• Received downlink throughput (L1 and Application layers) (average sample per second) 3 
• Downlink transmission mode 4 
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 5 
(average sample per second) 6 
Application server side: 7 
• Transmitted downlink throughput (Application layer) (average sample per second) 8 
When the UE is under excellent radio conditions (cell centre), the stable utilization of the highest possible downlink MCS, 9 
downlink transport block size and downlink MIMO rank should be observed. The UE should also receive the data with 10 
minimum downlink BLER. 11 
The Table 5-5 gives an example of the test results record (median and standard deviation from the captured samples 12 
should be calculated for each metric). In case of 5G SA and NSA, SS-RSRP and SS-SINR should be reported. In case 13 
of 5G NSA and dual connectivity (EN-DC), the values should be provided separately for both LTE and 5G paths. The 14 
spectral efficiency (see Section 3.8) should be calculated for benchmarking and comparison purposes in order to 15 
minimize the influence of different configured parameters such as bandwidth and TDD DL/UL ratio. 16 
Table 5-5 Example record of test results (median and standard deviation from the captured samples) 17 
 UDP TCP 
Received L1 DL throughput [Mbps]   
L1 DL Spectral efficiency [bps/Hz]    
Received Application DL throughput [Mbps]   
UE RSRP [dBm]   
UE RSRQ [dB]   
UE SINR [dB]   
MIMO rank   
PDSCH MSC   
DL PRB number   
PDSCH BLER [%]   
The following figures should be also included in the test report, and the stable behavior should be observed and evaluated. 18 
• Received UDP downlink throughput (L1 and Application layer) vs Time duration 19 
• PDSCH SINR vs Time duration 20 
• Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots vs Time duration 21 
The gap analysis should be provided for the measured and the expected target downlink throughputs which can be 22 
calculated based on the procedures from Section 5.1.   23 

  
Page 46 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
5.3 Uplink peak throughput 1 
5.3.1 Test description and applicability 2 
The purpose of the test is to measure the peak (i.e. maximum achievable) user data throughput in uplink direction (i.e. 3 
data transmitted from UE to application (traffic) server).  The stationary UE is placed under excellent radio conditions 4 
inside the isolated cell.   5 
5.3.2 Test setup and configuration 6 
The test setup is a single cell scenario (i.e. isolated cell without any inter-cell interference – see Section 3.7) with stationary 7 
UE (real or emulated UE) placed under excellent radio conditions as defined in Section 3.6 – using RSRP as the metric 8 
since this is an uplink test. Within the cell there should be only one active UE uploading data to the application server.  9 
The application server should be placed as close as possible to the c ore/core emulator and connected to the core /core 10 
emulator via a transport link with sufficient capacity so as not to limit the expected data throughput. The test is suitable 11 
for both lab and field environments.  12 
Test configuration : The test configuration is not specified . The utilized test configuration (parameters) should be  13 
recorded in the test report.  14 
Laboratory setup: The radio conditions experienced by the UE can be modified using a variable attenuator/fading 15 
generator inserted between the antenna connectors (if available) of the O-RU and the UE, or appropriately emulated using 16 
a UE emulator. The test environment should be setup to achieve excellent radio conditions (RSRP as defined in Section 17 
3.6) for the UE, but the minimum coupling loss (see Section 3.6) should not be exceeded.  The UE should be placed inside 18 
an RF shielded box or RF shielded room if the UE is not connected via cable. 19 
Field setup: The UE is placed in the centre of cell close to the radiated  eNB/gNB antenna(s), where excellent radio 20 
conditions (RSRP as defined in Section 3.6) should be observed . The minimum coupling loss (see 3.5) should not be 21 
exceeded. 22 
The test setups of 4G, 5G NSA and 5G SA are mentioned in Figure 5-1.  23 
5.3.3 Test Procedure 24 
1. The test setup is configured according to the test configuration. The test configuration should be recorded in the test 25 
report. The serving cell under test is activated and unloaded. All other cells are turned off.  26 
2. The UE (real or emulated UE)  is placed under excellent  radio condition (cell centre close to radiated eNB/gNB 27 
antenna) using RSRP thresholds as indicated in Section 3.6. The UE is powered on and attached to the network.  28 
3. The uplink full-buffer UDP and TCP data transmission (see Section 3.4) from UE to the application server should be 29 
verified by adjusting the connection settings (cabled environment) or UE position (OTA environment). The UE under 30 
excellent radio conditions that is achieving peak user throughput should see stable utilization of the highest possible 31 
uplink MCS and uplink transport block size. These KPIs should also be verified.  32 
4. The UE should be turned off or set to airplane mode, if possible, to empty the buffers . The uplink full-buffer UDP 33 
data transmission from UE to the application server is started. The application server should receive the sent data.  34 
5. All the required performance data (incl. the signalling and control data)  as specified in the following “Test 35 
requirements” section  is measured and captured at UE , eNB/gNB and Application server  sides using 36 
logging/measurement tools. The duration of test s hould be at least 3 minutes when the throughput is stable. The 37 

  
Page 47 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
location and position of the UE should remain unchanged during the entire measurement duration (capture of log 1 
data).  2 
6. The capture of log data is stopped. The uplink full-buffer UDP data transmission from UE to the application server 3 
is stopped.  4 
7. The UE should be turned off or set to airplane mode, if possible, to empty the buffers . The uplink full-buffer TCP 5 
data transmission from UE to the application server is started, and Step 5 is repeated.  6 
5.3.4 Test requirements (expected results) 7 
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 8 
should be captured and reported in the test report for the performance assessment 9 
UE side (real or emulated UE): 10 
• Radio parameters such as RSRP, RSRQ, CQI, PDSCH SINR (average sample per second) 11 
• PUSCH BLER, PUSCH MCS (average sample per second) 12 
• Transmit power on PUSCH  13 
• Transmitted uplink throughput (Application layer) (average sample per second) 14 
• Channel utilization, i.e. Number allocated/occupied uplink PRBs and Number of allocated/occupied slots  15 
(average sample per second) 16 
eNB/gNB side (if capture of logs is possible): 17 
• Radio parameters such as PUSCH SINR (average per second) 18 
• PUSCH BLER (average sample per second) 19 
Application server side: 20 
• Received uplink throughput (L1 and Application layers) (average sample per second) 21 
When the UE is in excellent radio condition (cell centre), t he stable utilization of the highest possible uplink MCS and 22 
uplink transport block size should be observed and e valuated. The eNB/gNB should also receive the data with the 23 
minimum uplink BLER.   24 
The Table 5-6 gives an example of the test results record (median and standard deviation from the measured samples 25 
should be provided for each metric). In case of 5G SA and NSA, SS-RSRP and SS-SINR should be reported. In case of 26 
5G NSA and dual connectivity (EN -DC), the values should be provided separately for both LTE and 5G. The spectral 27 
efficiency (see Section 3.8) should be calculated for benchmarking and comparison in order to minimize the influence of 28 
different configured parameters such as bandwidth and TDD DL/UL ratio. 29 
Table 5-6 Example record of test results (median and standard deviation from the measured samples) 30 
 UDP TCP 
Received L1 UL throughput [Mbps]   
L1 UL Spectral efficiency [bps/Hz]   
Received Application UL throughput [Mbps]   
UE RSRP [dBm]   
UE PDSCH SINR [dB]   

  
Page 48 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
PUSCH transmit power [dBm]   
PUSCH MSC   
UL RB number   
PUSCH BLER [%]   
The following figures should be also included in the test report, and the stable behavior should be observed and evaluated. 1 
• Received UDP uplink throughput (L1 and Application layers) vs Time duration 2 
• UE RSRP vs Time duration 3 
• Number of allocated/occupied uplink PRBs and Number of allocated/occupied slots vs Time duration 4 
The gap analysis should be provided for the measured and the expected target uplink throughputs which can be calculated 5 
based on the procedures from Section 5.1.   6 
5.4 Downlink throughput in different radio conditions 7 
5.4.1 Test description and applicability 8 
The purpose of the test is to measure the user experienced data throughput in the downlink direction while varying 9 
received radio signal quality (strength). The UE is placed in different stationary points inside the isolated cell.  10 
5.4.2 Test setup and configuration 11 
The test setup is a single cell scenario (i.e. isolated cell without any inter-cell interference – see Section 3.7) with stationary 12 
UE (real or emulated UE) placed in different radio conditions as defined in Section 3.6 - SINR should be considered in 13 
case of downlink. Note that in this case of single cell scenario, SINR is in fact SNR as inter -cell interferences are not 14 
present. The UE is sequentially placed in different radio conditions ranging from good  to poor. Note that the testing of 15 
peak downlink throughout in excellent radio conditions is already covered in Section 5.2 and so is skipped in this section. 16 
Within the cell there should be only one active UE in time downloading data from the application (traffic) server.  The 17 
application server should be placed as close as possible to the core/core emulator and connected to the core/core emulator 18 
via a transport link with sufficient capacity so as not to limit the expected data throughput. The test is suitable for lab as 19 
well as field environment. 20 
Test configuration : The test configuration is not specified . The utilized test configuration (parameters) should be  21 
recorded in the test report.  22 
Laboratory setup: The radio conditions experienced by the UE can be modified using a variable attenuator/fading 23 
generator inserted between the antenna connectors (if available) of the O-RU and the UE, or appropriately emulated using 24 
a UE emulator. The radio conditions of UE are initially set to good. The minimum coupling loss (see Section 3.6) should 25 
not be exceeded.  The change in radio conditions of UE, from excellent through fair and good to poor, is  achieved by 26 
increasing the attenuation of radio signal. The UE should be placed inside RF shielded box or RF shielded room if the 27 
UE is not connected via cable. 28 
Field setup: The test points with good, fair and poor radio conditions (as defined in Section 3.6) should be defined inside 29 
the serving cell. The minimum coupling loss (see 3.5) should not be exceeded.  The UE is initially placed where good 30 
radio conditions (SINR as defined in Section 3.6) should be observed. The change in radio conditions is achieved by 31 
moving the UE inside the serving cell from close to cell centre (with good radio conditions) to cell edge (with poor radio 32 
conditions).  33 
The test setups of 4G, 5G NSA and 5G SA are mentioned in Figure 5-1.  34 

  
Page 49 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
5.4.3 Test Procedure 1 
1. The test setup is configured according to the test configuration. The test configuration should be recorded in the test 2 
report. The serving cell under test is activated and unloaded. All other cells are turned off.  3 
2. The UE (real or emulated UE) is placed under good radio conditions (close to cell centre) using SINR thresholds as 4 
indicated in Section 3.6. The UE is powered on and attached to the network.  5 
3. The downlink full-buffer UDP and TCP data transmission (see Section 3.4) from the application server should be 6 
verified by adjusting the connection settings (cabled environment) or UE position (OTA environment)  to achieve 7 
good radio conditions.  8 
4. The UE should be turned off or set to airplane mode, if possible, to empty the buffers. The downlink full-buffer UDP 9 
data transmission from the application server to the UE is started. The UE should receive the sent data.  10 
5. All the req uired performance data (incl. the signalling and control data)  as specified in the following “Test 11 
requirements” section is measured and captured at UE and Application server sides using logging/measurement tools. 12 
The duration of test should be at least 3 minutes when the throughput is stable. The location and position of the UE 13 
should remain unchanged during the entire measurement duration (capture of log data).  14 
6. The capture of log data is stopped. The downlink full-buffer UDP data transmission from the ap plication server is 15 
stopped.  16 
7. The radio conditions of UE are changed to fair using SINR thresholds as indicated in Section 3.6. The steps 4 to 6 17 
are repeated.  18 
8. The radio conditions of UE are changed to poor (cell edge) radio condition  using SINR thresholds as indicated  in 19 
Section 3.6. The steps 4 to 6 are repeated.  20 
9. The steps 4 to 8 are repeated for downlink full-buffer TCP data transmission from the application server to the UE.  21 
5.4.4 Test requirements (expected results) 22 
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 23 
should be captured and reported in the test report for the performance assessment 24 
UE side (real or emulated UE): 25 
• Radio parameters such as RSRP, RSRQ, CQI, PDSCH SINR (average sample per second) 26 
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second) 27 
• Received downlink throughput (L1, and Application layers) (average sample per second) 28 
• Downlink transmission mode 29 
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 30 
(average sample per second) 31 
Application server side: 32 
• Transmitted downlink throughput (Application layer) (average sample per second) 33 
As the UE moves from good (close to cell centre), to fair, and to poor (cell edge) radio conditions, the changing radio 34 
conditions should cause the UE to report lower CQI and M IMO rank which results in assignment of lower MCS and 35 
lower data throughput in the downlink. 36 

  
Page 50 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
The Table 5-7 gives an example of the test results record (median and standard deviation from the captured samples 1 
should be calculated for each metric). In case of 5G SA and NSA, SS-RSRP and SS-SINR should be reported. In case of 2 
5G NSA and dual connectivity (EN -DC), the values should be provided separately for both LTE and 5G paths. The 3 
spectral efficiency (see Section 3.8) should be calculated for benchmarking and comparison in order to minimize the 4 
influence of different configured parameters such as bandwidth and TDD DL/UL ratio. 5 
Table 5-7 Example record of test results (median and standard deviation from the captured samples) 6 
 
Good Fair Poor 
(cell edge) 
UDP / TCP UDP / TCP UDP / TCP 
Received L1 DL throughput [Mbps]    
L1 DL Spectral efficiency [bps/Hz]     
Received Application DL throughput [Mbps]    
UE RSRP [dBm]    
UE PDSCH SINR [dB]    
MIMO rank    
PDSCH MSC    
DL RB number    
PDSCH BLER [%]    
The following figures should be also included in the test report, and the stable behavior should be observed and evaluated. 7 
• Received UDP downlink throughput (L1 and Application layer) vs Time duration 8 
• PDSCH SINR vs Time duration 9 
• Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots vs Time duration 10 
The gap analysis should be provided for the measured and the expected target downlink throughputs which can be 11 
calculated based on the procedures from Section 5.1.    12 
5.5 Uplink throughput in different radio conditions 13 
5.5.1 Test description and applicability 14 
The purpose of the test is to measure the user experienced data throughput in uplink while varying received radio signal 15 
quality (strength). The UE is placed in different stationary points inside the isolated cell.  16 
5.5.2 Test setup and configuration 17 
The test setup is a single cell scenario (i.e. isolated cell without any inter-cell interference – see Section 3.7) with stationary 18 
UE (real or emulated UE) placed in different radio conditions as defined in Section 3.6 - RSRP should be considered in 19 
case of uplink. The UE is gradually placed in different radio conditions from good (close to cell centre) to poor (cell edge 20 
– coverage limited cell edge in case of single cell scenario). Note that the testing of peak uplink throughout in excellent 21 
radio conditions is already covered in Section 5.3 and so is skipped in this section. Within the cell there should be only 22 
one active UE in time uploading data to the application (traffic) server.  The application server should be placed as close 23 
as possible to the core/core emulator and connected to the core/core emulator via a transport link with sufficient capacity 24 
so as not to limit the expected data throughput. The test is suitable for both lab and field environments. 25 

  
Page 51 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
Test configuration : The test configuration is not specified . The utilized test configuration (parameters) should be  1 
recorded in the test report.  2 
Laboratory setup: The radio conditions experienced by the UE can be modified using a variable attenuator/fading 3 
generator inserted between the antenna connectors (if available) of the O-RU and the UE, or appropriately emulated using 4 
a UE emulator. The radio conditions of UE are initially set to good. The minimum coupling loss (see Section 3.6) should 5 
not be exceeded.  The change in radio conditions of UE, from excellent through fair and good to poor, is achieved by 6 
increasing the attenuation of radio signal. The UE should be placed inside RF shielded box or RF shielded room if the 7 
UE is not connected via cable. 8 
Field setup: The test points with good, fair and poor radio conditions (as defined in Section 3.6) should be identified 9 
inside the serving cell. The minimum coupling loss (see 3.5) should not be exceeded. The UE is initially placed where 10 
good radio conditions (RSRP as defined in Section 3.6) should be observed. The change in radio conditions is achieved 11 
by moving the UE inside the serving cell from close to cell centre (with good radio conditions) to cell edge (with poor 12 
radio conditions). 13 
The test setups of 4G, 5G NSA and 5G SA are illustrated in Figure 5-1.  14 
5.5.3 Test Procedure 15 
1. The test setup is configured according to the test configuration. The test configuration should be recorded in the test 16 
report. The serving cell under test is activated and unloaded. All other cells are turned off.  17 
2. The UE (real or emulated UE) is placed under good radio condition (close to cell centre) using RSRP thresholds as 18 
indicated in Section 3.6. The UE is powered on and attached to the network.  19 
3. The uplink full-buffer UDP and TCP data transmission (see Section 3.4) from UE to the application server should be 20 
verified by adjusting the connection settings (cabled environment) or UE position (OTA environment)  to achieve 21 
good radio conditions.   22 
4. The UE should be turned off or set to airplane mode, if possible, to empty the buffers . The uplink full-buffer UDP 23 
data transmission from UE to the application server is started. The application server should receive the sent data.  24 
5. All the required performance data (incl. the signalling and control data)  as specified in the following “Test 25 
requirements” section  is measured and captured at UE , eNB/gNB and application server side using 26 
logging/measurement tools. The duration of test should be at least 3 minutes when the throughput is stable. The 27 
location and position of the UE should remain unchanged during the entire measurement duration (capture of log 28 
data).  29 
6. The capture of log data is stopped. The uplink full-buffer UDP data transmission from UE to the application server 30 
is stopped.  31 
7. The radio conditions of UE are changed to fair using RSRP thresholds as indicated in Section 3.6. The steps 4 to 6 32 
are repeated.  33 
8. The radio conditions of UE are changed to poor (cell edge) radio condition using RSRP thresholds as indicated in 34 
Section 3.6. The steps 4 to 6 are repeated.  35 
9. The steps 4 to 8 are repeated for uplink full-buffer TCP data transmission from UE to the application server.  36 
5.5.4 Test requirements (expected results) 37 
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 38 
should be captured and reported in the test report for the performance assessment 39 

  
Page 52 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
UE side (real or emulated UE): 1 
• Radio parameters such as RSRP, RSRQ, CQI, PDSCH SINR (average sample per second) 2 
• PUSHC BLER, PUSCH MCS (average sample per second) 3 
• Transmit power on PUSCH  4 
• Transmitted uplink throughput (Application layer) (average sample per second) 5 
• Channel utilization, i.e. Number allocated/occupied uplink PRBs and Number of allocated/occupied slots 6 
(average sample per second) 7 
eNB/gNB side (if capture of logs is possible): 8 
• Radio parameters such as PUSCH SINR (average per second) 9 
• PUSCH BLER (average per second) 10 
Application server side: 11 
• Received uplink throughput (L1 and Application layers) (average sample per second) 12 
The Table 5-8 gives an example of the test results record (median and standard deviation from the captured samples 13 
should be calculated for each metric). In case of 5G SA and NSA, SS-RSRP and SS-SINR should be reported. In case of 14 
5G NSA and dual connectivity (EN -DC), the values should be provided separately for both LTE and 5G paths. The 15 
spectral efficiency (see Section 3.8) should be calculated for benchmarking and comparison in order to minimize the 16 
influence of different configured parameters such as bandwidth and TDD DL/UL ratio. 17 
Table 5-8 Example record of test results (median and standard deviation from the measured samples) 18 
 
Good Fair Poor 
(cell edge) 
UDP / TCP UDP / TCP UDP / TCP 
Received L1 UL throughput [Mbps]    
L1 UL Spectral efficiency [bps/Hz]     
Received Application UL throughput [Mbps]    
UE RSRP [dBm]    
UE PDSCH SINR [dB]    
PUSCH transmit power [dBm]    
PUSCH MSC    
UL RB number    
PUSCH BLER [%]    
The following figures should be also included in the test report, and the stable behavior should be observed and evaluated. 19 
• Received UDP uplink throughput (L1 and Application layers) vs Time duration 20 
• UE RSRP vs Time duration 21 
• Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots vs Time duration 22 
The gap analysis should be provided for the measured and the expected target uplink throughputs which can be calculated 23 
based on the procedures from Section 5.1.    24 

  
Page 53 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
5.6 Bidirectional throughput in different radio conditions 1 
5.6.1 Test description and applicability 2 
The purpose of the test is to measure the user experienced data throughput in both downlink and uplink in parallel while 3 
varying received radio signal quality (strength). The UE is placed in different stationary points inside the isolated cell. 4 
The test also includes the measurement of peak (maximum achievable) data throughput of UE located in the excellent 5 
radio conditions, and cell edge coverage data throughput of UE located at the cell edge in the poor radio conditions.  6 
5.6.2 Test setup and configuration 7 
The test setup is a single cell scenario (i.e. isolated cell without any inter-cell interference – see Section 3.7) with stationary 8 
UE (real or emulated) placed in different radio conditions as defined in Section 3.6 –RSRP should be considered in this 9 
case. The UE is gradually placed in different radio conditions from excellent (cell centre) to poor (cell edge  – coverage 10 
limited cell edge in case of single cell scenario). Within the cell there should be only one active UE in time simultaneously 11 
downloading data from and uploading data to the application (traffic) server.  The application server should be placed as 12 
close as possible to the core /core emulator and connected to the core /core emulator via a transport link with sufficient 13 
capacity so as not to limit the expected data throughput. The test is suitable for both lab and field environments. 14 
Test configuration : The test configuration is not specified . The utilized test configuration (parameters) should be  15 
recorded in the test report.  16 
Laboratory setup: The radio conditions experienced by the UE can be modified using a variable attenuator/fading 17 
generator inserted between the antenna connectors (if available) of the O-RU and the UE, or appropriately emulated using 18 
a UE emulator. The radio conditions of UE are initi ally set to excellent. The minimum coupling loss (see  Section 3.6) 19 
should not be exceeded. The change in radio conditions of UE, from excellent through fair and good to poor, is achieved 20 
by increasing the attenuation of radio signal. The UE should be placed inside RF shielded box or RF shielded room if the 21 
UE is not connected via cable. 22 
Field setup: The test points with excellent, good, fair and poor radio conditions ( as defined in Section 3.6) should be 23 
identified inside the serving cell. The minimum coupling loss (see 3.5) should not be exceeded. The UE is initially placed 24 
in the cell center close to the eNB/gNB antenna(s), where excellent radio conditions (using RSRP as the metric as defined 25 
in Section 3.6) should be observed. The change in radio conditions is achieved by moving the UE inside the serving cell 26 
from cell centre (with excellent radio conditions) to cell edge (with poor radio conditions). 27 
The test setups of 4G, 5G NSA and 5G SA are illustrated in Figure 5-1.  28 
5.6.3 Test Procedure 29 
1. The test setup is configured according to the test configuration. The test configuration should be recorded in the test 30 
report. The serving cell under test is activated and unloaded. All other cells are turned off.  31 
2. The UE (real or emulated) is placed under excellent radio conditions as using RSRP thresholds as indicated in Section 32 
3.6. The UE is powered on and attached to the network.  33 
3. The simultaneous downlink and uplink  full-buffer UDP and TCP data tran smission (see Section 3.4) should be 34 
verified by adjusting the connection settings (cabled environment) or UE position (OTA environment). The UE under 35 
excellent radio conditions that is achieving peak uplink and downlink user throughput should see stable utilization of 36 
the highest possible MCS, downlink block size and MIMO rank (number of layers). These KPIs should also be 37 
verified.  38 

  
Page 54 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
4. The UE should be tu rned off or set to airplane mode, if possible, to empty the buffers . The simultaneous downlink 1 
and uplink full-buffer UDP data transmission is started. Both the UE and application server should receive the data.  2 
5. All the required performance data (incl. th e signalling and control data)  as specified in the “Test requirements” 3 
Section below are measured and captured at UE, eNB/gNB and application server side using logging/measurement 4 
tools. The duration of test should be at least 3 minutes when the throughput is stable. The location and position of the 5 
UE should remain unchanged during the entire measurement duration (capture of log data).  6 
6. The capture of log data is stopped. The simultaneous downlink and up link full-buffer UDP data transmission is 7 
stopped.  8 
7. The radio conditions of UE are changed to good as defined by both SINR and RSRP in Section 3.6, if possible. The 9 
steps 4 to 6 are repeated.  10 
8. The radio conditions of UE are changed to fair as defined by both SINR and RSRP in Section 3.6, if possible. The 11 
steps 4 to 6 are repeated. 12 
9. The radio conditions of UE are changed to poor (cell edge) radio condition as defined by both SINR and RSRP in 13 
Section 3.6, if possible. The steps 4 to 6 are repeated.  14 
10. The steps 4 to 9 are repeated for simultaneous downlink and uplink full-buffer TCP data transmission.  15 
5.6.4 Test requirements (expected results) 16 
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 17 
should be captured and reported in the test report for the performance assessment 18 
UE side (real or emulated): 19 
• Radio parameters such as RSRP, RSRQ, CQI, PDSCH SINR (average sample per second) 20 
• PDSCH BLER, PDSCH MCS, PUSCH BLER, PUSCH MCS (average sample per second) 21 
• DL MIMO rank (number of layers) (average sample per second) 22 
• Downlink transmission mode 23 
• Transmit power on PUSCH  24 
• Received downlink throughput (L1, and Application layers) (average sample per second) 25 
• Transmitted uplink throughput (Application layer) (average sample per second) 26 
• Channel utilization, i.e. Number allocated/occupied PRBs and Number of allocated/o ccupied slots  in both 27 
downlink and uplink directions (average sample per second) 28 
eNB/gNB side (if capture of logs is possible): 29 
• Radio parameters such as PUSH SINR (average per second) 30 
Application server side: 31 
• Received uplink throughput (L1 and Application layers) (average sample per second) 32 
• Transmitted downlink throughput (Application layer) (average sample per second) 33 

  
Page 55 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
When the UE is  under excellent radio conditions (cell centre), t he stable utilization of the highest possible MCS and 1 
transport block size in both uplink and downlink direction  should be observed and evaluated. The UE and eNB/gNB  2 
should also receive the data with the minimum downlink and uplink BLER, respectively.  3 
The Table 5-9 gives an example of the test results record (median and standard deviation from the captured samples 4 
should be calculated for each metric). In case of 5G SA and NSA, SS-RSRP and SS-SINR should be reported. In case of 5 
5G NSA and dual connectivity (EN -DC), the values should be provided separately for both LTE and 5G paths. The 6 
spectral efficiency (see Section 3.8) should be calculated for benchmarking and comparison in order to minimize the 7 
influence of different configured parameters such as bandwidth and TDD DL/UL ratio. 8 
Table 5-9 Example record of test results (median and standard deviation from the captured samples) 9 
 
Excellent 
(cell centre) Good Fair Poor 
(cell edge) 
UDP / TCP UDP / TCP UDP / TCP UDP / TCP 
Received L1 UL throughput [Mbps]     
L1 UL Spectral efficiency [bps/Hz]      
Received Application UL throughput [Mbps]     
Received L1 DL throughput [Mbps]     
L1 DL Spectral efficiency [bps/Hz]     
Received Application DL throughput [Mbps]     
UE RSRP [dBm]     
UE PDSCH SINR [dB]     
PUSCH transmit power [dBm]     
DL MIMO rank     
DL MSC     
UL MSC     
DL RB number     
UL RB number     
DL PDSCH BLER [%]     
UL PUSCH BLER [%]     
 10 
The following figures should be also included in the test report, and the stable behavior should be observed and evaluated. 11 
• Received UDP uplink throughput (L1 and Application layer) vs Time duration 12 
• Received UDP downlink throughput (L1 and Application layer) vs Time duration 13 
• UE RSRP vs Time duration 14 
• UE PDSCH SINR vs Time duration 15 
• Number of allocated/occupied downlink PRBs and Number of allocated/occupied downlink slots vs Time 16 
duration 17 
• Number of allocated/occupied uplink PRBs and Number of allocated/occupied uplink slots vs Time duration 18 
The bidirectional DL and UL throughputs should be compared with unidirectional downlink (Section 5.4) and uplink 19 
(Section 5.5) throughputs. Assuming the same test conditions (radio condi tions), the bidirectional and unidirectional 20 
throughputs are expected to be equal.  21 

  
Page 56 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
The gap analysis should be provided for the measured and the expected target downlink and uplink throughputs which 1 
can be calculated based on the procedures from Section 5.1.    2 
5.7 Downlink coverage throughput (link budget)  3 
5.7.1 Test description and applicability 4 
The purpose of the test is to measure the downlink user data throughput (i.e. data transmitted from application (traffic) 5 
server to UE) when radio conditions of UE change gradually. Test is verified by moving UE from center to edge of the 6 
isolated cell on the main lobe of eNB/gNB antenna until UE loses the coverage (call drop). Test assesses link adaptation 7 
and effect on scheduling, CQI, MCS, Number of layers (MIMO Rank) assignment etc. during the movement of UE from 8 
excellent radio conditions to poor radio conditions. 9 
5.7.2 Test setup and configuration 10 
The test setup is a single cell scenario (i.e. isolated cell without any inter-cell interference – see Section 3.7) with UE (real 11 
or emulated) slowly moving in the main lobe of eNB/gNB antenna out from cell centre to cell edge until UE loses the 12 
coverage (call drop). The drive route inside the cell should be defined to cover the whole range of SINR values from 13 
excellent (cell center) to poor (cell edge) radio conditions as defined in Section 3.6. Note that in case of single cell scenario 14 
SINR is in fact SNR as inter -cell interferences are not present. Within the cell there should be only one active UE 15 
downloading UDP/TCP data from the application server.  The application server should be placed as close as possible to 16 
the core/core emulator and connected to the core/core emulator via a transport link with sufficient capacity so as not to 17 
limit the expected data throughput. The test is suitable for both lab and field environments. 18 
Test configuration : The test configuration is not specified . The utilized test configuration (parameters) should be  19 
recorded in the test report.  20 
Laboratory setup: The radio conditions experienced by the UE can be modified using a variable attenuator/fading 21 
generator inserted between the antenna connectors (if available) of the O-RU and the UE, or appropriately emulated using 22 
a UE emulator. The radio conditions of UE are initially set to excellent. The minimum coupling loss (see  Section 3.6) 23 
should not be exceeded. The movement of UE out from cell centre to cell edge can be achieved by gradually increasing 24 
the attenuation of radio signal to cover the whole range of SINR from excellent through good and fair to poor (as defined 25 
in Section 3.6) until UE loses the coverage (call drop). The UE should be placed inside RF shielded box or RF shielded 26 
room if the UE is not connected via cable. 27 
Field setup: The drive route inside the iso lated cell should be defined to cover the whole range of SINR values from 28 
excellent (cell center) through good and fair to poor (cell edge) (as defined in Section 3.6) until UE loses the coverage 29 
(call drop). The UE is placed in the centre of cell close to the radiated eNB/gNB antenna(s). The minimum coupling loss 30 
(see 3.5) should not be exceeded.  The change in radio conditions is achieved by moving the UE along the drive route out 31 
from cell centre to cell edge until UE loses the coverage (call drop).  32 

  
Page 57 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
 1 
Figure 5-2 Testing of downlink link budget in the field setup 2 
5.7.3 Test Procedure 3 
1. The test setup is configured according to the test configuration. The test configuration should be recorded in the test 4 
report. The serving cell under test is activated and unloaded. All other cells are turned off.  5 
2. The UE (real or emulated) is placed under excellent radio condition (cell centre) using SINR thresholds as indicated 6 
in Section 3.6. The UE is powered on and attached to the network.  7 
3. The downlink full-buffer UDP and TCP data transmission (see Section 3.4) from the application server should be 8 
verified. The excellent radio conditions experiencing peak user throughput is identified with stable utilization of the 9 
highest possible downlink MCS, downlink transport block size and downlink MIMO rank (number of layers). These 10 
KPIs should also be verified.  11 
4. The UE should be turned off or set to airplane mode, if possible, to empty the buffers. The downlink full-buffer UDP 12 
data transmission from the application server to the UE is started. The UE should receive the sent data.  13 
5. All the required performance data (incl. the signalling and control  data) as specified in the following “Test 14 
requirements” section is measured and captured at UE and Application server sides using logging/measurement tools.  15 
6. In the field setup, the UE is moved along the defined drive route out from cell centre (excellent radio conditions) to 16 
cell edge (poor radio conditions)  on the main lobe of eNB/gNB antenna and with constant speed of around 30 kph 17 
until UE loses the coverage (call drop).  18 
7. In the lab setup, the attenuation between the antenna connectors of O -RU and UE is gradually increased until UE 19 
losses the coverage (call drop). 20 
8. The capture of log data is stopped. The downlink full-buffer UDP data transmission from the application server to 21 
UE is stopped.  22 
9. The UE should be turned off or set to airplane mode, if possible, to empty the buffers. The downlink full-buffer TCP 23 
data transmission from the application server to UE is started, and Steps 5 to 7 are repeated.   24 
5.7.4 Test requirements (expected results) 25 
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 26 
should be captured and reported in the test report for the performance assessment 27 
UE side (real or emulated): 28 
UE

  
Page 58 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
• Radio parameters such as RSRP, RSRQ, CQI, PDSCH SINR (average sample per second) 1 
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second) 2 
• Received downlink throughput (L1 and Application layers) (average sample per second) 3 
• Downlink transmission mode (average sample per second) 4 
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 5 
(average sample per second) 6 
• GPS coordinates (latitude, longitude) in the field setup 7 
Application server side: 8 
• Transmitted downlink throughput (Application layer) (average sample per second) 9 
Initially when the UE is in the excellent radio condition s (cell centre) , the stable u tilization of the highest possible 10 
downlink MCS, downlink transport block size and downlink MIMO rank should be observed and evaluated. The UE 11 
should also receive the data with the minimum downlink BLER. As the UE moves out from excellent radio conditions 12 
(cell centre), the radio conditions of the UE change gradually from excellent  through good and fair to poor (see Section 13 
3.6). The changing radio conditions may cause UE reporting lower CQI and MIMO rank which results in assignment of 14 
lower MCS and lower data throughput in downlink. Such a behavior should be observed and evaluated.  15 
Note that the test results can be affected by the various coverage enhancing features (e.g. transmit/receive diversity) at 16 
eNB/gNB and/or at UE sides. The used coverage enhancement features should be also listed in the test report.  17 
The following figures should be included in the test report, and the behavior should be evaluated. In case of 5G SA and 18 
NSA, SS-RSRP and SS-SINR should be reported. In case of 5G NSA and dual connectivity (EN-DC), the values should 19 
be provided separately f or both LTE and 5G paths. The spectral efficiency (see Section 3.8) should be calculated for 20 
benchmarking and comparison in order to minimize the influence of different configured parameters such as bandwidth 21 
and TDD DL/UL ratio. 22 
• Received UDP downlink throughput (L1 and Application layer) vs PDSCH SINR (all the samples as well as 23 
average curve with median values) 24 
• Received UDP L1 downlink spectral efficiency vs PDSCH SINR (all the samples as well as average curve with 25 
median values) 26 
• Cumulative distribution function of PDSCH SINR  27 
• Cumulative distribution function of PDSCH MCS  28 
• Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots vs PDSCH SINR (all the 29 
samples as well as average curve with median values) 30 
In the field environment, the cell coverage distance in meters (i.e. a straight line from the cell site) that corresponds to the 31 
cell-edge downlink application throughput of 6 Mbps for LTE and 50 Mbps for 5G NR should be recorded.  32 
5.8 Uplink coverage throughput (link budget)  33 
5.8.1 Test description and applicability 34 
The purpose of the test is to measure the uplink user data throughput (i.e. data transmitted from UE to application (traffic) 35 
server) when radio conditions of UE change gradually. Test is verified by moving UE from center to edge of the isolated 36 
cell on the main lobe of eNB/gNB antenna until UE loses the coverage (call drop). Test assesses link adaptation and effect 37 

  
Page 59 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
on scheduling, uplink transmit power (power control), CQI, MCS, etc. during the movement of UE from excellent radio 1 
conditions to poor radio conditions. 2 
5.8.2 Test setup and configuration 3 
The test setup is a single cell scenario (i.e. isolated cell without any inter-cell interference – see Section 3.7) with UE (real 4 
or emulated) slowly moving in the main lobe of eNB/gNB antenna out from cel l centre to cell edge until UE loses the 5 
coverage (call drop). The drive route inside the cell should be defined to cover the whole range of RSRP values from 6 
excellent (cell center) to poor (cell edge) radio conditions as defined in Section 3.6. Within the cell there should be only 7 
one active UE uploading UDP/TCP data to application server.  The application server should be placed as close as possible 8 
to the core/core emulator and connected to the core/core emulator via a transport link with sufficient capacity so as not to 9 
limit the expected data throughput. The test is suitable for both lab and field environments. 10 
Test configuration : The test configuration is not specified . The utilized test configuration (parameters) should be  11 
recorded in the test report.  12 
Laboratory setup: The radio conditions experienced by the UE can be modified using a variable attenuator/fading 13 
generator inserted between the antenna connectors (if available) of the O-RU and the UE, or appropriately emulated using 14 
a UE emulator. The radio conditions of UE are initially set to excellent. The minimum coupling loss (see  Section 3.6) 15 
should not be exceeded.  The movement of UE out from cell centre to cell edge can be achieved by gradually increasing 16 
the attenuation of radio signal to cover the whole range of RSRP from excellent through good and fair to poor (as defined 17 
in Section 3.6) until UE loses the coverage (call drop). The UE should be placed inside RF shielded box or RF shielded 18 
room if the UE is not connected via cable. 19 
Field setup: The drive route inside the isolated cell should be defined to cover the whole range of RSRP values from 20 
excellent (cell center) through good and fair to poor (cell edge) (as defined in Section 3.6) until UE loses the coverage 21 
(call drop). The UE is placed in the centre of cell close to the radiated eNB/gNB antenna(s). The minimum coupling 22 
loss (see 3.5) should not be exceeded.  The change in radio conditions is achieved by moving the UE along the drive 23 
route out from cell centre to cell edge until UE loses the coverage (call drop) – see Figure 5-2.  24 
5.8.3 Test Procedure 25 
1. The test setup is configured according to the test configuration. The test configuration should be recorded in the test 26 
report. The serving cell under test is activated and unloaded. All other cells are turned off.  27 
2. The UE (real or emulated) is placed under excellent radio condition (cell centre) using RSRP thresholds as indicated 28 
in Section 3.6. The UE is powered on and attached to the network.  29 
3. The uplink full-buffer UDP and TCP data transmission (see Section 3.4) from UE to the application server should be 30 
verified. The UE under excellent radio conditions that is achieving peak user throughput should see stable utilization 31 
of the highest possible uplink MCS and uplink transport block size. These KPIs should also be verified.  32 
4. The UE should be turned off or set to airplane mode, if possible, to empty the buffers . The uplink full-buffer UDP 33 
data transmission from the application server to the UE is started. The UE should receive the sent data.  34 
5. All the required performance data (incl. the signalling and control data)  as specified in the following “Test 35 
requirements” section  is measured and captured at UE , eNB/gNB and Application server sides using 36 
logging/measurement tools.  37 
6. In the field setup, the UE is moved along the defined drive route out from cell centre (excellent radio conditions) to 38 
cell edge (poor radio conditions) on the main lobe of eNB/gNB antenna and with constant speed of around 30 kph 39 
until UE loses the coverage (call drop).  40 

  
Page 60 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
7. In the lab setup, the attenuation between the antenna connectors of O -RU and UE is gradually increased until UE 1 
losses the coverage (call drop). 2 
8. The capture of log data is stopped. The uplink full-buffer UDP data transmission from UE to the application server 3 
is stopped.  4 
9. The UE should be turned off or set to airplane mode, if possib le, to empty the buffers . The uplink full-buffer TCP 5 
data transmission from UE to the application server is started, and Steps 5 to 7 are repeated.   6 
5.8.4 Test requirements (expected results) 7 
In addition to the common minimum set of configuration parameters (see Section 3.3), the following metrics and counters 8 
should be captured and reported in the test report for the performance assessment 9 
UE side (real or emulated): 10 
• Radio parameters such as RSRP, RSRQ, CQI, PDSCH SINR (average sample per second) 11 
• PUSHC BLER, PUSCH MCS (average sample per second) 12 
• Transmit power on PUSCH (average sample per second) 13 
• Transmitted uplink throughput (Application layer) (average sample per second) 14 
• Channel utilization, i.e. Number allocat ed/occupied uplink PRBs and Number of allocated/occupied slots 15 
(average sample per second) 16 
• GPS coordinates (latitude, longitude) in the field setup 17 
eNB/gNB side (if capture of logs is possible): 18 
• Radio parameters such as PUSCH SINR (average per second) 19 
• PUSCH BLER (average per second) 20 
Application server side: 21 
• Received uplink throughput (L1 and Application layers) (average sample per second) 22 
Initially when the UE is in the excellent radio conditions (cell centre), the stable utilization of the highest possible uplink 23 
MCS and uplink transport block size should be observed and evaluated. The eNB/gNB should also receive the data with 24 
the minimum uplink BLER. As the UE moves out from excellent radio conditions (cell cent re), the radio conditions of 25 
the UE change gradually from excellent through good and fair to p oor (see Section 3.6). With deteriorating channel 26 
conditions and increasing path loss, eNB/gNB would start experiencing worse PUSCH/PUCCH BLER.  To compensate 27 
the path loss and limit PUSCH/PUCCH BLER, eNB/gNB shall command the UE to increate PUSCH/PUCCH transmit 28 
power through closed loop Transmit Power Control (TPC) feature. The number of HARQ retransmissions may be also 29 
increased. If the power control does not reduce PUSCH/PUCCH BLER bellow a threshold, eNB/gNB shall schedule UE 30 
with lower MCS and lower MIMO rank in uplink which results in lower uplink data throughput. The UE moving further 31 
away from eNB/gNB can cause radio link failure and call drop  (due to UL Max RLC retransmissions, failure in CQI 32 
decoding, low uplink SINR, etc.)  which results in triggering of RRC connection re-establishment procedure. Such a 33 
behavior should be observed and evaluated.  34 
Note that the test results can be affected by the various coverage enhancing features (e.g. transmit/receive diversity) at 35 
eNB/gNB and/or at UE sides. The used coverage enhancement features should be also listed in the test report.  36 
The following figures should be included in the test report, and the behavior should be evaluated. In case of 5G SA and 37 
NSA, SS-RSRP and SS-SINR should be reported. In case of 5G NSA and dual connectivity (EN-DC), the values should 38 

  
Page 61 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
be provided separately for both LTE and 5G paths. The spectral efficiency (see Section 3.8) should be calculated for 1 
benchmarking and comparison in order to minimize the influence of different configured parameters such as bandwidth 2 
and TDD DL/UL ratio. 3 
• Received UDP uplink throughput (L1 and Application layer) vs RSRP (all the samples as well as average curve 4 
with median values) 5 
• Received UDP L1 uplink spectral efficiency vs RSRP (all the samples as well as average curve with median 6 
values) 7 
• PUSCH transmit power vs RSRP 8 
• Cumulative distribution function of RSRP  9 
• Cumulative distribution function of PUSCH MCS  10 
• Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots vs RSRP (all the samples 11 
as well as average curve with median values) 12 
In the field environment, the cell coverage distance in meters (i.e. a straight line from the cell site) that corresponds to 13 
the cell-edge uplink application throughput of 0.5 Mbps for LTE and 5 Mbps for 5G NR should be recorded.  14 

  
Page 62 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
 Services 1 
As a part of O-RAN testing, we need to ensure the O-RAN system can work in a telecom network, by inter-working with 2 
the other sub-systems to provide End-to-End services. This section of the document outlines the services which needs to 3 
be tested to validate that the O-RAN system can be deployed and optimized to deliver great end user service experience 4 
in a telecom network. 5 
The services which need to be tested are broadly classified as data service, video streaming service, voice service, video 6 
calling service in the eMBB slice and services supported using other slices like URLLC and mMT C.  Note that the test 7 
cases for URLLC and mMTC are FFS in this version of the specification. This section of the document also tests different 8 
scenarios like handover and different radio conditions to assess the impact of these scenarios on the end user s ervice 9 
experience.   10 
6.1 Data Services 11 
Data services forms one of the basic services supported as of today on a telecom network. This includes everything 12 
from web browsing, uploading/downloading content to traffic generated by all the different applications on the end user 13 
device. These services form an integral part of data traffic on a telecom network. This section comprises of two test 14 
scenarios which include web browsing and file upload/download. 15 
Along with the monitoring and validation of these services using user experience KPIs, the O-RAN systems also need 16 
to be monitored. The end user service experience can also be impacted by some of the features available on the O-RAN 17 
system. Some of these features have been included below. As a part of the data services testing, details of these features 18 
need to be included in the test report to get a comprehensive view of the setup used for testing. If additional features or 19 
functionalities have been enabled during this testing that impact the end user experience, those need be included in the 20 
test report as well.  21 
• Control Channel Beamforming 22 
• Connected Mode DRX 23 
• Massive MIMO 24 
• Coordinated Multi Point (DL and UL) 25 
• 256QAM Support (DL and UL) 26 
• SSB Power Boost 27 
• Beam Management 28 
• DL Common Channel Beamforming 29 
• Single-User MIMO Beamforming (DL and UL) 30 
• Multi-User MIMO Beamforming (DL and UL) 31 
• Single-User MIMO, TM Switching 32 
• Multi-User MIMO, TM Switching 33 
• TDD configuration support 34 
• RAN Slicing Framework (NR Low/Mid/High) 35 
• RACH Enhancements (PRACH Format 0) 36 
• PF (Proportional Fairness) Scheduling 37 
• QoS Scheduling 38 

  
Page 63 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
• Minimal Bit Rate Scheduling 1 
• Link Adaptation (DL and UL) 2 
• Uplink Data Compression UDC 3 
• PUSCH Frequency Hopping 4 
 5 
6.1.1 Web Browsing 6 
6.1.1.1 Test Description 7 
Web browsing forms an integral part of the 4G and 5G data network, and this test case is applicable to both NSA and 8 
SA deployments. This testing will need to be performed twice, once for NSA and repeated again for SA deployment as 9 
applicable. HTTP is the protocol used for web browsing and this protocol can be transported over TCP/TLS or 10 
UDP/QUIC. As a part of this effort, we will include scenarios which will include both these protocols. Testing can be 11 
performed using HTTP/TLS/TCP or HTTP/QUIC/UDP or both these protocols as applicable. 12 
Web browsing KPI 13 
• DNS Resolution time– Time taken from when the client sends a DNS query to when the DNS responds 14 
with an IP address in milliseconds/seconds.  This KPI should be recorded if DNS is used. 15 
• Time To First Byte (TTFB) – Time taken from when the client makes the HTTP request to when the first 16 
byte of the page is received in milliseconds/seconds. 17 
• Page Load Time – Time taken from when the client places the request to when the page is completely 18 
loaded in seconds. 19 
6.1.1.2 Test Setup 20 
The SUT in this test case would be O-eNB along with O-RU, O-DU and O-CU-CP/O-CU-UP for NSA deployments or 21 
just the O-RU, O-DU and O-CU-CP/O-CU-UP for SA deployments. The O-RAN setup should support the ability to 22 
perform this testing in different radio conditions as defined in Section 3.6. The 4G/5G core will be required to support 23 
the basic functionality to authenticate and register an end user device in order to setup a PDN connection/PDU session. 24 
The 4G core will be used for O-RAN NSA testing, whereas 5G core will be used for O-RAN SA testing. The 4G/5G 25 
core could be a completely emulated, partially emulated or a real non-emulated core. The application server should 26 
support web browsing and be accessible from the 4G/5G core. This application server(s) should support protocols like 27 
HTTP/TCP and/or HTTP/QUIC as applicable. The end user device (UE) used for testing could be a real UE or an 28 
emulated one. The test setup should include tools which have the ability to collect traces on the elements and/or packet 29 
captures of communication between the elements. This could be a built-in capability of the emulated/non-emulated 30 
network elements and end user device or an external tool. Optionally, if some of the network elements or application 31 
server is located remotely either in a cloud or on the internet, the additional latency should be calculated and accounted 32 
for.  33 
The O-eNB, O-RU, O-DU and O-CU-CP/O-CU-UP need to have the right configuration and software load. Tools 34 
which emulate latency need to be used and configured on the O-RAN system (between O-RU, O-DU and O-CU-CP/O-35 
CU-UP) to emulate real-world deployment conditions as applicable. The end user device must be configured with the 36 
right user credentials to be able to register and authenticate with the O-RAN system and the 4G/5G core. The end user 37 
device also needs to be provisioned with the right application like the web browser to perform the tests. The 4G/5G core 38 
network must be configured to support end user device used for testing. This includes supporting registration, 39 
authentication and PDN connection/PDU session establishment for this end user device. The application server should 40 
be configured to support web browsing, along with support for HTTP/TCP and/or HTTP/QUIC protocols as applicable. 41 
Web browsing has lot of variables which can impact the KPIs and in order to reduce the variables and make the test 42 

  
Page 64 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
outcomes consistent, it is recommended to set up a static web page of ~1.8-2MB size. The size of the web page is 1 
comparable to the average size of web pages on the internet. A static web page does not change the content on repeated 2 
requests, thus making the test results consistent and repeatable. The locations where the radio conditions are excellent, 3 
good, fair and poor need to be identified within the serving cell. 4 
All the elements in the network like O-RAN system, 4G/5G core and the application server need to have the ability to 5 
capture traces to validate the successful execution of the test cases. The end user device needs to have the capability to 6 
capture traces/packet capture to calculate the web browsing KPIs. Optionally, the network could have network taps 7 
deployed in various legs of the network to get packet captures to validate successful execution of the test cases. Finally, 8 
all these different components need to have connectivity with each other – the end user device should be able to connect 9 
to O-RAN system, which needs to be connected to the 4G/5G core which in turn should have connectivity to the 10 
application server. 11 
6.1.1.3 Test Methodology/Procedure 12 
Ensure the end user device, O-RAN system, 4G/5G core and the application server have all been configured as outlined 13 
in Section 6.1.1.2. All traces and packet captures need to be enabled for the duration of the testing to ensure all 14 
communication between network elements can be captured and validated. 15 
1. Power on the end user device in excellent radio condition and ensure it registers with the 4G/5G core for data 16 
services. 17 
2. Once the registration is complete, the end user device has to establish a PDU session with the 5G core (SA) or PDN 18 
connection with the 4G core (NSA). 19 
3. Open the browser or application on the end user device and start a web browsing session to a web content using 20 
HTTP/TCP and/or HTTP/QUIC protocol. If the intent is to execute this test case for both protocols, then this step 21 
needs to be performed twice, once for web content using HTTP/TCP followed by web content using HTTP/QUIC 22 
protocol. 23 
4. Validate the end user can get the content from the application server and view it on the end device. For every test 24 
collect the KPIs included in Section 6.1.1.1. 25 
5. Clear the browser cache on the end user device between tests.  26 
6. Repeat the test multiple times (>10 times) and gather results. 27 
7. Repeat the above steps 1 through 6 for the good, fair and poor radio conditions. 28 
 29 
6.1.1.4 Test Expectation (expected results) 30 
As a pre-validation, use the traces to validate a successful registration and PDN connection/PDU session setup by the 31 
end user device without any errors. This is a prerequisite before these tests can be validated. 32 
First and foremost, the end user device should be able to view the web browsing content for the web browsing test, with 33 
the content being viewable and readable. Use the packet captures to validate there is no packet drop or out of sequence 34 
packets which could impact customer experience and point to a misconfigured or flawed system. Any failures 35 
encountered during testing, will have to be investigated to identify the root cause.  36 
Calculate the web browsing KPIs included in Section 6.1.1.1 and include it in the test report. In a lab setup, the user of 37 
DNS could be bypassed by using IP Addresses instead of domain names. However, the DNS resolution time KPI must 38 
be recorded if a DNS is used for testing. The average range of values for these KPIs are included below for guidance, 39 
taking into consideration the size of the web page (~ 2MB). These values are applicable when the testing is performed 40 
in a controlled environment in good radio condition without the interference of external factors which could impact the 41 
KPIs, example: use of internet to connect to remote servers/hosts could add additional latency and packet loss issues to 42 
the connection, thus impacting the KPIs. As there are multiple variables which can impact the testing in this scenario, a 43 
KPI outcome outside the range does not necessarily point to a failed test case. Any test results outside the range of KPI 44 

  
Page 65 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
encountered during testing, will have to be investigated to identify the root cause as the issues may be due to the 1 
variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and the test has to be 2 
repeated.  3 
• DNS Resolution time (conditional mandatory) – < 1 second 4 
• Time To First Byte (TTFB) – < 3 seconds 5 
• Page Load Time – < 12 seconds 6 
 7 
As a part of gathering test data and reporting, ensure the minimum configuration parameters (see Section 3.3) are 8 
included in the test report. The following information should also be included in the test report to provide a 9 
comprehensive view of the test setup. 10 
End user device side (real or emulated): 11 
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second) 12 
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second) 13 
• Received downlink throughput for the duration of the page download (L1 and L3 PDCP layers) (average sample 14 
per second) 15 
• Downlink transmission mode 16 
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 17 
(average sample per second) 18 
The table below gives an example of the test report considering the mean and standard deviation of all test results that 19 
have been captured.  20 
Table 6-1 Example of Test Report for Web Browsing Testing 21 
 
Excellent 
(cell centre) Good Fair Poor 
(cell edge) 
HTTP over 
TCP/QUIC 
HTTP over 
TCP/QUIC 
HTTP over 
TCP/QUIC 
HTTP over 
TCP/QUIC 
DNS Resolution Time (Conditional 
mandatory)     
Time To First Byte (TTFB)     
Page Load Time     
L1 DL throughput [Mbps]     
L1 DL Spectral efficiency [bps/Hz]     
L3 DL PDCP throughput [Mbps]     
Application DL throughput [Mbps]     
UE RSRP [dBm]     
UE PDSCH SINR [dB]     
MIMO rank     
PDSCH MCS     
DL RB number     
UE CQI     
UE RSRQ     
UE PMI     

  
Page 66 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
UE RSSI     
UE Buffer status     
UE Packet delay      
PDSCH BLER [%]     
The web browsing experience can also be impacted by some of the features available (see Section 6.1) on the O-eNB, 1 
O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other features/functionality which 2 
could impact the end user’s web browsing experience should be included in the test report to provide a comprehensive 3 
view of the testing. 4 
6.1.2 File upload/download 5 
6.1.2.1 Test Description 6 
File Transfer Protocol (FTP) is a simple application layer protocol used to transfer file between remote locations. FTP 7 
along with different flavours of the protocols forms one of the fundamental methods to upload/download a file on the 8 
internet. This test case is applicable to both NSA and SA deployments and will need to be performed twice, once for 9 
NSA and repeated again for SA deployment as applicable. This scenario tests the end user experience to 10 
upload/download files to/from an FTP server over an O-RAN system. The KPIs used to measure the user experience 11 
have been included below 12 
1. Download throughput – This is the average application layer throughput to download the file in kbps 13 
2. Upload throughput – This is the average application layer throughput to upload the file in kbps. 14 
3. Time taken to Download file – This is the time taken to download the file in seconds. 15 
4. Time taken to Upload file – This is the time taken to upload the file in seconds. 16 
 17 
6.1.2.2 Test Setup 18 
The SUT in this test case would be O-eNB along with O-RU, O-DU and O-CU-CP/O-CU-UP for NSA deployments or 19 
just the O-RU, O-DU and O-CU-CP/O-CU-UP for SA deployments. The O-RAN setup should support the ability to 20 
perform this testing in different radio conditions as defined in Section 3.6. A 4G/5G core will be required with the basic 21 
functionality to authenticate and register an end user device in order to setup a PDN connection/PDU session. The 4G 22 
core will be used for O-RAN NSA testing, whereas 5G core will be used for O-RAN SA testing. The 4G/5G core be a 23 
completely emulated, partially emulated or a real non-emulated core. An FTP server acts as an application server for 24 
this test case. The end user device (UE) used for testing could be a real UE or an emulated one. The test setup should 25 
include tools which have the ability to collect traces on the elements and/or packet captures of communication between 26 
the elements. This could be a built-in capability of the emulated/non-emulated network elements or an external tool. 27 
Optionally, if some of the network elements are located remotely either in a cloud or on the internet, the additional 28 
latency should be calculated and accounted for.  29 
The O-eNB, O-RU, O-DU and O-CU-CP/O-CU-UP need to have the right configuration and software load. Tools 30 
which emulate latency need to be used and configured on the O-RAN system (between O-RU, O-DU and O-CU-CP/O-31 
CU-UP) to emulate real-world deployment conditions. The end user device must be configured with the right user 32 
credentials to be able to register and authenticate with the O-RAN system and the 4G/5G core. The end user device also 33 
needs to be provisioned with the right application like an FTP client to upload and download files. The 4G/5G core 34 
network must be configured to support end user device used for testing. This includes supporting registration, 35 
authentication and PDN connection/PDU session establishment for this end user device. The FTP server should host 36 
large files (>1 GB) which can be used for the download/upload testing. The locations where the radio conditions are 37 
excellent, good, fair and poor need to be identified within the serving cell. 38 

  
Page 67 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
All the elements in the network like O-RAN system, 4G/5G core and the application server need to have the ability to 1 
capture traces to validate the successful execution of the test cases. The end user device needs to have the capability to 2 
capture traces/packet capture to calculate the file upload/download KPIs. Optionally, the network could have network 3 
taps deployed in various legs of the network to get packet captures to validate successful execution of the test cases. 4 
Finally, all these different components need to have connectivity with each other – the end user device should be able to 5 
connect to O-RAN system, which needs to be connected to the 4G/5G core which in turn should have connectivity to 6 
the application server. 7 
6.1.2.3 Test Methodology/Procedure 8 
Ensure the end user device, O-RAN system, 4G/5G core and the FTP server have all been configured as outlined in 9 
Section 6.1.2.2. All traces and packet captures need to be enabled for the duration of the testing to ensure all 10 
communication between network elements can be captured and validated. 11 
1. Power on the end user device in excellent radio condition and ensure it registers with the 4G/5G core for data 12 
services. 13 
2. Once the registration is complete, the end user device has to establish a PDU session with the 5G core (SA) or PDN 14 
connection with the 4G core (NSA). 15 
3. Open the application on the end user device and upload the test file to the FTP server. Make a note of the time 16 
taken to upload the file and the average upload throughput. 17 
4. Next use the same application on the end user device to download a different test file from the FTP server. Make a 18 
note of the time taken to download the file and the average download throughput. 19 
5. Clear all the buffers and caches on the client and the server. Delete the test files – downloaded file on the client and 20 
the uploaded file on the FTP server. 21 
6. Repeat the test multiple times (>10 times) and gather results. 22 
7. Repeat the above steps 1 through 6 for the good, fair and poor radio conditions. 23 
 24 
6.1.2.4 Test Expectation (expected results) 25 
As a pre-validation, use the traces to validate a successful registration and PDN connection/PDU session setup by the 26 
end user device without any errors. This is a prerequisite before these tests can be validated. 27 
Validate the end user device can upload/download the complete file to/from the FTP server without interruption. 28 
Validate there isn’t a drop in throughput while uploading/downloading the file to/from the FTP server. Use the packet 29 
captures to validate there is no packet drop or out of sequence packets which could impact customer experience and 30 
point to a misconfigured or flawed system. 31 
Calculate the file upload/download KPIs included in Section 6.1.1.1 and include it in the test report. There are no target 32 
values for these KPIs as they are dependent on multiple factors used in the test configuration. Some comments on each 33 
of the KPIs are included below. 34 
• Download throughput – This value should be comparable to the results of the downlink throughput test 35 
performed in Section 5.2 and Section 5.4. 36 
• Upload throughput – This value should be comparable to the results of the uplink throughput test performed in 37 
Section 5.3 and Section 5.5. 38 
• Time taken to Download file – This value should be comparable to the value calculated using the formula 39 
included below  40 
• Time taken to Upload file – This value should be comparable to the value calculated using the formula included 41 
below. 42 
 43 

  
Page 68 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
𝑡𝑖𝑚𝑒 𝑡𝑎𝑘𝑒𝑛 𝑡𝑜 𝑢𝑝𝑙𝑜𝑎𝑑/𝑑𝑜𝑤𝑛𝑙𝑜𝑎𝑑 [𝑠𝑒𝑐𝑜𝑛𝑑𝑠] = Size of file in GB x 1000 x 1000 x 8
𝑢𝑝𝑙𝑖𝑛𝑘/𝑑𝑜𝑤𝑛𝑙𝑖𝑛𝑘/𝑢𝑝𝑙𝑖𝑛𝑘 𝑡ℎ𝑟𝑜𝑢𝑔ℎ𝑝𝑢𝑡 [𝑘𝑏𝑝𝑠] 1 
These KPI values included in Section 6.1.2.1 need to be included in the test report along with the minimum 2 
configuration parameters included in Section 3.3. The following information should also be included in the test report to 3 
provide a comprehensive view of the test setup. 4 
End user device side (real or emulated UE): 5 
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second) 6 
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second) 7 
• Received downlink throughput (L1 and L3 PDCP layers) (average sample per second) 8 
• Downlink transmission mode 9 
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 10 
(average sample per second) 11 
The table below gives an example of the test report considering the mean and standard deviation of all test results that 12 
have been captured.  13 
Table 6-2 Example Test Report for File Upload/Download Testing 14 
 
Excellent 
(cell centre) Good Fair Poor 
(cell edge) 
File 
Upload/Download 
File 
Upload/Download 
File 
Upload/Download 
File 
Upload/Download 
Upload/Download Throughput (kbps)     
Time taken to Upload/Download File     
L1 DL throughput [Mbps]     
L1 DL Spectral efficiency [bps/Hz]     
L3 DL PDCP throughput [Mbps]     
Application DL throughput [Mbps]     
UE RSRP [dBm]     
UE PDSCH SINR [dB]     
MIMO rank     
PDSCH MCS     
DL RB number     
UE CQI     
UE RSRQ     
UE PMI     
UE RSSI     
UE Buffer status     
UE Packet delay      
PDSCH BLER [%]     
The file upload/download experience can also be impacted by some of the features available (see Section 6.1) on the O-15 
eNB, O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other features/functionality 16 

  
Page 69 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
which could impact the end user’s web browsing experience should be included in the test report to provide a 1 
comprehensive view of the testing. 2 
6.2 Video Streaming 3 
Video makes up a major part of the internet traffic today and we are seeing similar trend even on the mobile data traffic. 4 
Mobile video accounts for close to two-thirds of the total mobile data traffic and is expected to increase in the coming 5 
years. Video streaming has evolved over time with newer and better audio and video codecs. The protocols used to 6 
stream the video packets has also been evolving over time, with Adaptive Bit Rate (ABR) being the most common 7 
protocol used for streaming video on the internet today. There are multiple flavours of ABR, but at the crux they all use 8 
HTTP protocol over TCP/TLS or UPD/QUIC to transfer audio-video packets to a client. A video streaming server 9 
which supports ABR hosts multiple versions of the same video content with each version of the video being encoded at 10 
different resolution and quality, hence different bit rates. The client uses the ABR protocol to get the list of all available 11 
versions of the requested video content and picks the best video content based on the available bandwidth on the client 12 
side. The ABR client continuously monitors the network condition and dynamically adjusts the quality and resolution of 13 
the video stream to match the available bandwidth. 14 
The ABR protocol provides the user with the best video streaming experience based on the available bandwidth 15 
between the client and the content server. This however adds a lot of variables to the streaming session 16 
• An ABR client may start with a lower resolution video and progressively switch to higher resolution video as it 17 
better estimates the bandwidth available at the client. 18 
• An ABR client might notice an improvement in the bandwidth and request a higher resolution/quality video 19 
content, thus improving the video quality mid-stream. 20 
• An ABR client might notice a degradation in the bandwidth and request a lower resolution/quality video 21 
content, thus deteriorating the video quality mid-stream. 22 
 23 
These variables make it challenging to quantify the video streaming experience of an end user. There have been many 24 
tools and organization which have defined different methods and algorithms to quantify the end user experience 25 
including QoE (Quality of Experience), SVQ (Streaming Video Quality), Video Multimethod Assessment Fusion 26 
(VMAF) etc. In this document we recommend using the Mean Opinion Score (MOS) as defined by ITU P.1203.3 to 27 
quantify the quality of experience of the end user. This mechanism however limits the testing to H264 video encoder 28 
with a resolution of HD quality (1080p resolution – 1920 x 1080 pixels) or below. 29 
Video Streaming KPIs 30 
• Video start time or Time to load first video frame – Time from when the video was selected to play to 31 
when the video starts playing in seconds. 32 
• Number of video stalls/buffering (Optional) – Number of times the video stalled or started buffering during 33 
the course of video streaming. This KPI has already been considered by ITU P.1203.3 to provide a 34 
cumulative MOS score. 35 
• Duration of stalls in the video (Optional) – The cumulative duration of all the stalls during the course of 36 
video streaming in seconds. This KPI has already been considered by ITU P.1203.3 to provide a 37 
cumulative MOS score. 38 
• Video MOS score – MOS score for the video streaming session as defined by ITU P1203.3. 39 
 40 
Along with the monitoring and validation of these services using user experience KPIs, the O-RAN systems also need 41 
to be monitored. The end user service experience can also be impacted by some of the features available on the O-RAN 42 
system. Some of these features have been included below. As a part of the video streaming testing, details of these 43 
features need to be included in the test report to get a comprehensive view of the setup used for testing. If additional 44 

  
Page 70 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
features or functionalities have been enabled during this testing that impact the end user experience, those need be 1 
included in the test report as well.  2 
• NR to LTE PS Redirection/Cell Reselection/Handover 3 
• Intra-frequency / Inter-Frequency Cell Reselection/Handover 4 
• NR Coverage-Triggered NR Session Continuity 5 
• LTE-NR & NR-NR Dual Connectivity and NR Carrier Aggregation 6 
• Direct data forwarding between NG-RAN and E-UTRAN nodes for inter-system mobility 7 
• Standard QCI Bearers Support 8 
• Control Channel Beamforming 9 
• Massive MIMO 10 
• 256QAM Support (DL and UL) 11 
• Beam Management 12 
• TDD configuration support 13 
• PF (Proportional Fairness) Scheduling 14 
• Link Adaptation (DL and UL) 15 
• Application Aware QoS 16 
6.2.1 Video Streaming – Stationary Test 17 
This scenario tests the video experience of a user streaming video over 4G and 5G network when the end user device is 18 
stationary. 19 
6.2.1.1 Test Description 20 
Majority of the video streaming on the internet today uses ABR (Adaptive Bit Rate) streaming using HTTP protocol 21 
over TCP/TLS or UDP/QUIC. As a part of this effort, we need to test the user’s video streaming experience when 22 
connected to a telecom network over O-RAN system. This test case is applicable to both NSA and SA deployment, and 23 
will need to be performed twice, once for NSA and repeated again for SA deployment as applicable. 24 
6.2.1.2 Test Setup 25 
The SUT in this test case would be O-eNB along with O-RU, O-DU and O-CU-CP/O-CU-UP for NSA deployments or 26 
just the O-RU, O-DU and O-CU-CP/O-CU-UP for SA deployments. The O-RAN setup should support the ability to 27 
perform this testing in different radio conditions as defined in Section 3.6. A 4G/5G core will be required to support the 28 
basic functionality to authenticate and register an end user device in order to setup a PDN connection/PDU session. The 29 
4G core will be used for O-RAN NSA testing, whereas 5G core will be used for O-RAN SA testing. The 4G/5G core 30 
could be a completely emulated, partially emulated or a real non-emulated core. The application server(s) for this 31 
testing should support video streaming using ABR protocol over HTTP/TCP and/or HTTP/QUIC as applicable. The end 32 
user device (UE) used for testing could be a real UE or an emulated one. The test setup should include tools which have 33 
the ability to collect traces on the elements and/or packet captures of communication between the elements. This could 34 
be a built-in capability of the emulated/non-emulated network elements or an external tool. Optionally, if some of the 35 
network elements or the application server are located remotely either in a cloud or on the internet, the additional 36 
latency should be calculated and accounted for.  37 

  
Page 71 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
The O-eNB, O-RU, O-DU and O-CU-CP/O-CU-UP need to have the right configuration and software load. Tools 1 
which emulate latency need to be used and configured on the O-RAN system (between O-RU, O-DU and O-CU-CP/O-2 
CU-UP) to emulate real-world deployment conditions. The end user device must be configured with the right user 3 
credentials to be able to register and authenticate with the O-RAN system and the 4G/5G core. The end user device also 4 
needs to be provisioned with the right application like a video streaming client to perform the tests. The 4G/5G core 5 
network must be configured to support end user device used for testing. This includes supporting registration, 6 
authentication and PDN connection/PDU session establishment for this end user device. The locations where the radio 7 
conditions are excellent, good, fair and poor need to be identified within the serving cell. 8 
All the elements in the network like O-RAN system, 4G/5G core and the application server need to have the ability to 9 
capture traces to validate the successful execution of the test cases. The end user device needs to have the capability to 10 
capture traces/packet capture to calculate the video streaming KPIs. Optionally, the network could have network taps 11 
deployed in various legs of the network to get packet captures to validate successful execution of the test cases. Finally, 12 
all these different components need to have connectivity with each other – the end user device should be able to connect 13 
to O-RAN system, O-RAN system needs to be connected to the 4G/5G core which in turn should have connectivity to 14 
the application server. 15 
6.2.1.3 Test Methodology/Procedure 16 
Ensure the end user device, O-RAN system, 4G/5G core and the application server have all been configured as outlined 17 
in Section 6.2.1.2. All traces and packet captures need to be enabled for the duration of the testing to ensure all 18 
communication between network elements can be captured and validated. 19 
1. Power on the end user device in excellent radio condition and ensure it registers with the 4G/5G core for data 20 
services. 21 
2. Once the registration is complete, the end user device has to establish a PDN Session with the 4G core or PDU 22 
session with the 5G core. 23 
3. Open the video streaming client on the end user device and start a video streaming session over HTTP/TCP 24 
protocol and let the video stream for at least 120 seconds.  25 
4. Optionally, repeat the test by streaming a video session over the HTTP/QUIC protocol and stream the video content 26 
for at least 120 seconds. 27 
5. Repeat the test multiple times (> 10 times) and gather results. 28 
6. Repeat the above steps 1 through 5 for the good, fair and poor radio conditions. 29 
 30 
6.2.1.4 Test Expectation (expected results) 31 
As a pre-validation, use the traces to validate a successful registration and PDN connection/PDU session setup by the 32 
end user device without any errors. This is a prerequisite before these tests can be validated. 33 
Validate the end user device is able to start streaming the video without delays and watch the video content. Ensure the 34 
video is able to stream without stalls or intermittent buffering. Use the packet captures to validate there is no packet 35 
drop or out of sequence packets which could impact customer experience and point to a misconfigured or flawed 36 
system. 37 
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 38 
is performed in a controlled environment in good radio condition without the interference of external factors which 39 
could impact the KPIs, example: use of internet to connect to remote servers/hosts could add latency, jitter and packet 40 
loss issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the testing in 41 
this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test results outside 42 
the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues may be 43 

  
Page 72 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and the test 1 
has to be repeated.  2 
• Video Start time or Time to load first video frame – ~1.5 seconds 3 
• Number of video stalls/buffering – < 1 4 
• Duration of stalls in the video – < 5 seconds 5 
• Video MOS Score – > 3.5 6 
 7 
The values for the end user video streaming KPIs defined in Section 6.2 need to be included in the test report along with 8 
the minimum configuration parameters included in Section 3.3. The following information should also be included in 9 
the test report for the testing performed in different radio conditions to provide a comprehensive view of the test setup. 10 
End user device side (real or emulated UE): 11 
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second) 12 
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second) 13 
• Downlink transmission mode 14 
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 15 
(average sample per second) 16 
The table below gives an example of the test report considering the mean and standard deviation of all test results that 17 
have been captured.  18 
Table 6-3 Example Test Report for Video Streaming – Stationary Test 19 
 
Excellent 
(cell centre) Good Fair Poor 
(cell edge) 
Video Streaming 
over TCP/QUIC 
Video Streaming 
over TCP/QUIC 
Video Streaming 
over TCP/QUIC 
Video Streaming 
over TCP/QUIC 
Video Start Time     
Number of Video Stalls/buffering     
Duration of stalls in the video     
Video MOS Score     
L1 DL Spectral efficiency [bps/Hz]     
UE RSRP [dBm]     
UE PDSCH SINR [dB]     
MIMO rank     
PDSCH MCS     
DL RB number     
UE CQI     
UE RSRQ     
UE PMI     
UE RSSI     
UE Buffer status     
UE Packet delay      
PDSCH BLER [%]     

  
Page 73 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
The end user video streaming experience can also be impacted by some of the features available (see Section 6.2) on the 1 
O-eNB, O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other 2 
features/functionality which could impact the end user’s video streaming experience should be included in the test 3 
report to provide a comprehensive view of the testing. 4 
6.2.2 Video Streaming – Handover between same Master eNB but different 5 
O-RUs – Intra O-DU 6 
This test scenario validates the user’s video streaming experience when the end user device (UE) is connected over 7 
NSA to a 4G core and is in the process of a handover between two O-RAN subcomponents (two O-RUs) on the 8 
Secondary gNB while remaining connected to the same Master eNB. As services like VoLTE or VoNR cannot be used 9 
to test handover of secondary gNB, video streaming is used for this testing. Hence, this test scenario is only applicable 10 
in an NSA deployment. 11 
6.2.2.1 Test Description 12 
The 5G O-RAN system has multiple sub-components like the O-RU, O-DU and the O-CU-CP/O-CU-UP. This setup 13 
leads to multiple handover scenarios when the O-RAN system is connected as a Secondary gNB in an NSA 14 
deployment. This scenario tests the end user’s video streaming experience when the end user device is connected over 15 
NSA to the 4G core and performs a handover between O-RUs which are connected to the same O-DU (and O-CU-16 
CP/O-CU-UP) on the Secondary gNB – Intra-O-DU handover. The end user device remains connected to the same 17 
Master eNB through the entire handover process. This handover is agnostic to the 4G core as the handover occurs on 18 
the O-RAN system. This test assesses the impact of the video streaming service on the end user device in this handover 19 
scenario by monitoring the end user video streaming KPIs included in Section 6.2. This test case streams video using 20 
ABR protocol over HTTP/TCP and/or HTTP/QUIC.  21 
6.2.2.2 Test Setup 22 
The SUT in this test case would be the Master eNB and a Secondary gNBs. The Master eNB will be an O-eNB and the 23 
Secondary gNB will constitute of a pair of O-RUs – O-RU1 and O-RU2, which will connect to the same O-DU and O-24 
CU-CP/O-CU-UP. The O-eNB, O-RUs, O-DU and O-CU-CP/O-CU-UP have to comply with the O-RAN 25 
specifications. A 4G core will be required to support the basic functionality to authenticate and register an end user 26 
device in order to setup a PDN connection. The 4G core could be a completely emulated, partially emulated or a real 27 
non-emulated core. The application server(s) for this testing should support video streaming using ABR protocol over 28 
HTTP/TCP and/or HTTP/QUIC as applicable. The end user device (UE) used for testing could be a real UE or an 29 
emulated one. The test setup should include tools which have the ability to collect traces on the elements and/or packet 30 
captures of communication between the elements. This could be a built-in capability of the emulated/non-emulated 31 
network elements or an external tool. Optionally, if some of the network elements or application server(s) are located 32 
remotely either in a cloud or on the internet, the additional latency should be calculated and accounted for.  33 
The O-eNB needs to have the right configuration and software load. The pair of O-RUs (O-RU1 and O-RU2) need to be 34 
connected to the O-DU and O-CU-CP/O-CU-UP and all the components need to have the right configuration and 35 
software load. Tools which emulate latency need to be used and configured on the O-RAN system (between O-RU, O-36 
DU and O-CU-CP/O-CU-UP) to emulate real-world deployment conditions. The end user device must be configured 37 
with the right user credentials to be able to register and authenticate with the O-RAN system and the 4G core. The end 38 
user device also needs to be provisioned with the right application like a video streaming client to perform the tests. The 39 
4G core network must be configured to support end user device used for testing. This includes supporting registration, 40 
authentication and PDN connection establishment for this end user device. 41 

  
Page 74 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
All the elements in the network like O-RAN system, 4G core and the application server need to have the ability to 1 
capture traces to validate the successful execution of the test cases. The end user device needs to have the capability to 2 
capture traces/packet capture to calculate the video streaming KPIs. Optionally, the network could have network taps 3 
deployed in various legs of the network to get packet captures to validate successful execution of the test cases. Finally, 4 
all these different components need to have connectivity with each other – the end user device should be able to connect 5 
to O-RAN system(O-eNB, O-RUs, O-DU and O-CU-CP/O-CU-UP), O-RAN system needs to be connected to the 4G 6 
core which in turn should have connectivity to the application server. 7 
6.2.2.3 Test Methodology/Procedure 8 
Ensure the end user device, O-RAN system, 4G core and the application server have all been configured as outlined in 9 
Section 6.2.2.2. All traces and packet captures need to be enabled for the duration of the testing to ensure all 10 
communication between network elements can be captured and validated. 11 
1. Power on the end user device and ensure it registers with the 4G core for data services over NSA by connecting 12 
over the O-eNB as Master eNB and O-RU1 of the O-RAN system as secondary gNB. 13 
2. Once the registration is complete, the end user device has to establish a PDN connection with the 4G core. 14 
3. Open the video streaming client on the end user device and start a streaming session over HTTP/TCP protocol.  15 
4. Once the video streaming session has started, move the device so it can handover from O-RU1 to O-RU2 on the 16 
Secondary gNB while it continues to use the O-eNB as the Master eNB.  17 
5. Allow the end user device to stream video through the entire handover process. Measure the KPIs included in 18 
Section 6.2 for this video streaming session. 19 
6. Optionally, repeat steps 1 through 5 for a video streaming session which uses HTTP/QUIC protocol for streaming. 20 
7. Repeat the test multiple times (> 10 times) and gather results. 21 
 22 
6.2.2.4 Test Expectation (expected results) 23 
As a pre-validation, use the traces to validate a successful registration and PDN connection setup by the end user device 24 
without any errors using the Master eNB and O-RU1, O-DU and O-CU-CP/O-CU-UP as the Secondary gNB. This is a 25 
prerequisite before these tests can be validated. 26 
Validate the end user device is able to start streaming the video without delays and watch the video content through the 27 
entire handover process. Ensure there is no stalls, downgrading of video quality or intermittent buffering of the video 28 
content, especially during the handover process. Use the packet captures to validate there is no packet drop or out of 29 
sequence packets which could impact customer experience and point to a misconfigured or flawed system. 30 
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 31 
is performed in a controlled environment in good radio condition without the interference of external factors which 32 
could impact the KPIs, example: use of internet to connect to remote servers/hosts could add latency, jitter and packet 33 
loss issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the testing in 34 
this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test results outside 35 
the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues may be 36 
due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and the test 37 
has to be repeated.  38 
• Video Start time or Time to load first video frame – ~1.5 seconds 39 
• Number of video stalls/buffering – < 1 40 
• Duration of stalls in the video – < 5 seconds 41 
• Video MOS Score – > 3.5 42 
 43 

  
Page 75 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
The values for the end user video streaming KPIs defined in Section 6.2 need to be included in the test report along with 1 
the minimum configuration parameters included in Section 3.3. The following information should also be included in 2 
the test report to provide a comprehensive view of the test setup. 3 
End user device side (real or emulated UE): 4 
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second) 5 
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second) 6 
• Received downlink throughput (L1 and L3 PDCP layers) (average sample per second) 7 
• Downlink transmission mode 8 
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 9 
(average sample per second) 10 
The table below gives an examp le of the test report considering the mean and standard deviation of all test results that 11 
have been captured.  12 
Table 6-4 Example Test Report for Video Streaming – Handover between same Master eNB but different O-RUs 13 
– Intra DU handover 14 
 Video Streaming over HTTP/TCP Video Streaming over HTTP/QUIC 
Video Start Time   
Number of Video Stalls/buffering   
Duration of stalls in the video   
Video MOS Score   
L1 DL Spectral efficiency [bps/Hz]   
UE RSRP [dBm]   
UE PDSCH SINR [dB]   
MIMO rank   
PDSCH MCS   
DL RB number   
UE CQI   
UE RSRQ   
UE PMI   
UE RSSI   
UE Buffer status   
UE Packet delay    
PDSCH BLER [%]   
The end user video streaming experience can also be impacted by some of the features available (see Section 6.2) on the 15 
O-eNB, O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other 16 
features/functionality which could impact the end user’s video streaming experience should be included in the test 17 
report to provide a comprehensive view of the testing. 18 

  
Page 76 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
6.2.3 Video Streaming – Handover between same MeNB but different O-DUs 1 
– Inter O-DU Intra O-CU 2 
This test scenario validates the user’s video streaming experience when the UE is connected over NSA to a 4G core and 3 
the UE is in the process of a handover between two O-RAN subcomponents (two O-RUs and O-DUs) on the Secondary 4 
gNB while remaining connected to the same Master eNB. As services like VoLTE or VoNR cannot be used to test 5 
handover of secondary gNB, video streaming is used for this testing. Hence, this test scenario is only applicable in an 6 
NSA deployment. 7 
6.2.3.1 Test Description 8 
The 5G O-RAN system has multiple sub-components like the O-RU, O-DU and the O-CU-CP/O-CU-UP. This setup 9 
leads to multiple handover scenarios when the O-RAN system is connected as a Secondary gNB in an NSA 10 
deployment. This scenario tests the end user’s video streaming experience when the end user device is connected over 11 
NSA to the 4G core and performs a handover between O-RUs which are connected to different O-DUs, which in turn 12 
are connected to the same O-CU-CP/O-CU-UP on the Secondary gNB – Inter-O-DU Intra-O-CU handover. The end 13 
user device remains connected to the same Master eNB through the entire handover process. This handover is agnostic 14 
to the 4G core as the handover occurs on the O-RAN system. This test assesses the impact of the video streaming 15 
service on the end user device in this handover scenario by monitoring the KPIs included in Section 6.2 This test case 16 
streams video using ABR protocol over HTTP/TCP and/or HTTP/QUIC.  17 
6.2.3.2 Test Setup 18 
The SUT in this test case would be the Master eNB and a Secondary gNBs. The Master eNB will be an O-eNB and the 19 
Secondary gNB will constitute of a pair of O-RUs – O-RU1 and O-RU2, which are connected to a pair of O-DUs -DU1 20 
and O-DU2, which are connected to the same O-CU-CP/O-CU-UP. The O-eNB, O-RUs, O-DUs and O-CU-CP/O-CU-21 
UP have to comply with the O-RAN specifications. The 4G core will be required to support the basic functionality to 22 
authenticate and register an end user device in order to setup a PDN connection. The 4G/5G core could be a completely 23 
emulated, partially emulated or a real non-emulated core. The application server(s) for this testing should support video 24 
streaming using ABR protocol over HTTP/TCP and/or HTTP/QUIC as applicable. The end user device (UE) used for 25 
testing could be a real UE or an emulated one. The test setup should include tools which have the ability to collect 26 
traces on the elements and/or packet captures of communication between the elements. This could be a built-in 27 
capability of the emulated/non-emulated network elements or an external tool. Optionally, if some of the network 28 
elements or application server(s) are located remotely either in a cloud or on the internet, the additional latency should 29 
be calculated and accounted for.  30 
The O-eNB needs to have right the configuration and software load. The pair of O-RUs (O-RU1 and O-RU2) need to be 31 
connected to the pair of O-DUs (O-DU1 and O-DU2), where O-RU1 is connected to O-DU1 and O-RU2 is connected 32 
to O-DU2. Both the O-DUs, O-DU1 and O-DU2 need to be connected to the same O-CU-CP/O-CU-UP. All the O-33 
RAN components (O-RUs, O-DUs and O-CU-CP/O-CU-UP) need to have the right configuration and software load. 34 
The end user device must be configured with the right user credentials to register and authenticate with the Master eNB 35 
and the 4G core. The end user device also needs to be provisioned with the right application like a video streaming 36 
client to perform the tests. The 4G core network must be configured to support end user device used for testing. This 37 
includes supporting registration, authentication and PDN connection establishment for this end user device. 38 
All the elements in the network like O-RAN system, 4G core and the application server need to have the ability to 39 
capture traces to validate the successful execution of the test cases. The end user device needs to have the capability to 40 
capture traces/packet capture to calculate the video streaming KPIs. Optionally, the network could have network taps 41 
deployed in various legs of the network to get packet captures to validate successful execution of the test cases. Finally, 42 
all these different components need to have connectivity with each other – the end user device should be able to connect 43 

  
Page 77 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
to O-RAN system (O-eNB, O-RUs, O-DUs and O-CU-CP/O-CU-UP), O-RAN system needs to be connected to the 4G 1 
core which in turn should have connectivity to the application server. 2 
6.2.3.3 Test Methodology/Procedure 3 
Ensure the end user device, O-RAN system, 4G core and the application server have all been configured as outlined in 4 
Section 6.2.3.2. All traces and packet captures need to be enabled for the duration of the testing to ensure all 5 
communication between network elements can be captured and validated. 6 
1. Power on the end user device and ensure it registers with the 4G core for data services over NSA by connecting 7 
over the O-eNB as Master eNB and O-RU1 of the O-RAN system as secondary gNB. 8 
2. Once the registration is complete, the end user device has to establish a PDN connection with the 4G core. 9 
3. Open the video streaming client on the end user device and start a streaming session over HTTP/TCP protocol.  10 
4. Once the video streaming session has started, move the device so it can handover from O-RU1 to O-RU2 (and in 11 
turn O-DU1 to O-DU2) on the Secondary gNB while it continues to use the O-eNB as the Master eNB.  12 
5. Allow the end user device to stream video through the entire handover process. Measure the KPIs included in 13 
Section 6.2 for this video streaming session. 14 
6. Optionally, repeat steps 1 through 5 for a video streaming session which uses HTTP/QUIC protocol for streaming. 15 
7. Repeat the test multiple times (> 10 times) and gather results. 16 
 17 
6.2.3.4 Test Expectation (expected results) 18 
As a pre-validation, use the traces to validate a successful registration and PDN connection setup by the end user device 19 
without any errors using the O-eNB as Master eNB and O-RU1, O-DU1 and O-CU-CP/O-CU-UP as the Secondary 20 
gNB. This is a prerequisite before these tests can be validated. 21 
Validate the end user device is able to start streaming the video without delays and watch the video content through the 22 
entire handover process. Ensure there is no stalls, downgrading of video quality or intermittent buffering of the video 23 
content, especially during the handover process. Use the packet captures to validate there is no packet drop or out of 24 
sequence packets which could impact customer experience and point to a misconfigured or flawed system. 25 
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 26 
is performed in a controlled environment in good radio condition without the interference of external factors which 27 
could impact the KPIs, example: use of internet to connect to remote servers/hosts could add latency, jitter and packet 28 
loss issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the testing in 29 
this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test results outside 30 
the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues may be 31 
due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and the test 32 
has to be repeated.  33 
• Video Start time or Time to load first video frame – ~1.5 seconds 34 
• Number of video stalls/buffering – < 1 35 
• Duration of stalls in the video – < 5 seconds 36 
• Video MOS Score – > 3.5 37 
 38 
The values for the end user video streaming KPIs defined in Section 6.2 need to be included in the test report along with 39 
the minimum configuration parameters included in Section 3.3. The following information should also be included in 40 
the test report to provide a comprehensive view of the test setup. 41 
End user device side (real or emulated UE): 42 

  
Page 78 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second) 1 
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second) 2 
• Downlink transmission mode 3 
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 4 
(average sample per second) 5 
The table below gives an example of the test report considering the mean and standard deviation of all test results that 6 
have been captured.  7 
Table 6-5 Example Test Report for Video Streaming – Handover between same Master eNB but different O-RUs 8 
and O-DUs – Inter-O-DU Intra-O-CU handover 9 
 Video Streaming over HTTP/TCP Video Streaming over HTTP/QUIC 
Video Start Time   
Number of Video Stalls/buffering   
Duration of stalls in the video   
Video MOS Score   
L1 DL Spectral efficiency [bps/Hz]   
UE RSRP [dBm]   
UE PDSCH SINR [dB]   
MIMO rank   
PDSCH MCS   
DL RB number   
UE CQI   
UE RSRQ   
UE PMI   
UE RSSI   
UE Buffer status   
UE Packet delay    
PDSCH BLER [%]   
The end user video streaming experience can also be impacted by some of the features available (see Section 6.2) on the 10 
O-eNB, O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other 11 
features/functionality which could impact the end user’s video streaming experience should be included in the test 12 
report to provide a comprehensive view of the testing. 13 
6.2.4 Video Streaming – Handover between same MeNB but different O-CUs 14 
– Inter O-CU 15 
This test scenario validates the user’s video streaming experience when the end user device (UE) is connected over 16 
NSA to a 4G core and is in the process of a handover between two Secondary gNB (O-RUs, O-DUs and O-CU-CP/O-17 
CU-UPs) while remaining connected to the same Master eNB. As services like VoLTE or VoNR cannot be used to test 18 
handover of secondary gNB, video streaming is used for this testing. Hence, this test scenario is only applicable in an 19 
NSA deployment. 20 
 21 

  
Page 79 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
6.2.4.1 Test Description 1 
This scenario tests the impact of a handover on the video streaming service. The end user device is connected over NSA 2 
to the 4G core and streaming video, while it performs a handover of the Secondary gNB (O-RU, O-DU and O-CU-3 
CP/O-CU-UP) to a new Secondary gNB (O-RU, O-DU and O-CU-CP/O-CU-UP), i.e. an inter-O-CU handover, when 4 
still being connected to the same Master eNB. This test assesses the impact of the video streaming service on the end 5 
user device in this handover scenario by monitoring the end user video streaming KPIs included in Section 6.2. This test 6 
case streams video using ABR protocol over HTTP/TCP and/or HTTP/QUIC. 7 
6.2.4.2 Test Setup 8 
The SUT in this test case would be the O-eNB which would be the Master eNB and a pair of Secondary gNBs – O-9 
RU1, O-DU1, O-CU-CP/O-CU-UP1 and O-RU2, O-DU2, O-CU-CP/O-CU-UP2. The O-eNB, O-RUs, O-DUs and O-10 
CU-CP/O-CU-UPs have to comply with the O-RAN specifications. A 4G core will be required to support the basic 11 
functionality to authenticate and register an end user device in order to setup a PDN connection. The 4G/5G core could 12 
be a completely emulated, partially emulated or a real non-emulated core. The application server(s) for this testing 13 
should support video streaming using ABR protocol over HTTP/TCP and/or HTTP/QUIC as applicable. The end user 14 
device (UE) used for testing could be a real UE or an emulated one. The test setup should include tools which have the 15 
ability to collect traces on the elements and/or packet captures of communication between the elements. This could be a 16 
built-in capability of the emulated/non-emulated network elements or an external tool. Optionally, if some of the 17 
network elements are located remotely either in a cloud or on the internet, the additional latency should be calculated 18 
and accounted for.  19 
The O-eNB, pair of gNBs (O-RUs, O-DUs and O-CU-CP/O-CU-UPs) need to have the right configuration and software 20 
load. The pair of gNBs will be connected – O-RU1 will be connected to O-DU1, which in turn will be connected to O-21 
CU-CP/O-CU-UP1 and similarly O-RU2 will be connected to O-DU2, which in turn will be connected to O-CU-CP/O-22 
CU-UP2. The end user device must be configured with the right user credentials to be able to register and authenticate 23 
with the Master eNB and the 4G core. The end user device also needs to be provisioned with the right application like a 24 
video streaming client to perform the tests. The 4G core network must be configured to support end user device used for 25 
testing. This includes supporting registration, authentication and PDN connection establishment for this end user device.  26 
All the elements in the network like O-RAN (O-eNB and gNBs), 4G core and the application server need to have the 27 
ability to capture traces to validate the successful execution of the test cases. The end user device needs to have the 28 
capability to capture traces/packet capture to calculate the video streaming KPIs. Optionally, the network could have 29 
network taps deployed in various legs of the network to get packet captures to validate successful execution of the test 30 
cases. Finally, all these different components need to have connectivity with each other – the end user device should be 31 
able to connect to O-RAN system(O-eNB and gNBs), O-RAN system needs to be connected to the 4G core which in 32 
turn should have connectivity to the application server. 33 
6.2.4.3 Test Methodology/Procedure 34 
Ensure the end user device, O-RAN system, 4G core and the application server have all been configured as outlined in 35 
Section 6.2.4.2. All traces and packet captures need to be enabled for the duration of the testing to ensure all 36 
communication between network elements can be captured and validated. 37 
1. Power on the end user device and ensure it registers with the 4G core for data services over NSA by connecting 38 
over the O-eNB as Master eNB and O-RU1, O-DU1, O-CU-CP/O-CU-UP1 as secondary gNB. 39 
2. Once the registration is complete, the end user device has to establish a PDN connection with the 4G core. 40 
3. Open the video streaming client on the end user device and start a streaming session over HTTP/TCP protocol.  41 

  
Page 80 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
4. Once the video streaming session has started, move the device so the secondary gNB is handed over from O-RU1 1 
to O-RU2 (i.e. from O-RU1, O-DU1 and O-CU-CP/O-CU-UP1, to O-RU2, O-DU2 and O-CU-CP/O-CU-UP2), 2 
while it continues to use the O-eNB as the Master eNB.  3 
5. Allow the end user device to stream video through the entire handover process. Measure the KPIs included in 4 
Section 6.2 for this video streaming session. 5 
6. Optionally, repeat steps 1 through 5 for a video streaming session which uses HTTP/QUIC protocol for streaming. 6 
7. Repeat the test multiple times (> 10 times) and gather results. 7 
 8 
6.2.4.4 Test Expectation (expected results) 9 
As a pre-validation, use the traces to validate a successful registration and PDN connection setup by the end user device 10 
without any errors using the Master eNB and the O-RU1, O-DU1 and O-CU-CP/O-CU-UP1 as secondary gNB. This is 11 
a prerequisite before these tests can be validated. 12 
Validate the end user device is able to start streaming the video without delays and watch the video content through the 13 
entire handover process. Ensure there is no stalls, downgrading of video quality or intermittent buffering of the video 14 
content, especially during the handover process. Use the packet captures to validate there is no packet drop or out of 15 
sequence packets which could impact customer experience and point to a misconfigured or flawed system. 16 
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 17 
is performed in a controlled environment in good radio condition without the interference of external factors which 18 
could impact the KPIs, example: use of internet to connect to remote servers/hosts could add latency, jitter and packet 19 
loss issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the testing in 20 
this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test results outside 21 
the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues may be 22 
due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and the test 23 
has to be repeated.  24 
• Video Start time or Time to load first video frame – ~1.5 seconds 25 
• Number of video stalls/buffering – < 1 26 
• Duration of stalls in the video – < 5 seconds 27 
• Video MOS Score – > 3.5 28 
 29 
The values for the end user video streaming KPIs defined in Section 6.2 need to be included in the test report along with 30 
the minimum configuration parameters included in Section 3.3. The following information should also be included in 31 
the test report to provide a comprehensive view of the test setup. 32 
End user device side (real or emulated UE): 33 
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second) 34 
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second) 35 
• Downlink transmission mode 36 
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 37 
(average sample per second) 38 
The table below gives an example of the test report considering the mean and standard deviati on of all test results that 39 
have been captured.  40 
Table 6-6 Example Test Report for Video Streaming – Handover between same Master eNB but different O-RUs 41 
– Inter CU handover 42 

  
Page 81 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
 Video Streaming over HTTP/TCP Video Streaming over HTTP/QUIC 
Video Start Time   
Number of Video Stalls/buffering   
Duration of stalls in the video   
Video MOS Score   
L1 DL Spectral efficiency [bps/Hz]   
UE RSRP [dBm]   
UE PDSCH SINR [dB]   
MIMO rank   
PDSCH MCS   
DL RB number   
UE CQI   
UE RSRQ   
UE PMI   
UE RSSI   
UE Buffer status   
UE Packet delay    
PDSCH BLER [%]   
The end user video streaming experience can also be impacted by some of the features available (see Section 6.2) on the 1 
O-eNB, O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other 2 
features/functionality which could impact the end user’s video streaming experience should be included in the test 3 
report to provide a comprehensive view of the testing. 4 
6.2.5 Video Streaming – Handover between different MeNB while staying 5 
connected to same SgNB 6 
This test scenario validates the user’s video streaming experience when the end user device (UE) is connected over 7 
NSA to a 4G core and the UE is in the process of a handover between two Master eNBs (two O-eNB) while staying 8 
connected to the same Secondary gNB (O-RU, O-DU and O-CU-CP/O-CU-UP). As services like VoLTE or VoNR 9 
cannot be used to test handover of secondary gNB, video streaming is used for this testing. Hence, this test scenario is 10 
only applicable in an NSA deployment. 11 
 12 
6.2.5.1 Test Description 13 
This scenario tests the impact of a handover on the video streaming service. The end user device is connected over NSA 14 
to the 4G core and streaming video while it performs handover of the Master eNB to a new Master eNB, i.e. O-eNB1 to 15 
O-eNB2, while staying connected to the same Secondary gNB (O-RU, O-DU and O-CU-CP/O-CU-UP).This test 16 
assesses the impact of the video streaming service on the end user device in this handover scenario by monitoring the 17 
end user video streaming KPIs included in Section 6.2. This test case streams video using ABR protocol over 18 
HTTP/TCP and/or HTTP/QUIC. 19 

  
Page 82 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
6.2.5.2 Test Setup 1 
The SUT in this test case would be a pair of O-eNBs (O-eNB1 and O-eNB2) which act as Master eNBs and a 2 
Secondary gNBs – (O-RU, O-DU and O-CU-CP/O-CU-UP). The pair of O-eNBs, O-RU, O-DU and O-CU-CP/O-CU-3 
UP have to comply with the O-RAN specifications. A 4G core will be required to support the basic functionality to 4 
authenticate and register an end user device in order to setup a PDN connection. The 4G/5G core could be a completely 5 
emulated, partially emulated or a real non-emulated core. The application server(s) for this testing should support video 6 
streaming using ABR protocol over HTTP/TCP and/or HTTP/QUIC as applicable. The end user device (UE) used for 7 
testing could be a real UE or an emulated one. The test setup should include tools which have the ability to collect 8 
traces on the elements and/or packet captures of communication between the elements. This could be a built-in 9 
capability of the emulated/non-emulated network elements or an external tool. Optionally, if some of the network 10 
elements are located remotely either in a cloud or on the internet, the additional latency should be calculated and 11 
accounted for.  12 
The pair of O-eNBs (O-eNB1 and O-eNB2) and the gNB (O-RU, O-DU and O-CU-CP/O-CU-UP) need to have the 13 
right configuration and software load. Tools which emulate latency need to be used and configured on the O-RAN 14 
system (between O-RU, O-DU and O-CU-CP/O-CU-UP) to emulate real-world deployment conditions. The end user 15 
device must be configured with the right user credentials to be able to register and authenticate with the O-RAN (O-16 
eNBs and gNB) and the 4G core. The end user device also needs to be provisioned with the right application like a 17 
video streaming client to perform the tests. The 4G core network must be configured to support end user device used for 18 
testing. This includes supporting registration, authentication and PDN session establishment for this end user device.  19 
All the elements in the network like O-RAN system, 4G core and the application server need to have the ability to 20 
capture traces to validate the successful execution of the test cases. The end user device needs to have the capability to 21 
capture traces/packet capture to calculate the video streaming KPIs. Optionally, the network could have network taps 22 
deployed in various legs of the network to get packet captures to validate successful execution of the test cases. Finally, 23 
all these different components need to have connectivity with each other – the end user device should be able to connect 24 
to O-RAN system(O-eNBs and gNB), O-RAN system needs to be connected to the 4G core which in turn should have 25 
connectivity to the application server. 26 
6.2.5.3 Test Methodology/Procedure 27 
Ensure the end user device, O-RAN system, 4G core and the application server have all been configured as outlined in 28 
Section 6.2.5.2. All traces and packet captures need to be enabled for the duration of the testing to ensure all 29 
communication between network elements can be captured and validated. 30 
1. Power on the end user device and ensure it registers with the 4G core for data services over NSA by connecting 31 
over the O-eNB1 as Master eNB and O-RU, O-DU and O-CU-CP/O-CU-UP as Secondary gNB. 32 
2. Once the registration is complete, the end user device has to establish a PDN connection with the 4G core. 33 
3. Open the video streaming client on the end user device and start a streaming session over HTTP/TCP protocol.  34 
4. Once the video streaming session has started, move the end user device so it performs a handover of the Master 35 
eNB from O-eNB1 to O-eNB2, while staying connected to the same Secondary gNB i.e. O-RU, O-DU and O-CU-36 
CP/O-CU-UP. 37 
5. Allow the end user device to stream video through the entire handover process. Measure the KPIs included in 38 
Section 6.2 for this video streaming session. 39 
6. Optionally, repeat steps 1 through 6 for a video streaming session which uses HTTP/QUIC protocol for streaming. 40 
7. Repeat the test multiple times (> 10 times) and gather results. 41 
 42 

  
Page 83 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
6.2.5.4 Test Expectation (expected results) 1 
As a pre-validation, use the traces to validate a successful registration and PDN connection setup by the end user device 2 
without any errors using O-eNB1 as the Master eNB and the O-RU, O-DU and O-CU-CP/O-CU-UP as secondary gNB. 3 
This is a prerequisite before these tests can be validated. 4 
Validate the end user device is able to start streaming the video without delays and watch the video content through the 5 
entire handover process. Ensure there is no stalls, downgrading of video quality or intermittent buffering of the video 6 
content, especially during the handover process. Use the packet captures to validate there is no packet drop or out of 7 
sequence packets which could impact customer experience and point to a misconfigured or flawed system. 8 
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 9 
is performed in a controlled environment in good radio condition without the interference of external factors which 10 
could impact the KPIs, example: use of internet to connect to remote servers/hosts could add latency, jitter and packet 11 
loss issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the testing in 12 
this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test results outside 13 
the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues may be 14 
due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and the test 15 
has to be repeated.  16 
• Video Start time or Time to load first video frame – ~1.5 seconds 17 
• Number of video stalls/buffering – < 1 18 
• Duration of stalls in the video – < 5 seconds 19 
• Video MOS Score – > 3.5 20 
 21 
The values for the end user video streaming KPIs defined in Section 6.2 need to be included in the test report along with 22 
the minimum configuration parameters included in Section 3.3. The following information should also be included in 23 
the test report to provide a comprehensive view of the test setup. 24 
End user device side (real or emulated UE): 25 
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second) 26 
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second) 27 
• Downlink transmission mode 28 
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 29 
(average sample per second) 30 
The table below gives an example of the test report considering the mean and standard deviation of al l test results that 31 
have been captured.  32 
Table 6-7 Example Test Report for Video Streaming – Handover between Master eNB while staying connected 33 
to the same Secondary gNB 34 
 Video Streaming over TCP/QUIC Video Streaming over TCP/QUIC 
Video Start Time   
Number of Video Stalls/buffering   
Duration of stalls in the video   
Video MOS Score   
L1 DL Spectral efficiency [bps/Hz]   

  
Page 84 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
UE RSRP [dBm]   
UE PDSCH SINR [dB]   
MIMO rank   
PDSCH MCS   
DL RB number   
UE CQI   
UE RSRQ   
UE PMI   
UE RSSI   
UE Buffer status   
UE Packet delay    
PDSCH BLER [%]   
The end user video streaming experience can also be impacted by some of the features available (see Section 6.2) on the 1 
O-eNB, O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other 2 
features/functionality which could impact the end user’s video streaming experience should be included in the test 3 
report to provide a comprehensive view of the testing. 4 
6.3 Voice Services – Voice over LTE (VoLTE) 5 
Voice service forms another basic service which is provided to a customer on a telecom network. Even though the 6 
earlier 2G/3G network provide voice service through circuit-switched network, this document focuses on voice service 7 
provided using a packet switched network. This Section of the document includes test case to validate the Voice over 8 
LTE service in different scenarios. The KPIs which will be monitored to assess the voice service are included below 9 
• CSSR – Call Setup Success Rate % – Total number of calls which were successful by the total number of 10 
calls made as a percentage. 11 
• CST – Call Setup Time – Time taken from the initial SIP INVITE to when the SIP 180 Ringing is received 12 
in seconds. 13 
• MOS Score – Mean Opinion Score for the voice call. This needs to be measured on both ends of the Voice 14 
call – mobile originated and mobile terminated. 15 
• Mute Rate – This is the percentage of calls which were muted in both directions (calls with RTP loss of > 16 
3-4s in both directions are considered muted call). This needs to be measured on both ends of the voice call 17 
– mobile originated & mobile terminated and counted only once per call. 18 
• One Way Calls – This is the percentage of calls which were muted in any one direction (calls with RTP 19 
loss of > 3-4s in one direction only are considered one-way calls). This needs to be monitored on both ends 20 
of the voice call – mobile originated & mobile terminated and counted only once per call. 21 
• RTP Packet Loss % - Number of RTP packets which were dropped/lost in uplink/downlink direction as a 22 
percentage of total packets. This needs to be measured on both ends of the voice call – mobile originated 23 
and mobile terminated. 24 
 25 
Along with the monitoring and validation of these services using user experience KPIs, the O-RAN systems also need 26 
to be monitored. The end user service experience can also be impacted by some of the features available on the O-RAN 27 
system. Some of these features have been included below. As a part of the voice services testing, details of these 28 
features need to be included in the test report to get a comprehensive view of the setup used for testing. If additional 29 
features or functionalities have been enabled during this testing that impact the end user voice experience, those need be 30 
included in the test report as well.  31 
• RLC in Unacknowledged Mode 32 

  
Page 85 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
• Robust Header Compression 1 
• DRX 2 
• Dynamic GBR Admission Control 3 
• TTI Bundling 4 
• VoLTE Inactivity Timer 5 
• Frequency Hopping 6 
• Multi-Target RRC Connection Re-Establishment 7 
• VoLTE HARQ 8 
• Coordinated Multi Point (DL and UL) 9 
• VoLTE Quality Enhancement 10 
• Packet Loss Detection 11 
• Voice Codec Aware scheduler 12 
• NR to LTE PS Redirection/Cell Reselection/Handover 13 
• LTE to NR PS Redirection/Cell Reselection/Handover 14 
 15 
6.3.1 VoLTE Stationary Test 16 
This scenario tests the voice service experience on an LTE network – Voice over LTE (VoLTE) when the end user 17 
device is stationary.  18 
6.3.1.1 Test Description 19 
With penetration of LTE, VoLTE has become the primary method of providing voice service. VoLTE uses IP packets 20 
to send and receive voice packets, with the IP packets being transferred over LTE. IP Multimedia System (IMS) forms 21 
an important part of VoLTE as it is used to setup control and data plane needed for VoLTE communication. As voice 22 
service is latency sensitive, the 4G core interacts with the IMS core to setup different bearers for Voice traffic – QCI-5 23 
for VoLTE control plane and QCI-1 for VoLTE data plane. This test case is applicable when UE is connected over an 24 
O-RAN system to a 4G core in an NSA deployment. 25 
6.3.1.2 Test Setup 26 
The SUT in this test case would be a O-eNB which would be the Master eNB and a Secondary gNB. As most of the 27 
current NSA deployment use the 4G eNB to provide voice services, the use of a secondary gNB is optional. We have 28 
however included the Secondary gNB in this test scenario, but it is only applicable if the gNB plays a role in 29 
establishing the control plane or data plane for a voice call. The O-eNB, gNB and the components within these have to 30 
comply with the O-RAN specifications. The O-RAN setup should support the ability to perform this testing in different 31 
radio conditions as defined in Section 3.6. A 4G core will be required to support the basic functionality to authenticate 32 
and register an end user device in order to setup a PDN connection. An IMS core will be required to register the end 33 
user device to support voice services on a 4G network. The 4G and IMS cores could be a completely emulated, partially 34 
emulated or real non-emulated cores. We will need at least two end user devices (UE) which can be a real UEs or an 35 
emulated one, and both have to support voice service using VoLTE. The end user devices will serve as Mobile 36 
Originated (MO) and Mobile Terminated (MT) end user devices forming the two ends of the voice call. Going forward 37 
in this section, these end user devices will be referred to as MO end user device and MT end user devices to represent 38 
the role they plan in the voice call. The test setup should include tools which have the ability to collect traces on the 39 

  
Page 86 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
elements and/or packet captures of communication between the elements. This could be a built-in capability of the 1 
emulated/non-emulated network elements or an external tool. Optionally, if some of the network elements are located 2 
remotely either in a cloud or on the internet, the additional latency should be calculated and accounted for.  3 
The Master eNB, Secondary gNB and their components (O-eNB, O-RU, O-DU and O-CU-CP/O-CU-UP) need to have 4 
the right configuration and software load. The end user device must be configured with the right user credentials to be 5 
able to register and authenticate with the O-RAN system and the 4G core. The end user devices also need to be 6 
provisioned with the user credentials to attach to the 4G core and register with the IMS core to perform voice call using 7 
VoLTE. The 4G core network and IMS core must be configured to support end user devices used for testing. This 8 
includes supporting registration, authentication and PDN connection establishment for these end user devices. This also 9 
includes provisioning the IMS core to support registration of the end user devices to make voice calls over VoLTE, and 10 
dynamically setting up dedicated bearers for voice calls. The locations where the radio conditions are excellent, good, 11 
fair and poor need to be identified within the serving cell. 12 
All the elements in the network like O-RAN system, 4G core and the IMS Core need to have the ability to capture 13 
traces to validate the successful execution of the test cases. The end user devices need to have the capability to capture 14 
traces/packet capture to calculate the VoLTE KPIs. Optionally, the network could have network taps deployed in 15 
various legs of the network to get packet captures to validate successful execution of the test cases. Finally, all these 16 
different components need to have connectivity with each other – the end user device should be able to connect to O-17 
RAN system(O-eNBs and gNBs), O-RAN system needs to be connected to the 4G core which in turn should have 18 
connectivity to the IMS Core. 19 
6.3.1.3 Test Methodology/Procedure 20 
Ensure the end user devices, O-RAN system, 4G core and the IMS Core have all been configured as outlined in Section 21 
6.3.1.2. In this test scenario, both the mobile originated and mobile terminated end user devices are going to use the 22 
same O-RAN (i.e. same O-eNB), 4G and IMS core to perform the end-to-end voice call. All traces and packet captures 23 
need to be enabled for the duration of the testing to ensure all communication between network elements can be 24 
captured and validated. 25 
1. Power on the two end user devices in excellent radio condition and ensure both of the end user devices connect to 26 
the 4G core over the Master O-eNB and optionally secondary gNB.  27 
2. Ensure both the MO and MT end user devices can establish a PDN connection with the 4G core. Once the PDN 28 
connection has been setup, both the end user devices have to register with the IMS core to support voice services. 29 
3. Use the MO end user device to call the MT end user device. Validate the MT end user device is able to receive and 30 
answer the call. 31 
4. Continue to have two-way voice communication on the voice call for at least 5 minutes before terminating it.  32 
5. Repeat the test multiple times (>10 times) and gather results. 33 
6. Repeat the above steps 1 through 5 for the MO and MT end user devices in good, fair and poor radio conditions. 34 
 35 
6.3.1.4 Test Expectation (expected results) 36 
As a pre-validation, use the traces to validate a successful PDN connection setup by the end user devices without any 37 
errors using the Master eNB and optionally the secondary gNB. Also validate both the end user devices are able to 38 
register with the IMS core for voice services. This is a prerequisite before these tests can be validated. 39 
Validate the end user devices are able to make a voice call between each other by dynamically setting up a QCI-1 bearer 40 
to transfer voice packets over RTP. Ensure the Call Setup Time is reasonable, the voice quality from both parties are 41 
clear and audible without one-way or intermittent muting. Use the packet captures to validate there is no RTP packet 42 
drops or high RTP packet jitter which could cause voice muting issues. Use the packet captures to ensure there are no 43 
out-of-sequence packets which could impact customer’s voice experience.  44 

  
Page 87 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 1 
is performed in a controlled environment in good radio condition without the interference of external factors which 2 
could impact the KPIs, example: use of internet to connect to remote network nodes could add latency, jitter and packet 3 
loss issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the testing in 4 
this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test results outside 5 
the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues may be 6 
due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and the test 7 
has to be repeated.  8 
• CSSR – Call Setup Success Rate % –> 99%. 9 
• CST – Call Setup Time – < 2.5s 10 
• MOS Score – > 3.5 11 
• Mute Rate % – < 1% 12 
• One Way Call % - < 1% 13 
• RTP Packet Loss % - < 1% 14 
 15 
These end user voice KPI values included in Section 6.3 need to be included in the test report along with the minimum 16 
configuration parameters included in Section 3.3. The following information should also be included in the test report 17 
for the testing performed in different radio conditions to provide a comprehensive view of the test setup. 18 
End user device side (real or emulated UE): 19 
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second) 20 
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second) 21 
• Downlink transmission mode 22 
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 23 
(average sample per second) 24 
The table below gives an examp le of the test report considering the mean and standard deviation of all test results that 25 
have been captured.  26 
Table 6-8 Example Test Report for Voice over LTE Testing – Stationary Test 27 
 
Excellent 
(cell centre) Good Fair Poor 
(cell edge) 
VoLTE MO/MT VoLTE MO/MT VoLTE MO/MT VoLTE MO/MT 
Call Setup Success Rate     
Call Setup time     
MOS Score     
Mute Rate     
One Way Call     
RTP Packet Loss     
L1 DL Spectral efficiency [bps/Hz]     
UE RSRP [dBm]     
UE PDSCH SINR [dB]     
MIMO rank     
PDSCH MCS     
DL RB number     

  
Page 88 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
UE CQI     
UE RSRQ     
UE PMI     
UE RSSI     
UE Buffer status     
UE Packet delay      
PDSCH BLER [%]     
The end user VoLTE experience can also be impacted by some of the features available (see Section 6.3) on the O-eNB, 1 
O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other features/functionality which 2 
could impact the end user’s voice service experience should be included in the test report to provide a comprehensive 3 
view of the testing. 4 
6.3.2 VoLTE Handover Test 5 
This test section is FFS 6 
6.3.3 Voice Service - LTE and NR handover tests 7 
This test scenario validates the user’s voice experience, when the UE is in a voice call, and performs a handover from 8 
LTE network to a 5G network and vice versa. This test scenario is applicable for a 5G SA deployment. 9 
6.3.3.1 Test Description 10 
Voice service is one of the basic services provided on the telecommunication network. Voice service on the 4G network 11 
is provided using VoLTE. Similarly, voice service on the 5G network is provided using packet switch technology called 12 
Voice over New Radio (VoNR). As 5G network is being deployed by a telecommunication service provider, the service 13 
provider will need to support 4G and 5G network, and thus support VoLTE, VoNR and the handover between the two 14 
voice services. This scenario tests the end user’s voice experience when the end user device performs a handover from 15 
VoLTE to VoNR and vice versa. 16 
6.3.3.2 Test Setup 17 
The SUT in this test case would be an O-eNB along with a gNB (O-RU, O-DU and O-CU-CP/O-CU-UP). We would 18 
also need an interworking 4G-5G core (referred to as 4G-5G core going forward) which supports a combined anchor 19 
point for 4G and 5G, i.e. SMF+PGW-C and UPF+PGW-U. The eNB connects to a 4G-5G core over the 4G interfaces 20 
like S1 to provide 4G LTE service and the O-CU-CP/O-CU-UP, O-DU and O-RU would connect to a 4G-5G core over 21 
the 5G interfaces like N2 and N3 to provide 5G SA service. The O-eNB, gNB and the components within these have to 22 
comply with the O-RAN specifications. The 4G-5G core will be required to support the basic functionality to 23 
authenticate and register an end user device in order to setup a PDN connection/PDU session. An IMS core will be 24 
required to register the end user device to support voice services on a 4G and 5G network. The 4G and 5G core have to 25 
interwork using the N26 interface between the MME and AMF to support seamless handover. We are recommending 26 
the use of N26 interface to ensure better customer experience when the end user device performs a handover between 27 
4G and 5G. The 4G-5G and IMS cores could be a completely emulated, partially emulated or real non-emulated cores. 28 
We will need at least two end user devices (UE) which can be a real UEs or an emulated one, and both have to support 29 
voice service using VoLTE, VoNR and the capability to handover from VoLTE to VoNR and vice versa. The end user 30 
devices will serve as Mobile Originated (MO) and Mobile Terminated (MT) end user devices forming the two ends of 31 
the voice call. Going forward in this section, these end user devices will be referred to as MO end user device and MT 32 
end user devices to represent their role in the voice call. The test setup should include tools which have the ability to 33 

  
Page 89 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
collect traces on the elements and/or packet captures of communication between the elements. This could be a built-in 1 
capability of the emulated/non-emulated network elements or an external tool. Optionally, if some of the network 2 
elements are located remotely either in a cloud or on the internet, the additional latency should be calculated and 3 
accounted for.  4 
The O-eNB, gNB and their components (O-RU, O-DU and O-CU-CP/O-CU-UP) need to have the right configuration 5 
and software load. The end user devices must be configured with the right user credentials to be able to register and 6 
authenticate with the O-RAN system and the 4G-5G core. The end user devices also need to be provisioned with the 7 
user credentials to support attach procedure to the 4G-5G core, registering to the IMS core and performing a voice call 8 
using VoLTE and VoNR. This includes supporting registration, authentication and PDN connection/PDU Session 9 
establishment for these end user devices. This also includes provisioning the IMS core to support registration of the end 10 
user devices to make voice calls over VoLTE and VoNR, and dynamically setting up dedicated bearers/QoS Flows for 11 
voice calls.  12 
All the elements in the network like O-RAN system, 4G-5G core and the IMS Core need to have the ability to capture 13 
traces to validate the successful execution of the test cases. The end user devices need to have the capability to capture 14 
traces/packet capture to calculate the VoLTE and VoNR KPIs. Optionally, the network could have network taps 15 
deployed in various legs of the network to get packet captures to validate successful execution of the test cases. Finally, 16 
all these different components need to have connectivity with each other – the end user device should be able to connect 17 
to O-RAN system(eNBs and gNBs), O-RAN system needs to be connected to the 4G-5G core which in turn should 18 
have connectivity to the IMS Core. 19 
6.3.3.3 Test Methodology/Procedure 20 
Ensure the end user devices, O-RAN system, 4G-5G core and the IMS Core have all been configured as outlined in 21 
Section 6.3.3.2. In this test scenario, both the mobile originated and mobile terminated end user devices are going to use 22 
the eNB and gNB to connect to the same 4G-5G and IMS core to perform the end-to-end voice call. All traces and 23 
packet captures need to be enabled for the duration of the testing to ensure all communication between network 24 
elements can be captured and validated. 25 
The below section gives the steps to perform a VoLTE to VoNR handover, followed by VoNR to VoLTE handover 26 
1. Power on the two end user devices and ensure both of the end user devices can connect to the 4G-5G core over the 27 
O-eNB.  28 
2. Ensure both the MO and MT end user devices can establish a PDN connection with the 4G-5G core. Once the PDN 29 
connection has been setup, both the end user devices have to register with the IMS core to support voice services. 30 
3. Use the MO end user device to call the MT end user device. Validate the MT end user device is able to receive and 31 
answer the call. 32 
4. Once the two-way call has been setup and communication is going back and forth between the two end user 33 
devices, move the MO device to the O-RU coverage of the gNB, thus forcing a handover from 4G to 5G, hence 34 
forcing a VoLTE to VoNR handover. Continue two-way voice communication through the handover process and 35 
terminate the call once the handover process is complete. Measure the voice KPIs included in Section 6.3.  36 
5. At this point in time, the MO end user device is in 5G coverage and registered to the 4G-5G core. The MT end user 37 
device is in 4G coverage and registered to the 4G-5G core. Both end user devices should still be registered to the 38 
IMS core. 39 
6. Use the MO end user device to call the MT end user device. Validate the MT end user device is able to receive and 40 
answer the call. 41 
7. Once the two-way call has been setup and communication is going back and forth between the two end user 42 
devices, move the MO device to the O-eNB coverage, thus forcing a handover from 5G to 4G, hence forcing a 43 
VoNR to VoLTE handover. Continue two-way voice communication through the handover process and terminate 44 
the call once the handover process is complete. Measure the voice KPIs included in Section 6.3.  45 
8. Repeat the test multiple times (> 10 times) and gather results. 46 
 47 

  
Page 90 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
6.3.3.4 Test Expectation (expected results) 1 
As a pre-validation, use the traces to validate a successful registration and PDN connection setup by the end user 2 
devices without any errors using the O-eNB when in 4G coverage. Similarly, use traces to validate successful 3 
registration and PDU session setup by the end user device without any errors using O-RU, O-DU and O-CU-CP/O-CU-4 
UP when in 5G coverage. Also validate both the end user devices are able to register with the IMS core for voice 5 
services. This is a prerequisite before these tests can be validated. 6 
Validate the end user devices are able to make a voice call between each other by dynamically setting up a QCI-1/5QI-1 7 
bearer to transfer voice packets over RTP. Ensure the Call Setup Time is reasonable, the voice quality from both parties 8 
are clear and audible without one-way or intermittent muting. Ensure the voice quality is not impacted during the 9 
handover process – VoLTE to VoNR and VoNR to VoLTE. Use the packet captures to validate there is no RTP packet 10 
drops or high RTP packet jitter which could cause voice muting issues, especially during the handover process. Use the 11 
packet captures to ensure there are no out-of-sequence packets which could impact customer’s voice experience.  12 
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 13 
is performed in a controlled environment in good radio condition without the interference of external factors which 14 
could impact the KPIs, example: use of internet to connect to remote network nodes could add latency, jitter and packet 15 
loss issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the testing in 16 
this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test results outside 17 
the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues may be 18 
due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and the test 19 
has to be repeated.  20 
• CSSR – Call Setup Success Rate % –> 99%. 21 
• CST – Call Setup Time – < 2.5s 22 
• MOS Score – > 3.5 23 
• Mute Rate % – < 1% 24 
• One Way Call % - < 1% 25 
• RTP Packet Loss % - < 1% 26 
 27 
These end user voice KPI values included in Section 6.3 need to be included in the test report along with the minimum 28 
configuration parameters included in Section 3.3. The following information should also be included in the test report 29 
for the testing performed in different radio conditions to provide a comprehensive view of the test setup. 30 
End user device side (real or emulated UE): 31 
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second) 32 
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second) 33 
• Received downlink throughput (L1 and L3 PDCP layers) (average sample per second) 34 
• Downlink transmission mode 35 
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 36 
(average sample per second) 37 
The table below gives an example of the test report considering the mean and standard deviation of all test results that 38 
have been captured.  39 
Table 6-9 Example Test Report for Voice service handover testing – LTE and NR 40 

  
Page 91 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
 VoLTE to VoNR handover VoNR to VoLTE handover 
Call Setup Success Rate   
Call Setup time   
MOS Score   
Mute Rate   
One Way Call   
RTP Packet Loss   
L1 DL Spectral efficiency [bps/Hz]   
UE RSRP [dBm]   
UE PDSCH SINR [dB]   
MIMO rank   
PDSCH MCS   
DL RB number   
UE CQI   
UE RSRQ   
UE PMI   
UE RSSI   
UE Buffer status   
UE Packet delay    
PDSCH BLER [%]   
The end user VoLTE/VoNR experience can also be impacted by some of the features available (see Section 6.3) on the 1 
O-eNB, O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other 2 
features/functionality which could impact the end user’s voice service experience should be included in the test report to 3 
provide a comprehensive view of the testing. 4 
6.4 Voice Service – EPS Fallback 5 
Voice service is one of the basic services which must be supported on every telecom network. Upgrade of telecom 6 
network occurs in phases, and this is no different for 5G. The telecom network may not be able to support voice service 7 
on 5G during this phase for multiple reasons – the 5G network may not be deployed nationwide, or 5G network may not 8 
be tuned to support voice service or the devices may not be able to support voice service on 5G. However, there can be 9 
no interruption to voice service during this transition phase. EPS fallback is the method used to support voice services 10 
during this phase, where voice services are continued to be supported on the legacy LTE network using VoLTE by 11 
forcing the device to fallback to LTE to make or receive a call. The KPIs which will be monitored to assess the voice 12 
service are included below 13 
• CSSR – Call Setup Success Rate % – Total number of calls which were successful by the total number of 14 
calls made as a percentage. 15 
• CST – Call Setup Time – Time taken from the initial SIP INVITE to when the SIP 180 Ringing is received 16 
in seconds. 17 
• MOS Score – Mean Opinion Score for the voice call. This needs to be measured on both ends of the Voice 18 
call – mobile originated and mobile terminated. 19 
• Mute Rate % – This is the percentage of calls which were muted in both directions (calls with RTP loss of 20 
> 3-4s in both directions are considered muted call). This needs to be measured on both ends of the Voice 21 
call – mobile originated and mobile terminated and counted only once per call. 22 

  
Page 92 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
• One Way Calls % – This is the percentage of calls which were muted in any one direction (calls with RTP 1 
loss of > 3-4s in one direction only are considered one-way calls). This needs to be monitored on both ends 2 
of the Voice call – mobile originated and mobile terminated and counted only once per call. 3 
• RTP Packet Loss % - Number of RTP packets which were dropped/lost in uplink/downlink direction as a 4 
percentage of total packets. This needs to be measured on both ends of the Voice call – mobile originated 5 
and mobile terminated. 6 
 7 
Along with the monitoring and validation of these services using user experience KPIs, the O-RAN systems also need 8 
to be monitored. The end user service experience can also be impacted by some of the features available on the O-RAN 9 
system. Some of these features have been included below. As a part of the voice services testing, details of these 10 
features need to be included in the test report to get a comprehensive view of the setup used for testing. If additional 11 
features or functionalities have been enabled during this testing that impact the end user voice experience, those need be 12 
included in the test report as well.  13 
• EPS Fallback for IMS Voice 14 
• NR to EPS Mobility 15 
• RLC in Unacknowledged Mode 16 
• Robust Header Compression 17 
• DRX 18 
• Dynamic GBR Admission Control 19 
• TTI Bundling 20 
• VoLTE Inactivity Timer 21 
• Frequency Hopping 22 
• Multi-Target RRC Connection Re-Establishment 23 
• VoLTE HARQ 24 
• Coordinated Multi Point (DL and UL) 25 
• VoLTE Quality Enhancement 26 
• Packet Loss Detection 27 
• Voice Codec Aware scheduler 28 
 29 
6.4.1 EPS Fallback Test 30 
This scenario tests the voice service when an end user device (UE) is in 5G SA coverage and performs an EPS fallback 31 
to 4G to make or receive a voice call. 32 
6.4.1.1 Test Description 33 
This section tests the voice service on a 5G SA network when it uses EPS fallback mechanism to fallback to the LTE 34 
network and use VoLTE to support voice service. This testing only applies to 5G SA deployment and in this scenario 35 
the UE is connected to the 5G SA Core and registered with the IMS core for voice service. When the end user device 36 
(UE) wants to make a voice call or receive a voice call, the network informs the device to fallback to the LTE network 37 
to make/receive the voice call. The EPS fallback does increase the Call Setup Time due to the time needed to fallback 38 
before making/receiving the voice call.  39 

  
Page 93 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
There are two mechanism in which EPS fallback is supported on the 5G core, and the methodology used impacts the 1 
time taken to perform the fallback to LTE, and hence impacts the Call Setup Time. The two mechanisms that could be 2 
used to perform EPS fallback is included below. These two mechanisms do change the test setup but does not change 3 
the testing procedure. 4 
• With N26 interface – The AMF in the 5G core communicates with the MME in the 4G over the N26 interface. 5 
In this scenario the UE performs a handover from the 5G network to the 4G network. 6 
• Without N26 interface – The AMF in the 5G core does not communicate directly with the 4G core, instead 7 
uses the UDM/HSS to store and transfer relevant session information to the 4G core. In this scenario the UE 8 
performs a Release with Redirect from the 5G network to the 4G network. 9 
6.4.1.2 Test Setup 10 
The SUT in this test case would be an O-eNB along with a gNB (O-RU, O-DU and O-CU-CP/O-CU-UP). We would 11 
also need an interworking 4G-5G core (referred to as 4G-5G core going forward) which supports a combined anchor 12 
point, i.e. SMF+PGW-C and UPF+PGW-U. The O-eNB connects to a 4G-5G core over the 4G interfaces like S1 to 13 
provide 4G LTE service and the gNB (O-RU, O-DU and O-CU-CP/O-CU-UP) would connect to the 4G-5G core over 14 
the 5G interfaces like N2 and N3 to provide 5G SA service. The O-eNB, gNB and the components within these have to 15 
comply with the O-RAN specifications and support EPS fallback. The 4G-5G core will be required to support the basic 16 
functionality to authenticate and register an end user device in order to setup a PDN connection/PDU session. The IMS 17 
core will be required to register the end user device and needs to be integrated with the 4G-5G core to support voice 18 
services over VoLTE and EPS fallback. The 4G-5G core have to interwork either using the N26 interface or without 19 
using N26 interface depending on the desired core network configuration. The existence of the N26 interface does 20 
reduce the EPS fallback time, hence reduce Call Setup Time and provide better end user experience. The 4G-5G and 21 
IMS cores could be a completely emulated, partially emulated or real non-emulated cores. We will need at least two end 22 
user devices (UE) which can be a real UEs or an emulated one, and both have to support voice service using VoLTE 23 
and EPS fallback procedure. The end user devices will serve as Mobile Originated (MO) and Mobile Terminated (MT) 24 
end user devices forming the two ends of the voice call. For the sake of clarity, we will address these end user devices 25 
as UE-1 and UE-2 in this section. The test setup should include tools which have the ability to collect traces on the 26 
elements and/or packet captures of communication between the elements. This could be a built-in capability of the 27 
emulated/non-emulated network elements or an external tool. Optionally, if some of the network elements are located 28 
remotely either in a cloud or on the internet, the additional latency should be calculated and accounted for.  29 
The O-eNB, gNB and their components (O-RU, O-DU and O-CU-CP/O-CU-UP) need to have the right configuration 30 
and software load. The end user device must be configured with the right user credentials to be able to register and 31 
authenticate with the O-RAN system and the 4G-5G core. The end user devices also need to be provisioned with the 32 
user credentials to attach to the 4G-5G core and register with the IMS core to perform voice call using VoLTE and EPS 33 
fallback. The 4G-5G core network and IMS core must be configured to support voice service on the end user devices 34 
used for testing, which includes dynamically setting up dedicated bearers for voice calls.  35 
All the elements in the network like O-RAN system, 4G-5G core and the IMS Core need to have the ability to capture 36 
traces to validate the successful execution of the test cases. The end user devices need to have the capability to capture 37 
traces/packet capture to calculate the VoLTE KPIs. Optionally, the network could have network taps deployed in 38 
various legs of the network to get packet captures to validate successful execution of the test cases. Finally, all these 39 
different components need to have connectivity with each other – the end user device should be able to connect to O-40 
RAN system(O-eNBs and gNBs), O-RAN system needs to be connected to the 4G-5G core which in turn should have 41 
connectivity to the IMS Core. 42 
6.4.1.3 Test Methodology/Procedure 43 
Ensure the end user devices, O-RAN system, 4G-5G core and the IMS Core have all been configured as outlined in 44 
Section 6.4.1.2. In this test scenario, one of the end user device is going to be connected over the 4G O-eNB to the 4G-45 

  
Page 94 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
5G core, while the other end user device is going to be connected over 5G gNB (O-RU, O-DU and O-CU-CP/O-CU-1 
UP) to the 4G-5G core. All traces and packet captures need to be enabled for the duration of the testing to ensure all 2 
communication between network elements can be captured and validated. 3 
1. Power on the two end user devices and ensure UE-1 is in LTE coverage, when UE-2 is in the 5G coverage. 4 
Validate both end user devices register to the 4G-5G core. 5 
2. Once the registration is complete, UE-1 and UE-2 have to establish a PDN connection and PDU session 6 
respectively with the 4G-5G core. Once the PDN connection and PDU session have been setup, both the end user 7 
devices have to register with the IMS core to support voice services. 8 
3. Use UE-1 as the MO end user device to call UE-2. Validate the UE-2 performs EPS fallback to LTE to receive the 9 
call.  10 
4. Answer the call on UE-2 and continue to have two-way communication for at least 5 minutes and terminate the 11 
call. Measure the voice KPIs included in Section 6.4.  12 
5. At this point UE-2 should have completed the call and moved back to connect to the 4G-5G Core over 5G gNB. 13 
6. Use UE-2 as the MO end user device to call UE-1. Validate the UE-2 performs EPS fallback to LTE to before 14 
making the call.  15 
7. Answer the call on UE-1 and continue to have two-way communication for at least 5 minutes and terminate the 16 
call. Measure the voice KPIs included in Section 6.4.  17 
8. Repeat the test multiple times (>10 times) and gather results. 18 
 19 
6.4.1.4 Test Expectation (expected results) 20 
As a pre-validation, use the traces to validate end user device UE-1 is able to perform a successful registration and PDN 21 
connection setup while using the O-eNB in LTE coverage. Similarly, use the traces to validate end user device UE-2 is 22 
able to perform a successful registration and PDU session setup while using gNB in 5G coverage. Also validate both the 23 
end user devices are able to register with the IMS core for voice services. This is a prerequisite before these tests can be 24 
validated. 25 
Validate the end user devices are able to make a voice call between each other by dynamically setting up a QCI-1 bearer 26 
to transfer voice packets over RTP. Validate the device which is in the 5G coverage falls back to 4G before receiving or 27 
making a voice call, in other words the EPS fallback procedure has been executed successfully. The Call Setup Time 28 
will be higher than VoLTE due to the delay associated with executing the EPS fallback procedure. Even though this test 29 
case does not directly impact the quality of the voice call, for the sake of consistency, ensure the voice quality from both 30 
parties are clear and audible without one-way or intermittent muting. Use the packet captures to validate there is no 31 
RTP packet drops or high RTP packet jitter which could cause voice muting issues. Use the packet captures to ensure 32 
there are no out-of-sequence packets which could impact customer’s voice experience.  33 
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 34 
is performed in a controlled environment in good radio condition without the interference of external factors which 35 
could impact the KPIs, example: use of internet to connect to remote network nodes could add latency, jitter and packet 36 
loss issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the testing in 37 
this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test results outside 38 
the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues may be 39 
due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and the test 40 
has to be repeated.  41 
Table 6-10 Typical Voice KPI values in controlled environments with good radio conditions 42 
KPI With N26 Interface Without N26 Interface 
CSSR – Call Setup Success Rate % >99% >99% 

  
Page 95 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
CST – Call Setup Time 3.5s 4 s 
MOS Score 3.5 3.5 
Mute Rate % < 1% < 1% 
One Way Call % < 1% < 1% 
RTP Packet Loss % < 1% < 1% 
 1 
These end user voice KPI values included in Section 6.3 need to be included in the test report along with the minimum 2 
configuration parameters included in Section 3.3. The following information should also be included in the test report to 3 
provide a comprehensive view of the test setup. 4 
End user device side (real or emulated UE): 5 
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second) 6 
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second) 7 
• Downlink transmission mode 8 
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 9 
(average sample per second) 10 
The table below gives an example of the test report considering the mean and standard deviation of all  test results that 11 
have been captured.  12 
Table 6-11 Example Test Report for Voice Service Testing – EPS Fallback 13 
 VoLTE MO VoLTE MT 
Call Setup Success Rate   
Call Setup time   
MOS Score   
Mute Rate   
One Way Call   
RTP Packet Loss   
L1 DL Spectral efficiency [bps/Hz]   
UE RSRP [dBm]   
UE PDSCH SINR [dB]   
MIMO rank   
PDSCH MCS   
DL RB number   
UE CQI   
UE RSRQ   
UE PMI   
UE RSSI   
UE Buffer status   
UE Packet delay    

  
Page 96 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
PDSCH BLER [%]   
The end user VoLTE experience can also be impacted by some of the features available (see Section 6.3) on the O-eNB, 1 
O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other features/functionality which 2 
could impact the end user’s voice service experience should be included in the test report to provide a comprehensive 3 
view of the testing. 4 
6.5 Voice Service – Voice over NR (VoNR) 5 
Voice services on 5G are called Voice over New Radio (VoNR). VoNR also packetizes voice and uses IP packets for 6 
voice communication. On the Core and IMS side, VoNR is very similar to VoLTE. VoNR also uses IMS system for 7 
voice service, and the IMS interacts with the 5G core to setup separate bearer for voice service. This section tests the 8 
voice service over VoNR in different scenarios. The KPIs which will be monitored to assess the voice service are 9 
included below 10 
• CSSR – Call Setup Success Rate % – Total number of calls which were successful by the total number of 11 
calls made as a percentage. 12 
• CST – Call Setup Time – Time taken from the initial SIP INVITE to when the SIP 180 Ringing is received 13 
in seconds. 14 
• MOS Score – Mean Opinion Score for the voice call. This needs to be measured on both ends of the Voice 15 
call – mobile originated and mobile terminated. 16 
• Mute Rate % – This is the percentage of calls which were muted in both directions (calls with RTP loss of 17 
> 3-4s in both directions are considered muted call). This needs to be measured on both ends of the Voice 18 
call – mobile originated and mobile terminated and counted only once per call. 19 
• One Way Calls % – This is the percentage of calls which were muted in any one direction (calls with RTP 20 
loss of > 3-4s in one direction only are considered one-way calls). This needs to be monitored on both ends 21 
of the Voice call – mobile originated and mobile terminated and counted only once per call. 22 
• RTP Packet Loss % - Number of RTP packets which were dropped/lost in uplink/downlink direction as a 23 
percentage of total packets. This needs to be measured on both ends of the Voice call – mobile originated 24 
and mobile terminated. 25 
 26 
End user voice service experience can also be impacted by some of the features available on the O-RU, O-DU and O-27 
CU-CP/O-CU-UP. Some of these features have been included below. As a part of the voice services testing, details of 28 
these features need to be included in the test report to get a comprehensive view of the setup used for testing. If 29 
additional features or functionalities have been enabled during this testing that impact the end user voice experience, 30 
those need be included in the test report as well. 31 
• Basic Voice over NR 32 
• RLC in Unacknowledged Mode 33 
• Robust Header Compression 34 
• DRX 35 
• Dynamic GBR Admission Control 36 
• TTI Bundling 37 
• Automated Neighbor Relations ANR 38 
• Connected mode mobility (Handover) 39 
• idle mode reselection 40 
• Intra-frequency Cell Reselection/Handover 41 
• Inter-frequency Redirection/Cell Reselection/Handover 42 

  
Page 97 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
• NR Coverage-Triggered NR Session Continuity 1 
 2 
6.5.1 Voice over NR Test 3 
This test scenario validates the user’s voice experience when the end user device is in 5G coverage and the user makes a 4 
voice call over NR – VoNR. 5 
6.5.1.1 Test Description 6 
This section tests VoNR user experience on an O-RAN system. VoNR is similar to VoLTE where it uses IP packets to 7 
send and receive voice packets, with the IP packets being transferred over NR or 5G radio. Just like VoLTE, IMS is 8 
used to setup the control and data plane for the voice communication. As voice service is latency sensitive, the 5G core 9 
interacts with the IMS core to setup different QoS flows for voice traffic – 5QI-5 for VoNR control plane and 5QI-1 for 10 
VoNR data plane. This test case is applicable when end user device (UE) is connected over a 5G SA network.  11 
6.5.1.2 Test Setup 12 
The SUT in this test case would be a gNB (O-RU, O-DU and O-CU-CP/O-CU-UP) which is used to test the voice 13 
service. A 5G SA Core would be required to support basic functionality to authenticate and register the end user device 14 
to establish a PDU session. An IMS core will be required to register the end user device to support voice services on a 15 
5G network. The 5G and IMS cores could be a completely emulated, partially emulated or real non-emulated cores. We 16 
will need at least two end user devices (UE) which can be a real UEs or an emulated one, and both have to support 17 
voice service using VoNR. The end user devices will serve as Mobile Originated (MO) and Mobile Terminated (MT) 18 
end user devices forming the two ends of the voice call. Going forward in this section, these end user devices will be 19 
referred to as MO end user device and MT end user device to represent their role in the voice call. The test setup should 20 
include tools which have the ability to collect traces on the elements and/or packet captures of communication between 21 
the elements. This could be a built-in capability of the emulated/non-emulated network elements or an external tool. 22 
Optionally, if some of the network elements are located remotely either in a cloud or on the internet, the additional 23 
latency should be calculated and accounted for.  24 
The O-RU, O-DU and O-CU-CP/O-CU-UP need to have the right configuration and software load. The SUT will also 25 
need to be setup to run this testing in different radio conditions as outlined in Section 3.6. The end user device must be 26 
configured with the right user credentials to be able to register and authenticate with the O-RAN system and the 5G 27 
core. The end user devices also need to be provisioned with the user credentials to register and setup PDU session with 28 
the 5G core and register with the IMS core to perform voice call using VoNR. The 5G core network and IMS core must 29 
be configured to support voice service on the end user devices used for testing, which includes the ability to 30 
dynamically set up QoS Flows for voice calls. The locations where the radio conditions are excellent, good, fair and 31 
poor need to be identified within the serving cell. 32 
All the elements in the network like O-RAN system, 5G core and the IMS Core need to have the ability to capture 33 
traces to validate the successful execution of the test cases. The end user devices need to have the capability to capture 34 
traces/packet capture to calculate the VoNR KPIs. Optionally, the network could have network taps deployed in various 35 
legs of the network to get packet captures to validate successful execution of the test cases. Finally, all these different 36 
components need to have connectivity with each other – the end user device should be able to connect to O-RAN 37 
system(O-RU, O-DU and O-CU-CP/O-CU-UP), O-RAN system needs to be connected to the 5G core which in turn 38 
should have connectivity to the IMS Core. 39 

  
Page 98 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
6.5.1.3 Test Methodology/Procedure 1 
Ensure the end user device, O-RAN system, 5G core and the IMS core have all been configured as outlined in Section 2 
6.5.1.2. In this test scenario, both the mobile originated and mobile terminated end user devices are going to use the 3 
same O-RAN system, 5G and IMS core to perform the end-to-end voice call. All traces and packet captures need to be 4 
enabled for the duration of the testing to ensure all communication between network elements can be captured and 5 
validated. 6 
1. Power on the two end user devices in excellent radio condition and ensure both of devices registers with the 5G 7 
core for voice services over SA by connecting over the O-RAN (O-RU, O-DU and O-CU-CP/O-CU-UP).  8 
2. Once the registration is complete, the MO and MT end user devices have to establish a PDU session with the 5G 9 
core. Once the PDU session has been setup, both the end user devices have to register with the IMS core to support 10 
voice services. 11 
3. Use the MO end user device to call the MT end user device. Validate the MT end user device is able to receive and 12 
answer the call. 13 
4. Continue to have two-way voice communication on the voice call for at least 5 minutes before terminating it.  14 
5. Repeat the test multiple times (> 10 times) and gather results. 15 
6. Repeat the above steps 1 through 5 for the good, fair and poor radio conditions. 16 
 17 
6.5.1.4 Test Expectation (expected results) 18 
As a pre-validation, use the traces to validate a successful registration and PDU session setup by the end user devices 19 
without any errors over O-RU, O-DU and O-CU-CP/O-CU-UP. Also validate both the end user devices are able to 20 
register with the IMS core for voice services. This is a prerequisite before these tests can be validated. 21 
Validate the end user devices are able to make a voice call between each other by dynamically setting up a 5QI-1 bearer 22 
to transfer voice packets over RTP. Ensure the Call Setup Time is reasonable, the voice quality from both parties are 23 
clear and audible without one-way or intermittent muting. Use the packet captures to validate there is no RTP packet 24 
drops or high RTP packet jitter which could cause voice muting issues. Use the packet captures to ensure there are no 25 
out-of-sequence packets which could impact customer’s voice experience.  26 
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 27 
is performed in a controlled environment in good radio condition without the interference of external factors which 28 
could impact the KPIs, example: use of internet to connect to remote servers/hosts could add latency, jitter and 29 
reliability issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the 30 
testing in this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test results 31 
outside the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues 32 
may be due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and 33 
the test has to be repeated.  34 
• CSSR – Call Setup Success Rate % –> 99%. 35 
• CST – Call Setup Time – < 2.5s 36 
• MOS Score – > 3.5 37 
• Mute Rate % – < 1% 38 
• One Way Call % - < 1% 39 
• RTP Packet Loss % - < 1% 40 
 41 
As a part of gathering data, ensure the minimum configuration parameters (see Section 3.3) are included in the test 42 
report. The following information should also be included in the test report to provide a comprehensive view of the test 43 
setup. 44 

  
Page 99 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
UE side (real or emulated UE): 1 
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second) 2 
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second) 3 
• Received downlink throughput (L1 and L3 PDCP layers) (average sample per second) 4 
• Downlink transmission mode 5 
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 6 
(average sample per second) 7 
Table 6-12 Example Test Report for VoNR – Stationary Testing 8 
 
Excellent 
(cell centre) Good Fair Poor 
(cell edge) 
VoNR MO/MT VoNR MO/MT VoNR MO/MT VoNR MO/MT 
Call Setup Success Rate     
Call Setup Time     
MOS Score     
Mute Rate %     
One Way Call %     
RTP Packet Loss %     
L1 DL Spectral efficiency [bps/Hz]     
UE RSRP [dBm]     
UE PDSCH SINR [dB]     
MIMO rank     
PDSCH MCS     
DL RB number     
UE CQI     
UE RSRQ     
UE PMI     
UE RSSI     
UE Buffer status     
UE Packet delay      
PDSCH BLER [%]     
The end user voice service experience can also be impacted by some of the features available (see Section 6.5) on the 9 
O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other features/functionality which 10 
could impact the end user’s voice service experience should be included in the test report to provide a comprehensive 11 
view of the testing. 12 
6.5.2 VoNR – Intra-Distributed Unit (O-DU) handover 13 
This test scenario validates the user’s voice experience when the end user device (UE) is on a VoNR call and performs a 14 
handover between two O-RUs which connect to the same O-DU (Intra-O-DU handover) 15 

  
Page 100 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
6.5.2.1 Test Description 1 
The 5G O-RAN system has multiple sub-components like the O-RU, O-DU and the O-CU-CP/O-CU-UP. This setup 2 
leads to multiple handover scenarios between the O-RAN sub-components. This particular scenario tests the end user’s 3 
voice experience during the handover between two O-RUs which are connected to the same O-DU (and O-CU-CP/O-4 
CU-UP), hence an Intra-O-DU handover. This handover will be agnostic to the 5G core as the handover occurs on the 5 
O-RAN system. This test assesses the impact on the end user’s voice service in this handover scenario by monitoring 6 
the KPIs included in Section 6.5. 7 
6.5.2.2 Test Setup 8 
The SUT in this test case would be a pair of O-RUs which connect to the same O-DU and O-CU-CP/O-CU-UP (refer 9 
Section 4.4). This O-RAN setup is used to test the voice service during a handover. A 5G SA Core would be required to 10 
support basic functionality to authenticate and register the end user device to establish a PDU session. An IMS core will 11 
be required to register the end user device to support voice services on a 5G network. The 5G and IMS cores could be a 12 
completely emulated, partially emulated or real non-emulated cores. We will need at least two end user devices (UE) 13 
which can be a real UEs or an emulated one, and both have to support voice service using 5G – Voice over NR(VoNR). 14 
The end user devices will serve as Mobile Originated (MO) and Mobile Terminated (MT) end user devices forming the 15 
two ends of the voice call. Going forward in this section, these end user devices will be referred to as MO end user 16 
device and MT end user device to represent their role in the voice call. The test setup should include tools which have 17 
the ability to collect traces on the elements and/or packet captures of communication between the elements. This could 18 
be a built-in capability of the emulated/non-emulated network elements or an external tool. Optionally, if some of the 19 
network elements are located remotely either in a cloud or on the internet, the additional latency should be calculated 20 
and accounted for.   21 
The pair of O-RUs (O-RU1 and O-RU2) need to be connected to the O-DU and O-CU-CP/O-CU-UP and all the 22 
components need to have the right configuration and software load. The end user devices must be configured with the 23 
right user credentials to be able to register and authenticate with the O-RAN system and the 5G core. The end user 24 
devices also need to be provisioned with the user credentials to register and setup PDU session with the 5G core and 25 
register with the IMS core to perform voice call using VoNR. The 5G core network and IMS core must be configured to 26 
support voice service on the end user devices used for testing, which includes the ability to dynamically set up QoS 27 
Flows for voice calls. 28 
All the elements in the network like O-RAN system, 5G core and the IMS Core need to have the ability to capture 29 
traces to validate the successful execution of the test cases. The end user devices need to have the capability to capture 30 
traces/packet capture to calculate the VoNR KPIs. Optionally, the network could have network taps deployed in various 31 
legs of the network to get packet captures to validate successful execution of the test cases. Finally, all these different 32 
components need to have connectivity with each other – the end user device should be able to connect to O-RAN 33 
system(O-RUs connected to O-DU and O-CU-CP/O-CU-UP), O-RAN system needs to be connected to the 5G core 34 
which in turn should have connectivity to the IMS Core. 35 
6.5.2.3 Test Methodology/Procedure 36 
Ensure the end user devices, O-RAN system, 5G core and the IMS Core have all been configured as outlined in Section 37 
6.5.2.2. In this test scenario, both the mobile originated and mobile terminated end user devices are going to use the 38 
same O-RAN system, 5G and IMS core to perform the end-to-end voice call. All traces and packet captures need to be 39 
enabled for the duration of the testing to ensure all communication between network elements can be captured and 40 
validated. 41 

  
Page 101 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
1. Power on the two end user devices and ensure both of the devices registers with the 5G core for voice services over 1 
SA by connecting over the O-RAN (O-RU, O-DU and O-CU-CP/O-CU-UP). Ensure both the MO & MT end user 2 
devices are in the coverage area of the same O-RU – O-RU1. 3 
2. Once the registration is complete, the MO and MT end user devices have to establish PDU session with the 5G 4 
core. Once the PDU session has been setup, both the end user devices have to register with the IMS core to support 5 
voice services. 6 
3. Use the MO end user device to call the MT end user device. Validate the MT end user device is able to receive and 7 
answer the call. 8 
4. Once the call has been setup, move the MO end user device from the coverage area of O-RU1 to coverage area of 9 
O-RU2, thus causing a handover from O-RU1 to O-RU2.  10 
5. Continue the two-way voice communication between MO and MT end user devices until the handover procedure is 11 
complete before terminating the voice call.  12 
6. Repeat the test (steps 1 through 5) multiple times (> 10 times) and gather results. 13 
 14 
6.5.2.4 Test Expectation (expected results) 15 
As a pre-validation, use the traces to validate a successful registration and PDU session setup by the end user devices 16 
without any errors over O-RU, O-DU and O-CU-CP/O-CU-UP. Also validate both the end user devices are able to 17 
register with the IMS core for voice services. This is a prerequisite before these tests can be validated. 18 
Validate the end user devices are able to make a voice call between each other by dynamically setting up a 5QI-1 bearer 19 
to transfer voice packets over RTP. Ensure the Call Setup Time is reasonable, the voice quality from both parties are 20 
clear and audible without one-way or intermittent muting through the duration of the call, especially during the 21 
handover process. Use the packet captures to validate there is no RTP packet drops or high RTP packet jitter which 22 
could cause voice muting issues. Use the packet captures to ensure there are no out-of-sequence packets which could 23 
impact customer’s voice experience.  24 
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 25 
is performed in a controlled environment in good radio condition without the interference of external factors which 26 
could impact the KPIs, example: use of internet to connect to remote servers/hosts could add latency, jitter and 27 
reliability issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the 28 
testing in this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test results 29 
outside the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues 30 
may be due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and 31 
the test has to be repeated.  32 
• CSSR – Call Setup Success Rate % –> 99%. 33 
• CST – Call Setup Time – < 2.5s 34 
• MOS Score – > 3.5 35 
• Mute Rate % – < 1% 36 
• One Way Call % - < 1% 37 
• RTP Packet Loss % - < 1% 38 
 39 
As a part of gathering data, ensure the minimum configuration parameters (see Section 3.3) are included in the test 40 
report. The following information should also be included in the test report to provide a comprehensive view of the test 41 
setup. 42 
UE side (real or emulated UE): 43 
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second) 44 
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second) 45 

  
Page 102 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
• Received downlink throughput (L1 and L3 PDCP layers) (average sample per second) 1 
• Downlink transmission mode 2 
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 3 
(average sample per second) 4 
Table 6-13 Example Test Report for VoNR – Intra-O-DU Handover 5 
 VoNR MO VoNR MT 
Call Setup Success Rate   
Call Setup Time   
MOS Score   
Mute Rate %   
One Way Call %   
RTP Packet Loss %   
L1 DL Spectral efficiency [bps/Hz]   
UE RSRP [dBm]   
UE PDSCH SINR [dB]   
MIMO rank   
PDSCH MCS   
DL RB number   
UE CQI   
UE RSRQ   
UE PMI   
UE RSSI   
UE Buffer status   
UE Packet delay    
PDSCH BLER [%]   
The end user voice service experience can also be impacted by some of the features available (see Section 6.5) on the 6 
O-RUs, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other features/functionality which 7 
could impact the end user’s voice service experience should be included in the test report to provide a comprehensive 8 
view of the testing. 9 
6.5.3 VoNR – Intra-Central Unit (O-CU) Inter-Distributed Unit (O-DU) 10 
handover  11 
This test scenario validates the user’s voice experience when the end user device (UE) is on a VoNR call and performs a 12 
handover between O-RUs which connect to different O-DUs, which in turn are connected to the same O-CU-CP/O-CU-13 
UP (Intra-O-CU Inter-O-DU handover) 14 
6.5.3.1 Test Description 15 
The 5G O-RAN system has multiple sub-components like the O-RU, O-DU and the O-CU-CP/O-CU-UP. This setup 16 
leads to multiple handover scenarios between the O-RAN sub-components. This particular scenario tests the end user’s 17 
voice experience during the handover between two O-RUs, where the O-RUs are connected to different O-DUs which 18 

  
Page 103 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
in turn are connected to the same O-CU-CP/O-CU-UP, hence an Intra-O-CU Inter-O-DU handover. This handover will 1 
be agnostic to the 5G core as the handover occurs on the O-RAN system. This test assesses the impact on the end user’s 2 
voice service in this handover scenario by monitoring the KPIs included in Section 6.5. 3 
6.5.3.2 Test Setup 4 
The SUT in this test case would be a pair of O-RUs which connect to a different pair of O-DUs which in turn connect to 5 
the same O-CU (refer Section 4.5). This O-RAN setup is used to test the voice service during a handover. A 5G SA 6 
Core would be required to support basic functionality to authenticate and register the end user device to establish a PDU 7 
session. An IMS core will be required to register the end user device to support voice services on a 5G network. The 5G 8 
and IMS cores could be a completely emulated, partially emulated or real non-emulated cores. We will need at least two 9 
end user devices (UE) which can be a real UEs or an emulated one, and both have to support voice service using VoNR. 10 
The end user devices will serve as Mobile Originated (MO) and Mobile Terminated (MT) end user devices forming the 11 
two ends of the voice call. Going forward in this section, these end user devices will be referred to as MO end user 12 
device and MT end user device to represent their role in the voice call. The test setup should include tools which have 13 
the ability to collect traces on the elements and/or packet captures of communication between the elements. This could 14 
be a built-in capability of the emulated/non-emulated network elements or an external tool. Optionally, if some of the 15 
network elements are located remotely either in a cloud or on the internet, the additional latency should be calculated 16 
and accounted for.  17 
The pair of O-RUs (O-RU1 and O-RU2) need to be connected to a pair of O-DUs (O-DU1 and O-DU2) and O-CU-18 
CP/O-CU-UP. As for the O-RU to O-DU connection, ensure O-RU1 connects to O-DU1, and O-RU2 connects to O-19 
DU2, and both the O-DUs connect to the same O-CU-CP/O-CU-UP. All the O-RAN components need to have the right 20 
configuration and software load. The end user devices also need to be provisioned with the user credentials to register 21 
and setup PDU session with the 5G core and register with the IMS core to perform voice call using VoNR. The 5G core 22 
network and IMS core must be configured to support voice service on the end user devices used for testing, which 23 
includes the ability to dynamically set up QoS Flows for voice calls. 24 
All the elements in the network like O-RAN system, 5G core and the IMS Core need to have the ability to capture 25 
traces to validate the successful execution of the test cases. The end user devices need to have the capability to capture 26 
traces/packet capture to calculate the VoNR KPIs. Optionally, the network could have network taps deployed in various 27 
legs of the network to get packet captures to validate successful execution of the test cases. Finally, all these different 28 
components need to have connectivity with each other – the end user device should be able to connect to O-RAN 29 
system(O-RUs connected to O-DU and O-CU), O-RAN system needs to be connected to the 5G core which in turn 30 
should have connectivity to the IMS Core. 31 
6.5.3.3 Test Methodology/Procedure 32 
Ensure the end user devices, O-RAN system, 5G core and the IMS core have all been configured as outlined in Section 33 
6.5.3.2. In this test scenario, both the mobile originated and mobile terminated end user devices are going to use the 34 
same O-RAN system, 5G and IMS core to perform the end-to-end voice call. All traces and packet captures need to be 35 
enabled for the duration of the testing to ensure all communication between network elements can be captured and 36 
validated. 37 
1. Power on the two end user devices and ensure both of devices registers with the 5G core for voice services over SA 38 
by connecting over the O-RAN (O-RU, O-DU and O-CU). Ensure both the MO & MT end user devices are in the 39 
coverage area of the same O-RU – O-RU1. 40 
2. Once the registration is complete, the MO and MT end user devices have to establish a PDU session with the 5G 41 
core. Once the PDU session has been setup, both the end user devices have to register with the IMS core to support 42 
voice services. 43 
3. Use the MO end user device to call the MT end user device. Validate the MT end user device is able to receive and 44 
answer the call. 45 

  
Page 104 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
4. Once the call has been setup, move the MO end user device from the coverage area of O-RU1 to coverage area of 1 
O-RU2, thus causing a handover from O-RU1 to O-RU2, and O-DU1 to O-DU2.  2 
5. Continue the two-way voice communication between the end user devices until the handover procedure is complete 3 
before terminating the voice call.  4 
6. Repeat the test (steps 1 through 5) multiple times (> 10 times) and gather results. 5 
 6 
6.5.3.4 Test Expectation (expected results) 7 
As a pre-validation, use the traces to validate a successful registration and PDU session setup by the end user devices 8 
without any errors over O-RU, O-DU and O-CU. Also validate both the end user devices are able to register with the 9 
IMS core for voice services. This is a prerequisite before these tests can be validated. 10 
Validate the end user devices are able to make a voice call between each other by dynamically setting up a 5QI-1 bearer 11 
to transfer voice packets over RTP. Ensure the Call Setup Time is reasonable, the voice quality from both parties are 12 
clear and audible without one-way or intermittent muting through the duration of the call, especially during the 13 
handover process. Use the packet captures to validate there is no RTP packet drops or high RTP packet jitter which 14 
could cause voice muting issues. Use the packet captures to ensure there are no out-of-sequence packets which could 15 
impact customer’s voice experience.  16 
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 17 
is performed in a controlled environment in good radio condition without the interference of external factors which 18 
could impact the KPIs, example: use of internet to connect to remote servers/hosts could add latency, jitter and 19 
reliability issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the 20 
testing in this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test results 21 
outside the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues 22 
may be due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and 23 
the test has to be repeated.  24 
• CSSR – Call Setup Success Rate % –> 99%. 25 
• CST – Call Setup Time – < 2.5s 26 
• MOS Score – > 3.5 27 
• Mute Rate % – < 1% 28 
• One Way Call % - < 1% 29 
• RTP Packet Loss % - < 1% 30 
 31 
As a part of gathering data, ensure the minimum configuration parameters (see Section 3.3) are included in the test 32 
report. The following information should also be included in the test report to provide a comprehensive view of the test 33 
setup. 34 
UE side (real or emulated UE): 35 
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second) 36 
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second) 37 
• Downlink transmission mode 38 
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 39 
(average sample per second) 40 
Table 6-14 Example Test Report for VoNR – Inter-O-DU Intra-O-CU Handover 41 
 VoNR MO VoNR MT 

  
Page 105 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
Call Setup Success Rate   
Call Setup Time   
MOS Score   
Mute Rate %   
One Way Call %   
RTP Packet Loss %   
L1 DL Spectral efficiency [bps/Hz]   
UE RSRP [dBm]   
UE PDSCH SINR [dB]   
MIMO rank   
PDSCH MCS   
DL RB number   
UE CQI   
UE RSRQ   
UE PMI   
UE RSSI   
UE Buffer status   
UE Packet delay    
PDSCH BLER [%]   
The end user voice service experience can also be impacted by some of the features available (see Section 6.5) on the 1 
O-RUs, O-DU and O-CU. The details of these features, along with any other features/functionality which could impact 2 
the end user’s voice service experience should be included in the test report to provide a comprehensive view of the 3 
testing. 4 
6.5.4 VoNR – Inter-Central Unit (O-CU) handover  5 
This test scenario validates the user’s voice experience when the end user device (UE) is on a VoNR call and performs a 6 
handover between O-RUs which connect to different O-DUs and different O-CUs (Inter-O-CU Inter-O-DU handover) 7 
6.5.4.1 Test Description 8 
The 5G O-RAN system has multiple sub-components like the O-RU, O-DU and the O-CU. This setup leads to multiple 9 
handover scenarios between the O-RAN sub-components. This particular scenario tests the end user’s voice experience 10 
during the handover between two O-RUs, where the O-RUs are connected to different O-DUs which in turn are 11 
connected to different O-CUs, hence an Inter-O-CU Inter-O-DU handover. This handover occurs on the O-RAN system 12 
and the 5G core is aware of the handover as it needs to send data to a new O-CU as a part of the handover process. This 13 
test assesses the impact on the end user’s voice service in this handover scenario by monitoring the KPIs included in 14 
Section 6.5. 15 
6.5.4.2 Test Setup 16 
The SUT in this test case would be a pair of O-RAN subsystems, a set of O-RU, O-DU and O-CU which interconnects 17 
with another set of O-RU, O-DU and O-CU (refer Section 4.6). This O-RAN setup is used to test the voice service 18 
during a handover. A 5G SA Core would be required to support basic functionality to authenticate and register the end 19 
user device to establish a PDU session. An IMS core will be required to register the end user device to support voice 20 

  
Page 106 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
services on a 5G network. The 5G and IMS cores could be a completely emulated, partially emulated or real non-1 
emulated cores. We will need at least two end user devices (UE) which can be a real UEs or an emulated one, and both 2 
have to support voice service using VoNR. The end user devices will serve as Mobile Originated (MO) and Mobile 3 
Terminated (MT) end user devices forming the two ends of the voice call. Going forward in this section, these end user 4 
devices will be referred to as MO end user device and MT end user device to represent their role in the voice call. The 5 
test setup should include tools which have the ability to collect traces on the elements and/or packet captures of 6 
communication between the elements. This could be a built-in capability of the emulated/non-emulated network 7 
elements or an external tool. Optionally, if some of the network elements are located remotely either in a cloud or on the 8 
internet, the additional latency should be calculated and accounted for.  9 
The pair of O-RAN (O-RAN1 and O-RAN2) subsystems will be connected – O-RU1 will be connected to O-DU1, 10 
which in turn will be connected to O-CU1 and similarly O-RU2 will be connected to O-DU2, which in turn will be 11 
connected to O-CU2. The O-CU1 and O-CU2 nodes will be connected to each other and the 5G core. All the O-RAN 12 
components need to have the right configuration and software load. The end user devices must be configured with the 13 
right user credentials to be able to register and authenticate with the O-RAN system and the 5G core. The end user 14 
devices also need to be provisioned with the user credentials to register and setup PDU session with the 5G core and 15 
register with the IMS core to perform voice call using VoNR. The 5G core network and IMS core must be configured to 16 
support voice service on the end user devices used for testing, which includes the ability to dynamically set up QoS 17 
Flows for voice calls. 18 
All the elements in the network like O-RAN system, 5G core and the IMS Core need to have the ability to capture 19 
traces to validate the successful execution of the test cases. The end user devices need to have the capability to capture 20 
traces/packet capture to calculate the VoNR KPIs. Optionally, the network could have network taps deployed in various 21 
legs of the network to get packet captures to validate successful execution of the test cases. Finally, all these different 22 
components need to have connectivity with each other – the end user device should be able to connect to O-RAN 23 
system(O-RUs connected to O-DUs which are connected to the O-CUs), O-RAN system needs to be connected to the 24 
5G core which in turn should have connectivity to the IMS Core. 25 
6.5.4.3 Test Methodology/Procedure 26 
Ensure the end user devices, O-RAN system, 5G core and the IMS Core have all been configured as outlined in Section 27 
6.5.4.2. In this test scenario, both the mobile originated and mobile terminated end user devices are going to use the 28 
same O-RAN system, 5G and IMS core to perform the end-to-end voice call. All traces and packet captures need to be 29 
enabled for the duration of the testing to ensure all communication between network elements can be captured and 30 
validated. 31 
1. Power on the two end user devices and ensure both of devices registers with the 5G core for voice services over SA 32 
by connecting over the O-RAN1 system (O-RU1, O-DU1 and O-CU1). Ensure both the MO & MT end user 33 
devices are in the coverage area of the same O-RU – O-RU1. 34 
2. Once the registration is complete, the MO and MT end user devices have to establish a PDU session with the 5G 35 
core. Once the PDU session has been setup, both the end user devices have to register with the IMS core to support 36 
voice services. 37 
3. Use the MO end user device to call the MT end user device. Validate the MT end user device is able to receive and 38 
answer the call. 39 
4. Once the call has been setup, move the MO end user device from the coverage area of O-RU1 to coverage area of 40 
O-RU2, thus causing a handover from O-RAN1 to O-RAN2 subsystem - O-RU1 to O-RU2, O-DU1 to O-DU2 and 41 
O-CU1 to O-CU2.  42 
5. Continue the two-way voice communication between MO and MT end user devices until the handover procedure is 43 
complete before terminating the voice call.  44 
6. Repeat the test (steps 1 through 5) multiple times (> 10 times) and gather results. 45 
 46 

  
Page 107 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
6.5.4.4 Test Expectation (expected results) 1 
As a pre-validation, use the traces to validate a successful registration and PDU session setup by the end user devices 2 
without any errors over O-RU, O-DU and O-CU. Also validate both the end user devices are able to register with the 3 
IMS core for voice services. This is a prerequisite before these tests can be validated. 4 
Validate the end user devices are able to make a voice call between each other by dynamically setting up a 5QI-1 bearer 5 
to transfer voice packets over RTP. Ensure the Call Setup Time is reasonable, the voice quality from both parties are 6 
clear and audible without one-way or intermittent muting through the duration of the call, especially during the 7 
handover process. Use the packet captures to validate there is no RTP packet drops or high RTP packet jitter which 8 
could cause voice muting issues. Use the packet captures to ensure there are no out-of-sequence packets which could 9 
impact customer’s voice experience.  10 
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 11 
is performed in a controlled environment in good radio condition without the interference of external factors which 12 
could impact the KPIs, example: use of internet to connect to remote servers/hosts could add latency, jitter and 13 
reliability issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the 14 
testing in this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test results 15 
outside the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues 16 
may be due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and 17 
the test has to be repeated.  18 
• CSSR – Call Setup Success Rate % –> 99%. 19 
• CST – Call Setup Time – < 2.5s 20 
• MOS Score – > 3.5 21 
• Mute Rate % – < 1% 22 
• One Way Call % - < 1% 23 
• RTP Packet Loss % - < 1% 24 
 25 
As a part of gathering data, ensure the minimum configuration parameters (see Section 3.3) are included in the test 26 
report. The following information should also be included in the test report to provide a comprehensive view of the test 27 
setup. 28 
UE side (real or emulated UE): 29 
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second) 30 
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second) 31 
• Received downlink throughput (L1 and L3 PDCP layers) (average sample per second) 32 
• Downlink transmission mode 33 
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 34 
(average sample per second) 35 
Table 6-15 Example Test Report for VoNR – Inter-O-CU Handover 36 
 VoNR MO VoNR MT 
Call Setup Success Rate   
Call Setup Time   
MOS Score   
Mute Rate %   

  
Page 108 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
One Way Call %   
RTP Packet Loss %   
L1 DL Spectral efficiency [bps/Hz]   
UE RSRP [dBm]   
UE PDSCH SINR [dB]   
MIMO rank   
PDSCH MCS   
DL RB number   
UE CQI   
UE RSRQ   
UE PMI   
UE RSSI   
UE Buffer status   
UE Packet delay    
PDSCH BLER [%]   
The end user voice service experience can also be impacted by some of the features available (see Section 6.5) on the 1 
O-RUs, O-DU and O-CU. The details of these features, along with any other features/functionality which could impact 2 
the end user’s voice service experience should be included in the test report to provide a comprehensive view of the 3 
testing. 4 
6.6 Video Service – Video over LTE (ViLTE) 5 
Voice service has been one of the basic services provided on the mobile device since launch. With increasing LTE 6 
penetration and the availability of higher speeds, video calls are slowly replacing voice calls. With the launch of 5G 7 
which promises much higher throughput, the shift towards using video calls is only going to get faster. This section of 8 
the document validates the user’s video calling experience in different conditions on the LTE network. This section of 9 
the document only applies to the video calling service provided by the telecom service provider, which uses the telecom 10 
operator’s IMS core to establish dedicated bearer to provide superior video calling experience. The KPIs which will be 11 
monitored to assess the voice service are included below 12 
• CSSR – Call Setup Success Rate % – Total number of calls which were successful by the total number of 13 
calls made as a percentage. 14 
• CST – Call Setup Time – Time taken from the initial SIP INVITE to when the SIP 180 Ringing is received 15 
in seconds. 16 
• MOS Score – Mean Opinion Score for the video call. This needs to be measured on both ends of the video 17 
call – mobile originated and mobile terminated. 18 
• Mute Rate % – This is the percentage of video calls which were muted or video freezes in both directions 19 
(calls with RTP loss of > 3-4s in both directions are considered muted call). This needs to be measured on 20 
both ends of the video call – mobile originated and mobile terminated and counted only once per call. 21 
• One Way Calls % – This is the percentage of video calls which were muted, or video is not transmitted in 22 
any one direction only (video calls with RTP loss of > 3-4s in one direction only are considered one-way 23 
calls). This needs to be monitored on both ends of the Voice call – mobile originated and mobile terminated 24 
and counted only once per call. 25 
• RTP Packet Loss % - Number of RTP packets which were dropped/lost in uplink/downlink direction as a 26 
percentage of total packets. This needs to be measured on both ends of the Voice call – mobile originated 27 
and mobile terminated. 28 
 29 

  
Page 109 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
Along with the monitoring and validation of these services using user experience KPIs, the O-RAN systems also need 1 
to be monitored. The end user service experience can also be impacted by some of the features available on the O-RAN 2 
system. Some of these features have been included below. As a part of the video calling services testing, details of these 3 
features need to be included in the test report to get a comprehensive view of the setup used for testing. If additional 4 
features or functionalities have been enabled during this testing that impact the end user voice experience, those need be 5 
included in the test report as well.  6 
• RLC in Unacknowledged Mode 7 
• Robust Header Compression 8 
• DRX 9 
• Dynamic GBR Admission Control 10 
• TTI Bundling 11 
• VoLTE Inactivity Timer 12 
• Frequency Hopping 13 
• Multi-Target RRC Connection Re-Establishment 14 
• VoLTE HARQ 15 
• Coordinated Multi Point (DL and UL) 16 
• VoLTE Quality Enhancement 17 
• Packet Loss Detection 18 
• Voice Codec Aware scheduler 19 
• NR to LTE PS Redirection/Cell Reselection/Handover 20 
• LTE to NR PS Redirection/Cell Reselection/Handover 21 
 22 
6.6.1 ViLTE Stationary Test 23 
This test scenario validates the user’s video calling experience when the end user device (UE) is in LTE coverage and 24 
performs a video call. 25 
6.6.1.1 Test Description 26 
Telecom service providers are providing video calling service to their customers and it has been gaining popularity 27 
replacing voice calls. Video over LTE (ViLTE) uses the same mechanism as VoLTE, i.e. IP packets to send and receive 28 
video & audio packets, with the IP packets being transferred over LTE. Just like VoLTE, IMS is used to setup control 29 
and data plane needed for ViLTE communication. As two-way video service is latency sensitive, the EPC core interacts 30 
with the IMS core to setup different bearers for Video traffic – QCI-5 for ViLTE control plane and QCI-2 for ViLTE 31 
data plane. This test case is applicable when UE is connected over an NSA network. This section tests ViLTE user 32 
experience on an O-RAN system. 33 
6.6.1.2 Test Setup 34 
The SUT in this test case would be a O-eNB which would the Master eNB and a Secondary gNB. As most of the 35 
current NSA deployment use the 4G eNB to provide video calling services, the use of a secondary gNB is optional. We 36 
will however include the Secondary gNB in this test scenario, but it is only applicable if the gNB plays a role in 37 
establishing the control plane or data plane for a video call. The O-eNB, gNB and the components within these have to 38 

  
Page 110 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
comply with the O-RAN specifications. The O-RAN setup should support the ability to perform this testing in different 1 
radio conditions as defined in Section 3.6. A 4G core will be required to support the basic functionality to authenticate 2 
and register an end user device in order to setup a PDN connection. An IMS core will be required to register the end 3 
user device to support video calling services on a 4G network. The 4G and IMS cores could be a completely emulated, 4 
partially emulated or real non-emulated cores. We will need at least two end user devices (UE) which can be a real UEs 5 
or an emulated one, and both have to support video calling service using ViLTE. The end user devices will serve as 6 
Mobile Originated (MO) and Mobile Terminated (MT) end user devices forming the two ends of the video call Going 7 
forward in this section, these end user devices will be referred to as MO end user device and MT end user devices to 8 
represent the role they plan in the video call. The test setup should include tools which have the ability to collect traces 9 
on the elements and/or packet captures of communication between the elements. This could be a built-in capability of 10 
the emulated/non-emulated network elements or an external tool. Optionally, if some of the network elements are 11 
located remotely either in a cloud or on the internet, the additional latency should be calculated and accounted for.  12 
The Master eNB, Secondary gNB and their components (O-eNB, O-RU, O-DU and O-CU) need to have the right 13 
configuration and software load. The end user device must be configured with the right user credentials to be able to 14 
register and authenticate with the O-RAN system and the 4G core. The end user devices also need to be provisioned 15 
with the user credentials to attach to the 4G core and register with the IMS core to perform video call using ViLTE. The 16 
4G core network and IMS core must be configured to support end user devices used for testing. This includes 17 
supporting registration, authentication and PDN connection establishment for these end user devices. The also includes 18 
provisioning the IMS core to support registration of the end user devices to make video calls over ViLTE. The locations 19 
where the radio conditions are excellent, good, fair and poor need to be identified within the serving cell. 20 
All the elements in the network like O-RAN system, 4G core and the IMS Core need to have the ability to capture 21 
traces to validate the successful execution of the test cases. The end user devices need to have the capability to capture 22 
traces/packet capture to calculate the VoLTE KPIs. Optionally, the network could have network taps deployed in 23 
various legs of the network to get packet captures to validate successful execution of the test cases. Finally, all these 24 
different components need to have connectivity with each other – the end user device should be able to connect to O-25 
RAN system(O-eNBs and gNBs), O-RAN system needs to be connected to the 4G core which in turn should have 26 
connectivity to the IMS Core. 27 
6.6.1.3 Test Methodology/Procedure 28 
Ensure the end user devices, O-RAN system, 4G core and the IMS Core have all been configured as outlined in Section 29 
6.6.1.2. In this test scenario, both the mobile originated and mobile terminated end user devices are going to use the 30 
same O-RAN (i.e. same O-eNB), 4G and IMS core to perform the end-to-end video call. All traces and packet captures 31 
need to be enabled for the duration of the testing to ensure all communication between network elements can be 32 
captured and validated. 33 
1. Power on the two end user devices in excellent radio condition and ensure both of the end user devices connect to 34 
the 4G core over the Master O-eNB and optionally secondary gNB.  35 
2. Ensure both the MO and MT end user devices can establish a PDN connection with the 4G core. Once the PDN 36 
connection has been setup, both the end user devices have to register with the IMS core to support video calling 37 
services. 38 
3. Use the MO end user device to video call the MT end user device. Validate the MT end user device is able to 39 
receive and answer the call. 40 
4. Continue to have two-way voice and video communication on the video call for at least 5 minutes before 41 
terminating it.  42 
5. Repeat the test multiple times (>10 times) and gather results. 43 
6. Repeat the above steps 1 through 5 for the MO and MT end user devices in good, fair and poor radio conditions. 44 
 45 

  
Page 111 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
6.6.1.4 Test Expectation (expected results) 1 
As a pre-validation, use the traces to validate a successful PDN connection setup by the end user devices without any 2 
errors using the Master eNB and optionally the secondary gNB. Also validate both the end user devices are able to 3 
register with the IMS core for voice services. This is a prerequisite before these tests can be validated. 4 
Validate the end user devices are able to make a video call between each other by dynamically setting up a QCI-2 5 
bearer to transfer voice and video packets over RTP. Ensure the Call Setup Time is reasonable, the video and voice 6 
quality from both parties are clear and audible without one-way or intermittent muting or video freezing. Use the packet 7 
captures to validate there is no RTP packet drops or high RTP packet jitter which could cause voice muting, video 8 
freezing or video lag issues. Use the packet captures to ensure there are no out-of-sequence packets which could impact 9 
customer’s video calling experience.  10 
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 11 
is performed in a controlled environment in good radio condition without the interference of external factors which 12 
could impact the KPIs, example: use of internet to connect to remote network nodes could add latency, jitter and packet 13 
loss issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the testing in 14 
this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test results outside 15 
the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues may be 16 
due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and the test 17 
has to be repeated.  18 
• CSSR – Call Setup Success Rate % –> 99%. 19 
• CST – Call Setup Time – < 2.5s 20 
• MOS Score – > 3.5 21 
• Mute Rate % – < 1% 22 
• One Way Call % - < 1% 23 
• RTP Packet Loss % - < 1% 24 
 25 
These end user video calling KPI values included in Section 6.6 need to be included in the test report along with the 26 
minimum configuration parameters included in Section 3.3. The following information should also be included in the 27 
test report for the testing performed in different radio conditions to provide a comprehensive view of the test setup. 28 
End user device side (real or emulated UE): 29 
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second) 30 
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second) 31 
• Received downlink throughput (L1 and L3 PDCP layers) (average sample per second) 32 
• Downlink transmission mode 33 
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 34 
(average sample per second) 35 
The table below gives an example of the test report considering the mean and standard deviation of all test results that 36 
have been captured.  37 
Table 6-16 Example Test Report for Video over LTE Testing – Stationary Test 38 
 
Excellent 
(cell centre) Good Fair Poor 
(cell edge) 
ViLTE MO/MT ViLTE MO/MT ViLTE MO/MT ViLTE MO/MT 

  
Page 112 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
Call Setup Success Rate     
Call Setup time     
MOS Score     
Mute Rate     
One Way Call     
RTP Packet Loss     
L1 DL Spectral efficiency [bps/Hz]     
UE RSRP [dBm]     
UE PDSCH SINR [dB]     
MIMO rank     
PDSCH MCS     
DL RB number     
UE CQI     
UE RSRQ     
UE PMI     
UE RSSI     
UE Buffer status     
UE Packet delay      
PDSCH BLER [%]     
The end user ViLTE experience can also be impacted by some of the features available (see Section 6.3) on the O-eNB, 1 
O-RU, O-DU and O-CU. The details of these features, along with any other features/functionality which could impact 2 
the end user’s video calling service experience should be included in the test report to provide a comprehensive view of 3 
the testing. 4 
6.6.2 ViLTE Handover Test 5 
This test section is for FFS. 6 
6.6.3 ViLTE - LTE to NR handover test 7 
This test scenario validates the user’s video calling experience when the UE is in video call on LTE and performs a 8 
handover from LTE network to a 5G network and vice versa. This test scenario is applicable for a 5G SA deployment. 9 
6.6.3.1 Test Description 10 
Voice service is one of the basic services provided on the telecommunication network. Video service on the 4G network 11 
was provided using ViLTE. Similarly, video service on the 5G network is provided using packet switch technology 12 
called Video over New Radio. As 5G network is being deployed by a telecommunication service provider, the service 13 
provider will need to support 4G and 5G network, and thus support Video over LTE, Video over NR and the handover 14 
between the two video calling services. This scenario tests the end user’s video calling experience when the end user 15 
device performs a handover from video over LTE to Video over NR and vice versa. 16 

  
Page 113 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
6.6.3.2 Test Setup 1 
The SUT in this test case would be an O-eNB along with a gNB (O-RU, O-DU and O-CU). We would also need an 2 
interworking 4G-5G core (referred to as 4G-5G core going forward) which supports a combined anchor point for 4G 3 
and 5G, i.e. SMF+PGW-C and UPF+PGW-U. The eNB connects to a 4G-5G core over the 4G interfaces like S1 to 4 
provide 4G LTE service and the O-CU, O-DU and O-RU would connect to a 4G-5G core over the 5G interfaces like N2 5 
and N3 to provide 5G SA service. The O-eNB, gNB and the components within these have to comply with the O-RAN 6 
specifications. The 4G-5G core will be required to support the basic functionality to authenticate and register an end 7 
user device in order to setup a PDN connection/PDU session. An IMS core will be required to register the end user 8 
device to support video calling services on a 4G and 5G network. The 4G and 5G core have to interwork using the N26 9 
interface between the MME and AMF to support seamless handover. We are recommending the use of N26 interface to 10 
ensure better customer experience when the end user device performs a handover between 4G and 5G. The 4G-5G and 11 
IMS cores could be a completely emulated, partially emulated or real non-emulated cores. We will need at least two end 12 
user devices (UE) which can be a real UEs or an emulated one, and both have to support video calling service using 13 
ViLTE, Video over NR and the capability to handover from ViLTE to Video over NR and vice versa. The end user 14 
devices will serve as Mobile Originated (MO) and Mobile Terminated (MT) end user devices forming the two ends of 15 
the video call. Going forward in this section, these end user devices will be referred to as MO end user device and MT 16 
end user devices to represent their role in the video call. The test setup should include tools which have the ability to 17 
collect traces on the elements and/or packet captures of communication between the elements. This could be a built-in 18 
capability of the emulated/non-emulated network elements or an external tool. Optionally, if some of the network 19 
elements are located remotely either in a cloud or on the internet, the additional latency should be calculated and 20 
accounted for. 21 
The O-eNB, gNB and their components (O-RU, O-DU and O-CU) need to have the right configuration and software 22 
load. The end user devices must be configured with the right user credentials to be able to register and authenticate with 23 
the O-RAN system and the 4G-5G core. The end user devices also need to be provisioned with the user credentials to 24 
support attach procedure to the 4G-5G core, registering to the IMS core and performing a video call over LTE and NR. 25 
This includes supporting registration, authentication and PDN connection/PDU Session establishment for these end user 26 
devices. This also includes provisioning the IMS core to support registration of the end user devices to make video calls 27 
over ViLTE and Video over NR, which includes dynamically setting up dedicated bearers/QoS Flows for video calls.  28 
All the elements in the network like O-RAN system, 4G-5G core and the IMS Core need to have the ability to capture 29 
traces to validate the successful execution of the test cases. The end user devices need to have the capability to capture 30 
traces/packet capture to calculate the ViLTE and Video over NR KPIs. Optionally, the network could have network taps 31 
deployed in various legs of the network to get packet captures to validate successful execution of the test cases. Finally, 32 
all these different components need to have connectivity with each other – the end user device should be able to connect 33 
to O-RAN system(O-eNBs and gNBs), O-RAN system needs to be connected to the 4G-5G core which in turn should 34 
have connectivity to the IMS Core. 35 
6.6.3.3 Test Methodology/Procedure 36 
Ensure the end user devices, O-RAN system, 4G-5G core and the IMS Core have all been configured as outlined in 37 
Section 6.6.3.2. In this test scenario, both the mobile originated and mobile terminated end user devices are going to use 38 
the O-eNB and gNB to connect to the same 4G-5G and IMS core to perform the end-to-end video call. All traces and 39 
packet captures need to be enabled for the duration of the testing to ensure all communication between network 40 
elements can be captured and validated. 41 
The below section gives the steps to perform a video over LTE to video over NR handover, followed by video over NR 42 
to video over LTE handover 43 
1. Power on the two end user devices and ensure both of the end user devices can connect to the 4G-5G core over the 44 
O-eNB.  45 

  
Page 114 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
2. Ensure both the MO and MT end user devices can establish a PDN connection with the 4G-5G core. Once the PDN 1 
connection has been established, both the end user devices have to register with the IMS core to support video 2 
calling services. 3 
3. Use the MO end user device to video call the MT end user device. Validate the MT end user device is able to 4 
receive and answer the video call. 5 
4. Once the two-way call has been setup and communication is going back and forth between the two end user 6 
devices, move the MO device to the O-RU coverage of the gNB, thus forcing a handover from 4G to 5G, hence 7 
forcing a ViLTE to Video over NR handover. Continue two-way voice and video communication through the 8 
handover process and terminate the call once the handover process is complete. Measure the video KPIs included in 9 
Section 6.6.  10 
5. At this point in time, the MO end user device is in 5G coverage and registered to the 4G-5G core. The MT end user 11 
device is in 4G coverage and registered to the 4G-5G core. Both end user devices should still be registered to the 12 
IMS core. 13 
6. Use the MO end user device to video call the MT end user device. Validate the MT end user device is able to 14 
receive and answer the call. 15 
7. Once the two-way call has been setup and communication is going back and forth between the two end user 16 
devices, move the MO device to the O-eNB coverage, thus forcing a handover from 5G to 4G, hence forcing a 17 
Video over NR to ViLTE handover. Continue the two-way voice and video communication through the handover 18 
process and terminate the call once the handover process is complete. Measure the video KPIs included in Section 19 
6.6.  20 
8. Repeat the test multiple times (> 10 times) and gather results. 21 
 22 
6.6.3.4 Test Expectation (expected results) 23 
As a pre-validation, use the traces to validate a successful registration and PDN connection setup by the end user 24 
devices without any errors using the O-eNB when in 4G coverage. Similarly, use traces to validate successful 25 
registration and PDU session setup by the end user device without any errors using O-RU, O-DU and O-CU when in 5G 26 
coverage. Also validate both the end user devices are able to register with the IMS core for voice services. This is a 27 
prerequisite before these tests can be validated. 28 
Validate the end user devices are able to make a video call between each other by dynamically setting up a QCI-2/5QI-2 29 
bearer to transfer voice and video packets over RTP. Ensure the Call Setup Time is reasonable, the video and voice 30 
quality from both parties are clear and audible without one-way or intermittent muting or video freezing. Ensure the 31 
voice and video quality is not impacted during the handover process – ViLTE to Video over NR and Video over NR to 32 
ViLTE. Use the packet captures to validate there is no RTP packet drops or high RTP packet jitter which could cause 33 
voice muting, video freezing or video lag issues. Use the packet captures to ensure there are no out-of-sequence packets 34 
which could impact customer’s video calling experience.  35 
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 36 
is performed in a controlled environment in good radio condition without the interference of external factors which 37 
could impact the KPIs, example: use of internet to connect to remote network nodes could add latency, jitter and packet 38 
loss issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the testing in 39 
this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test results outside 40 
the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues may be 41 
due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and the test 42 
has to be repeated.  43 
• CSSR – Call Setup Success Rate % –> 99%. 44 
• CST – Call Setup Time – < 2.5s 45 
• MOS Score – > 3.5 46 
• Mute Rate % – < 1% 47 

  
Page 115 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
• One Way Call % - < 1% 1 
• RTP Packet Loss % - < 1% 2 
 3 
These end user video calling KPI values included in Section 6.6 need to be included in the test report along with the 4 
minimum configuration parameters included in Section 3.3. The following information should also be included in the 5 
test report for the testing performed in different radio conditions to provide a comprehensive view of the test setup. 6 
End user device side (real or emulated UE): 7 
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second) 8 
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second) 9 
• Downlink transmission mode 10 
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 11 
(average sample per second) 12 
The table below gives an example of the test report considering the mean and standard deviation of all test results that 13 
have been captured.  14 
Table 6-17 Example Test Report for Video over LTE Testing – Stationary Test 15 
 
Excellent 
(cell centre) Good Fair Poor 
(cell edge) 
ViLTE MO/MT ViLTE MO/MT ViLTE MO/MT ViLTE MO/MT 
Call Setup Success Rate     
Call Setup time     
MOS Score     
Mute Rate     
One Way Call     
RTP Packet Loss     
L1 DL Spectral efficiency [bps/Hz]     
UE RSRP [dBm]     
UE PDSCH SINR [dB]     
MIMO rank     
PDSCH MCS     
DL RB number     
UE CQI     
UE RSRQ     
UE PMI     
UE RSSI     
UE Buffer status     
UE Packet delay      
PDSCH BLER [%]     
The end user ViLTE experience can also be impacted by some of the features available (see Section 6.3) on the O-eNB, 16 
O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other features/functionality which 17 
could impact the end user’s video calling service experience should be included in the test report to provide a 18 
comprehensive view of the testing. 19 

  
Page 116 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
6.7 Video Service – EPS Fallback 1 
Video service is quickly replacing voice service and turning into one of the basic services offered by telecommunication 2 
service providers. Upgrade of telecom network occurs in phases, and this is no different for 5G. The telecom network 3 
may not be able to support video calling service on 5G during this phase for multiple reasons – the 5G network may not 4 
be deployed nationwide, or 5G network may not be tuned to support voice/video service or the devices may not be able 5 
to support video/voice service on 5G. EPS fallback is the method used to support voice/video services during this 6 
transition phase, where voice/video services are continued to be supported on the legacy LTE network using video over 7 
LTE (ViLTE). This section of the document only applies to the video calling service provided by the telecom service 8 
provider, which uses the telecom operator’s IMS core to establish dedicated bearer to provide superior video calling 9 
experience. The KPIs which will be monitored to assess the video calling service are included below 10 
• CSSR – Call Setup Success Rate % – Total number of calls which were successful by the total number of 11 
calls made as a percentage. 12 
• CST – Call Setup Time – Time taken from the initial SIP INVITE to when the SIP 180 Ringing is received 13 
in seconds. 14 
• MOS Score – Mean Opinion Score for the video call. This needs to be measured on both ends of the video 15 
call – mobile originated and mobile terminated. 16 
• Mute Rate % – This is the percentage of video calls which were muted or video freezes in both directions 17 
(calls with RTP loss of > 3-4s in both directions are considered muted call). This needs to be measured on 18 
both ends of the video call – mobile originated and mobile terminated and counted only once per call. 19 
• One Way Calls % – This is the percentage of video calls which were muted, or video is not transmitted in 20 
any one direction only (video calls with RTP loss of > 3-4s in one direction only are considered one-way 21 
calls). This needs to be monitored on both ends of the Voice call – mobile originated and mobile terminated 22 
and counted only once per call. 23 
• RTP Packet Loss % - Number of RTP packets which were dropped/lost in uplink/downlink direction as a 24 
percentage of total packets. This needs to be measured on both ends of the Voice call – mobile originated 25 
and mobile terminated. 26 
 27 
Along with the monitoring and validation of these services using user experience KPIs, the O-RAN systems also need 28 
to be monitored. The end user service experience can also be impacted by some of the features available on the O-RAN 29 
system. Some of these features have been included below. As a part of the voice services testing, details of these 30 
features need to be included in the test report to get a comprehensive view of the setup used for testing. If additional 31 
features or functionalities have been enabled during this testing that impact the end user voice experience, those need be 32 
included in the test report as well.  33 
• EPS Fallback for IMS Voice 34 
• NR to EPS Mobility 35 
• RLC in Unacknowledged Mode 36 
• Robust Header Compression 37 
• DRX 38 
• Dynamic GBR Admission Control 39 
• TTI Bundling 40 
• VoLTE Inactivity Timer 41 
• Frequency Hopping 42 
• Multi-Target RRC Connection Re-Establishment 43 
• VoLTE HARQ 44 

  
Page 117 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
• Coordinated Multi Point (DL and UL) 1 
• VoLTE Quality Enhancement 2 
• Packet Loss Detection 3 
• Voice Codec Aware scheduler 4 
 5 
6.7.1 Video Service – EPS Fallback testing 6 
This scenario tests the video calling service when an end user device (UE) is in 5G SA coverage and performs EPS 7 
fallback to 4G to make or receive a video call. 8 
6.7.1.1 Test Description 9 
This section tests the video service on a 5G SA network when it uses EPS fallback mechanism to fallback to the LTE 10 
network to use video over LTE to support video calling service. This testing only applies to 5G SA deployment and in 11 
this scenario the UE is connected to the 5G SA Core and registered with the IMS core for video calling service. As 12 
video calling service is not supported on 5G yet, the UE is forced to fallback to 4G when it makes/receives a video call. 13 
The EPS fallback does increase the Call Setup Time due to the time needed to fallback before making/receiving the 14 
video call.  15 
There are two mechanism in which EPS fallback is supported on the 5G core, and the methodology used impacts the 16 
time taken to perform the fallback to LTE, and hence impacts the Call Setup Time. The two mechanisms that could be 17 
used to perform EPS fallback is included below. These two mechanisms do change the test setup but does not change 18 
the testing procedure. 19 
• With N26 interface – The AMF in the 5G core communicates with the MME in the 4G over the N26 interface. 20 
In this scenario the UE performs a handover from the 5G network to the 4G network. 21 
• Without N26 interface – The AMF in the 5G core does not communicate directly with the 4G core, instead 22 
uses the UDM/HSS to store and transfer relevant session information to the 4G core. In this scenario the UE 23 
performs a Release with Redirect from the 5G network to the 4G network. 24 
 25 
6.7.1.2 Test Setup 26 
The SUT in this test case would be an O-eNB along with a gNB (O-RU, O-DU and O-CU-CP/O-CU-UP). We would 27 
also need an interworking 4G-5G core (referred to as 4G-5G core going forward) which supports a combined anchor 28 
point, i.e. SMF+PGW-C and UPF+PGW-U. The O-eNB connects to a 4G-5G core over the 4G interfaces like S1 to 29 
provide 4G LTE service and the gNB (O-RU, O-DU and O-CU-CP/O-CU-UP) would connect to the 4G-5G core over 30 
the 5G interfaces like N2 and N3 to provide 5G SA service. The O-eNB, gNB and the components within these have to 31 
comply with the O-RAN specifications and support EPS fallback. The 4G-5G core will be required to support the basic 32 
functionality to authenticate and register an end user device in order to setup a PDN connection/PDU session. The IMS 33 
core will be required to register the end user device and needs to be integrated with the 4G-5G core to support video 34 
calling services over LTE (ViLTE) and EPS fallback. The 4G-5G core has to interwork either using the N26 interface 35 
or without using N26 interface depending on the desired core network configuration. The existence of the N26 interface 36 
does reduce the EPS fallback time, hence reduce Call Setup Time and provide better end user experience. The 4G-5G 37 
and IMS cores could be a completely emulated, partially emulated or real non-emulated cores. We will need at least two 38 
end user devices (UE) which can be a real UEs or an emulated one, and both have to support video calling service using 39 
ViLTE and EPS fallback procedure. The end user devices will serve as Mobile Originated (MO) and Mobile 40 
Terminated (MT) end user devices forming the two ends of the video call. For the sake of clarity, we will address these 41 
end user devices as UE-1 and UE-2 in this section. The test setup should include tools which have the ability to collect 42 

  
Page 118 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
traces on the elements and/or packet captures of communication between the elements. This could be a built-in 1 
capability of the emulated/non-emulated network elements or an external tool. Optionally, if some of the network 2 
elements are located remotely either in a cloud or on the internet, the additional latency should be calculated and 3 
accounted for.  4 
The O-eNB, gNB and their components (O-RU, O-DU and O-CU-CP/O-CU-UP) need to have the right configuration 5 
and software load. The end user device must be configured with the right user credentials to be able to register and 6 
authenticate with the O-RAN system and the 4G-5G core. The end user devices also need to be provisioned with the 7 
user credentials to attach to the 4G-5G core and register with the IMS core to perform video call using ViLTE and EPS 8 
fallback. The 4G-5G core network and IMS core must be configured to support video calling service on the end user 9 
device used for testing, which includes dynamically setting up dedicated bearers for voice calls.  10 
All the elements in the network like O-RAN system, 4G-5G core and the IMS Core need to have the ability to capture 11 
traces to validate the successful execution of the test cases. The end user devices need to have the capability to capture 12 
traces/packet capture to calculate the ViLTE KPIs. Optionally, the network could have network taps deployed in 13 
various legs of the network to get packet captures to validate successful execution of the test cases. Finally, all these 14 
different components need to have connectivity with each other – the end user device should be able to connect to O-15 
RAN system(O-eNBs and gNBs), O-RAN system needs to be connected to the 4G-5G core which in turn should have 16 
connectivity to the IMS Core. 17 
6.7.1.3 Test Methodology/Procedure 18 
Ensure the end user devices, O-RAN system, 4G-5G core and the IMS Core have all been configured as outlined in 19 
Section 6.7.1.2. In this test scenario, one of the end user devices (UE-1) is going to be connected over the 4G O-eNB to 20 
the 4G-5G core, while the other end user device (UE-2) is going to be connected over 5G gNB (O-RU, O-DU and O-21 
CU-CP/O-CU-UP) to the 4G-5G core. All traces and packet captures need to be enabled for the duration of the testing 22 
to ensure all communication between network elements can be captured and validated. 23 
1. Power on the two end user devices and ensure UE-1 is in LTE coverage, when UE-2 is in the 5G coverage. 24 
Validate both end user devices register to the 4G-5G core. 25 
2. Once the registration is complete, UE-1 and UE-2 have to establish a PDN connection and PDU session 26 
respectively with the 4G-5G core. Once the PDN connection and PDU session have been setup, both the end user 27 
devices have to register with the IMS core to support video calling services. 28 
3. Use UE-1 as the MO end user device to video call UE-2. Validate the UE-2 performs EPS fallback to LTE to 29 
receive the video call.  30 
4. Answer the call on UE-2 and continue to have two-way audio/video communication for at least 5 minutes and 31 
terminate the call. Measure the voice KPIs included in Section 6.7.  32 
5. At this point UE-2 should have completed the video call and moved back to connect to the 4G-5G Core over 5G 33 
gNB (i.e. O-RU). 34 
6. Use UE-2 as the MO end user device to call UE-1. Validate the UE-2 performs EPS fallback to LTE to before 35 
making the video call.  36 
7. Answer the call on UE-1 and continue to have two-way communication for at least 5 minutes and terminate the 37 
call. Measure the voice KPIs included in Section 6.7.  38 
8. Repeat the test multiple times (>10 times) and gather results. 39 
 40 
6.7.1.4 Test Expectation (expected results) 41 
As a pre-validation, use the traces to validate end user device UE-1 is able to perform a successful registration and PDN 42 
connection setup while using the O-eNB in LTE coverage. Similarly, use the traces to validate end user device UE-2 is 43 
able to perform a successful registration and PDU session setup while using gNB in 5G coverage. Also validate both the 44 

  
Page 119 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
end user devices are able to register with the IMS core for voice services. This is a prerequisite before these tests can be 1 
validated. 2 
Validate the end user devices are able to make a video call between each other by dynamically setting up a QCI-2 3 
bearer to transfer voice and video packets over RTP. Validate the device which is in the 5G coverage falls back to 4G 4 
before receiving or making a video call, in other words the EPS fallback procedure has been executed successfully. The 5 
Call Setup Time will be higher than Video over LTE due to the delay associated with executing the EPS fallback 6 
procedure. Even though this test case does not directly impact the quality of the video call, for the sake of consistency, 7 
ensure the voice and video quality from both parties are clear and audible without one-way or intermittent muting and 8 
video freezing. Use the packet captures to validate there is no RTP packet drops or high RTP packet jitter which could 9 
cause voice muting, video freezing and video lag issues. Use the packet captures to ensure there are no out-of-sequence 10 
packets which could impact customer’s video calling experience.  11 
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 12 
is performed in a controlled environment in good radio condition without the interference of external factors which 13 
could impact the KPIs, example: use of internet to connect to remote network nodes could add latency, jitter and packet 14 
loss issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the testing in 15 
this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test results outside 16 
the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues may be 17 
due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and the test 18 
has to be repeated.  19 
Table 6-18 Typical Video KPI values in controlled environments with good radio conditions 20 
KPI With N26 Interface Without N26 Interface 
CSSR – Call Setup Success Rate % >99% >99% 
CST – Call Setup Time 3.5s 4 s 
MOS Score 3.5 3.5 
Mute Rate % < 1% < 1% 
One Way Call % < 1% < 1% 
RTP Packet Loss % < 1% < 1% 
 21 
These end user video calling KPI values included in Section 6.7 need to be included in the test report along with the 22 
minimum configuration parameters included in Section 3.3. The following information should also be included in the 23 
test report for the testing performed to provide a comprehensive view of the test setup. 24 
End user device side (real or emulated UE): 25 
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second) 26 
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second) 27 
• Received downlink throughput (L1 and L3 PDCP layers) (average sample per second) 28 
• Downlink transmission mode 29 
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 30 
(average sample per second) 31 

  
Page 120 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
The table below gives an example of the test report considering the mean and standard deviation of all test results that 1 
have been captured.  2 
Table 6-19 Example Test Report for Video Calling Service Testing – EPS Fallback 3 
 EPS Fallback MO EPS Fallback MT 
Call Setup Success Rate   
Call Setup time   
MOS Score   
Mute Rate   
One Way Call   
RTP Packet Loss   
L1 DL Spectral efficiency [bps/Hz]   
UE RSRP [dBm]   
UE PDSCH SINR [dB]   
MIMO rank   
PDSCH MCS   
DL RB number   
UE CQI   
UE RSRQ   
UE PMI   
UE RSSI   
UE Buffer status   
UE Packet delay    
PDSCH BLER [%]   
The end user ViLTE experience can also be impacted by some of the features available (see Section 6.7) on the O-eNB, 4 
O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other features/functionality which 5 
could impact the end user’s video calling experience should be included in the test report to provide a comprehensive 6 
view of the testing. 7 
6.8 Video Service – Video over NR 8 
Video calling service is slowly replacing voice as a basic calling service. The improvement in end user throughput on 9 
LTE has facilitated the move from voice call to video, and the high speeds on 5G is only going to accelerate this 10 
migration process. This section of the document validates the user’s video calling experience in different conditions on 11 
the 5G network. This section of the document only applies to the video calling service provided by the telecom service 12 
provider, which uses the telecom operator’s IMS core to establish dedicated QoS flows to provide superior video calling 13 
experience. The KPIs which will be monitored to assess the video calling service are included below 14 
• CSSR – Call Setup Success Rate % – Total number of calls which were successful by the total number of 15 
calls made as a percentage. 16 
• CST – Call Setup Time – Time taken from the initial SIP INVITE to when the SIP 180 Ringing is received 17 
in seconds. 18 
• MOS Score – Mean Opinion Score for the video call. This needs to be measured on both ends of the video 19 
call – mobile originated and mobile terminated. 20 

  
Page 121 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
• Mute Rate % – This is the percentage of video calls which were muted or video freezes in both directions 1 
(calls with RTP loss of > 3-4s in both directions are considered muted call). This needs to be measured on 2 
both ends of the video call – mobile originated and mobile terminated and counted only once per call. 3 
• One Way Calls % – This is the percentage of video calls which were muted, or video is not transmitted in 4 
any one direction only (video calls with RTP loss of > 3-4s in one direction only are considered one-way 5 
calls). This needs to be monitored on both ends of the Voice call – mobile originated and mobile terminated 6 
and counted only once per call. 7 
• RTP Packet Loss % - Number of RTP packets which were dropped/lost in uplink/downlink direction as a 8 
percentage of total packets. This needs to be measured on both ends of the Voice call – mobile originated 9 
and mobile terminated. 10 
 11 
End user video calling service experience can also be impacted by some of the features available on the O-RU, O-DU 12 
and O-CU-CP/O-CU-UP. Some of these features have been included below. As a part of the video calling services 13 
testing, details of these features need to be included in the test report to get a comprehensive view of the setup used for 14 
testing. If additional features or functionalities have been enabled during this testing that impact the end user voice 15 
experience, those need be included in the test report as well. 16 
• Basic Voice over NR 17 
• RLC in Unacknowledged Mode 18 
• Robust Header Compression 19 
• DRX 20 
• Dynamic GBR Admission Control 21 
• TTI Bundling 22 
• Automated Neighbor Relations ANR 23 
• Connected mode mobility (Handover) 24 
• idle mode reselection 25 
• Intra-frequency Cell Reselection/Handover 26 
• Inter-frequency Redirection/Cell Reselection/Handover 27 
• NR Coverage-Triggered NR Session Continuity 28 
 29 
6.8.1 Video over NR – Stationary Testing 30 
This test scenario validates the user’s video calling experience when the end user device (UE) makes a video call on the 31 
5G network. 32 
6.8.1.1 Test Description 33 
This section of the document tests video over NR user experience on an O-RAN system. Video over NR is similar to 34 
ViLTE where it uses IP packets to send and receive audio/video packets, with the IP packets being transferred over NR 35 
or 5G radio. Just like ViLTE, IMS is used to setup the control and data plane for the video communication. As video 36 
calling service is latency sensitive, the 5G core interacts with the IMS core to setup different QoS flows for video traffic 37 
– 5QI-5 for control plane and 5QI-2 for data plane. This test case is applicable when UE is connected over a 5G SA 38 
network.  39 

  
Page 122 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
6.8.1.2 Test Setup 1 
The SUT in this test case would be a gNB (O-RU, O-DU and O-CU-CP/O-CU-UP) which is used to test the video 2 
calling service. A 5G SA Core would be required to support basic functionality to authenticate and register the end user 3 
device to establish a PDU session. An IMS core will be required to register the end user device to support video calling 4 
services on a 5G network. The 5G and IMS cores could be a completely emulated, partially emulated or real non-5 
emulated cores. We will need at least two end user devices (UE) which can be a real UEs or an emulated one, and both 6 
have to support video calling service using Video over NR. The end user devices will serve as Mobile Originated (MO) 7 
and Mobile Terminated (MT) end user devices forming the two ends of the video call. Going forward in this section, 8 
these end user devices will be referred to as MO end user device and MT end user device to represent their role in the 9 
video call. The test setup should include tools which have the ability to collect traces on the elements and/or packet 10 
captures of communication between the elements. This could be a built-in capability of the emulated/non-emulated 11 
network elements or an external tool. Optionally, if some of the network elements are located remotely either in a cloud 12 
or on the internet, the additional latency should be calculated and accounted for.  13 
The O-RU, O-DU and O-CU-CP/O-CU-UP need to have the right configuration and software load. The SUT will also 14 
need to be setup to run this testing in different radio conditions as outlined in Section 3.6. The end user device must be 15 
configured with the right user credentials to be able to register and authenticate with the O-RAN system and the 5G 16 
core. The end user devices also need to be provisioned with the user credentials to register and setup PDU session with 17 
the 5G core and register with the IMS core to perform video call using Video over NR. The 5G core network and IMS 18 
core must be configured to support video calling service on the end user devices used for testing, which includes the 19 
ability to dynamically set up QoS Flows for video calls. The locations where the radio conditions are excellent, good, 20 
fair and poor need to be identified within the serving cell. 21 
All the elements in the network like O-RAN system, 5G core and the IMS Core need to have the ability to capture 22 
traces to validate the successful execution of the test cases. The end user devices need to have the capability to capture 23 
traces/packet capture to calculate the Video over NR KPIs. Optionally, the network could have network taps deployed 24 
in various legs of the network to get packet captures to validate successful execution of the test cases. Finally, all these 25 
different components need to have connectivity with each other – the end user device should be able to connect to O-26 
RAN system(O-RU, O-DU and O-CU-CP/O-CU-UP), O-RAN system needs to be connected to the 5G core which in 27 
turn should have connectivity to the IMS Core. 28 
6.8.1.3 Test Methodology/Procedure 29 
Ensure the end user devices, O-RAN system, 5G core and the IMS core have all been configured as outlined in Section 30 
6.8.1.2. In this test scenario, both the mobile originated and mobile terminated end user devices are going to use the 31 
same O-RAN system, 5G and IMS core to perform the end-to-end video call. All traces and packet captures need to be 32 
enabled for the duration of the testing to ensure all communication between network elements can be captured and 33 
validated. 34 
1. Power on the two end user devices in excellent radio condition and ensure both of devices registers with the 5G 35 
core for video calling services over SA by connecting over the O-RAN (O-RU, O-DU and O-CU-CP/O-CU-UP).  36 
2. Once the registration is complete, the MO and MT end user devices have to establish a PDU session with the 5G 37 
core. Once the PDU session has been setup, both the end user devices have to register with the IMS core to support 38 
video calling services. 39 
3. Use the MO end user device to video call the MT end user device. Validate the MT end user device is able to 40 
receive and answer the call. 41 
4. Continue to have two-way audio/video communication on the video call for at least 5 minutes before terminating it.  42 
5. Repeat the test multiple times (> 10 times) and gather results. 43 
6. Repeat the above steps 1 through 5 for the good, fair and poor radio conditions. 44 
 45 

  
Page 123 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
6.8.1.4 Test Expectation (expected results) 1 
As a pre-validation, use the traces to validate a successful registration and PDU session setup by the end user devices 2 
without any errors over O-RU, O-DU and O-CU-CP/O-CU-UP. Also validate both the end user devices are able to 3 
register with the IMS core for video calling services. This is a prerequisite before these tests can be validated. 4 
Validate the end user devices are able to make a video call between each other by dynamically setting up a 5QI-2 bearer 5 
to transfer voice and video packets over RTP. Ensure the Call Setup Time is reasonable, the voice and video quality 6 
from both parties are clear and audible without one-way or intermittent muting and video freezing. Use the packet 7 
captures to validate there is no RTP packet drops or high RTP packet jitter which could cause voice muting, video 8 
freezing or video lag issues. Use the packet captures to ensure there are no out-of-sequence packets which could impact 9 
customer’s voice experience.  10 
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 11 
is performed in a controlled environment in good radio condition without the interference of external factors which 12 
could impact the KPIs, example: use of internet to connect to remote servers/hosts could add latency, jitter and 13 
reliability issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the 14 
testing in this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test results 15 
outside the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues 16 
may be due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and 17 
the test has to be repeated.  18 
• CSSR – Call Setup Success Rate % –> 99%. 19 
• CST – Call Setup Time – < 2.5s 20 
• MOS Score – > 3.5 21 
• Mute Rate % – < 1% 22 
• One Way Call % - < 1% 23 
• RTP Packet Loss % - < 1% 24 
 25 
As a part of gathering data, ensure the minimum configuration parameters (see Section 3.3) are included in the test 26 
report. The following information should also be included in the test report to provide a comprehensive view of the test 27 
setup. 28 
UE side (real or emulated UE): 29 
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second) 30 
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second) 31 
• Downlink transmission mode 32 
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 33 
(average sample per second) 34 
Table 6-20 Example Test Report for Video over NR – Stationary Testing 35 
 
Excellent 
(cell centre) Good Fair Poor 
(cell edge) 
Video over NR 
MO/MT 
Video over NR 
MO/MT 
Video over NR 
MO/MT 
Video over NR 
MO/MT 
Call Setup Success Rate     
Call Setup Time     
MOS Score     

  
Page 124 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
Mute Rate %     
One Way Call %     
RTP Packet Loss %     
L1 DL Spectral efficiency [bps/Hz]     
UE RSRP [dBm]     
UE PDSCH SINR [dB]     
MIMO rank     
PDSCH MCS     
DL RB number     
UE CQI     
UE RSRQ     
UE PMI     
UE RSSI     
UE Buffer status     
UE Packet delay      
PDSCH BLER [%]     
The end user video calling service experience can also be impacted by some of the features available (see Section 6.8) 1 
on the O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other features/functionality 2 
which could impact the end user’s video calling service experience should be included in the test report to provide a 3 
comprehensive view of the testing. 4 
6.8.2 Video over NR – Intra-Distributed Unit (DU) handover 5 
This test scenario validates the user’s video calling experience when the end user device (UE) is on a video call over 6 
NR and performs a handover between O-RUs which connect to the same O-DU (Intra-O-DU handover). 7 
6.8.2.1 Test Description 8 
The 5G O-RAN system has multiple sub-components like the O-RU, O-DU and the O-CU-CP/O-CU-UP. This setup 9 
leads to multiple handover scenarios between the O-RAN sub-components. This particular scenario tests the end user’s 10 
video calling experience during the handover between two O-RUs which are connected to the same O-DU (and O-CU-11 
CP/O-CU-UP), hence an Intra-O-DU handover. This handover will be agnostic to the 5G core as the handover occurs 12 
on the O-RAN system. This test assesses the impact on the end user’s voice service in this handover scenario by 13 
monitoring the KPIs included in Section 6.8. 14 
6.8.2.2 Test Setup 15 
The SUT in this test case would be a pair of O-RUs which connect to the same O-DU and O-CU-CP/O-CU-UP. This O-16 
RAN setup is used to test the video calling service during a handover (refer Section 4.4). A 5G SA Core would be 17 
required to support basic functionality to authenticate and register the end user device to establish a PDU session. An 18 
IMS core will be required to register the end user device to support video calling services on a 5G network. The 5G and 19 
IMS cores could be a completely emulated, partially emulated or real non-emulated cores. We will need at least two end 20 
user devices (UE) which can be a real UEs or an emulated one, and both have to support video calling service using 5G 21 
– Video over NR. The end user devices will serve as Mobile Originated (MO) and Mobile Terminated (MT) end user 22 
devices forming the two ends of the video call. Going forward in this section, these end user devices will be referred to 23 
as MO end user device and MT end user device to represent their role in the video call. The test setup should include 24 

  
Page 125 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
tools which have the ability to collect traces on the elements and/or packet captures of communication between the 1 
elements. This could be a built-in capability of the emulated/non-emulated network elements or an external tool. 2 
Optionally, if some of the network elements are located remotely either in a cloud or on the internet, the additional 3 
latency should be calculated and accounted for.   4 
The pair of O-RUs (O-RU1 and O-RU2) need to be connected to the O-DU and O-CU-CP/O-CU-UP and all the 5 
components need to have the right configuration and software load. The end user devices must be configured with the 6 
right user credentials to be able to register and authenticate with the O-RAN system and the 5G core. The end user 7 
devices also need to be provisioned with the user credentials to register and setup PDU session with the 5G core and 8 
register with the IMS core to perform video call using Video over NR. The 5G core network and IMS core must be 9 
configured to support video service on the end user devices used for testing, which includes the ability to dynamically 10 
set up QoS Flows for voice calls. 11 
All the elements in the network like O-RAN system, 5G core and the IMS Core need to have the ability to capture 12 
traces to validate the successful execution of the test cases. The end user devices need to have the capability to capture 13 
traces/packet capture to calculate the Video over NR KPIs. Optionally, the network could have network taps deployed 14 
in various legs of the network to get packet captures to validate successful execution of the test cases. Finally, all these 15 
different components need to have connectivity with each other – the end user device should be able to connect to O-16 
RAN system(O-RUs connected to O-DU and O-CU-CP/O-CU-UP), O-RAN system needs to be connected to the 5G 17 
core which in turn should have connectivity to the IMS Core. 18 
6.8.2.3 Test Methodology/Procedure 19 
Ensure the end user devices, O-RAN system, 5G core and the IMS core have all been configured as outlined in Section 20 
6.8.2.2. In this test scenario, both the mobile originated and mobile terminated end user devices are going to use the 21 
same O-RAN system, 5G and IMS core to perform the end-to-end video call. All traces and packet captures need to be 22 
enabled for the duration of the testing to ensure all communication between network elements can be captured and 23 
validated. 24 
1. Power on the two end user devices and ensure both of the devices registers with the 5G core for video calling 25 
services over SA by connecting over the O-RAN (O-RU, O-DU and O-CU-CP/O-CU-UP). Ensure both the MO & 26 
MT end user devices are in the coverage area of the same O-RU – O-RU1. 27 
2. Once the registration is complete, the MO and MT end user devices have to establish PDU session with the 5G 28 
core. Once the PDU session has been setup, both the end user devices have to register with the IMS core to support 29 
voice services. 30 
3. Use the MO end user device to video call the MT end user device. Validate the MT end user device is able to 31 
receive and answer the call. 32 
4. Once the call has been setup, move the MO end user device from the coverage area of O-RU1 to coverage area of 33 
O-RU2, thus causing a handover from O-RU1 to O-RU2.  34 
5. Continue the two-way video and voice communication between MO and MT end user devices until the handover 35 
procedure is complete before terminating the video call.  36 
6. Repeat the test (steps 1 through 5) multiple times (> 10 times) and gather results. 37 
 38 
6.8.2.4 Test Expectation (expected results) 39 
As a pre-validation, use the traces to validate a successful registration and PDU session setup by the end user devices 40 
without any errors over O-RU, O-DU and O-CU-CP/O-CU-UP. Also validate both the end user devices are able to 41 
register with the IMS core for video calling services. This is a prerequisite before these tests can be validated. 42 
Validate the end user devices are able to make a video call between each other by dynamically setting up a 5QI-2 bearer 43 
to transfer voice and video packets over RTP. Ensure the Call Setup Time is reasonable, the voice and video 44 

  
Page 126 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
qualityfrom both parties are clear and audible without one-way or intermittent muting and video freezing, through the 1 
duration of the call, especially during the handover process. Use the packet captures to validate there is no RTP packet 2 
drops or high RTP packet jitter which could cause voice muting, video freezing or video lag issues. Use the packet 3 
captures to ensure there are no out-of-sequence packets which could impact customer’s voice experience.  4 
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 5 
is performed in a controlled environment in good radio condition without the interference of external factors which 6 
could impact the KPIs, example: use of internet to connect to remote servers/hosts could add latency, jitter and 7 
reliability issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the 8 
testing in this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test results 9 
outside the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues 10 
may be due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and 11 
the test has to be repeated.  12 
• CSSR – Call Setup Success Rate % –> 99%. 13 
• CST – Call Setup Time – < 2.5s 14 
• MOS Score – > 3.5 15 
• Mute Rate % – < 1% 16 
• One Way Call % - < 1% 17 
• RTP Packet Loss % - < 1% 18 
 19 
As a part of gathering data, ensure the minimum configuration parameters (see Section 3.3) are included in the test 20 
report. The following information should also be included in the test report to provide a comprehensive view of the test 21 
setup. 22 
UE side (real or emulated UE): 23 
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second) 24 
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second) 25 
• Downlink transmission mode 26 
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 27 
(average sample per second) 28 
Table 6-21 Example Test Report for Video over NR – Intra-O-DU Handover 29 
 Video over NR MO Video over NR MT 
Call Setup Success Rate   
Call Setup Time   
MOS Score   
Mute Rate %   
One Way Call %   
RTP Packet Loss %   
L1 DL Spectral efficiency [bps/Hz]   
UE RSRP [dBm]   
UE PDSCH SINR [dB]   
MIMO rank   
PDSCH MCS   

  
Page 127 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
DL RB number   
UE CQI   
UE RSRQ   
UE PMI   
UE RSSI   
UE Buffer status   
UE Packet delay    
PDSCH BLER [%]   
The end user video calling service experience can also be impacted by some of the features available (see Section 6.8) 1 
on the O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other features/functionality 2 
which could impact the end user’s video calling service experience should be included in the test report to provide a 3 
comprehensive view of the testing. 4 
6.8.3 Video over NR – Intra-Central Unit (CU) Inter-Distributed Unit (DU) 5 
handover  6 
This test scenario validates the user’s video calling experience when the end user device (UE) is on a video call and 7 
performs a handover between O-RUs which connect to different O-DUs which in turn are connected to the same O-CU-8 
CP/O-CU-UP (Intra-O-CU Inter-O-DU handover) 9 
6.8.3.1 Test Description 10 
The 5G O-RAN system has multiple sub-components like the O-RU, O-DU and the O-CU-CP/O-CU-UP. This setup 11 
leads to multiple handover scenarios between the O-RAN sub-components. This particular scenario tests the end user’s 12 
video calling experience during the handover between two O-RUs, where the O-RUs are connected to different O-DUs 13 
which in turn are connected to the same O-CU-CP/O-CU-UP, hence an Intra-O-CU Inter-O-DU handover. This 14 
handover will be agnostic to the 5G core as the handover occurs on the O-RAN system. This test assesses the impact on 15 
the end user’s voice service in this handover scenario by monitoring the KPIs included in Section 6.8. 16 
6.8.3.2 Test Setup 17 
The SUT in this test case would be a pair of O-RUs which connect to a different pair of O-DUs which in turn connect to 18 
the same O-CU-CP/O-CU-UP (refer Section 4.5). This O-RAN setup is used to test the voice service during a handover. 19 
A 5G SA Core would be required to support basic functionality to authenticate and register the end user device to 20 
establish a PDU session. An IMS core will be required to register the end user device to support voice services on a 5G 21 
network. The 5G and IMS cores could be a completely emulated, partially emulated or real non-emulated cores. We 22 
will need at least two end user devices (UE) which can be a real UEs or an emulated one, and both have to support 23 
video calling service using Video over NR. The end user devices will serve as Mobile Originated (MO) and Mobile 24 
Terminated (MT) end user devices forming the two ends of the video call. Going forward in this section, these end user 25 
devices will be referred to as MO end user device and MT end user device to represent their role in the voice call. The 26 
test setup should include tools which have the ability to collect traces on the elements and/or packet captures of 27 
communication between the elements. This could be a built-in capability of the emulated/non-emulated network 28 
elements or an external tool. Optionally, if some of the network elements are located remotely either in a cloud or on the 29 
internet, the additional latency should be calculated and accounted for.  30 
The pair of O-RUs (O-RU1 and O-RU2) need to be connected to a pair of O-DUs (O-DU1 and O-DU2) and O-CU-31 
CP/O-CU-UP. As for the O-RU to O-DU connection, ensure O-RU1 connects to O-DU1, and O-RU2 connects to O-32 
DU2, and both the O-DUs connect to the same O-CU-CP/O-CU-UP. All the O-RAN components need to have the right 33 

  
Page 128 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
configuration and software load. The end user devices also need to be provisioned with the user credentials to register 1 
and setup PDU session with the 5G core and register with the IMS core to perform video call using Video over NR. The 2 
5G core network and IMS core must be configured to support video calling service on the end user devices used for 3 
testing, which includes the ability to dynamically set up QoS Flows for video calls. 4 
All the elements in the network like O-RAN system, 5G core and the IMS Core need to have the ability to capture 5 
traces to validate the successful execution of the test cases. The end user devices need to have the capability to capture 6 
traces/packet capture to calculate the Video over NR KPIs. Optionally, the network could have network taps deployed 7 
in various legs of the network to get packet captures to validate successful execution of the test cases. Finally, all these 8 
different components need to have connectivity with each other – the end user device should be able to connect to O-9 
RAN system(O-RUs connected to O-DU and O-CU-CP/O-CU-UP), O-RAN system needs to be connected to the 5G 10 
core which in turn should have connectivity to the IMS Core. 11 
6.8.3.3 Test Methodology/Procedure 12 
Ensure the end user devices, O-RAN system, 5G core and the IMS Core have all been configured as outlined in Section 13 
6.8.3.2. In this test scenario, both the mobile originated and mobile terminated end user devices are going to use the 14 
same O-RAN system, 5G and IMS core to perform the end-to-end video call. All traces and packet captures need to be 15 
enabled for the duration of the testing to ensure all communication between network elements can be captured and 16 
validated. 17 
1. Power on the two end user devices and ensure both of devices registers with the 5G core for video calling services 18 
over SA by connecting over the O-RAN (O-RU, O-DU and O-CU-CP/O-CU-UP). Ensure both the MO & MT end 19 
user devices are in the coverage area of the same O-RU – O-RU1. 20 
2. Once the registration is complete, the MO and MT end user devices have to establish a PDU session with the 5G 21 
core. Once the PDU session has been setup, both the end user devices have to register with the IMS core to support 22 
video calling services. 23 
3. Use the MO end user device to video call the MT end user device. Validate the MT end user device is able to 24 
receive and answer the call. 25 
4. Once the video call has been setup, move the MO end user device from the coverage area of O-RU1 to coverage 26 
area of O-RU2, thus causing a handover from O-RU1 to O-RU2, and O-DU1 to O-DU2.  27 
5. Continue the two-way video and voice communication between the end user devices until the handover procedure 28 
is complete before terminating the video call.  29 
6. Repeat the test (steps 1 through 5) multiple times (> 10 times) and gather results. 30 
 31 
6.8.3.4 Test Expectation (expected results) 32 
As a pre-validation, use the traces to validate a successful registration and PDU session setup by the end user devices 33 
without any errors over O-RU, O-DU and O-CU-CP/O-CU-UP. Also validate both the end user devices are able to 34 
register with the IMS core for video calling services. This is a prerequisite before these tests can be validated. 35 
Validate the end user devices are able to make a video call between each other by dynamically setting up a 5QI-2 bearer 36 
to transfer voice and video packets over RTP. Ensure the Call Setup Time is reasonable, the voice and video quality 37 
from both parties are clear and audible without one-way or intermittent muting and video freezing, through the duration 38 
of the call, especially during the handover process. Use the packet captures to validate there is no RTP packet drops or 39 
high RTP packet jitter which could cause voice muting, video freezing or video lag issues. Use the packet captures to 40 
ensure there are no out-of-sequence packets which could impact customer’s voice experience.  41 
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 42 
is performed in a controlled environment in good radio condition without the interference of external factors which 43 
could impact the KPIs, example: use of internet to connect to remote servers/hosts could add latency, jitter and 44 

  
Page 129 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
reliability issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the 1 
testing in this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test results 2 
outside the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues 3 
may be due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and 4 
the test has to be repeated.  5 
• CSSR – Call Setup Success Rate % –> 99%. 6 
• CST – Call Setup Time – < 2.5s 7 
• MOS Score – > 3.5 8 
• Mute Rate % – < 1% 9 
• One Way Call % - < 1% 10 
• RTP Packet Loss % - < 1%  11 
 12 
As a part of gathering data, ensure the minimum configuration parameters (see Section 3.3) are included in the test 13 
report. The following information should also be included in the test report to provide a comprehensive view of the test 14 
setup. 15 
UE side (real or emulated UE): 16 
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second) 17 
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second) 18 
• Downlink transmission mode 19 
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 20 
(average sample per second) 21 
Table 6-22 Example Test Report for Video over NR – Intra-O-CU Inter-O-DU Handover 22 
 Video over NR MO Video over NR MT 
Call Setup Success Rate   
Call Setup Time   
MOS Score   
Mute Rate %   
One Way Call %   
RTP Packet Loss %   
L1 DL Spectral efficiency [bps/Hz]   
UE RSRP [dBm]   
UE PDSCH SINR [dB]   
MIMO rank   
PDSCH MCS   
DL RB number   
UE CQI   
UE RSRQ   
UE PMI   
UE RSSI   

  
Page 130 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
UE Buffer status   
UE Packet delay    
PDSCH BLER [%]   
The end user video calling service experience can also be impacted by some of the features available (see Section 6.8) 1 
on the O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other features/functionality 2 
which could impact the end user’s video calling service experience should be included in the test report to provide a 3 
comprehensive view of the testing. 4 
6.8.4 Video over NR – Intra-Central Unit (CU) handover  5 
This test scenario validates the user’s video calling experience when the end user device (UE) is on a video call and 6 
performs a handover between O-RUs which connected to different O-DUs and O-CU-CP/O-CU-UPs (Inter-O-CU Inter-7 
O-DU handover) 8 
6.8.4.1 Test Description 9 
The 5G O-RAN system has multiple sub-components like the O-RU, O-DU and the O-CU-CP/O-CU-UP. This setup 10 
leads to multiple handover scenarios between the O-RAN sub-components. This particular scenario tests the end user’s 11 
video calling experience during the handover between two O-RUs, where the O-RUs are connected to different O-DUs 12 
which in turn are connected to different O-CU-CP/O-CU-UPs, hence an Inter-O-CU Inter-O-DU handover. This 13 
handover occurs on the O-RAN system and the 5G core is aware of the handover as it needs to send data to a new O-14 
CU-CP/O-CU-UP as a part of the handover process. This test assesses the impact on the end user’s voice service in this 15 
handover scenario by monitoring the KPIs included in Section 6.8. 16 
6.8.4.2 Test Setup 17 
The SUT in this test case would be a pair of O-RAN subsystems, a set of O-RU, O-DU and O-CU-CP/O-CU-UP which 18 
interconnects with another set of O-RU, O-DU and O-CU-CP/O-CU-UP (refer Section 4.6). This O-RAN setup is used 19 
to test the video calling service during a handover. A 5G SA Core would be required to support basic functionality to 20 
authenticate and register the end user device to establish a PDU session. An IMS core will be required to register the 21 
end user device to support voice services on a 5G network. The 5G and IMS cores could be a completely emulated, 22 
partially emulated or real non-emulated cores. We will need at least two end user devices (UE) which can be a real UEs 23 
or an emulated one, and both have to support video calling service using Video over NR. The end user devices will 24 
serve as Mobile Originated (MO) and Mobile Terminated (MT) end user devices forming the two ends of the video call. 25 
Going forward in this section, these end user devices will be referred to as MO end user device and MT end user device 26 
to represent their role in the video call. The test setup should include tools which have the ability to collect traces on the 27 
elements and/or packet captures of communication between the elements. This could be a built-in capability of the 28 
emulated/non-emulated network elements or an external tool. Optionally, if some of the network elements are located 29 
remotely either in a cloud or on the internet, the additional latency should be calculated and accounted for.  30 
The pair of O-RAN (O-RAN1 and O-RAN2) subsystems will be connected – O-RU1 will be connected to O-DU1, 31 
which in turn will be connected to O-CU-CP/O-CU-UP1 and similarly O-RU2 will be connected to O-DU2, which in 32 
turn will be connected to O-CU-CP/O-CU-UP2. The O-CU-CP/O-CU-UP1 and O-CU-CP/O-CU-UP2 nodes will be 33 
connected to each other and the 5G core. All the O-RAN components need to have the right configuration and software 34 
load. The end user devices must be configured with the right user credentials to be able to register and authenticate with 35 
the O-RAN system and the 5G core. The end user devices also need to be provisioned with the user credentials to 36 
register and setup PDU session with the 5G core and register with the IMS core to perform video call using Video over 37 
NR. The 5G core network and IMS core must be configured to support video calling service on the end user devices 38 
used for testing, which includes the ability to dynamically set up QoS Flows for voice calls. 39 

  
Page 131 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
All the elements in the network like O-RAN system, 5G core and the IMS Core need to have the ability to capture 1 
traces to validate the successful execution of the test cases. The end user devices need to have the capability to capture 2 
traces/packet capture to calculate the Video over NR KPIs. Optionally, the network could have network taps deployed 3 
in various legs of the network to get packet captures to validate successful execution of the test cases. Finally, all these 4 
different components need to have connectivity with each other – the end user device should be able to connect to O-5 
RAN system(O-RUs connected to O-DUs which are connected to the O-CU-CP/O-CU-UPs), O-RAN system needs to 6 
be connected to the 5G core which in turn should have connectivity to the IMS Core. 7 
6.8.4.3 Test Methodology/Procedure 8 
Ensure the end user devices, O-RAN system, 5G core and the IMS Core have all been configured as outlined in Section 9 
6.8.4.2. In this test scenario, both the mobile originated and mobile terminated end user devices are going to use the 10 
same O-RAN system, 5G and IMS core to perform the end-to-end video call. All traces and packet captures need to be 11 
enabled for the duration of the testing to ensure all communication between network elements can be captured and 12 
validated. 13 
1. Power on the two end user devices and ensure both of devices registers with the 5G core for video calling services 14 
over SA by connecting over the O-RAN1 system (O-RU1, O-DU1 and O-CU-CP/O-CU-UP1). Ensure both the 15 
MO & MT end user devices are in the coverage area of the same O-RU – O-RU1. 16 
2. Once the registration is complete, the MO and MT end user devices have to establish a PDU session with the 5G 17 
core. Once the PDU session has been setup, both the end user devices have to register with the IMS core to support 18 
video calling services. 19 
3. Use the MO end user device to video call the MT end user device. Validate the MT end user device is able to 20 
receive and answer the video call. 21 
4. Once the call has been setup, move the MO end user device from the coverage area of O-RU1 to coverage area of 22 
O-RU2, thus causing a handover from O-RAN1 to O-RAN2 subsystem - O-RU1 to O-RU2, O-DU1 to O-DU2 and 23 
O-CU-CP/O-CU-UP1 to O-CU-CP/O-CU-UP2.  24 
5. Continue the two-way video and voice communication between MO and MT end user devices until the handover 25 
procedure is complete before terminating the video call.  26 
6. Repeat the test (steps 1 through 5) multiple times (> 10 times) and gather results. 27 
 28 
6.8.4.4 Test Expectation (expected results) 29 
As a pre-validation, use the traces to validate a successful registration and PDU session setup by the end user devices 30 
without any errors over O-RU, O-DU and O-CU-CP/O-CU-UP. Also validate both the end user devices are able to 31 
register with the IMS core for video calling services. This is a prerequisite before these tests can be validated. 32 
Validate the end user devices are able to make a video call between each other by dynamically setting up a 5QI-2 bearer 33 
to transfer voice and video packets over RTP. Ensure the Call Setup Time is reasonable, the voice and video quality 34 
from both parties are clear and audible without one-way or intermittent muting and video freezing, through the duration 35 
of the call, especially during the handover process. Use the packet captures to validate there is no RTP packet drops or 36 
high RTP packet jitter which could cause voice muting, video freezing or video lag issues. Use the packet captures to 37 
ensure there are no out-of-sequence packets which could impact customer’s voice experience.  38 
The average range of values for the KPIs are included below for guidance. These values are applicable when the testing 39 
is performed in a controlled environment in good radio condition without the interference of external factors which 40 
could impact the KPIs, example: use of internet to connect to remote servers/hosts could add latency, jitter and 41 
reliability issues to the connection, thus impacting the KPIs. As there are multiple variables which can impact the 42 
testing in this scenario, a KPI outcome outside the range does not necessarily point to a failed test case. Any test results 43 
outside the range of KPI encountered during testing, will have to be investigated to identify the root cause as the issues 44 

  
Page 132 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
may be due to the variables outside the SUT. If the root cause is not in the SUT, then the issues have to be resolved and 1 
the test has to be repeated.  2 
• CSSR – Call Setup Success Rate % –> 99%. 3 
• CST – Call Setup Time – < 2.5s 4 
• MOS Score – > 3.5 5 
• Mute Rate % – < 1% 6 
• One Way Call % - < 1% 7 
• RTP Packet Loss % - < 1% 8 
 9 
As a part of gathering data, ensure the minimum configuration parameters (see Section 3.3) are included in the test 10 
report. The following information should also be included in the test report to provide a comprehensive view of the test 11 
setup. 12 
UE side (real or emulated UE): 13 
• Radio parameters such as RSRP, RSRQ, PDSCH SINR (average sample per second) 14 
• PDSCH BLER, PDSCH MCS, MIMO rank (number of layers) (average sample per second) 15 
• Downlink transmission mode 16 
• Channel utilization, i.e. Number of allocated/occupied downlink PRBs and Number of allocated/occupied slots 17 
(average sample per second) 18 
Table 6-23 Example Test Report for Video over NR – Inter-O-CU Handover 19 
 Video over NR MO Video over NR MT 
Call Setup Success Rate   
Call Setup Time   
MOS Score   
Mute Rate %   
One Way Call %   
RTP Packet Loss %   
L1 DL Spectral efficiency [bps/Hz]   
UE RSRP [dBm]   
UE PDSCH SINR [dB]   
MIMO rank   
PDSCH MCS   
DL RB number   
UE CQI   
UE RSRQ   
UE PMI   
UE RSSI   
UE Buffer status   
UE Packet delay    
PDSCH BLER [%]   

  
Page 133 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
The end user video calling service experience can also be impacted by some of the features available (see Section 6.8) 1 
on the O-RU, O-DU and O-CU-CP/O-CU-UP. The details of these features, along with any other features/functionality 2 
which could impact the end user’s video calling service experience should be included in the test report to provide a 3 
comprehensive view of the testing. 4 
  5 

  
Page 134 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
 Security 1 
This chapter describes the tests evaluating and assessing the security aspects of the E2E of a radio access network. The 2 
general test methodologies and configurations outlined in Chapter 3 should be followed.  3 
Because the whole O-RAN system is the System under Test (SUT) and can be viewed as an integrated black box in the 4 
context of the E2E testing, the security test cases listed here fall into the following categories: 5 
1. gNB security assurance specification required by 3GPP SA3 (applicable to 5G-NR NSA/SA); 6 
2. Optional: Denial of Service (DoS), fuzzing, and exploitation types of security test cases against the O-RAN 7 
system executed on L2 (Ethernet) and L3 (IP) layers with easy to access target information (MAC/IP address) 8 
(applicable to 5G-NR NSA/SA); 9 
3. Optional: Resource exhaustion types of security test case against the underlying cloud infrastructure hosting 10 
the O-RAN system and its network slice(s) (applicable to 5G-NR NSA/SA)   11 
Any other security test cases requiring specific access to a major interface or an internal functionality of the O-RAN 12 
component(s) is out of scope. 13 
7.1 gNB Security Assurance Specification (SCAS) required by 14 
3GPP SA3 15 
An E2E O-RAN system (SUT) is equivalent to a gNB from both network architecture and functional perspectives, 16 
therefore any gNB specific security requirements, threats and test cases outlined in 3GPP TS 33.511 [27] shall be 17 
followed in this E2E of radio access network security evaluation and assessment. The following table illustrates all the 18 
required test cases listed in Section 4.2.2 of 3GPP TS 33.511[27]. 19 
Table 7-1 List of gNB SCAS Test Cases 20 
Test Case 
(O-RAN 
Ref. #) 
Test Case 
(3GPP 
Ref. #) 
Test Name Description 
7.1.1 4.2.2.1.1 Integrity protection 
of RRC-signalling 
Verify that the RRC-signaling data sent between UE and gNB over 
the NG RAN air interface are integrity protected 
7.1.2 4.2.2.1.2 Integrity protection 
of user data between 
the UE and the gNB 
Verify that the user data packets are integrity protected over the NG 
RAN air interface. 
7.1.3 4.2.2.1.4 RRC integrity check 
failure 
Verify that RRC integrity check failure is handled correctly by the 
gNB 
7.1.4 4.2.2.1.5 UP integrity check 
failure 
Verify that UP integrity check failure is handled correctly by the 
gNB. 
7.1.5 4.2.2.1.6 Ciphering of RRC-
signalling 
Verify that the RRC-signaling data sent between UE and gNB over 
the NG RAN air interface are confidentiality protected. 
7.1.6 4.2.2.1.7 Ciphering of user 
data between the UE 
and the gNB 
Verify that the user data packets are confidentiality protected over 
the NG RAN air interface. 
7.1.7 4.2.2.1.8 Replay protection of 
user data between 
Verify that the user data packets are replay protected over the NG 
RAN air interface. 

  
Page 135 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
the UE and the gNB 
7.1.8 4.2.2.1.9 Replay protection of 
RRC-signalling 
Verify the replay protection of RRC-signaling between UE and gNB 
over the NG RAN air interface. 
7.1.9 4.2.2.1.10 Ciphering of user 
data based on the 
security policy sent 
by the SMF 
Verify that the user data packets are confidentiality protected based 
on the security policy sent by the SMF via AMF 
7.1.10 4.2.2.1.11 Integrity of user data 
based on the security 
policy sent by the 
SMF 
Verify that the user data packets are integrity protected based on the 
security policy sent by the SMF. 
7.1.11 4.2.2.1.12 AS algorithms 
selection 
Verify that the eNB/gNB selects the algorithms with the highest 
priority in its configured list. 
7.1.12 4.2.2.1.13 Key refresh at the 
gNB 
Key refresh at gNB 
7.1.13 4.2.2.1.14 Bidding down 
prevention in Xn-
handovers 
Verify that bidding down is prevented in Xn-handovers. 
7.1.14 4.2.2.1.15 AS protection 
algorithm selection 
in gNB change 
Verify that AS (Algorithm Selection) protection algorithm is selected 
correctly 
7.1.15 4.2.2.1.16 Control plane data 
confidentiality 
protection over 
N2/Xn interface 
Verify the control plane data confidentiality protection over N2/Xn 
interface 
7.1.16 4.2.2.1.17 Control plane data 
integrity protection 
over N2/Xn 
interface 
Verify the control plane data integrity protection over N2/Xn 
interface 
7.1.17 4.2.2.1.18 Key update at the 
gNB on dual 
connectivity 
Key update at the gNB on dual connectivity – 2 test cases 
 1 
7.2 Optional: DoS, fuzzing and blind exploitation types of security 2 
test 3 
Due to the open and disaggregated nature of the O-RAN system (SUT), the attack surfaces associated with some of its 4 
critical transport protocols and major interfaces of the O-RAN system become easy targets for potential attackers. 5 
Cyberattacks like DoS, fuzzing and blind exploitation types are easy to launch, require little information on the target 6 
system, and could cause significant performance degradation, or even the service interruption if not properly mitigated. 7 
As the WG1 Security Task Group (STG) is continuously working on the O-RAN system security analysis and security 8 
requirements specifications, the proposed security test cases in this section are optional for this E2E test specification 9 
release.  10 
As we learn more about the O-RAN system security posture against these types of attacks through the proposed test 11 
cases, we will reevaluate their necessity in the subsequent E2E test specification releases. 12 

  
Page 136 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
7.2.1 S-Plane PTP DoS Attack (Network layer) 1 
7.2.1.1 Test description & applicability 2 
The purpose of the test is to verify that a predefined volumetric DoS attack against O-DU S-Plane will not degrade 3 
service availability or performance of the SUT in a meaningful way. 4 
7.2.1.2 Test setup and configuration 5 
The test requires easy to access MAC address information of the O-DU’s open fronthaul interface and L2 connectivity 6 
(e.g. over L2 network switching device) to the target from the emulated attacker.  7 
Please refer to the diagram below for the test setup and configuration: 8 
 9 
7.2.1.3 Test procedure 10 
• Ensure normal UE procedures1 and user-plane traffic can be handled properly through the  SUT. It is 11 
recommended to use Section 5.6 Bidirectional throughput in different radio conditions and  Section 6.1 Data 12 
Services tests as a benchmark for indicating correct behavior of the SUT. 13 
• Use test tool to generate various level of volumetric DoS attack against the MAC address of the O-DU S-Plane 14 
o Volumetric tiers: 10Mbps, 100Mbps, 1Gbps 15 
o DoS Traffic types: generic Ethernet frames, PTP announce/sync message 16 
o DoS source address: spoofed MAC of PTPGM, random source MACs and broadcast MAC 17 
• Observe the functional and performance impact of the SUT 18 
 
1 UE procedures include initial acquisition, registration/de-registration, attach/detach, paging, service 
request, handover, … 
5GC or Emulated 5GC 
UE or Emulated UE 
S-Plane DoS attack 

  
Page 137 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
7.2.1.4 Test requirements (expected results) 1 
• No meaningful degradation of service availability and performance of the SUT. It is recommended to use Sec.  2 
5.6 Bidirectional throughput in different radio conditions and Section 6.1 Data Services tests as a benchmark 3 
for indicating correct behavior of the SUT. 4 
• DoS attacks should be mitigated by O-DU 5 
7.2.2 C-Plane eCPRI DoS Attack (Network layer) 6 
7.2.2.1 Test description & applicability 7 
The purpose of the test is to verify that a predefined volumetric DoS attack against O-DU C-Plane will not degrade 8 
service availability or performance of the SUT in a meaningful way. 9 
7.2.2.2 Test setup and configuration 10 
The test requires easy to access MAC address information of the O-DU’s open fronthaul interface and L2 connectivity 11 
(e.g. over L2 network switching device) to the target from the emulated attacker.  12 
Please refer to the diagram below for the test setup and configuration: 13 
 14 
7.2.2.3 Test procedure 15 
• Ensure normal UE procedures1 and user-plane traffic can be handled properly through the SUT. It is 16 
recommended to use Section 5.6 Bidirectional throughput in different radio conditions and  Section 6.1 Data 17 
Services tests as a benchmark for indicating correct behavior of the SUT. 18 
• Use test tool to generate various level of volumetric DoS attack against the MAC address of the O-DU C-Plane 19 
o Volumetric tiers: 10Mbps, 100Mbps, 1Gbps 20 
o DoS Traffic types: eCPRI real-time ctrl data message over Ethernet 21 
o DoS source address: spoofed MAC of O-RU(s), random source MACs or broadcast MAC 22 
• Observe the functional and performance impact of the SUT 23 
5GC or Emulated 5GC 
UE or Emulated UE 
C-Plane DoS attack 

  
Page 138 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
7.2.2.4 Test requirements (expected results) 1 
• No meaningful degradation of service availability and performance of the SUT. It is recommended to use 2 
Section  5.6 Bidirectional throughput in different radio conditions and Section 6.1 Data Services tests as a 3 
benchmark for indicating correct behavior of the SUT. 4 
• DoS attacks should be mitigated by O-DU. 5 
7.2.3 Near-RT RIC A1 Interface DoS Attack (Network layer) 6 
7.2.3.1 Test description & applicability 7 
The purpose of the test is to verify that a predefined volumetric DoS attack against Near-RT RIC A1 interface will not 8 
degrade service availability or performance of the SUT in a meaningful way 9 
7.2.3.2 Test setup and configuration 10 
The test requires easy to access IP address information of the Near-RT RIC’s A1 interface and a routable path to the 11 
target from the emulated attacker.  12 
Please refer to the diagram below for the test setup and configuration: 13 
 14 
7.2.3.3 Test procedure 15 
• Ensure normal UE procedures1 and user-plane traffic can be handled properly through the SUT. It is 16 
recommended to use Section 5.6 Bidirectional throughput in different radio conditions and  Section 6.1 Data 17 
Services tests as a benchmark for indicating correct behavior of the SUT. 18 
• Use test tool to generate various level of volumetric DoS attack against the IP address of the Near-RT RIC A1 19 
interface 20 
o Volumetric tiers: 10Mbps, 100Mbps, 1Gbps 21 
o DoS Traffic types: generic UDP packets, HTTP/HTTPs REST API calls 22 
o DoS source address: spoofed IP of Non-RT RIC, random source IPs or broadcast IP (UDP only) 23 
• Observe the functional and performance impact of the SUT 24 
5GC or Emulated 5GC 
UE or Emulated UE 
A1 interface DoS attack 

  
Page 139 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
7.2.3.4 Test requirements (expected results) 1 
• No meaningful degradation of service availability and performance of the SUT. It is recommended to use 2 
Section 5.6 Bidirectional throughput in different radio conditions and Section 6.1 Data Services tests as a 3 
benchmark for indicating correct behavior of the SUT. DoS attacks should be mitigated by O-RAN system 4 
perimeter security or Near-RT RIC. 5 
7.2.4 S-Plane PTP Unexpected Input (Network layer) 6 
7.2.4.1 Test description & applicability 7 
The purpose of the test is to verify that an unexpected (not in-line with protocol specification) input sent towards O-DU 8 
S-Plane will not compromise the security of the SUT 9 
7.2.4.2 Test setup and configuration 10 
The test requires easy to access MAC address information of the O-DU’s open fronthaul interface and L2 connectivity 11 
(e.g. over L2 network switching device) to the target from the emulated attacker.  12 
Please refer to the diagram below for the test setup and configuration: 13 
 14 
7.2.4.3 Test procedure 15 
• Ensure normal UE procedures1 and user-plane traffic can be handled properly through the SUT. It is 16 
recommended to use Section 5.6 Bidirectional throughput in different radio conditions and  Section 6.1 Data 17 
Services tests as a benchmark for indicating correct behavior of the SUT. 18 
• Use packet capture tool to capture sample of legitimate PTP message sent towards the O-DU S-Plane 19 
• Use fuzzing tool to replay the captured PTP message while mutating its content and keeping original 20 
source/destination MAC address. Send at least 250,000 iterations of mutated PTP message based on a random 21 
seed 22 
• Observe the functional and performance impact of the SUT 23 
5GC or Emulated 5GC 
UE or Emulated UE 
S-Plane Unexpected Input 

  
Page 140 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
7.2.4.4 Test requirements (expected results) 1 
• No meaningful degradation of service availability and performance of the SUT. It is recommended to use 2 
Section  5.6 Bidirectional throughput in different radio conditions and Section 6.1 Data Services tests as a 3 
benchmark for indicating correct behavior of the SUT. 4 
7.2.5 C-Plane eCPRI Unexpected Input (Network layer) 5 
7.2.5.1 Test description & applicability 6 
The purpose of the test is to verify that an unexpected (not in-line with protocol specification) input sent towards O-DU 7 
C-Plane will not compromise the security of the SUT 8 
7.2.5.2 Test setup and configuration 9 
The test requires easy to access MAC address information of the O-DU’s open fronthaul interface and L2 connectivity 10 
(e.g. over L2 network switching device) to the target from the emulated attacker.  11 
Please refer to the diagram below for the test setup and configuration: 12 
 13 
7.2.5.3 Test procedure 14 
• Ensure normal UE procedures1 and user-plane traffic can be handled properly through the SUT. It is 15 
recommended to use Section 5.6 Bidirectional throughput in different radio conditions and  Section 6.1 Data 16 
Services tests as a benchmark for indicating correct behavior of the SUT. 17 
• Use packet capture tool to capture sample of legitimate eCPRI message sent towards the O-DU C-Plane 18 
• Use fuzzing tool to replay the captured eCPRI message while mutating its content and keeping original 19 
source/destination MAC address. Send at least 250,000 iterations of mutated eCPRI message based on a random 20 
seed 21 
• Observe the functional and performance impact of the SUT 22 
5GC or Emulated 5GC 
UE or Emulated UE 
C-Plane Unexpected Input 

  
Page 141 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
7.2.5.4 Test requirements (expected results) 1 
• No meaningful degradation of service availability and performance of the SUT. It is recommended to use 2 
Section  5.6 Bidirectional throughput in different radio conditions and Section 6.1 Data Services tests as a 3 
benchmark for indicating correct behavior of the SUT. 4 
7.2.6 Near-RT RIC A1 Interface Unexpected Input (Network layer)  5 
7.2.6.1 Test description & applicability 6 
The purpose of the test is to verify that an unexpected (not in-line with protocol specification) input sent towards Near-7 
RT RIC A1 interface will not compromise the security of the SUT 8 
7.2.6.2 Test setup and configuration 9 
The test requires easy to access IP address information of the Near-RT RIC’s A1 interface and a routable path to the 10 
target from the emulated attacker.  11 
Please refer to the diagram below for the test setup and configuration: 12 
 13 
7.2.6.3 Test procedure 14 
• Ensure normal UE procedures1 and user-plane traffic can be handled properly through the SUT. It is 15 
recommended to use Section 5.6 Bidirectional throughput in different radio conditions and  Section 6.1 Data 16 
Services tests as a benchmark for indicating correct behavior of the SUT 17 
• Use packet capture tool to capture sample of legitimate HTTP/HTTPs REST API message sent towards the 18 
Near-RT RIC A1 interface 19 
• Use fuzzing tool to replay the captured HTTP/HTTPs REST API message while mutating its content and 20 
keeping original source/destination IP/port. Send at least 250,000 iterations of mutated HTTP/HTTPs REST 21 
API message based on a random seed 22 
• Observe the functional and performance impact of the SUT 23 
5GC or Emulated 5GC 
UE or Emulated UE 
A1 interface Unexpected Input 

  
Page 142 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
7.2.6.4 Test requirements (expected results) 1 
• No meaningful degradation of service availability and performance of the SUT. It is recommended to use 2 
Section  5.6 Bidirectional throughput in different radio conditions and Section 6.1 Data Services tests as a 3 
benchmark for indicating correct behavior of the SUT. 4 
7.2.7 Blind exploitation of well-known vulnerabilities over Near-RT RIC A1 5 
interface (Network layer) 6 
7.2.7.1 Test description & applicability 7 
The purpose of the test is to verify that exploitation attempts of well-known vulnerabilities executed blindly against 8 
Near-RT RIC A1 interface will not compromise security of the SUT 9 
7.2.7.2 Test setup and configuration 10 
The test requires easy to access IP address information of the Near-RT RIC’s A1 interface and a routable path to the 11 
target from the emulated attacker.  12 
Please refer to the diagram below for the test setup and configuration: 13 
 14 
7.2.7.3 Test procedure 15 
• Ensure normal UE procedures1 and user-plane traffic can be handled properly through the SUT. It is 16 
recommended to use Section 5.6 Bidirectional throughput in different radio conditions and  Section 6.1 Data 17 
Services tests as a benchmark for indicating correct behavior of the SUT 18 
• Make sure that vulnerability scanning tool has up-to-date database of well-known vulnerabilities 19 
(signatures/plugins) based on Common Vulnerabilities and Exposures (CVE). Document the actual version of 20 
vulnerability database (signatures/plugins) for further reference 21 
• Use vulnerability scanning tool to execute a scan against the IP address of the Near-RT RIC A1 interface. The 22 
scan should have the following parameters defined: 23 
o TCP Ports: 0-65535 24 
o UDP Ports: Top 1000 25 
5GC or Emulated 5GC 
UE or Emulated UE 
Exploitation over A1 interface 

  
Page 143 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
o Safe Checks: Disabled (to make sure that exploitation attempts of the vulnerabilities will be 1 
performed) 2 
• Observe the functional and performance impact of the SUT 3 
7.2.7.4 Test requirements (expected results) 4 
• No meaningful degradation of service availability and performance of the SUT. It is recommended to use 5 
Section  5.6 Bidirectional throughput in different radio conditions and Section 6.1 Data Services tests as a 6 
benchmark for indicating correct behavior of the SUT. 7 
• Identification of well-known vulnerabilities is out-of-scope of the test 8 
 9 
7.3 O-Cloud resource exhaustion type of security test (Virtualization 10 
layer) 11 
Because the O-RAN system is running on a virtualized platform, O-Cloud, security attacks against this underlying 12 
infrastructure could have profound performance and service impacts on the O-RAN system. 13 
Among various types of threats to the O-Cloud infrastructure, resource exhaustion attack, such as the O-Cloud side-14 
channel noisy neighbor attack, is one of the easiest to launch and could impact the network slice(s) performance, 15 
especially on a small size edge cloud with shared physical hosts/resources. 16 
As the WG1 Security Task Group (STG) is continuously working on the O-RAN system security analysis and security 17 
requirements specifications, the proposed security test case in this section will be optional for this E2E test specification 18 
release.  19 
As we learn more about the O-RAN system security posture against these types of attacks through the proposed test 20 
case, we will re-evaluate its necessity in the following E2E test specification release. 21 
7.3.1 O-Cloud side-channel DoS attack 22 
7.3.1.1 Test description & applicability 23 
The purpose of the test is to verify that a noisy neighbor DoS attack against O-Cloud for resource starvation will not 24 
degrade service availability or performance of the O-RAN system served by the network slice under test. 25 
7.3.1.2 Test setup and configuration 26 
The test requires access to the O-Cloud platform hosting the network slice(s) of the O-RAN system.  27 
Please refer to the diagram below for the test setup and configuration: 28 

  
Page 144 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
 1 
Noisy Neighbor VNF(s) can be deployed into an existing slice or a new slice of the shared resources with the existing 2 
slice under test 3 
7.3.1.3 Test procedure 4 
• Ensure normal UE procedures1 and user-plane traffic can be handled properly through the E2E O-RAN system 5 
served by the O-Cloud RAN slice under test. It is recommended to use Section 5.6 Bidirectional throughput in 6 
different radio conditions and  Section 6.1 Data Services tests as a benchmark for indicating correct behavior 7 
of the SUT 8 
• Use test tool (through O-Cloud MANO) to instantiate noisy neighbor VNFs  9 
o Noisy Neighbor tenant: existing slice or a new slice of the shared resources with the existing slice  10 
• Observe the functional and performance impact of the E2E O-RAN system served by the O-Cloud RAN slice 11 
under test 12 
7.3.1.4 Test requirements (expected results) 13 
• No degradation of service availability and performance of the E2E O-RAN system served by the network slice 14 
under test. It is recommended to use Section 5.6 Bidirectional throughput in different radio conditions and  15 
Section 6.1 Data Services tests as a benchmark for indicating correct behavior of the SUT 16 
• The Noisy Neighbor attack should be properly logged and alerted by the O-Cloud 17 
 18 
 19 
 20 
5GC or Emulated 5GC 
UE or Emulated UE 
O-Cloud 
Side-channel DoS attack 

  
Page 145 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
Annex A: Template of test report 1 
A. GENERAL INFORMATION 2 
A1 Name of test campaign 
 
A2 Version of the report – reference ID 
 
A3 Date(s) of testing 
 
A4 Contact person (tester) – incl. Name, Organization, E-mail address 
 
A4 Test location (lab) – incl. the address 
 
A5 Description of test campaign, summary of test results, conclusions 
 
 
 
 
 
 
 
 
 
 
 
 
 
List of tests - details of each test can be found in Section E. 3 
Test No. Test name  Test status [ PASS / FAIL / - ] 
01   
02   
03   
 4 
B. TEST AND MEASUREMENT EQUIPMENT AND TOOLS 5 
# Equipment or tool Type Manufacture Version (HW/SW) Notes# 
01      
02      
03      
# Specific details such as frequency range, date of last calibration, Type and version of Operating System at end -user device/application server, etc. 6 

  
Page 146 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
 1 
C. SYSTEM UNDER TEST 2 
C1 Total number of DUTs included in SUT  
 
C2 Deployment architecture (indoor/outdoor, micro/pico/ macro) 
 
C3 Description of SUT – block diagram – connection scheme 
 
 
 
 
 
DUT 1# 3 
C3 Type 
 
C4 Serial Number C5 Supplier (manufacture) 
C6 SW version 
 
C7 HW version 
 
C8 Interface/IOT profile(s) if applied  
 
C9 Description incl. parameters, setting/configuration 
 
 
# If SUT contains more DUTs, please copy the table . 4 
 5 
D. TEST CONFIGURATION 6 
D1 Carrier (central) frequency – (NR-)ARFCN 
 
D2 Total transmission (effective) bandwidth  
 
D3 Number of total Resource Blocks  
 
D4 Sub-carrier spacing 
 
D5 Carrier prefix length 
 
D6 Slot length 
 
D5 Duplex mode 
 
D6 TDD DL/UL ratio (configuration)  D7 IPv4 / IPv6 
eNB/gNB UE 
D8 Number of supported MIMO layers at eNB/gNB  
 
D9 Number of supported MIMO layers at UE  
 
D10 Antenna configuration - number of Tx/Rx at eNB/gNB 
  
D11 Antenna configuration - number of Tx/Rx at UE 
 
D12 Total antenna gain at eNB/gNB  D13 Total antenna gain at UE 

  
Page 147 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
D14 Total transmit power at antenna connectors at eNB/gNB  
 
D15 Total transmit power at antenna connectors at UE  
 
 1 
E. TEST RESULTS 2 
E1 Test No.  
 
E2 Test name 
 
E3 Date(s) of test execution 
  
E4 Reference to test specification  
 
E5 Utilized test and measurement equipment and tools , incl. the specific setting/configuration – reference to Section B 
 
  
E6 Test setup – connection/block diagram – deployment scenario (dense urban/urban/rural, LOS/NLOS/nLOS)  
 
 
 
 
E7 Test procedure – describe differences in comparison with the test procedure defined in test spec. – limitations  
 
E8 Test results – measured KPIs – incl. attachment of raw log file(s)  
 
 
 
E9 Notes, incl. observed issues with the solutions  
 
E10 Conclusions – pass/fail – assessment of measured KPIs (in comparison with the expected KPIs) – gap analysis 
 
 
 
 3 
 4 

  
Page 148 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
Annex ZZZ: O-RAN Adopter License Agreement 1 
BY DOWNLOADING, USING OR OTHERWISE ACCESSING ANY O-RAN SPECIFICATION, ADOPTER 2 
AGREES TO THE TERMS OF THIS AGREEMENT. 3 
This O-RAN Adopter License Agreement (the “Agreement”) is made by and between the O-RAN Alliance and the entity 4 
that downloads, uses or otherwise accesses any O-RAN Specification, including its Affiliates (the “Adopter”). 5 
This is a license agreement for entities who wish to adopt any O-RAN Specification. 6 
Section 1: DEFINITIONS 7 
1.1 “Affiliate” means an entity that directly or indirectly controls, is controlled by, or is under common control with 8 
another entity, so long as such control exists. For the purpose of this Section, “Control” means beneficial ownership of 9 
fifty (50%) percent or more of the voting stock or equity in an entity. 10 
1.2 “Compliant Implementation” means any system, device, method or operation (whether implement ed in hardware, 11 
software or combinations thereof) that fully conforms to a Final Specification. 12 
1.3 “Adopter(s)” means all entities, who are not Members, Contributors or Academic Contributors, including their 13 
Affiliates, who wish to download, use or otherwise access O-RAN Specifications. 14 
1.4 “Minor Update” means an update or revision to an O-RAN Specification published by O-RAN Alliance that does not 15 
add any significant new features or functionality and remains interoperable with the prior version of an O -RAN 16 
Specification. The term “O-RAN Specifications” includes Minor Updates. 17 
1.5 “Necessary Claims” means those claims of all present and future patents and patent applications, other than design 18 
patents and design registrations, throughout the world, which ( i) are owned or otherwise licensable by a Member, 19 
Contributor or Academic Contributor during the term of its Member, Contributor or Academic Contributorship; (ii) such 20 
Member, Contributor or Academic Contributor has the right to grant a license without the payment of consideration to a 21 
third party; and (iii) are necessarily infringed by a Compliant Implementation (without considering any Contributions not 22 
included in the Final Specification). A claim is necessarily infringed only when it is not possible on technical (but not 23 
commercial) grounds, taking into account normal technical practice and the state of the art generally available at the date 24 
any Final Specification was published by the O -RAN Alliance or the date the patent claim first came into existenc e, 25 
whichever last occurred, to make, sell, lease, otherwise dispose of, repair, use or operate a Compliant Implementation 26 
without infringing that claim. For the avoidance of doubt in exceptional cases where a Final Specification can only be 27 
implemented by technical solutions, all of which infringe patent claims, all such patent claims shall be considered 28 
Necessary Claims. 29 
1.6 “Defensive Suspension” means for the purposes of any license grant pursuant to Section 3, Member, Contributor, 30 
Academic Contributor, Adopter, or any of their Affiliates, may have the discretion to include in their license a term 31 
allowing the licensor to suspend the license against a licensee who brings a patent infringement suit against the licensing 32 
Member, Contributor, Academic Contributor, Adopter, or any of their Affiliates. 33 
Section 2: COPYRIGHT LICENSE 34 
2.1 Subject to the terms and conditions of this Agreement, O -RAN Alliance hereby grants to Adopter a nonexclusive, 35 
nontransferable, irrevocable, non -sublicensable, worldwide copyright  license to obtain, use and modify O -RAN 36 
Specifications, but not to further distribute such O -RAN Specification in any modified or unmodified way, solely in 37 
furtherance of implementations of an O-RAN Specification. 38 
2.2 Adopter shall not use O-RAN Specifications except as expressly set forth in this Agreement or in a separate written 39 
agreement with O-RAN Alliance. 40 

  
Page 149 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
Section 3: FRAND LICENSE 1 
3.1 Members, Contributors and Academic Contributors and their Affiliates are prepared to grant based on a separate 2 
Patent License Agreement to each Adopter under Fair Reasonable And Non - Discriminatory (FRAND) terms and 3 
conditions with or without compensation (royalties) a nonexclusive, non -transferable, irrevocable (but subject to 4 
Defensive Suspension), non-sublicensable, worldwide patent license under their Necessary Claims to make, have made, 5 
use, import, offer to sell, lease, sell and otherwise distribute Compliant Implementations; provided, however, that such 6 
license shall not extend: (a) to any part or function of a product in which a Compliant Implementation is incorporated that 7 
is not itself part of the Compliant Implementation; or (b) to any Adopter if that Adopter is not making a reciprocal grant 8 
to Members, Contributors and Academic Contributors, as set forth in Sec tion 3.3. For the avoidance of doubt, the 9 
foregoing licensing commitment includes the distribution by the Adopter’s distributors and the use by the Adopter’s 10 
customers of such licensed Compliant Implementations. 11 
3.2 Notwithstanding the above, if any Member , Contributor or Academic Contributor, Adopter or their Affiliates has 12 
reserved the right to charge a FRAND royalty or other fee for its license of Necessary Claims to Adopter, then Adopter 13 
is entitled to charge a FRAND royalty or other fee to such Member, Contributor or Academic Contributor, Adopter and 14 
its Affiliates for its license of Necessary Claims to its licensees. 15 
3.3 Adopter, on behalf of itself and its Affiliates, shall be prepared to grant based on a separate Patent License Agreement 16 
to each Members, Contributors, Academic Contributors, Adopters and their Affiliates under Fair Reasonable And Non-17 
Discriminatory (FRAND) terms and conditions with or without compensation (royalties) a nonexclusive, non -18 
transferable, irrevocable (but subject to Defensive Suspension), non-sublicensable, worldwide patent license under their 19 
Necessary Claims to make, have made, use, import, offer to sell, lease, sell and otherwise distribute Compliant 20 
Implementations; provided, however, that such license will not extend: (a) to any part or function of a product in which a 21 
Compliant Implementation is incorporated that is not itself part of the Compliant Implementation; or (b) to any Members, 22 
Contributors, Academic Contributors, Adopters and their Affiliates that is not makin g a reciprocal grant to Adopter, as 23 
set forth in Section 3.1. For the avoidance of doubt, the foregoing licensing commitment includes the distribution by the 24 
Members’, Contributors’, Academic Contributors’, Adopters’ and their Affiliates’ distributors and the use by the 25 
Members’, Contributors’, Academic Contributors’, Adopters’ and their Affiliates’ customers of such licensed Compliant 26 
Implementations. 27 
Section 4: TERM AND TERMINATION 28 
4.1 This Agreement shall remain in force, unless early terminated according to this Section 4. 29 
4.2 O-RAN Alliance on behalf of its Members, Contributors and Academic Contributors may terminate this Agreement 30 
if Adopter materially breaches this Agreement and does not cure or is not capable of curing such breach within thirty (30) 31 
days after being given notice specifying the breach. 32 
4.3 Sections 1, 3, 5 - 11 of this Agreement shall survive any termination of this Agreement. Under surviving Section 3, 33 
after termination of this Agreement, Adopter will continue to grant licenses (a) to entities who become Adopters after the 34 
date of termination; and (b) for future versions of O-RAN Specifications that are backwards compatible with the version 35 
that was current as of the date of termination. 36 
Section 5: CONFIDENTIALITY 37 
Adopter will use the  same care and discretion to avoid disclosure, publication, and dissemination of O -RAN 38 
Specifications to third parties, as Adopter employs with its own confidential information, but no less than reasonable care. 39 
Any disclosure by Adopter to its Affiliates,  contractors and consultants should be subject to an obligation of 40 
confidentiality at least as restrictive as those contained in this Section. The foregoing obligation shall not apply to any 41 
information which is: (1) rightfully known by Adopter without any limitation on use or disclosure prior to disclosure; (2) 42 
publicly available through no fault of Adopter; (3) rightfully received without a duty of confidentiality; (4) disclosed by 43 
O-RAN Alliance or a Member, Contributor or Academic Contributor to a third party without a duty of confidentiality on 44 
such third party; (5) independently developed by Adopter; (6) disclosed pursuant to the order of a court or other authorized 45 

  
Page 150 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
governmental body, or as required by law, provided that Adopter provides reasonable pri or written notice to O -RAN 1 
Alliance, and cooperates with O-RAN Alliance and/or the applicable Member, Contributor or Academic Contributor to 2 
have the opportunity to oppose any such order; or (7) disclosed by Adopter with O-RAN Alliance’s prior written approval. 3 
Section 6: INDEMNIFICATION 4 
Adopter shall indemnify, defend, and hold harmless the O -RAN Alliance, its Members, Contributors or Academic 5 
Contributors, and their employees, and agents and their respective successors, heirs and assigns (the “Indemnitees ”), 6 
against any liability, damage, loss, or expense (including reasonable attorneys’ fees and expenses) incurred by or imposed 7 
upon any of the Indemnitees in connection with any claims, suits, investigations, actions, demands or judgments arising 8 
out of Adopter’s use of the licensed O-RAN Specifications or Adopter’s commercialization of products that comply with 9 
O-RAN Specifications. 10 
Section 7: LIMITATIONS ON LIABILITY; NO WARRANTY 11 
EXCEPT FOR BREACH OF CONFIDENTIALITY, ADOPTER’S BREACH OF SECTION 3, AND ADOPTER’S 12 
INDEMNIFICATION OBLIGATIONS, IN NO EVENT SHALL ANY PARTY BE LIABLE TO ANY OTHER PARTY 13 
OR THIRD PARTY FOR ANY INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE OR CONSEQUENTIAL 14 
DAMAGES RESULTING FROM ITS PERFORMANCE OR NON-PERFORMANCE UNDER THIS AGREEMENT, 15 
IN EACH CASE WHETHER UNDER CONTRACT, TORT, WARRANTY, OR OTHERWISE, AND WHETHER OR 16 
NOT SUCH PARTY HAD ADVANCE NOTICE OF THE POSSIBILITY OF SUCH DAMAGES. O -RAN 17 
SPECIFICATIONS ARE PROVIDED “AS IS” WITH NO WARRANTIES OR CONDITIONS WHATSOEVER, 18 
WHETHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE. THE O -RAN ALLIANCE AND THE 19 
MEMBERS, CONTRIBUTORS OR ACADEMIC CONTRIBUTORS EXPRESSLY DISCLAIM ANY WARRANTY 20 
OR CONDITION OF MERCHANTABILITY, SECURITY, SATISFACTORY QUALITY, NONINFRINGEMENT, 21 
FITNESS FOR ANY PARTICULAR PURPOSE , ERROR -FREE OPERATION, OR ANY WARRANTY OR 22 
CONDITION FOR O-RAN SPECIFICATIONS. 23 
Section 8: ASSIGNMENT 24 
Adopter may not assign the Agreement or any of its rights or obligations under this Agreement or make any grants or 25 
other sublicenses to this Agreement, ex cept as expressly authorized hereunder, without having first received the prior, 26 
written consent of the O -RAN Alliance, which consent may be withheld in O -RAN Alliance’s sole discretion. O-RAN 27 
Alliance may freely assign this Agreement. 28 
Section 9: THIRD-PARTY BENEFICIARY RIGHTS 29 
Adopter acknowledges and agrees that Members, Contributors and Academic Contributors (including future Members, 30 
Contributors and Academic Contributors) are entitled to rights as a third -party beneficiary under this Agreement, 31 
including as licensees under Section 3. 32 
Section 10: BINDING ON AFFILIATES 33 
Execution of this Agreement by Adopter in its capacity as a legal entity or association constitutes that legal entity’s or 34 
association’s agreement that its Affiliates are likewise bound to the obligations that are applicable to Adopter hereunder 35 
and are also entitled to the benefits of the rights of Adopter hereunder. 36 
Section 11: GENERAL 37 
This Agreement is governed by the laws of Germany without regard to its conflict or choice of law provisions.  38 

  
Page 151 
 
O-RAN.TIFG.E2E-Test.0-v01.00 
This Agreement constitutes the entire agreement between the parties as to its express subject matter and expressly 1 
supersedes and replaces any prior or contemporaneous agreements between the parties, whether written or oral, relating 2 
to the subject matter of this Agreement.  3 
Adopter, on behalf of itself and its Affiliates, agrees to comply at all times with all applicable laws, rules and regulations 4 
with respect to its and its Affiliates’ performance under this Agreement, including without limitation, export control and 5 
antitrust laws. Without limiting the generality of the foregoing, Adopter acknowledges that this Agreement prohibits any 6 
communication that would violate the antitrust laws. 7 
By execution hereof, no form of any partnership, joint venture or other special relationship is created between Adopter, 8 
or O -RAN Alliance or its Members, Contributors or Academic Contributors. Except as expressly  set forth in this 9 
Agreement, no party is authorized to make any commitment on behalf of Adopter, or O -RAN Alliance or its Members, 10 
Contributors or Academic Contributors. 11 
In the event that any provision of this Agreement conflicts with governing law or if any provision is held to be null, void 12 
or otherwise ineffective or invalid by a court of competent jurisdiction, (i) such provisions will be deemed stricken from 13 
the contract, and (ii) the remaining terms, provisions, covenants and restrictions of this Agr eement will remain in full 14 
force and effect. 15 
Any failure by a party or third party beneficiary to insist upon or enforce performance by another party of any of the 16 
provisions of this Agreement or to exercise any rights or remedies under this Agreement or otherwise by law shall not be 17 
construed as a waiver or relinquishment to any extent of the other parties’ or third party beneficiary’s right to assert or 18 
rely upon any such provision, right or remedy in that or any other instance; rather the same shall be a nd remain in full 19 
force and effect. 20 