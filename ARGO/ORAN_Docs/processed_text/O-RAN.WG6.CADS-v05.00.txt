                                                                                                            
 
                                                   O-RAN.WG6.CADS-v05.00 
                                                                                                                         Technical Report  
 
 
O-RAN Working Group 6 (Cloudification and Orchestration) 
Cloud Architecture and Deployment Scenarios 
 for O-RAN Virtualized RAN 
  
 
Copyright © 2023 by the O-RAN ALLIANCE e.V. 
The copying or incorporation into any other work of part or all of the material available in this document in any form without the prior 
written permission of O-RAN ALLIANCE e.V.  is prohibited, save that you may print or download extracts of the material of this document 
for your personal use, or copy the material of this document for the purpose of sending to individual third parties for their information 
provided that you acknowledge O-RAN ALLIANCE as the source of the material and that you inform the th ird party that these conditions 
apply to them and that they must comply with them. 
 
O-RAN ALLIANCE e.V., Buschkauler Weg 27, 53347 Alfter, Germany 
Register of Associations, Bonn VR 11238, VAT ID DE321720189 
 
 1 


                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
2 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
 
Table of Contents 
Table of Contents ............................................................................................................................................... 2 
Table of Figures .................................................................................................................................................. 3 
Table of Tables ................................................................................................................................................... 4 
1 Scope ........................................................................................................................................................ 5 
1.1 Context; Relationship to Other O-RAN Work ................................................................................................... 5 
1.2 Objectives .......................................................................................................................................................... 5 
2 References ................................................................................................................................................ 7 
3 Definitions and Abbreviations ................................................................................................................. 8 
3.1 Definitions ......................................................................................................................................................... 8 
3.2 Abbreviations ..................................................................................................................................................... 9 
4 Overall Architecture ............................................................................................................................... 11 
4.1 O-RAN Functions Definitions ......................................................................................................................... 12 
4.2 Degree of Openness ......................................................................................................................................... 13 
4.3 Decoupling of Hardware and Software ............................................................................................................ 13 
 The O-Cloud............................................................................................................................................... 14 
 Key O-Cloud Concepts .............................................................................................................................. 15 
 O-Cloud Platform Capabilities ................................................................................................................... 19 
4.4 O-Cloud Multi-Site Networking ...................................................................................................................... 19 
 O-Cloud and Transport Network Shared Connectivity Information .......................................................... 20 
5 Deployment Scenarios:  Common Considerations ................................................................................. 21 
5.1 Mapping Logical Functionality to Physical Implementations ......................................................................... 21 
 Technical Constraints that Affect Hardware Implementations................................................................... 21 
 Service Requirements that Affect Implementation Design ........................................................................ 21 
 Rationalization of Centralizing O-DU Functionality ................................................................................. 22 
5.2 Performance Aspects ....................................................................................................................................... 25 
 User Plane Delay ........................................................................................................................................ 25 
5.3 Hardware Acceleration and Acceleration Abstraction Layer (AAL) ............................................................... 28 
 Accelerator Deployment Model ................................................................................................................. 28 
 Acceleration Abstraction Layer (AAL) Interface ....................................................................................... 29 
 Accelerator Management and Orchestration Considerations ..................................................................... 29 
5.4 Cloud Considerations ....................................................................................................................................... 29 
 Networking requirements ........................................................................................................................... 29 
5.4.1.1 Support for Multiple Networking Interfaces ................................................................................... 30 
5.4.1.2 Support for High Performance N-S Data Plane .............................................................................. 30 
5.4.1.3 Support for High-Performance E-W Data Plane ............................................................................. 30 
5.4.1.4 Support for Service Function Chaining .......................................................................................... 31 
5.4.1.5 Support for VLAN based networking ............................................................................................. 31 
5.4.1.6 Support for O-Cloud Gateways to connect O-Cloud Sites with external Transport Networks ....... 32 
 Assignment of Acceleration Resources ...................................................................................................... 32 
 Real-time / General Performance Feature Requirements ........................................................................... 32 
5.4.3.1 Host Linux OS ................................................................................................................................ 32 
5.4.3.1.1 Support for Pre-emptive Scheduling .................................................................................. 32 
5.4.3.2 Support for Node Feature Discovery .............................................................................................. 33 
5.4.3.3 Support for CPU Affinity and Isolation .......................................................................................... 33 
5.4.3.4 Support for Dynamic HugePages Allocation .................................................................................. 33 
5.4.3.5 Support for Topology Manager ...................................................................................................... 33 
5.4.3.6 Support for Scale In/Out ................................................................................................................. 34 
5.4.3.7 Support for Device Plugin .............................................................................................................. 34 
5.4.3.8 Support for Direct IRQ Assignment ............................................................................................... 35 
5.4.3.9 Support for No Over Commit CPU ................................................................................................ 35 
5.4.3.10 Support for Specifying CPU Model ................................................................................................ 35 
 Storage Requirements ................................................................................................................................ 35 

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
3 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
 Notification Subscription Framework ........................................................................................................ 35 
5.4.5.1 O-Cloud Notification Subscription Requirements .......................................................................... 35 
5.5 Sync Architecture ............................................................................................................................................ 36 
 Cloud Platform Time Synchronization Architecture .................................................................................. 36 
5.5.1.1 Edge Cloud Site Level – LLS-C3 Synchronization Topology ........................................................ 37 
5.5.1.1.1 LLS-C3 Synchronization Topology Edge Site Time Synchronization Architecture .......... 37 
5.5.1.1.2 LLS-C3 Synchronization Topology Edge Site Requirements ............................................ 38 
5.5.1.1.2.1 Software .................................................................................................................................... 38 
5.5.1.1.2.2 Hardware ................................................................................................................................... 38 
5.5.1.2 Edge Cloud Site Level – LLS-C1 Synchronization Topology ........................................................ 38 
5.5.1.2.1 LLS-C1 Synchronization Topology Edge Site Time Synchronization Architecture .......... 38 
5.5.1.2.2 LLS-C1 Synchronization Topology Edge Site Requirements ............................................ 40 
5.5.1.2.2.1 Software .................................................................................................................................... 40 
5.5.1.2.2.2 Hardware ................................................................................................................................... 40 
 Loss of Synchronization Notification ......................................................................................................... 40 
5.6 Operations and Maintenance Considerations ................................................................................................... 41 
5.7 Transport Network Architecture ...................................................................................................................... 42 
 Fronthaul Gateways ................................................................................................................................... 42 
5.8 Overview of Deployment Scenarios ................................................................................................................ 43 
6 Deployment Scenarios and Implementation Considerations .................................................................. 44 
6.1 Scenario A ....................................................................................................................................................... 44 
 Key Use Cases and Drivers ........................................................................................................................ 44 
6.2 Scenario B ........................................................................................................................................................ 44 
 Key Use Cases and Drivers ........................................................................................................................ 46 
6.3 Scenario C ........................................................................................................................................................ 46 
 Key Use Cases and Drivers ........................................................................................................................ 47 
 Scenario C.1, and Use Case and Drivers .................................................................................................... 47 
 Scenario C.2, and Use Case and Drivers .................................................................................................... 48 
6.4 Scenario D ....................................................................................................................................................... 50 
6.5 Scenario E ........................................................................................................................................................ 50 
 Key Use Cases and Drivers ........................................................................................................................ 51 
 Scenario E.1 vO-DU with O-RU ................................................................................................................ 51 
6.6 Scenario F ........................................................................................................................................................ 51 
 Key Use Cases and Drivers ........................................................................................................................ 52 
6.7 Scenarios of Initial Interest .............................................................................................................................. 52 
7 Appendix A (informative):  Extensions to Current Deployment Scenarios to Include NSA ................. 52 
7.1 Scenario A ....................................................................................................................................................... 53 
7.2 Scenario B ........................................................................................................................................................ 53 
7.3 Scenario C ........................................................................................................................................................ 53 
7.4 Scenario C.2 ..................................................................................................................................................... 53 
7.5 Scenario D ....................................................................................................................................................... 54 
 
Table of Figures  
Figure 1:  Relationship of this Document to Scenario Documents and O -RAN Management Documents ........................ 5 
Figure 2:  Major Components Related to the Orchestration and Cloudification Effort  ...................................................... 6 
Figure 3:  Example of Tiered Clouds/mapped to O-Clouds and Sites ................................................................................ 7 
Figure 4: High Level Architecture of O-RAN .................................................................................................................. 12 
Figure 5:  Logical Architecture of O-RAN ....................................................................................................................... 12 
Figure 6:  Decoupling, and Illustration of the O-Cloud Concept ...................................................................................... 13 
Figure 7:  Relationship between RAN Functions and Demands on Cloud Infrastructure and Hardware  ......................... 15 
Figure 8: Key Components Involved in/with an O-Cloud ................................................................................................ 15 
Figure 9: Example of O-Cloud Resources mapped to O-Cloud Nodes and O-Cloud Networks in an O-Cloud Node 
Cluster ............................................................................................................................................................................... 17 
Figure 10: Edge O-Cloud Site deployment example with an O-Cloud Site Network Fabric (Hub) ................................. 18 
Figure 11: Edge O-Cloud Site deployment example without an O-Cloud Site Network Fabric (Single servers) ............ 18 
Figure 12:  Simple Centralization of O-DU Resources .................................................................................................... 23 
Figure 13:  Pooling of Centralized O-DU Resources........................................................................................................ 23 

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
4 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
Figure 14:  Comparison of Merit of Centralization Options vs. Number of Cell Sites in a Pool  ...................................... 24 
Figure 15:  Major User Plane Latency Components, by 5G Service Slice and Function Placement  ................................ 25 
Figure 16: Hardware Abstraction Considerations ............................................................................................................. 28 
Figure 17: Accelerator APIs/Libraries in Container and Virtual Machine Impl ementations ............................................ 29 
Figure 18:  Illustration of the Network Interfaces Attached to a Pod, as Provisioned by Multus CNI  ............................. 30 
Figure 19:  Illustration of the Userspace CNI Plugin ........................................................................................................ 31 
Figure 20:  Example Illustration of Two NUMA Regions ............................................................................................... 34 
Figure 21: O-Cloud Notification Framework Architecture .............................................................................................. 36 
Figure 22: Edge Cloud Site Time Sync Architecture for LLS-C3 .................................................................................... 37 
Figure 23: Hyperconverged Edge Cloud Time Sync Architecture for L LS-C3 ................................................................ 38 
Figure 24: Edge Cloud Site Time Sync Architecture for LLS-C1 .................................................................................... 39 
Figure 25: Hyperconverged Edge Cloud Time Sync Architecture for LLS -C1 ................................................................ 40 
Figure 26: vO-DU Subscribes to PTP Notification .......................................................................................................... 41 
Figure 27:  High-Level Comparison of Scenarios ............................................................................................................ 43 
Figure 28:  Scenario A ...................................................................................................................................................... 44 
Figure 29:  Scenario B – NR Stand-alone ......................................................................................................................... 45 
Figure 30: Scenario B – MR-DC (inter-RAT NR and E-UTRA) regardless of the Core Network type (either EPC or 
5GC) ................................................................................................................................................................................. 45 
Figure 31:  Scenario C ...................................................................................................................................................... 46 
Figure 32:  Treatment of Network Slices:  MEC for URLLC at Edge Cloud, Centralized Control, Single vO -DU ........ 47 
Figure 33:  Scenario C.1 ................................................................................................................................................... 48 
Figure 34:  Treatment of Network Slice: MEC for URLLC at Edge Cloud, Separate vO-DUs ....................................... 49 
Figure 35:  Single O-RU Being Shared by More than One Operator ............................................................................... 49 
Figure 36:  Scenario C.2 ................................................................................................................................................... 50 
Figure 37:  Scenario D ...................................................................................................................................................... 50 
Figure 38:  Scenario E ...................................................................................................................................................... 51 
Figure 39: Scenario E.1 .................................................................................................................................................... 51 
Figure 40:  Scenario F ....................................................................................................................................................... 52 
Figure 41:  Scenario A, Including NSA ............................................................................................................................ 53 
Figure 42:  Scenario C, Including NSA ............................................................................................................................ 53 
Figure 43:  Scenario C.2, Including NSA ......................................................................................................................... 54 
Figure 44:  Scenario D, Including NSA ............................................................................................................................ 54 
Table of Tables 
Table 1:  Service Delay Constraints and Major Delay Contributors ................................................................................. 26 
Table 2:  Cardinality and Delay Performance for Scenario B........................................................................................... 46 
Table 3:  Cardinality and Delay Performance for Scenario C........................................................................................... 47 
Table 4:  Cardinality and Delay Performance for Scenario C.1........................................................................................ 48 
 

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
5 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
1 Scope  
This Technical Report has been produced by the O-RAN Alliance. 
The contents of the present document are subject to continuing work within O -RAN and may change following formal 
O-RAN approval. Should O-RAN modify the contents of the present document, it will be re-released by O-RAN with an 
identifying change of release date and an increase in version number as follows: 
Version x.y.z 
where: 
x the first digit  is incremented for all changes of substance, i.e. technical enhancements, corrections, updates, 
etc. (the initial approved document will have x=01). 
y the second digit is incremented when editorial only changes have been incorporated in the document. 
z the third digit included only in working versions of the document indicating incremental changes during the 
editing process. 
1.1 Context; Relationship to Other O-RAN Work 
This document introduces and examines different scenarios and use cases for O -RAN deployments of Network 
Functionality into Cloud Platforms , O -RAN Cloudified NFs  and O-RAN Physical NFs .  D eployment scenarios are 
associated with meeting customer and service requirements, while considering technological constraints and the need to 
create cost-effective solutions. It will also reference management considerations covered in more depth elsewhere.  
The following O-RAN documents will be referenced (see Section 5.6): 
• OAM architecture specification [8] 
• OAM interface specification (O1) [9] 
• O-RAN Architecture Description [10] 
The details of implementing each identified scenario will be covered in separate Scenario documents, shown in green in 
Figure 1.   
 
Figure 1:  Relationship of this Document to Scenario Documents and O-RAN Management Documents 
This document also draws on some other work from other O-RAN working groups, as well as sources from other industry 
bodies.   
1.2 Objectives  
The O-RAN Alliance seeks to improve RAN flexibility and deployment velocity, while at the same time reducing the 
capital and operating costs through the adoption of cloud architectures. The structure of the Orchestration and 
Cloudification work is shown gr aphically below.  This document focuses on the Cloudification deployment aspects as 
indicated.  
Scenario 
Reference 
Design
…
Cloud Architecture 
and Deployment 
Scenarios
OAM 
Architecture
OAM Interface 
Specification 
Management 
documents
Scenario  
Reference 
Design
Scenario  
Reference 
Design
O-RAN 
Architecture 
Description
Architecture 
documents

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
6 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
Editor’s note: O-RU cloudification and O-RU AAL are future study items.  
  
Figure 2:  Major Components Related to the Orchestration and Cloudification Effort 
A key principle is the decoupling of RAN hardware and software for all components including near-RT RIC, O-CU (O-
CU-CP and O -CU-UP), O -DU, and O -RU, and the deployment of software c omponents on commodity server 
architectures supplemented with programmable accelerators where necessary.   
Key characteristics of cloud architectures which we will reference in this document are:  
a) Decoupling of hardware from software.  This aims to improve flexibility and choice for operators by decoupling 
selection and deployment of hardware infrastructure from software selection,  
b) Standardization of hardware specifications across software implementations , to simplify physi cal deployment 
and maintenance.  This aims to promote the availability of a multitude of software implementation choices for a 
given hardware configuration.   
c) Sharing of hardware.  This aims to promote the availability of a multitude of hardware implementation choices 
for a given software implementation. 
d) Flexible instantiation and lifecycle management through orchestration automation.   This aims to reduce 
deployment and ongoing maintenance costs by promoting simplification and automation throughout the 
hardware and software lifecycle through common chassis specifications and standardized orchestration 
interfaces.   
This document will define various deployment scenarios that can be supported by the O -RAN specifications and are of 
either current or relatively near-term interest.  Each scenario is identified by a specific grouping of functionality (Tiered 
Clouds) at different key locations (Cell Site, Edge Cloud, Regional Cloud and Central Cloud, which will be defined 
shortly), and an identification of whether functionality  at a given location is provided by an O-RAN Physical NF based 
solution where software and hardware are tightly integrated and sharing a single identity , or by a n O -RAN cloud 
infrastructure architecture (O-Cloud) that meets the above requirements. 
The scope of this work clearly includes supporting all 5G technologies, i.e. E-UTRA and NR with both EPC-based Non-
Standalone (NSA) and 5GC architectures. This implies that cloud/orchestration aspects of NSA (E -UTRA) are also  
supported. However, this version primarily addresses 5G SA deployments. 
This technical report examines the constraints that drive a specific solution, and discuss the hierarchical properties of each 
solution, including a rough scale of the size of each Tiered Cloud and a sense of the number of sub clouds expected to be 
served by a higher tiered cloud.  Figure 3 shows as example of  how multiple Cell Sites feed into a smaller number of 
Edge Clouds, and how in turn multiple Edge Clouds feed into a Regional Cloud that can feed into a Central Cloud.  For 
a given scenario, the Logical Functions are distributed in a certain way among each type of Tiered C loud, and the 
“cardinality” of the different functions will be discussed. 
The present document describes that the Cloud Infrastructure is deployed as managed O -Cloud(s). Tiered Cloud is a 
conceptual construct of grouped functions in an O -RAN. Their physical manifestation is realized in O -Clouds on Cloud 
Sites. Cell Sites connect into the Cloud Site O -Cloud Resources through the Fronthaul network. Cloud Sites and Cell 
Sites could be collocated at the same site location or separately on their own individual site locations. The document also 
Orchestration
S/W
H/W
Cloud stack  ( Containers/VMs, OS, Cloud Mgmt. )
O-CU O-DU O-RU
Centralized CU/DU
(C-RAN)
CU/DU split Distributed 
CU/DU 
(D-RAN)
Blackbox
BBU
Multitude of deployment
models: CloudRAN, 
CU-DU split, 
dRAN on whitebox or DC
All RAN modulesFlexible
Orch.
Inventory,
Discovery, 
Registration
Policy,
Metrics
Support 10,000s
of distributed
cloud sites
Multitude of silicon
accelerators
Common LCM
mechanisms
across VNF &
PNFs
ASIC
Cloudification
AAL AAL AAL

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
7 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
describes how the O -Cloud Resources are used in O -Cloud Node Clusters that execute the Network Function 
Deployments. All of these are orchestrated from the Service Management and Orchestration framework (SMO).  
This has implications on the processing power needed in each type of cloud, as well as implications on the environmental 
requirements.  This document will also discuss considerations of hardware chassis and components that are reasonable in 
each scenario, and the implications of managing such a cloud.   
 
 
 
Figure 3:  Example of Tiered Clouds/mapped to O-Clouds and Sites 
Additional major areas for this document are listed below:   
• Mapping of logical functions to physical elements and locations, and implications of that mapping. 
• High-level assessment of critical performance requirements, and how that influences architecture. 
• Processor and accelerator options (e.g., x86, FPGA, GPU ).  In order to determine whether a Network F unction 
is a candidate for openness, there need s to be the possibility to have m ultiple suppliers of software for given 
hardware, and multiple sources of required chip/accelerators.   
• The Hardware Abstraction Layer, aka “Acceleration Abstraction Layer” nee ds to be addressed in light of 
various hardware options that could be used. 
• Cloud infrastructure makeup.  This includes considerations such as: 
• Deployments are allowed to use VMs, Containers in VMs, or just Containers.  
• Multiple Operating Systems are expec ted to be supported; e.g., open source Ubuntu, CentOS Linux , or 
Yocto Linux-based distributions, or selected proprietary OSs.   
• Management of a cloudified RAN introduces some new management considerations, because the mapping 
between Network Functionality and cloud platforms can be done in multiple ways, depending on the scenario 
that is chosen.  Thus, management of aspects that ar e related to platform aspects rather than RAN functional 
aspects need to be designed with flexibility in mind from the start.  For example, logging of physical functions, 
scale out actions, and survivability considerations are affected.   
• These management considerations are introduced in this document, but management documents will 
address the solutions. 
• The transport layer will be discussed, but only to the extent that it affects the architecture and design of the 
network.  For example, the chosen L1 technology may affect the performance of transport.  As another example, 
the use of a Fronthaul Gateway will affect economics as well as the placement options of certain Network 
Functions.  And of course, t he existence of L2 switches in a cloud platform deploym ent will be required for 
efficient use of server resources. 
Additional areas could be considered in the future.   
2 References 
The following documents contain provisions which, through reference in this text, constitute provisions of this report. 


                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
8 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
[1] 3GPP TS 38.470, NG-RAN; F1 general aspects and principles. 
[2] 3GPP TR 21.905, Vocabulary for 3GPP Specifications. 
[3] eCPRI Interface Specification V1.2, Common Public Radio Interface:  eCPRI Interface Specification. 
[4] eCPRI Transport Network V1.2, Requirements Specification, Common Public Radio Interface:  
Requirements for the eCPRI Transport Network. 
[5] IEEE Std 802.1CM-2018,  Time-Sensitive Networking for Fronthaul.  
[6] ITU-T Technical Report, GSTR-TN5G - Transport network support of IMT-2020/5G.  
[7] O-RAN WG4, Control, User and Synchronization Plane Specification, Technical Specification.  See 
https://www.o-ran.org/specifications. 
[8] O-RAN WG1, Operations and Maintenance Architecture, Technical Specification.  See https://www.o-
ran.org/specifications. 
[9] O-RAN WG1, Operations and Maintenance Interface Specification, Technical Specification.  See 
https://www.o-ran.org/specifications.  
[10] O-RAN WG1, O-RAN Architecture Description, Technical Specification. See https://www.o-
ran.org/specifications.  
[11] 3GPP TS 28.622, Telecommunication management; Generic Network Resource Model (NRM) Integration 
Reference Point (IRP); Information Service (IS). 
[12] O-RAN WG6, Cloud Platform Reference Design for Deployment Scenario B, Technical Specification.  See 
https://www.o-ran.org/specifications. 
[13] O-RAN WG7 OMAC HAR 0-v01.00 O-RAN White Box Hardware Working Group Outdoor Macrocell 
Hardware Architecture and Requirements (FR1) Specification. 
[14] O-RAN WG1, Use Cases Detailed Specifications – v05.00, Technical Specification. See https://www.o-
ran.org/specifications. 
3 Definitions and Abbreviations 
3.1 Definitions 
For the purposes of the present document, the terms and definitions given in 3GPP TR 21.905 [2] and the following apply. 
A term defined in the present  document takes precedence over the definition of the same term, if any, in 3GPP 
TR 21.905 [2].  
Cell Site This refers to the  location of Radio Unit s (RUs); e.g., placed on same structure as the Radio 
Unit or at the base.  The Cell Site in general will support multiple sectors  and hence multiple 
O-RUs. 
Cloud Infrastructure (CInf) This refers to a set of computation, storage and networking equipment with related software 
that offers physical and/or virtual cloud resources and services into the O -Cloud as an under-
cloud from the Cloud Infrastructure provider organization that could b e operator internal or 
external. The Cloud Infrastructure resources are not addressed by the present document and 
not specified as part of O2ims and O2dms. 
CInf Management This is a vendor or operator software operated by the Cloud Infrastructure provider 
organization. It handles discovery, health and maintenance of the Cloud Infrastructure 
included equipment and its offered physical and logical services that can be distributed over 
multiple Cloud Sites. The Cloud Infrastructure Management are not addressed by the present 
document and not specified as part of O2ims and O2dms. 
Central Cloud This is the highest location tier, that supports virtualized RAN and other functions, and 
provides the centralization of functionality that has the least strict network la tency 
requirements. 

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
9 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
Cloud Site This refers to a physical place that  has Cloud Infrastructure resources that can be used for O -
Clouds and potentially other non O-Cloud resources. 
CIDR Classless Inter-Domain Routing (also known as Subnet Mask) 
Edge Cloud This is a location that supports virtualized RAN functions for multiple Cell Sites, and provides 
centralization of functions for those sites and associated economies of scale.  An Edge Cloud 
might serve a large physical area or a relatively small one  close to its cell sites, depending on 
the Operator’s use case.  However, the sites served by the Edge Cloud must be near enough to 
the O-RUs to meet the network latency requirements of the O-DU functions. 
F1 Interface  The open interface between O-CU and O-DU in this document is the same as that defined by 
the CU and DU split in 3GPP TS 38.473.  It consists of an F1-u part and an F1-c part. 
Managed Element  Refer to the 3GPP TS 28.622 [11] . 
Managed Function  Refer to the 3GPP TS 28.622 [11] 
Network Function The O -RAN Network Functions (O -RAN NFs) are defined in the O -RAN Architecture 
Description [10].  
Regional Cloud This is a location that supports virtualized RAN functions for many Cell Sites in multiple Edge 
Clouds, and provides high centralization of functionality. The sites served by the Regional 
Cloud must be near enough to the O-DUs to meet the network latency requirements of the O-
CU and near-RT RIC.  
O-Cloud This refers to a collection of O-Cloud Resource Pools at one or more location and the software 
to manage Nodes and Deployments hosted on them.  An O-Cloud will include functionality to 
support both Deployment-plane and Management services . The O -Cloud provides a single 
logical reference point for all O-Cloud Resource Pools within the O-Cloud boundary. 
O-RAN Physical NF  A RAN NF software deployed on tightly integrated hardware sharing a single Managed 
Element identity. 
Cloudified NF  A RAN Network Function software that is deployed in the O -Cloud via one or more NF 
Deployments. 
NF Deployment A software deployment on O-Cloud resources that realizes, all or part of, a Cloudified NF. 
PE Provider Edge 
Tiered Cloud The definition of Tiered Clouds is a grouping of O-RAN related functionality called Cell Site, 
Edge Cloud, Regional Cloud and Central Cloud where each tier has increasing latency and 
increasing maximum area covered per tier.  
Note: The Tiered Cloud and its different locations are conceptual and intended to enable discussions 
around functional placement and its requirements in O-RAN. It is not an exact definition, and it 
doesn’t have a direct mapping to the realization of the functionality. 
 
3.2 Abbreviations 
For the purposes of this document, the abbreviations given in 3GPP TR  21.905 [2] and the following apply.  
An abbreviation defined in the present document takes precedence over the definition of the same abbreviation, if any, in 
3GPP TR 21.905 [2]. 
3GPP Third Generation Partnership Project 
5G Fifth-Generation Mobile Communications 
AAL Acceleration Abstraction Layer 
API Application Programming Interface 
ASIC Application-Specific Integrated Circuit  
BBU BaseBand Unit 
BS Base Station 
CI Cloud Infrastructure 
CoMP   Co-Ordinated Multi-Point transmission/reception 
CNF Cloud-Native Network Function  
CNI Container Networking Interface 
CPU Central Processing Unit 

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
10 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
CR Cell Radius 
CU Centralized Unit as defined by 3GPP 
DFT Discrete Fourier Transform 
DL Downlink 
DPDK Data Plan Development Kit  
DMS Deployment Management Services 
DU Distributed Unit as defined by 3GPP 
eMBB enhanced Mobile BroadBand 
EPC Evolved Packet Core 
E-UTRA Evolved UMTS Terrestrial Radio Access 
FCAPS Fault Configuration Accounting Performance Security  
FEC  Forward Error Correction 
FFT Fast Fourier Transform 
FH Fronthaul 
FH GW Fronthaul Gateway 
FPGA Field Programmable Gate Array 
GNSS Global Navigation Satellite System 
GPP General Purpose Processor 
GPS Global Positioning System 
GPU Graphics Processing Unit  
HARQ Hybrid Automatic Repeat ReQuest 
HW Hardware 
IEEE Institute of Electrical and Electronics Engineers 
IM Information Modelling, or Information Model 
IMS Infrastructure Management Services 
IRQ Interrupt ReQuest  
ISA Instruction Set Architecture 
ISD Inter-Site Distance 
ITU International Telecommunications Union 
KPI Key Performance Indicator 
LCM Life Cycle Management 
LDPC  Low-Density Parity-Check 
LLS Lower Layer Split   
LTE Long Term Evolution 
LVM Logic Volume Manager 
MEC Mobile Edge Computing 
mMTC massive Machine Type Communications 
MNO Mobile Network Operator 
NF Network Function 
NFD Node Feature Discovery 
NFVI Network Function Virtualization Infrastructure 
NIC Network Interface Card 
NMS Network Management System  
NR  New Radio 
NSA Non-Standalone 
NTP Network Time Protocol 
NUMA Non-Uniform Memory Access  
NVMe Non-Volatile Memory Express 
O-Cloud O-RAN Cloud Platform 
OCP  Open Compute Project 
O-CU O-RAN Central Unit  
O-CU-CP O-CU Control Plane 
O-CU-UP O-CU User Plane 
O-DU O-RAN Distributed Unit (uses Lower-level Split) 
O-RU O-RAN Radio Unit 
OTII Open Telecom IT Infrastructure 
OWD One-Way Delay 
PCI Peripheral Component Interconnect 
PNF Physical Network Function 
PoE Power over Ethernet 
PoP Point of Presence 
PRTC Primary Reference Time Clock 

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
11 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
PTP Precision Time Protocol 
QoS  Quality of Service  
RAN Radio Access Network 
RAT Radio Access Technology 
RIC RAN Intelligent Controller  
RT Real Time 
RTT Round Trip Time 
RU Radio Unit  
SA Standalone 
SFC Service Function Chaining  
SMO Service Management and Orchestration 
SMP Symmetric MultiProcessing 
SoC System on Chip 
SR-IOV Single Root Input/ Output Virtualization 
SW Software 
TCO Total Cost of Ownership 
TNE Transport Network Element 
TR Technical Report 
TRP Transmission Reception Point 
TS Technical Specification 
TSC (T-TSC) Telecom Slave Clock 
Tx Transmitter 
UE User Equipment 
UL Uplink 
UMTS Universal Mobile Telecommunications System 
UP User Plane 
UPF User Plane Function 
URLLC Ultra-Reliable Low-Latency Communications 
vCPU virtual CPU 
VIM Virtualized Infrastructure Manager 
VM Virtual Machine  
VNF Virtualized Network Function 
vO-CU Virtualized O-RAN Central Unit  
vO-CU-CP Virtualized O-CU Control Plane 
vO-CU-UP Virtualized O-CU User Plane 
vO-DU Virtualized O-RAN Distributed Unit 
4  Overall Architecture  
This section address es the overall architecture in terms of the Network Functions and infrastructure (O-RAN Physical 
NFs, servers, and clouds) that are in scope. Figure 4 provides a high-level view of the O-RAN architecture as depicted in 
[10].  

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
12 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
 
 
Figure 4: High Level Architecture of O-RAN 
4.1 O-RAN Functions Definitions 
This section reviews key O-RAN functions definitions in O-RAN.  
• The O-DU/ O-RU split is defined as using Option 7-2x.  See [7].  
• The O-CU/ O-DU split is defined as using the CU/ DU split F1 as defined in 3GPP TS 38.470 [1].    
This document assumes these two splits.  
Figure 5 shows the logical architecture of O-RAN (as depicted in [10]) with O-Cloud platform at the bottom, where any 
given O-RAN function could be supported by O-Cloud, depending on the deployment scenario.  For example, the figure 
here illustrates a case where the O-RU is implemented as an O -RAN Physical NF, and the other functions within the 
dashed line are supported by O-Cloud.   
 
Figure 5:  Logical Architecture of O-RAN 
OFH CUS-Plane 
F1-c
A1
F1-u
E1
E2
Service Management and Orchestration Framework
Non-Real Time RIC 
Near-Real Time RAN 
Intelligent Controller (RIC)
O-DU
NG-c
NG-u
X2-c
Xn-u
O-Cloud
O2
E2
E2
E2
X2-u
Xn-c
O-CU-CP
O-CU-UP
O-eNB
O1
O-RU
O1
OFH M-Plane
Open 
Fronthaul 
M-Plane

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
13 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
4.2 Degree of Openness 
In theory, every architecture component could be open in every sense imaginable, but in practice it is likely that different 
components will have varying degrees of openness due to economic and other implementation considerations.  Some 
factors are significantly affected by the deployment scenario; for example, what might be viable in an indoor deployment 
might not be viable in an outdoor deployment.   
Increasing degrees of openness for a n O-RAN Physical Network Function or O-RAN Cloudified Network Function(s) 
are: 
A. Interfaces among Network Functions are open; e.g., E2, F1, and Open Fronthaul are used. Therefore, Network 
Functions in different O-RAN Physical NFs/clouds from different vendors can interconnect. 
B. In addition to having open c onnections as described above , the chassis of servers in a cloud are open and can 
accept blades/sleds from multiple vendors.  However, the blades/sleds have RAN software that is not decoupled 
from the hardware. 
C. In addition to having open connections and an open chassis, a specific blade/sled uses software that is decoupled 
from the hardware.  In this scenario, the software could be from one supplier, the blade/sled could be from another, 
and the chassis could be from another.   
Categories A and B have O-RAN Physical NFs/clouds, while Category C is an open solution that we are calling a n O-
Cloud, and is subject to the cloudification discussion and requirements. 
In this document, the degree of openness for each O-RAN Physical NF/cloud can vary by scenario. The question of which 
Network Functions should be split vs. combined, and the degree of openness in each one, is addressed in the discussion 
of scenarios.  
4.3 Decoupling of Hardware and Software  
Editor’s note: O-RU AAL is a future study item.  
There are three layers that we must consider when we discuss decoupling of hardware and software:  
• The hardware layer, shown at the bottom in Figure 6.  (In the case of a VM deployment, this maps basically to 
the ETSI NFVI hardware sub-layer.) 
• A middle layer that includes Cloud Stack functions as well as Acceleration Abstraction Layer functions.  (In the 
case of a VM deployment, these map to the ETSI NFVI virtualization sub-layer + VIM.) 
• A top layer that supports the virtual RAN functions.  
Each layer can come from a different supplier.  The first aspect of decoupling has to do with ensuring that a Cloud Stack 
can work on multiple suppliers’ hardware; i.e., it does not require vendor-specific hardware.   
The second aspect of decoupling has to do with ensuring that a Cloud Platform can support RAN virtualized functions 
from multiple RAN software suppliers.  If this is possible, then we say that the Cloud Platform (which includes the 
hardware that it runs on) is an O-RAN Cloud Platform, or “O-Cloud”.  See Figure 6 below.   
 
Figure 6:  Decoupling, and Illustration of the O-Cloud Concept 
Cloud stack  ( Containers/VMs, OS, Cloud Mgmt. )
O-CU O-DU O-RU
ASIC Hardware
O-Cloud
AAL AAL AAL

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
14 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
 The O-Cloud  
The general definition of the O-Cloud Platform includes the following characteristics: 
The O-Cloud Platform is a set of hardware and software components that provide cloud computing capabilities and 
services to execute RAN network functions. 
The O-Cloud Platform hardware include s compute, networking and storage components, and may also include 
various acceleration technologies required by the RAN network functions to meet their performance objectives.  
The O-Cloud Platform software exposes open and well -defined APIs that enable the orchestration and management 
of the NF Deployment’s life cycle. 
The O-Cloud Platform software exposes open and well -defined APIs that enable the orchestration and management 
of the O-Cloud. 
The O-Cloud Platform software is decoupled from the O-Cloud Platform hardware (i.e., it can typically be sourced 
from different vendors). 
The management aspects of the O -Cloud platform are discussed in 5.6. The scope of this document includes listing 
specific requirements of the O-Cloud Platform to support orchestration and execution of the various O -RAN Network 
Functions. 
An example of a O-Cloud Platform is an OpenStack and/or a Kubernetes Cluster deployment on a set of COTS servers 
(including FPGA and GPU cards), interconnected by a spine/leaf networking fabric.  
There is an important interplay between specific virtualized RAN functions and the hardware that is needed to meet 
performance requirements and to s upport the functionality economically.  Therefore, a hardware/ cloud platform 
combination that can support, say, a vO -CU function might not be appropri ate to adequately support a vO-DU function.  
When RAN functions are combined in different ways in each sp ecific deployment scenario, these aspects must be 
considered. 
Such infrastructure requirements of the Cloudified NFs and/or their constituent NF Deployments are among the Service 
Management and Orchestration (SMO) considerations for the homing decision. Th e SMO is responsible to make the 
homing decision, which results in the SMO selection of the appropriate O-Cloud Node Cluster(s) matching the requested 
capabilities for the NF Deployment(s) and makes the determination of the specific Deployment Management S ervice 
(DMS) that the SMO deems adequate for an NF Deployment.  
The O-Cloud DMS(s) is responsible to decide on the placement of the workloads for an NF Deployment, internally inside 
the O-Cloud Node Cluster(s) based on the SMO’s homing decision and using other NF Deployment requirements that it 
receives through the O2dms. 
Below is a high -level conceptual example of how different accelerators, along with their associated cloud capabilities, 
can be required for different RAN functions.  Although we do not specify any particular hardware requirement or cloud 
capability here, we can note some general themes.  For example, any RAN function that involves real-time movement of 
user traffic will require the O-Cloud Platform to control for delay and jitter, which may in turn require features such as 
real-time OSs, avoidance of frequent interrupts, CPU pinning, etc.   

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
15 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
 
Figure 7:  Relationship between RAN Functions and Demands on Cloud Infrastructure and Hardware  
Please note that any cloud that has features required for a given function (e.g., for O -DU) can also support functions that 
do not require such features.  For example, a cloud that can support O -DU can also support functions such as O-CU-CP.  
 
 
 Key O-Cloud Concepts  
Figure 8 illustrates key components of an O-Cloud and its management. 
 
 
 
Figure 8: Key Components Involved in/with an O-Cloud 
Key terms in this figure are defined below: 
O-Cloud 
 ervice  anagement   Orchestration  rame or 
Infrastructure  anagement  ervices   ogical    istributed     
 eployment  anagement  ervices   ogical    istributed    ..n 
 eployment  lane   ogical N   eployment N   eployment N   art  eployment
Resource  ool 
  -Acc
  -Acc
  -Acc
 ite Net or   abric
 ite Net or 
 ite Net or 
 ite Net or 
 ate ay
 R 
 R 
 R 
O-Cloud 
…
 istributed 
across 
multiple 
locations
O-Cloud  ite O-Cloud  ite O-Cloud  ite
I  
   
N   eployment
Resource  oolm
Compute
Compute
Compute
Resource  ool 
Compute
Compute
Compute
Node Cluster 
Node  roup
Node
Node  roup N 
NodeNode
Node Cluster Net or 
Node  roup
Node
Node  roup N 
NodeNode
Node Clustern
Node  roup
Node
Node  roup N 
NodeNode
Node Cluster Net or 
Node  roup
Node
Node  roup N 
NodeNode
Node Cluster 
NodeNodeNode
Node Cluster Net or 
Resource  ool 
Compute
Compute
Compute
 ite Net or   abric
 ite Net or 
 ite Net or 
O ims O dms
         n
Resource  ool 
 torage
 torage
 torage
Node Cluster 
NodeNodeNode
Node Cluster Net or 
     
 nspecified 
Resource  ool
Compute  torage 
 torage Compute 
Resource  ool 
Compute Including:
- is s
- CI cardsCompute Including:
- is s
- CI cards

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
16 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
• The SMO is defined in [10]. 
• An O-Cloud refers to a collection of O-Cloud Resources, Resource Pools and O-Cloud Services at one or more 
O-Cloud Sites including the software to manage O -Cloud Resource provisioning, Nodes, Clusters and 
Deployments hosted on them.  An O -Cloud will include functionality to support both Deployment -plane (aka. 
user-plane) and Management services. The O -Cloud provides a single logical reference point for all O -Cloud 
Resources, Resource Pools and Services within the O-Cloud boundary, i.e. for the distributed O-Cloud. 
• An O-Cloud Site refers to a set of O-Cloud Resources at a Cloud Site with a geographical location. The size of 
the O-Cloud Site can be from a single to thousands of O-Cloud Resources. O-Cloud Resources are generally 
interconnected through one or more O-Cloud Site Network Fabrics that are the demarcation for direct O-Cloud 
Site internal L2 switching.  Multiple O -Cloud Sites can be in terconnected into a distributed O -Cloud which 
generally would require bridging, routing or stitching on any other networking layer in between each O -Cloud 
Site and its respective external transport network attachment point.  Note: Very small O-Cloud Sites with just a 
few O-Cloud Resources can be directly connected to external networks e.g. fronthaul and backhaul networks , 
without an O-Cloud Site Network Fabric. 
• The O2 Interfaces are the interfaces associated with a collection of O-Cloud Services that are provided by the 
O-Cloud platform to the SMO. The services are categorized into two logical groups: (i) Infrastructure 
Management Services (IMS), which include the subset of O2 functions that are responsible for deploying and 
managing cloud infrastructure. (ii) Deployment Management Services (DMS), which include the subset of O2 
functions that are responsible for managing the lifecycle of virtualized/containerized deployments on the cloud 
infrastructure. The O2 interfaces associated with the O -Cloud Infrastructure and Deployment Management 
Services will be specified in the upcoming O2 specification. Any definitions of SMO functional elements needed 
to consume these services shall be described in OAM architecture. Further details of these key concepts and how 
they relate to each other can be found in the O-RAN O2 Interface General Aspects and Principles (GAnP). 
• O-Cloud IMS related concepts and views of the Cloud Infrastructure 
o An O-Cloud Resource represent a unit of defined capabilities and characteristics within an O -Cloud 
Cloud Site that can be provisioned and used for the O -Cloud Deployment Plane . There are some 
different sorts of O -Cloud Resources e.g. Compute, HW-Accelerator, Storage, Gateway and Site 
Network Fabric. Note: Exact classes of O -Cloud Resources are FFS and needs alignment to existing 
other specifications e.g. GAnP and IMS Interface Specification. 
o An O-Cloud Resource Pool is a collection of O-Cloud Resources with homogeneous capabilities and 
characteristics as defined by the operator within an O-Cloud Site.  
o The Unspecified O-Cloud Resource Pool is the collection of O -Cloud Resources that are exposed in 
the O-Cloud IMS inventory without a classification or being placed in any O -Cloud Resource Pool. 
Note: Exact classes of O-Cloud Resources are FFS. 
o An O-Cloud Site Network Fabric is an O-Cloud Resource that connects the O-Cloud Resources that 
can connect to other O-Cloud Resources in an O-Cloud Site. 
o An O-Cloud Site Network is a provisioned Network Resource with its configured defined capabilities 
and characteristics out of an O-Cloud Site Network Fabric. 
• O-Cloud DMS related concepts and views of the O -Cloud Resources that are created/updated through IMS 
provisioning 
o O-Cloud Deployment Plane is a logical construct representing the O-Cloud Nodes, O-Cloud Networks 
and O-Cloud Node Clusters which are used to create NF Deployments. The O-Cloud Deployment Plane 
is created using IMS provisioned O-Cloud Resources from O-Cloud Resource Pools and O-Cloud Site 
Network Fabrics. 
o NF Deployment, see term definition in 3.1. 
o An O-Cloud Node is a network connected (physical and/or logical) computer or a network connection 
terminating function. An O-Cloud Node can be provisioned by the IMS into the O-Cloud Node Cluster. 
O-Cloud Nodes are typically comprised of physical and/or logical CPUs, Memories, Storages, NICs, 
HW Accelerators, etc. and a loaded Operating System with relevant Cluster SW . The O-Cloud Node 
software discovers, abstracts and exposes the IMS -assigned O-Cloud Resources or partitions of them  
as O-Cloud Deployment Plane constructs.  Note that an O-Cloud Node could also exist as a stand-alone 
O-Cloud Node. 
o An O-Cloud Node Cluster is a collection of O -Cloud Nodes that work in concert with each other , 
through a set of interconnecting O -Cloud Node Cluster Networks. The O -Cloud Nodes Operating 
System and Cluster SW discover its capabilities, characteristics and initial parameters with additional 
configuration done through the IMS. The cluster concepts w ill be further specified in the GAnP 
document. 
o An O-Cloud Node Cluster Network is an O-Cloud Site Network assigned to an O-Cloud Node Cluster. 
o An O-Cloud Node Group is a set of O-Cloud Nodes within an O-Cloud Node Cluster that are to be 
treated as equal by some aspects e.g. the O-Cloud Node Cluster scheduler. These O-Cloud Nodes are 
interconnected through the set of O-Cloud Node Cluster Networks and an optional set of O-Cloud Node 

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
17 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
Group Networks. These O-Cloud Nodes would commonly have similar capabilities and characteristics 
exposed from their set of used computational, storage and networking Resources. 
o An O-Cloud Node Group Network is an O-Cloud Site Network assigned to an O-Cloud Node Group 
in a O-Cloud Node Cluster. 
 
Figure 9 illustrates an example of how O -Cloud Resources and parts of O -Cloud Resources are mapped into O -Cloud 
Nodes and O-Cloud Node Clusters which is done through the O-Cloud IMS Provisioning services. The depicted O-Cloud 
Node Groups and their related O-Cloud Node Group Networks are dashed to indicate that this grouping level is optional. 
  
 
Figure 9: Example of O-Cloud Resources mapped to O-Cloud Nodes and O-Cloud Networks in an O-Cloud 
Node Cluster 
An O-Cloud Resource Pool comprises one or more O-Cloud Resources, each with one or more network connections and 
optionally, one or more internal HW accelerators and storage devices. The O-Cloud Site Network Fabric Resources may 
provide connectivity between the pooled O-Cloud Resources of O-Cloud Compute, O-Cloud HW-Accelerator, O-Cloud 
Storage and O-Cloud Gateway Resources. and to the O-RU through an O-RAN 7.2x compliant Fronthaul transport. The 
O-Cloud Gateways may bridge or stitch O -Cloud Site Networks across multiple O -Cloud Resource Pools in different 
Cloud Sites inside a distributed O-Cloud. The O -Cloud Site Network Fabrics are managed by the Infrastructure 
Management Services (IMS) described earlier. Interconnection of the different O-Cloud Sites in a distributed O-Cloud is 
typically done through an externally provisioned and managed WAN Transport, but could also be done through Cloud 
Infrastructure internally managed WAN Transport.  Figure 10 shows an example of the architecture and usage of one or 
more O-Cloud Compute Resource Pool s, comprising multiple servers  interconnected over an O -Cloud Site Network 
Fabric.  
  
 
O-Cloud
Infrastructure  anagement  ervices   ogical    istributed 
 eployment  anagement  ervices   ogical    istributed    ..n 
 eployment  lane   ogical N   eployment N   eployment N   art  eployment
 ite Net or   abric  ate ay
 R 
 R 
 R 
O-Cloud  ite
Node Cluster
Node  roup
Node
Node  roup N 
Node
Node Cluster Net or 
Node  roup
Node
Node  roup N 
Node Node
 ervice  anagement   Orchestration  rame or 
O ims O dms
     
Resource  ool 
 torage
 torage
 torage
Resource  ool 
Compute Including:
- is s
- CI cards
Resource  ool 
  -Acc
  -Acc
Compute Including:
- is s
- CI cards
Resource  ool 
Compute
Compute
Compute
 ite Net or 
 ite Net or 
 ite Net or 
Node Cluster Net or 
 ite Net or 

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
18 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
 
 
 
Figure 10: Edge O-Cloud Site deployment example with an O-Cloud Site Network Fabric (Hub) 
An O-Cloud Resource Pool may also comprise one or a set of single servers without any associated O-Cloud Site 
Network Fabric, e.g., infrastructure deployed at a cell site. In such a scenario where an O-Cloud Site Network Fabric is 
not present, the O-Cloud Compute Resources may be directly connected to the O-RU through an O-RAN compliant 
front haul connection and to an externally provisioned backhaul or midhaul Transport. Figure 11 shows an example of 
the architecture and usage of an O-Cloud Compute Resource Pool in such a configuration without the O-Cloud Site 
Network Fabric. 
 
 
Figure 11: Edge O-Cloud Site deployment example without an O-Cloud Site Network Fabric (Single servers) 
                 
   
O-Cloud  ite
Net or   abric
 eaf  eaf  eaf
 pine  opt 
                       
         
                  
                
                
I  
O-Cloud Resource  ool s 
 ac haul or
midhaul
transport ronthaul
transport
O-RAN
 . x   
AA 
O 
O ims
 eployments
nRT-RIC,
vO-C , vO-  xN xN xN xN xN xN 
      
O-Cloud
Node 
Cluster
O-Cloud
Node
Cluster
O dms
Node
 erver
Acc NIC
O 
Node
 erver
Acc NIC
O Node
 erver
Acc NIC
O 
Node
 erver
Acc NIC
O Node
 erver
Acc NIC
O 
Node
 erver
Acc NIC
O 
                 
O-Cloud Resource  ool s 
O-Cloud
Node 
Cluster
O-Cloud
Node
Cluster
   
                       
         
                                
I  
 ac haul or
midhaul
transport
 ronthaul
transport
O-RAN
 . x   
O 
O ims
 eployments
nRT-RIC,
vO-C , vO-  xN xN xN xN xN xN 
O dms
                
Node
 erver
Acc NIC
O Node
 erver
Acc NIC
O Node
 erver
Acc NIC
O 
AA 
      

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
19 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
The requirements on the O -Cloud Site Network Fabric such as  clock/sync requirements , latency and jitter 
recommendations shall be described in a future version of this document. We note that the architecture of a regional cloud 
may be similar to that of an edge cloud but may not include some requirements such as time source.  
 O-Cloud Platform Capabilities 
An O-Cloud Platform can provide a degree of automation and autonomous handling of its functionalities (including DMS 
and IMS). In such cases, the O -Cloud Platform can detect, trigger and handle autonomously and without any SMO 
intervention, tasks with a certain level of complexity. An example is an O-Cloud Platform based on Kubernetes® (as 
specified by the CNCF®), where th is O-Cloud Platform is capable of placing the workloads (for NF Deployments) on 
suitable O-Cloud Nodes based on deployment artifacts and orchestration policies, as well as detect, trigger and execute 
O-Cloud Node self -repair, or to execute autonomously a hor izontal auto -scaling of running workloads for the NF 
Deployments based on the scaling data made available in the deployment artifacts.   
 
Examples of the main tasks that the DMS handles based on the information received over O2dms, includes (non -
exhaustive list): 
- The placement of the workloads for O-RAN NF Deployments, internally within the O-Cloud Node Cluster(s); 
- The LCM of the respective workloads, including: 
o The allocation of resources inside the O-Cloud Node Clusters that the workloads will run on,  
o the configuration of the allocated resources as needed for the deployment (e.g., IP addresses, connection 
points, etc). 
o The execution, either autonomously/automati cally or on -demand, of any necessary workload LCM 
operations within the allocated O-Cloud Node Cluster, such as scaling out/in, or self-healing.  
o The move of NF Deployment workloads in different O -Cloud Nodes within the same O -Cloud Node 
Cluster when error s in the allocated resources fail to meet the expected service levels previously 
indicated over O2dms, 
o The termination of the deployment of NF Deployments when requested by SMO over O2dms,  
- The u pdates to the O-Cloud inventory with latest status information  about O -Cloud resources used for NF 
Deployment workloads that it lifecycle manages.   
  
Example of main tasks (non-exhaustive list) that the IMS handles based on the information received over O2ims:  
- Provisioning requests for the O -Cloud Node Clusters, with an allocation of resources and their configuration. 
The main task is to keep track of available resources, their capabilities, their capacities and to allocate them into 
the SMO requested Node Clust ers with appropriate configuration that makes the Node Clusters usable for the 
NF Deployments.  
o The Node Cluster and Resource capabilities and capacities, along with the resulting mapping of Node 
Clusters to Resources, are exposed over the O2ims inventory service. 
- Fault management of the O -Cloud Node Clusters and O -Cloud Resources that will limit the O -Cloud total 
capabilities and capacities. Alarms are available for SMO consumption over the O2 ims interface with relevant 
information about the faults.  
- Performance management and reporting of the O -Cloud Node Clusters and O -Cloud Resources. The 
measurements are available for SMO consumption over the O2ims interface with relevant information about the 
measurements. 
- O-Cloud inventory reporting, exposed via O2ims inventory services, of the O -Cloud Sites, Deployment 
Management Services, Node Clusters and Resources including their capabilities, capacities , allocations and 
availabilities that enables the SMO to understand how requested allocations have been met by the O -Cloud and 
what the O-Cloud available capabilities and capacities are. O-Cloud life cycle management, where the O-Cloud 
infrastructure management Services, Sites, Deployment Manager Services and Resources are registered, 
structured and configured to have agreed settings and API versions for O2ims communication with SMO . 
- Performing maintenance operations in the O-Cloud Platform, either autonomously/automatically or on demand, 
e.g., switching the op erational mode of O -Cloud Node(s) into the maintenance mode when an O -Cloud Node 
within an O-Cloud Node Cluster requires maintenance or is scheduled for decommissioning . 
 
4.4 O-Cloud Multi-Site Networking 
 For disaggregated O -RAN deployments, the Network Funct ions (VNFs/ CNFS) may be deployed in multiple O -
Clouds or O-Cloud sites. These sites or O-Clouds are generally interconnected via Transport Networks.  

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
20 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
 
For these disaggregated NFs to communicate with each other, end -to-end connectivity needs to be establis hed, which 
includes networking within O-Cloud site and as well Transport Network as shown in the figure below.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
    Figure: Logical view of multi-site connectivity through Public Transport Network 
 
O-Cloud site network connects the NFs to the O-Cloud Gateway. If an O-Cloud site have site fabric, this connectivity is 
achieved through the fabric. This connectivity is local within O-Cloud site. However, the connectivity between the O-
Cloud Gateway and Provider Edge (a.k.a. Transport Endpoint) needs to be established to achieve end -to-end 
connectivity. 
 O-Cloud and Transport Network Shared Connectivity Information 
O-Cloud and Transport Networks generally belong to different administrative domains and are orchestrated 
independently; hence a mechanism is needed to orchestrate the connectivity between the O -Cloud Gateway and the 
Provider Network Edge.  
 
The subnet between the O-Cloud gateway and the PE device needs to be configured with appropriate Identifiers (such as 
VLAN ID, subnet CIDR, Getaway IP). These identifiers must match on both sides of the administr ative domains to 
achieve successful configuration of the subnet. Hence, an appropriate mechanism is needed so that these identifiers can 
be exchanged to allow each side independently to configure their side of the subnet.  
 
Depending upon the deployment sc enarios, the operators may make the choice as to which domain will manage the 
selection and allocation of these identifiers. For instance, if O-Cloud is the manager for the allocation of these identifiers, 
it will first configure the O -Cloud gateway port c onnecting to the PE device. These identifiers are then passed to the 
Transport Network domain to configure the PE device port that connects to the O -Cloud gateway. 
Conversely, if Transport Network Domain is the manager for allocation of these identifiers, PE device is configured first 
and then the identifiers are passed to the O-Cloud.  
O-Cloud Site 
Transport Network  
O-Cloud Site 
Network 
 O-Cloud Site 
Network 
Transport 
Network 
NF(S) 
O-CLOUD 
GW 
PE 
 PE 
Site Fabric 
NF(S) 
 O-CLOUD 
GW 
O-Cloud Site 

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
21 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
5 Deployment Scenarios:  Common Considerations 
In any implementation of logical network functionality, decisions need to be made regarding which logical functions are 
mapped to which Cloud Platforms, and therefore which functions are to be co-located with other logical functions .  In 
this document we do not prescribe one specific implementation, but we do understand that in order to establish agreements 
and requirements, the manner in which the Network Functions are mapped to the same or different Cloud Platforms must 
be considered.   
We refer to each specific mapping as a “deployment scenario”.  In this section, we examine the deployment scenarios that 
are receiving the most consideration.  Then we will select the one or ones that should be the focus of initial scenario 
reference design efforts. 
5.1 Mapping Logical Functionality to Physical Implementations 
There are many aspects that need to be considered when deciding to implement logical function s in distinct O-Clouds.  
Some aspects have to do with f undamental technical constraints  and economic considerations , while others have to do 
with the nature of the services that are being offered.   
 Technical Constraints that Affect Hardware Implementations   
Below are some factors that will affect the cost of implementations, and can drive a carrier to require  separation of or 
combining of different logical functions.   
• Environment:  Equipment may be deployed in indoor controlled environments (e.g., Central Offices), semi -
controlled environments (e.g., cabinets with fans and heaters), and exposed environments (e.g., Radio Units on 
a tower).  In general, the less controlled the environm ent, the more difficult and expensive  the equipment will 
be.  The required temperature range is a key design factor, and can drive higher power requirements.   
• Dimensions:  The physical dimensions can also drive deployment constraints – e.g., the need to f it into a tight 
cabinet, or to be placed safely on a tower or pole.   
• Transport technology:   The transport technology used for Fronthaul , Midhaul, and Backhaul is often fiber, 
which has an extremely low and acceptable loss rate.  However, there are options  other than fiber, in particular 
wireless/ microwave, where the potential for data loss must be considered.  This will be discussed further  in the 
next section. 
• Acceleration Hardware:   The need for acceleration hardware can be driven by the need to meet ba sic 
performance requirements, but can also be tied to some of the above considerations.  For example, a hardware 
acceleration chip (COTS or proprietary) can result in lower power use, less generated heat, and smaller physical 
dimensions than if acceleration is not used.  On the other hand, some types of hardware acceleration chips might 
not be “hardened” (i.e., they might only operate properly in a restricted environment), and could require a more 
controlled environment such as in a central office. 
The acceleration hardware most often referred to includes: 
• Field Programmable Gate Arrays (FPGAs) 
• Graphical Processing Units (GPUs) 
• System on Chip (SoC) 
• Standardized H ardware:  Use of standardized hardware designs and standardized form factors  can have 
advantages such as helping to reduce operations complexity, e.g., when an operator makes periodic technology 
upgrades of selected components.  An example would be to use an Open Compute Project (OCP)  or Open 
Telecom IT Infrastructure (OTII) –based design.   
 Service Requirements that Affect Implementation Design  
RANs can serve a wide range of services and customer  requirements, and e ach market can drive some unique  
requirements.  Some examples are below. 
• Indoor or outdoor deployment:  Indoor deployments (e.g., in a public venue like a sports stadium, train station, 
shopping mall, etc.) often enjoy a controlled environment for all elements, including the Radio Units.  This can 
improve the economics of some indoor deployment scenarios.  The distance between Network Functions tends 

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
22 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
to be much lower, and the devices that support O -RU functionality may be much easier and ch eaper to install 
and maintain. This can affect the density of certain deployments, and the frequency that certain scenarios are 
deployed.   
• Bands supported, and Macro cell vs. Small cell :  The choice of bands (e.g., Sub-6 GHz vs. mmWave) might 
be driven by whether the target customers are mobile vs. fixed, and whether a clear line of sight to the customer 
is available or is needed. The bands to be supported  will of course affect O-RU design.  In addition, b ecause 
mmWave carriers can support much higher channel width (e.g., 400 MHz vs. 20 MHz), mmWave deployments 
can require a great deal more O -DU and O -CU processing power.   And of course the operations costs of  
deploying Macro cells vs. Small cells differ in other ways.   
• Performance r equirements of the Application / Network Slice:   Ultimately, user applications drive 
performance requirements, and RANs are expected to support a very wide range of applications.  For example, 
the delay requirements to support a Conne cted Car application using Ultra Reliable Low Latency 
Communications (URLLC) will be more demanding than the delay requirements for other types of applications.  
In our discussion of 5G, we can start by considering requirements separately for URLLC, enhanc ed Mobile 
Broadband (eMBB), and massive Machine Type Communications (mMTC). 
The consideration of performance requirements is a primary one, and is the subject of Section 5.2.  
 Rationalization of Centralizing O-DU Functionality 
Almost all Scenarios to be discussed in this document involve a degree of centralization of O -DU.  In this section it is  
assumed that O-DU resources for a set of O-RUs are centralized at the same location.   
Editor’s Note:  While most Scenarios also centralize O -CU-CP , O-CU-UP , and near-RT RIC in one form or 
another, the benefits of centralizing them are not discussed in this section.  
Managing O-DU in equipment at individual cell sites (via on-site BBUs today) has multiple challenges, including: 
• If changes are needed at a site (e.g., adding radio carriers), then adding equipment is a coarse -grained activity – 
i.e., one cannot generally just add “another 1/5 of a box”, if that is all that is needed.  Adding the minimum 
increment of additional capacity might result in poor utilization and thereby prevent expansion at that site.   
• Cell sites are in many separate locations, and each requires  establishment and maintenance of an acceptable 
environment for the equipment.  In turn this requires separate visits for any physical operations.  
• Micro sites tend to have much lower average utilization than macro sites, but each can experience considerabl e 
peaks. 
• “Planned obsolescence” occurs, due to ongoing evolution of smartphone capabilities and throughput 
improvements, as well as introduction of new features and services.  It is common practice today to upgrade 
(“forklift replace”) BBUs every 36-60 months. 
These factors motivate the centralization of resources where possible.  For the O-DU function, we can think of two types 
of centralization: simple centralization and pooled centralization.   
If the equipment uses O -DU centralization in an Edge Cloud, at any given hour an O-RU will be using a single specific 
O-DU resource that is assigned to it (e.g. via Kubernetes).  On a broad time scale, traffic from any cell site can be rehomed, 
without any physical work, to use other/additional resources that are a vailable at that Edge Cloud location.  This would 
likely be done infrequently; e.g., about as often as cell sites are expanded.   
Centralization can have some additional benefits, such as only having to maintain a single large controlled environment 
for many cell sites rather than creating and maintaining many distributed locations that might be less controlled (e.g., 
outside cabinets or huts).  Capacity can be added at the central site and assigned to cell sites as needed.  Note that simple 
centralization still assigns each O-RU to a single O-DU resource1, as shown below, and that traffic from one O-RU is not 
split into subsets that could be assigned to different O -DUs.  Also note that a Fronthaul (FH) Gateway ( GW) may exist 
between the cell site and the c entralized resources, not only to improve economics but also to enable traffic re -routing 
when desired.  
 
1 In this figure, each O-DU block can be thought of as a unit of server resources that includes a hardware accelerator, a GPP, memory and any other 
associated hardware. 

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
23 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
 
Figure 12:  Simple Centralization of O-DU Resources 
By comparison, with pooled centralization, traffic from an O-RU (or subsets of the O-RU’s traffic) can be assigned more 
dynamically to any of several shared O-DU resources.  So if one cell site is mostly idle and another experiences high 
traffic demand, the traffic can be routed to the appropriate O-DU resources in the shared pool.  The total resources of this 
shared pool can be smaller than resources of distributed locations, because the peak of the sum of the traffic will be 
markedly lower than the sum of the individual cell site traffic peaks.   
 
Figure 13:  Pooling of Centralized O-DU Resources 
We note that being able to share O -DU resources somewhat dynamically is expected to be a solvable problem, although 
we understand that it is by no means a trivial problem.  There are management considerations, among others.  There may 
be incremental steps toward true shared pooling, where rehoming of O -RUs to different O-DUs can be performed more 
dynamically, based on traffic conditions. 
It is noted that O-DU centralization benefits the most dense networks where several cell sites are within the O -RU to O-
DU latency limits.  Sparsely populated areas most probably will be addressed by vO -CU centralization only.   
Figure 14 shows the results of an analysis of a simulated greenfield deployment as an attempt to visualize the relative 
merit of simple centralization of O -DU (“oDU”) vs. pooled centralization of O -DU (“poDU”) vs. legacy DU (“BBU”), 
plotted against the realizable Cell Site pool size.  


                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
24 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
 
Figure 14:  Comparison of Merit of Centralization Options vs. Number of Cell Sites in a Pool 
An often-used measure is related to the power required to  support a given number of carrier MHz.  The lower the power 
used per carrier, the more efficient is the implementation.  In Figure 14, the values of eac h curve are normalized to the 
metric of Watts/MHz for distributed legacy BBUs, normalized to equal 1.  Please note that in this diagram, a lower value 
is better.  The following assumptions apply to the figure:   
• A legacy BBU processes X MHz (for carriers) and consumes Y watts.  For example, a specific BBU might 
process 1600 MHz and consume 160 watts.  
• N legacy BBUs will process N x X  MHz and consume N x Y watts and have a merit figure of 1, per 
normalization.  If a given site requires less than X MHz, it will still be necessary to deploy an X MHz BBU.  For 
example, we may need only 480 MHz but still deploy a 1600 MHz BBU.  
• Simple Centralization (the “oDU” line):  In this case, active TRPs are statically mapped to s pecific VMs and 
vO-DU tiles2.  Fewer vO-DU tiles are required to support the same number of  TRPs, because MHz per site is 
not a constant. 
• Independent of resources to support active user traffic, a fixed power level is required to power Ethernet 
“frontplane” switches and hardware to support management and orchestration processes.  
• In a pool, processing capacity will be added over time as required. 
• Due to mobility traffic behavior, tiles will not be fully utilized, although centralization of resources will 
improve utilization when compared with a legacy BBU approach.   
• Centralization with more dynamic pooling (the “poDU” line): In addition to active load balancing,  individual 
traffic flows (which can last from a few hundreds of msecs  to several seconds) will b e routed to the least used 
tile, further optimizing (reducing) vO-DU tile requirements.   
• As in the simple centralization approach above, there is a fixed power level required for hardware that 
supports switching, management and orchestration processes. 
As a final note, any form of centralization requires efficient transport between the O-RU and the O-DU resources.  When 
O-RU functionality is distributed over a relatively large area (e.g., not concentrated in a single large building), the 
existence of a Fronthaul Gateway is a key enabler.   
 
2 A “vO-DU tile” refers to a chip or System on Chip (SoC) that provides hardware acceleration for math-intensive functionality such as that required 
for Digital Signal Processing.  With the Option 7.2x split, acceleration of Forward Error Correction (FEC) functionality is required (FEC is 
optional for e.g. low band.), and other functionality could be considered for acceleration if desired.  


                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
25 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
5.2 Performance Aspects 
Performance requirements drive architectural and design considerations.   Performance can include attributes such as 
delay, packet loss, transmission loss, and delay variation (aka “jitter”).   
Editor’s Note:  While all aspects are of interest, delay has the largest impact on network design and will 
be the focus of this version.  Future versions can address other performance aspects if desired and is 
FFS.   
 User Plane Delay 
This section discusses the framework for discussing delay of user-plane packets3, and also general delay numbers that it 
can be agreed that apply across all scenarios.  Details relevant to a specific Scenario will be discussed  in each Scenario’s 
subsection, as applicable. The purpose of these high -level targets is to act as a baseline for allocating the total latency 
budget to subsystems that are on the path of each constraint, as required for system engineering and dimensioning 
calculations, and to assess the impact on the function placement within the specific network site tiers.   
The goal is to establish reasonable maximum delay targets, as well as to identify and document the major infrastructure 
as well as O -RAN NF-specific delay contributing components. For each service o r element, minimum delay should be 
considered to be zero. The implication of this is that any of the elements can be moved towards the Cell Site (e.g. in a 
fully distributed Cloud RAN configuration, all of O-CU-UP, O-DU and O-RU would be distributed to Cell Site).  
In real network deployments, the expectation is that, depending on the operator-specific implementation constraints such 
as location and fiber availability, deployment area density, etc., deployments result in anything between the fully 
distributed and maximally centralized configuration. Even on one operator’s network, it is common that there are many 
different sizes of Edge Cloud instances, and combinations of Centralized and Distributed architectures in same network 
are also common (e.g. networ k operator may choose to centralize the deployments on dense Metro areas to the extent 
possible and distribute the configurations on suburban/rural areas with larger cell sizes / cell density that do not translat e 
to pooling benefits from more centralized architecture). However, the maximum centralization within the constraints of 
latencies that can be tolerable is useful for establishing the basis for dimensioning of the maximum sizes, especially for 
the Edge and Regional cloud PoPs. Figure 15 below illustrates the relationship among some key delay parameters.   
 
Figure 15:  Major User Plane Latency Components, by 5G Service Slice and Function Placement 
Please note the following: 
 
3 Delay of control plane or OAM traffic is not considered in this section.  


                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
26 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
• NOTE 1: If the T2 or/and T3 transport network(s) is/are Packet Transport Network(s), then time allocation for 
the transport network elements  processing and queuing delays will require some portion of maximum latency 
allocation, and will require reduction of the maximum area accordingly. 
• NOTE 2: Site Internal / fabric networks are not shown for clarity, but need some latency allocation (effectively 
extensions or part of transport delays; per PoP tier designations TE1, TE2, TE3 and TC). 
• NOTE 3: To maximize the potential for resource pooling benefits, minimize network function redundancy cost, 
and minimize the amount of hardware / power in progressively more distributed sites (towards UEs), target 
design should attempt to maximize the distances and therefore latencies available for transport networks within 
the service- and RAN-specific time constraints, especially for TT1. 
• NOTE 4: UPF, like EC /MEC, is outside of the scope of O -RAN, so UPF shown as  a “black box” to illustra te 
where it needs to be placed in context of specific services to be able to ta ke advantage of the RAN service -
specific latency improvements. 
Figure 15 represents User Equipment locations on the right, and network tiers towards the left, with increasing latency 
and increasing maximum area covered per tier towards the left. These Mobile Network Operator’s (MNO’s) Edge tiers 
are nominated as Cell Site, Edge Cloud, and Regional Cloud, with one additional tier nominated as Central Cloud in the 
figure. 
The summary of the associated latency constraints as well as major latency contributing components as depicted in Figure 
15 above is given in Table 1, below. 
Table 1:  Service Delay Constraints and Major Delay Contributors 
RAN Service-Specific User Plane Delay Constraints 
Identifier Brief Description 
Max. OWD 
(ms) 
Max. RTT 
(ms) 
URLLC Ultra-Reliable Low Latency Communications (3GPP) 0.5 1 
URLLC Ultra-Reliable Low Latency Communications (ITU) 1 2 
eMBB enhanced Mobile Broadband 4 8 
mMTC massive Machine Type Communications 15 30 
Transport Specific Delay Components 
TAIR Transport propagation delay over air interface     
TE1 Cell Site Switch/Router delay     
TT1 Transport delay between Cell Site and Edge Cloud 0.1 0.2 
TE2 Edge Cloud Site Fabric delay     
TT2 Transport delay between Edge and Regional Cloud 1 2 
TE3 Regional Cloud Site Fabric delay     
TT3 Transport delay between Regional  and Central Cloud 10 20 
TC Central Cloud Site Fabric delay     
Network Function Specific Delay Components 
TUE Delay Through the UE SW and HW stack     
TRU Delay Through the O-RU User Plane     
TDU Delay Through the O-DU User Plane     
TCU-UP Delay Through the O-CU User Plane     
 
The transport network delays are specified as maximums, and link speeds are considered to be symmetric for all 
components with exception of the air interface (TAIR).  For the S-Plane services utilizing PTP protocol, it is a requirement 
that the link lengths, link speeds and forward-reverse path routing for PTP are all symmetric. 
Radios (O-RUs) are always located in the Cell Site tier, while O-DU can be located “up to” Edge Cloud tier. It is possible 
to move any of the user plane NF instances closer towards the cell site, as implicitly they would be inside the target 

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
27 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
maximum delay, but it is not necessarily possible to move them further away from the Cell Sites while remainin g within 
the RAN internal and/or RAN service-specific timing constraints.  A common expected deployment case is one where O-
DU instances are moved towards or even to the Cell Site and O -RUs (e.g. in Distributed Cloud-RAN configurations), or 
in situations where the Edge Cloud needs to be located closer to the Cell Site due to fiber and/or location availability , or 
other constraints. While this is expected to work well from the delay constraints perspective,  the centralization and 
pooling-related benefits will be potentially reduced or even eliminated in the context of such deployment scenarios.  
The maximum transport network latency between the site hosting O -DU(s) and sites hosting associated O -RU(s) is 
primarily determined by the RAN internal processes time constraints (such as HARQ loop, scheduling, etc., time-sensitive 
operations). For the purposes o f this document, we use 100us latency, which is commonly used as a target maximum 
latency for this transport segment in related industry specifications for user -plane, specifically “High100” on E -CPRI 
transport requirements [4] section 4.1.1, as well as “Fronthaul” latency requirement in ITU technical report GSTR-TN5G 
[6], section 7 -2, and IEEE Std 802.1CM -2018 [5], section 6.3.3.1.  Based on the 5us/km fiber propagation delay, this 
implies that in a 2D Manhattan tessellation model, which is a common simple topology model for dense urban area fiber 
routing, the maximum area that can be covered from a single Edge Cloud tier site hosting O-DUs is up to a 400km2  area 
of Cell Sites and associated RUs .  Based on the radio inter -site distances, number of bands and other radio network 
dimensioning specific parameters, this can be used to estimate the maximum number of Cell Sites and cell sectors that 
can be covered from single Edge Cloud tier location, as well as maximum number of UEs in this coverage area. 
The maximum transport network latencies towards the entities located at higher tiers are constrained by the lower of F1 
interface latency (max 10 ms as per GSTR -TN5G [6], section 7.2), or alternatively service -specific latency constraints, 
for the edge -located services that are positioned to take advantage of improved latencies.   For eMBB, UE -CU latency 
target is 4m s one -way delay, while for the U RLLC it is 0.5ms as per 3GPP (or 1ms as per ITU requirements). The 
placement of the O-CU-UP as well as associated UPF, to be able to provide URLLC services would have to be at most at 
the Edge Cloud tier to satisfy the service latency constraint. For the eMBB service s with 4ms OWD target, it is possible 
to locate O-CU-UP and UPF on next higher latency location tier, i.e. Regional Cloud tier. Note that while not shown i n 
the picture, Edge compute / Multi-Access Edge Compute (MEC) services for a given RAN service type are expected to 
be collocated with the associated UPF function to take advantage of the associated service latency reduction potential.  
For the services that do not have specific low-latency targets, the associated O-CU-UP and UPF can be located on higher 
tier, similar to deployments in typical LTE network designs. This is designated as Central Cloud tier in the example in 
Figure 15 above.  For eMBB services, if there are no local service instances in the Edge or Regional clouds to take 
advantage of the 4ms OWD enabled by eMBB service definition, but the associated service s are provided from either 
Central Clouds, external networks or from other Edge Cloud / RAN instances (in case of user -to-user traffic), the 
associated non-constrained (i.e. over 4ms from subscriber) eMBB O-CU-UP and UPF instances can be located in Central 
Cloud sites without perceivable impact to the service user, as in such cases the transport and/or service -specific latencies 
are dominant latency components.  
The intent of this section is not to micromanage the latency budget, but to rather establish a reasonable baseline for 
dimensioning purposes, particularly to provide basic assessment to enable sizing of the cloud tiers wi thin the context of 
the service-specific constraints and transport allocations. As such, we get the following “allowances” for the aggregate 
unspecified elements: 
• URLLC3GPP: 0.5ms - 0.1ms (TT1) = 0.4ms  ≥ TUE + TAIR + TE1 + TRU + 2(TE2) + TDU + TCU-UP 
• URLLCITU: 1ms - 0.1ms (TT1) = 0.9ms  ≥ TUE + TAIR + TE1 + TRU + 2(TE2) + TDU + TCU-UP 
• eMBB: 4ms - 0.1ms (TT1) - 1ms (TT2) = 2.9ms ≥ TUE + TAIR + TE1 + TRU + 2(TE2) + TDU + TE3 + TCU-UP 
• mMTC15: 15ms - 0.1ms (TT1) - 1ms (TT2) - 10ms (TT3) = 3.9ms ≥ TUE + TAIR + TE1 + TRU + 2(TE2) + TDU + TE3 + 
TCU-UP + TC 
 
If required, we may provide more specific allocations in later versions of the document, as we gain more implementation 
experience and associated test data, but at this stage it is considered to be premature to do so. It should also be noted that 
the URLLC specification is still work i n progress at this stage in 3GPP , so likely first implementations will focus on 
eMBB service, which leaves 2.9ms for combined O-RAN NFs, air interface, UE and cloud fabric latencies. 
It is possible that network queuing delays may be the dominant delay contributor for some service classes. However, these 
delay components should be understood to be  in context of the most latency -sensitive services, particularly on RU -DU 
interfaces, and relevant to the system l evel dimensioning. It is expected that if we will have multiple QoS classes, then 
the delay and loss parameters are specified on per-class basis, but such specification is outside of scope of this section.  
The delay components in this section are based on  presently supported O -RAN splits, i.e. 3GPP reference split 
configurations 7-2 & 8 for the RU-DU split (as defined in O-RAN), and 3GPP split 2 for F1 (as defined in O -RAN) and 
associated transport allocations, and constraints are based on the 5G serv ice requirements from ITU & 3GPP.  

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
28 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
Other extensions have been approved and included in version 2.0 of the O-RAN Fronthaul specification [7], which allow 
for so called “non -ideal” Fronthaul. It should be noted that while they allow substantially larger delays (e.g. 10 ms FH 
splits have been described and implemented outside of O-RAN), they cannot be considered for all possible 5G use cases, 
as for example it is clearly impossible to meet the 5G service -specification requirements over such large delay values 
over the FH for URLLC or even 4 ms eMBB services. In addition, in specific s cenarios (e.g. high-speed users), adding 
latency to the fronthaul interface can result in reduced performance, and lower potential benefits, e.g. in Co -Ordinated 
Multi-Point (CoMP) mechanisms. 
5.3 Hardware Acceleration and Acceleration Abstraction Layer 
(AAL) 
As stated in Section 4.3.2, an O-Cloud Node is a collection of CPUs, Memory, Storage, NICs, BIOSes, BMCs, etc., and 
may include hardware accelerators to offload computational-intense functions with the aim of optimizing the performance 
of the O-RAN Cloudified NF (e.g., O-RU, O-DU, O-CU-CP, O-CU-UP, near-RT RIC).  There are many different types 
of hardware accelerators, such as FPGA, ASIC, DSP, GPU, and many different types of acceleration functions, such as 
Low-Density Parity -Check (LDPC) , Forward Error Correction (FEC) , end-to-end high -PHY for O -DU, security 
algorithms for O -CU, and Artificial Intelligence for RIC .  The combination of hardware accelerator and acceleration 
function, and indeed the option to use hardware acceleration, is the vendor’s choice; however , all types of hardware 
acceleration on the cloud platform should ensure the decoupling of software from hardware. This decoupling implies the 
following key objectives:  
• Multiple vendors of hardware GPP CPUs and accelerators (e.g., FGPA, ASIC, DSP, or GPU) can be used in O-
Cloud platforms (including agreed -upon Acceleration Abstraction Layer as defined in an upcoming 
specification) from multiple vendors, which in turn can support the software providing RAN functionality.  
• A given hardware and cloud platform shall support RAN software (including near-RT RIC, O-CU-CP, O-CU-
UP, O-DU, and possibly O-RU functionality in the future) from multiple vendors.  
There are different concepts that should be considered for the hardware acceleration abstraction layer on the cloud 
platform; these are usually the following:  
• Accelerator Deployment Model  
• Acceleration Abstraction Layer (AAL) Interface (i.e., the APIs used by the NFs)  
 
Figure 16: Hardware Abstraction Considerations 
 Accelerator Deployment Model  
Figure 16 above presents two common hardware accelerator deployment models  as examples : an abstracted 
implementation utilizing a vhost_user and virtIO  type deployment, and a pass -through model using SR -IOV. While the 
abstracted model allows a full decoupling of the Network Function (NF) from the hardware accelerator, this model may 
not suit real-time latency sensitive NFs such as the O-DU. For better acceleration capabilities, SR-IOV pass through may 
be required, as it is supported in both VM and container environments.  


                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
29 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
 Acceleration Abstraction Layer (AAL) Interface 
To allow multiple NF vendors to utilize a given accelerator through its Acceleration Abstraction Layer (AAL) interface, 
the a ccelerators must provide an open -sourced API.  Likewise, this API shall allow NFs applications to discover , 
configure, select and use (one or more) acceleration functions provided by a given accelerator on the cloud platform . 
Moreover, this API shall also support different offload architectures including look aside , inline and any combination of 
both. Examples of open APIs include DPDK’s CryptoDev, EthDev, EventDev, and Base Band Device (BBDEV).  
When delivering an NF to an Operator, it is assumed that the supplier of that Network Function will provide not only the 
Network Function, but it will also package the appropriate Accelerator Driver (possibly provided by a 3rd party) and will 
indicate the corresponding AAL profile needed in the Operator’s O -Cloud. Figure 17 illustrates this for both Container 
and Virtual Machine (VM) deployments.  
  
 
Figure 17: Accelerator APIs/Libraries in Container and Virtual Machine Implementations 
 Accelerator Management and Orchestration Considerations 
Note that Figure 17 shows the APIs/Libraries as used by the NF application running in a Container or a VM, but there are 
several entities that require management. Accordingly, the figure also shows the Accelerator Management and 
Accelerator Driver in the O -Cloud.  As will be discussed in Section 5.6, these entities ( in addition to  any hardware 
accelerator considerations) will be managed via O2, specifically the Infrastructure Management Services.  Figure 17 also 
shows that the Accelerator Driver (e.g., the PMD driver) needs to be supported both by the O-Cloud Platform, by the 
Guest OS in case of VMs, and by the NF packaged into a container.   
In general, t he hardware accelerators shall be capable of being managed and orchestrated. In particular, hardware 
accelerators shall support feature discovery and life cycle management.  Existing Open Source solutions may be leveraged 
for both VMs and containers as defined in an upcomingO2 specification.  Examples include OpenStack Nova and Cyborg, 
while in Kubernetes we can leverage the device plugin framework for vendors to advertise their device and associated 
resources for the accelerator management.   
5.4 Cloud Considerations 
In this section we talk about the list of cloud platform capabilities which is expected to be provided by the cloud platform 
to be able to support the deployment of the scenarios which are covered by this document.   
It is assumed that some or all deployment scenarios may be using VM orchestrated/managed by OpenStack and / or 
Container managed/orchestrated by Kubernetes, and therefore this section will cover both options. 
The discussion in most sub-sections of this section is structured into (up to) three parts:  (1) Common, (2) Container 
only, and (3) VM only.  
 Networking requirements 
A Cloud Platform should have the ability to support high performance N – S and E – W networking, with high throughput 
and low latency.  
O-Cloud ite
Compute vc, ithAcceleration
 ostO Acc. river
 irtual achine N Application
A I  
 uestO 
O-RAN
Net or  unctionsina
 irtual achine
 eployment
 anagement ervices
Infrastructure anagement
 ervices Acc. gmt.
Acc. river
e.g.,AA   Cprofile
A Is  ibraries
O-CloudNode
O-Cloud ite
 eployment
 anagement ervices
Compute vc, ithAcceleration
Infrastructure anagement
 ervices Acc. gmt.
 ostO 
Acc. river
e.g.,AA   Cprofile
N Application
A Is  ibraries
Acc. river
O-RAN
Net or  unctionsina
Container
O-CloudNode

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
30 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
5.4.1.1 Support for Multiple Networking Interfaces 
Common:  In the different scenarios, near -RT RIC, vO -CU, and vO -DU all depend on having su pport for multiple 
network interfaces. The Cloud Platform is required to support the ability to assign multiple networking interfaces to a 
single container or VM instance, so that the cloud platform could support successful deployment for the different 
scenarios.  
Container-only:  For example, the cloud platform can achieve this by supporting the implementation of Multus Container 
Networking Interface (CNI) Plugin. For more details, please see https://github.com/intel/multus-cni. 
 
Figure 18:  Illustration of the Network Interfaces Attached to a Pod, as Provisioned by Multus CNI  
VM-only:  OpenStack provides the Neutron component for networking. For more details, please see 
https://docs.openstack.org/neutron/stein/  
5.4.1.2 Support for High Performance N-S Data Plane 
Common:  The Fronthaul connection between the O -RU/RU and vO -DU requires high performance a nd low latency. 
This means handling packets at high speed and low latency. As per the different scenarios covered in this document, 
multiple vO-DUs may be running on the same physical cloud platform, which will result in the need for sharing the same 
physical networking interface with multiple functions. Typically, the SR-IOV networking interface is used for this. 
The cloud platform will need to provide support for assigning SR -IOV networking interfaces to a container or VM 
instance, so the instance can use the network interface (physical function or virtual function) directly without using a 
virtual switch.  
If only one container needs to use the networking interface, the PCI pass -through network interface can provide high 
performance and low latency without using a virtual switch. 
In general, the following two items are needed for high performance N -S data throughput: 
• Support for SR-IOV; i.e., the ability to assign SR-IOV NIC interfaces to the containers/ VMs 
• Support for PCI pass-through for direct access to the NIC by the container/ VM  
Container-only:  When containers are used, the cloud platform can achieve this by supporting the implementation of SR-
IOV Network device plugin for Kubernetes. For more details, please refer to https://github.com/intel/sriov-network-
device-plugin  
VM-only: OpenStack provides the Neutron component for networking. For more details, please see 
https://docs.openstack.org/neutron/stein/admin/config-sriov.html . 
5.4.1.3 Support for High-Performance E-W Data Plane 
Common:  High-performance E-W data plane throughput is a requirement for the implementation of the different near -
RT RIC, vO-CU, and vO-DU scenarios which are covered in this document.  
One of commonly used options for E-W high-performance data plane is the use of a virtual switch which provides basic 
communication capability for instances deployed at either the same machine or different machines. It provides L2 and L3 
network functions.  


                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
31 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
To get the high performance required, one of the options  is to use a Data Plan Development Kit (DPDK) -based virtual 
switch.  Using this method, the packets will not go into Linux kernel space networking, and instead will implement 
userspace networking which will improve the throughput and latency. To support th is, the container or VM instance will 
need to use DPDK to accelerate packet handling.  
The cloud platform will need to provide the mechanism  to support the implementation of userspace networking for 
container(s) / VM(s). 
Container-only:  As an example, the  cloud platform can achieve this by supporting implementation of Userspace CNI 
Plugin. For more details, please refer to https://github.com/intel/userspace-cni-network-plugin. 
 
Figure 19:  Illustration of the Userspace CNI Plugin 
VM-only:  OVS DPDK is an example of a Host userspace virtual switch and could provide high performance L2/L3 
packet receive and transmit.   
5.4.1.4 Support for Service Function Chaining  
Common:  Support for a Service Function Chaining  (SFC) capability requires the ability to create a service function 
chain between multiple VMs or containers. In the virtualization environment, multiple instances will usually be deployed, 
and being able to efficiently connect the instances to provide service will be a fundamental requirement.  
The ability to dynamically configure traffic flow will provide flexibility to Operators.  When the service requirement or 
flow direction needs to be changed, the Service Function Chaining capability can be used to easily implement it instead 
of having to restart and reconfigure the services, networking configuration and Containers/VMs.  
Container-only: An example of SFC functionality is found at: https://networkservicemesh.io/ 
VM only:  The OpenStack Neutron SFC and OpenFlow -based SFC are examples of solutions that can implement the 
Service Function Chaining capability. 
5.4.1.5 Support for VLAN based networking 
Common:  VLAN based networking is the most common and fundamental for m of networking. VLANs are typically 
used to provide the isolation of various types of traffic in cloud environments. Cloud platforms must support the traffic 
isolation requirements of the application.  
The O-RAN slicing use cases specified in [14] require the use of VLANs by O-RAN NFs to distinguish traffic belonging 
to different slices. To support this requirement,  the O-Cloud platform must provide support for trunked VLAN network 
interfaces to be made available to Cloudified NFs (VMs and Containers) so that packets tagged with different VLANs 
can be transported on the same virtual network interface. 
 
VLANs may also be used to differentiate slices in the transpo rt network once the appropriate VLAN tags are applied by 
Cloudified NFs in the Data Center as specified in [14]. Therefore, the O -Cloud must also ensure that any VLAN  tags 
applied by the O-RAN NFs are carried over to the transport network. 


                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
32 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
 
Container-only: For example, the cloud platform can achieve this by supporting the implementation of Multus Container 
Networking Interface (CNI) Plugin. For more details, please see https://github.com/k8snetworkplumbingwg/multus-cni 
VM only:  OpenStack provides the Neutron component for networking. For more details, please see 
https://docs.openstack.org/neutron/stein/  
 
5.4.1.6 Support for O-Cloud Gateways to connect O-Cloud Sites with external Transport 
Networks 
In disaggregated O-RAN deployments, the Network Functions (NFs) may be deployed in multiple O-Clouds or different 
locations within a given distributed O-Cloud.  As an example, O-DU and O-CU-CP may be deployed in two different O-
Clouds or different distributed O -Cloud sites. For O -CU-CP to communicate with O -DU, the networking needs to span 
across the O-Clouds or distributed O-Cloud sites via external transport networks.   
For a transport network to interconnect different O-Clouds or O-Cloud sites, it needs an endpoint, herein referred to as 
O-Cloud gateway, in each of the O-Clouds. For the sake of this architecture, the O-Cloud gateway is a logical endpoint 
inside the O-Cloud that connects the O-Cloud to the outside world. 
 
Beside the interconnection of O-Clouds and distributed O-Cloud sites, there are other external connections that also need 
to terminate the O-Clouds and O-Cloud sites domains in an O -Cloud gateway function to ensure a clear demarcation of 
the different network domains. It is FFS which other gateway functions are needed as how they are to be named and how 
they are to be managed for example seeking inspiration in the ETSI GS NVF-SOL.005. 
 
O-Cloud shall provide support for one or more O-Cloud gateway instances to provide connectivity to one or more external 
networks. This does not restrict or impose any networking models within the O-Cloud as long as the O-Cloud provides a 
mechanism to connect the NFs to the O-Cloud gateway so that the deployed NFs could reach other NFs deployed in other 
O-Clouds or O-Cloud sites, while maintaining the segmentation of the traffic between the NFs. It shall also be noted that 
each network domain can have its own networking model and segmentation scheme. 
 
O-Cloud gateway augments the O-Cloud architecture model depicted by figure 10 and 11 in section 4.3.2. 
 Assignment of Acceleration Resources 
Common:  For both container and VM solutions, specific devices such as accelerator (e.g., FPGA, GPU) may be needed. 
In this case, the cloud platform needs to be able to assign the specified device to container instance or VM instance.  
For example, some L1 protocols require an FFT algorithm (to compute the DFT) that could be implemented in an FPGA 
or GPU, and the vO-DU would need the PCI Pass-Through to assign the accelerator device to the vO-DU for access and 
use. 
 Real-time / General Performance Feature Requirements 
5.4.3.1 Host Linux OS 
5.4.3.1.1 Support for Pre-emptive Scheduling  
Support may be required to support Pre-emptive Scheduling (real time Linux uses the preempt_rt patch). Generally, 
without real time features, it is very difficult for an application to get deterministic response times for events, interrupt s 
and other reasons4. In addition, during the housekeeping processes in Linux system, the application also cannot guarantee 
the running time (CPU cycle), so from the wireless application design perspective, it needs the real time feature. In 
addition, to support the requi rements of high throughput, multiple accesses and low latency, some wireless applications 
need the priority-based OS environment.  
 
4 Other options include things such as Linux signal, softwareirq, and perhaps using a common process. Because the pre-emptive kernel could 
interrupt the low priority process and occupy the CPU, it will get more chance to run the high priority process. Then through proper application 
design, it will have guaranteed time/resource and can have deterministic performance. 

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
33 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
5.4.3.2 Support for Node Feature Discovery 
Common:  Automated and dynamic placement of Cloud-Native Network Functions (CNFs) / microservices and VMs is 
needed, based on the hardware requirements imposed on the vO -DU, vO-CU and near-RT RIC functions.  This requires 
the cloud platform to support the ability to discover the hardware capabilities on each node and advertise it via labels vs.  
nodes, and allows O-RAN Cloudified NFs’  descriptions to have hardware requirements via labels. This mechanism is 
also known as Node Feature Discovery (NFD). 
Container-only:  For example, the cloud platform can achieve this by supporting implementation of NFD for Kubernetes. 
For more details, please see https://github.com/kubernetes-sigs/node-feature-discovery. 
VM-only:  VMs can use OpenStack mechanisms.  For example, the OpenStack No va filter, host aggregates and 
availability zones can be used to implement the same function. 
5.4.3.3 Support for CPU Affinity and Isolation 
Common:  The vO-DU, vO-CU and even the near-RT RIC are performance sensitive and require the ability to consume 
a large amount of CPU cycles to work correctly.  They depend on the ability of the cloud platform to provide a mechanism 
to guarantee performance determinism even when there are noisy neig hbors.  
Container-only:  This requires the cloud platform to support using affinity and isolation of cores, so high performance 
Kubernetes Pod cores also can be dedicated to specified tasks.  For example, the cloud platform can achieve this by 
implementing CPU Manager for Kubernetes. For more details, please refer to https://github.com/intel/CPU-Manager-for-
Kubernetes . 
VM-only:  For example the modern Linux operating system uses the Symmetric MultiProcessing (SMP) mode, so the 
system process and application will be located at different CPU cores. To run the VM and guarantee the VM performance, 
the capability to assign the specif ic CPU cores to a VM is the way to do that. And at the same time, CPU isolation will 
reduce the inter-core affinity.  Please refer to https://docs.openstack.org/senlin/pike/scenarios/affinity.html 
5.4.3.4 Support for Dynamic HugePages Allocation 
Common:  When an application requires high performance and performance determinism, the reduction of paging is very 
helpful. vO-DU, vO-CU and even near -RT RIC can require performance determinism.  The cloud platform needs to be 
able to support the ability to provide this mechanism to applications that require it.  
This requires the cloud platform to support ability to dynamically allocate the necessary amount of the faster memory 
(a.k.a. HugePages) to the container or VM as necessary, and also to relinquish this memory allocation in the event of 
unexpected termination.  
Container-only:  For example, the cloud platform can achieve this by supporting implementation of Manage HugePages 
in Kubernetes. For more details please refer to https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-
hugepages/ . 
VM-only:  For example, the OpenStack Nova flavor setting can be used to configure the HugePage size for a VM instance.  
See https://docs.openstack.org/nova/pike/admin/huge-pages.html  
5.4.3.5 Support for Topology Manager 
Common:  Some of the cloud infrastructure which is targeted in the scenarios in this document may have servers which 
utilize a multiple-socket configuration which comes with multiple memory regions. Each core5 is connected to a memory 
region. While each CPU on one socket can access the memory region of the CPUs on another socket of the same board, 
the access time is significantly slower when crossing socket boundaries, an d this will affect performance significantly.  
The configuration of hardware with multiple memory regions is also known as Non -Uniform Memory Access (NUMA) 
regions. To support automated and dynamic placement of CNFs/microservices or VMs based on cloud infrastructure that 
has multiple NUMA regions and guarantee the response time of the application (especially for vO-DU), it is critical to be 
able to ensure that all the containers/VMs are associated with core(s) which are connected to the same NUMA region. In  
 
5 In this document, we use the terms core and socket in the following way.  A socket, or more precisely the multichip platform that fits into a server 
socket, contains multiple cores, each of which is a separate CPU.  Each core in a socket has some dedicated memory, and also some shared 
memory among other cores of the same socket, which are within the same NUMA zone.  

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
34 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
addition, if the application relies on access to hardware accelerators and/or I/O which uses memory as a way to interact 
with the application, it is also critical that those also use the same NUMA region that the application uses.  
The cloud platform will need to provide the mechanism to enable managing the NUMA topology to ensure the placement 
of specified containers/VMs on cores which are on the same NUMA region, as well as making sure that the devices which 
the application uses are also connected to the same NUMA region.  
 
Figure 20:  Example Illustration of Two NUMA Regions 
5.4.3.6 Support for Scale In/Out 
Common:  The act of scaling in/out of containers/ VMs can be based on triggers such as CPU load, network load, and 
storage consumption. The network service usually is not just a single container or VM, and in order to leverage the 
container/ VM benefit, the netwo rk service usually will have multiple containers/ VMs. But if demand is changing 
dynamically, especially for the O -CU, the service needs to be scaled in/out according to service requirements such as 
subscriber quantity.  
For example, when the number of subscribers increases, the system needs to start more container/ VM instances to ensure 
the service quality. From the cloud platform perspective, it could monitor the CPU load; if the load reaches a level such 
as 80%, it needs to scale out. If the CPU load drops 40%, it could then scale in. 
Different services can scale in/out depending on different criteria, such as the CPU load, network load and storage 
consumption.  Support for scale in/out can be helpful in implementing on-demand services.  
Editor’s Note:  Support for scale up/down is not discussed at this time, but may be revisited in the 
future.   
5.4.3.7 Support for Device Plugin 
Common:  For vO-DU, vO-CU and near -RT RIC applications, hardware accelerators such as SmartNICs, FPGAs and 
GPUs may be required to meet  performance objectives that can’t be met by using software only implementations.  In 
other cases, such accelerators can be useful as an option to reduce the consumption of CPU cycles to achieve better cost 
efficiency. 
The cloud platform will need to provide the mechanism to support those accelerators. This in turn requires support the 
ability to discover, advertise, schedule and manage devices such as SR-IOV, GPU, and FPGA.   
Container-only:  For example, the cloud platform can achieve this by supporting implementation of Device Plugins in 
Kubernetes. For more details please check: https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-
net/device-plugins/. 
VM-only:  The PCI passthrough feature in OpenStack allows full access and direct control of a physical PCI device in 
guests. This mechanism is generic for any kind of PCI device, and runs with a Network Interface Card (NIC), Graphics 
Processing Unit (GPU), or any other devices that can be attached to a PCI bus.  Correct driver installation is the only 
requirement for the guest to properly use the devices. 
Some PCI devices p rovide Single Root I/O Virtualization and Sharing (SR -IOV) capabilities. When SR -IOV is used, a 
physical device is virtualized and appears as multiple PCI devices. Virtual PCI devices are assigned to the same or 
different guests. In the case of PCI passthrough, the full physical device is assigned to only one guest and cannot be shared. 


                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
35 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
See https://wiki.openstack.org/wiki/Cyborg 
5.4.3.8 Support for Direct IRQ Assignment 
VM-only:  The general -purpose platform ha s many devices that will generate the IRQ to the system. To develop a 
performance-sensitive application, inclusion of low-latency and deterministic timing features, and assigning the IRQ to a 
specific CPU core, will reduce the impact of housekeeping processes and decrease the response time to desired IRQs. 
5.4.3.9 Support for No Over Commit CPU 
VM-only:  The “No Over Commit CPU” VM creation option is able to guarantee VM performance with a “dedicated 
CPU” model. 
In traditional telecom equipment design, this will ma intain the level of CPU utilization to avoid burst and congestion 
situations. In a virtualization environment, performance-sensitive applications such as vO-DU, vO-CU, and near-RT RIC 
will need the platform to provide a mechanism to secure the CPU resource .  
5.4.3.10 Support for Specifying CPU Model 
VM-only:  OpenStack can use the CPU model setting to configure the vCPU for a VM.  For example, QEMU allows the 
CPU options to be “Nehalem”, “Westmere”, “SandyBridge” or “IvyBridge”, or alternatively it could be configur ed as 
“host-passthrough”. This allows VMs to leverage advanced features of selected CPU architectures. For the vO -CU and 
vO-DU design and implementation, there will be some algorithm and computing functions that can leverage host CPU 
instructions to realize some benefits such as performance. The cloud platform needs to provide this capability to VMs.  
 Storage Requirements 
The storage requirements are the same for both VM and Container based implementations.  
For O-RAN components, the O-RAN Cloudified NF needs storage for the image and for the O-RAN Cloudified NF itself.  
It should support different scale , e.g., for a  Regional Cloud vs. an Edge Cloud.  The cloud platform need s to support a 
large-scale storage solution with redundancy, medium and small -scale storage solution s for two or  more servers, and a 
very small-scale solution for a single server.  
 Notification Subscription Framework 
Applications should have the ability to retrieve notifications that are nec essary for their functionality. For example, vO-
DU needs to know that the node that it starts on has a PTP clock in sync with the master clock.  
Rationale – Application functionality often relies on but is not limited to O-Cloud platform HW resources such as FPGA, 
GPU, PHC. Hence, these application(s) should have the ability to select the resources that will provide them notifications 
about the status of these resources, initial state and changing state. This requires the applications to use a privilege mod e 
in order to access the O -Cloud platform drivers and retrieve the status . However, in a Cloud Native environment , 
applications should not have a privilege mode for accessing the O -Cloud resources. This framework allows applications 
to subscribe for their necessary notifications without claiming a privilege mode and comply with O -Cloud Native 
requirements.    
5.4.5.1 O-Cloud Notification Subscription Requirements 
Tracking function: 
• tracks for resource(s) state of relevant data (for example, change in class of a master clock)    
• tracking function can be configured with tracking frequency per the resource being tracked (default value will 
be defined) 
Registration function: 
• allows application(s) and/or SMO (or other entities) to query for the resources that provide notifications  
• allows application(s) and/or SMO (or other entities) to subscribe to receiving notifications from the selected 
resource(s) 
• allows application(s) and/or SMO (or other entities) to sub scribe to pulling notifications/data from the selected 
resource(s) 

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
36 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
• allows application(s) and/or SMO (or other entities) to unsubscribe to notification(s) which  were previously 
subscribed to for either receiving or pulling notifications 
• The registration function updates the notification function about the state of the subscription and its request type 
(receiving or pulling notifications) 
Notification function: 
• used by the tracking function to message registered listeners of the resource state and/or its rele vant data 
• pulls the tracking function per the application and/or SMO request  
• as soon as an application and/or SMO registers it receives a notification of the resource(s) status it is subscribed 
to 
Figure 21 illustrates the architecture for implementing a framework for notification subscription. This diagram shows the 
functionally and interaction from a logical perspective , however, where these functions reside or how they are 
implemented is not in scope of this document and will be described by Cloud  Platform Reference Design [12].  
 
Figure 21: O-Cloud Notification Framework Architecture 
5.5 Sync Architecture 
Synchronization mechanisms and options are receiving significant attention in the industry.   
Editor’s Note:  O-RAN Working Groups 4 and 5 are addressing some aspects of synchronization, and more 
discussion of Sync is expected in future versions of this document.   
Version 2 of the Control, User and Synchronization  (CUS) Plane Specification [7] discusses, in chapter 9.2.2, “Clock 
Model and Synchronization Topology”, four topology configuration options Lower Layer Split Control Plane 1 – 4 (LLS-
C1 – LLS-C4) that are required to support different O-RAN deployment scenarios.  Configuration LLS-C3 is seen as the 
most likely initial option for deployment and is discussed below.  This section will provide a summary of what is required 
to support the LLS-C1 and LLS-C3 synchronization topology from the cloud platform perspective. 
Note that in  chapter 6 “Deployment Scenarios and Implementation Considerations” of this document , we call the site 
which runs the vO-DU the “Edge Cloud”, while the Control, User and Synchronizat ion (CUS) Plane Specification [7] 
calls it the “Central Site”.  However, the meaning is the same. 
 Cloud Platform Time Synchronization Architecture 
The Time Sync deployment architecture which is described below relies on usage of Precision Time Protocol (PTP) IEEE 
1588-2008 (a.k.a. IEEE 1588 Version 2) to synchronize clocks throughout the Edge Cloud site.  


                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
37 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
For LLS-C3 in the CUS specification [7], vO-DU may act as a Telecom Slave Clock (T-TSC) and select the time source 
the same SyncE and PTP distribution from fronthaul as O -RU. Please note that the following synchronization topology  
for LLS-C3 will address only the case where O-DU and O-RU are synchronized from the same time source connected to 
the fronthaul network, other cases are for Further Study. 
For LLS -C1, the O -Cloud running the vO -DU acts as synchronization master towards the fronthaul interface to 
synchronize the O -RU. Please note that the following synchronization topology for LLS -C1 will address only the case 
where O-DU synchronization source is from a local PRTC (GNSS receiver), other cases are for Further Study . 
 
5.5.1.1 Edge Cloud Site Level – LLS-C3 Synchronization Topology 
This section outlines what the time synchronization architecture  should be  from the cloud platform perspective , and 
identifies requirements that the Cloud Platform and Edge Site need to support in order to support the O-RAN deployment 
scenarios that use the LLS-C3 synchronization topology described in CUS specification [7]. 
5.5.1.1.1 LLS-C3 Synchronization Topology Edge Site Time Synchronization Architecture 
The deployment architecture at the Edge Cloud site level includes: 
• Primary Reference Time Clock (PRTC)-traceable time source (i.e., Grandmaster Clocks):  
o External precision time source for the PTP networks, usually ba sed on Global Navigation 
Satellite System/Global Positioning System (GNSS/GPS) 
• Compute Nodes:  
o Compute Nodes synchronize their clocks to a Grandmaster Clock via the Fronthaul Network 
• Controller Nodes: 
o Controller Nodes synchronize their clocks to the Network Time Protocol (NTP) via the 
Management Network 
 
Figure 22 illustrates the relationship of these entities where the Controller functions are hosted on separate 
nodes from the Compute nodes.  Figure 23 illustrates the relationships where each Compute node also includes 
the Controller functions (i.e., the hyperconverged case). 
  
  
Figure 22: Edge Cloud Site Time Sync Architecture for LLS-C3 
O-Cloud 
Compute 
Node
O-Cloud 
Management 
(OCM) 
compute-0
S
compute-1 compute-N
S
controller-0 controller-N
Management Network
S
Fronthaul Network
Data Network
Cell Site (O-RU) Cell Site(O-RU) … Cell Site (O-RU)
MasterM
SlaveS
Clock Source
M
Grandmaster
Clock(s)
T-BC
PTP Switch
…
PTP
NTP
Management Network
…
S S S

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
38 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
  
Figure 23: Hyperconverged Edge Cloud Time Sync Architecture for LLS-C3 
5.5.1.1.2 LLS-C3 Synchronization Topology Edge Site Requirements 
To support time synchronization at the Edge site, the cloud platform (O -Cloud) used at the Edge site needs to support 
implementation of the PTP IEEE 1588-2008 (a.k.a. IEEE 1588 Version 2) standard. The compute nodes should meet the 
“Time and Frequency Synchronization Requirements” described in CUS specification [7]. The following software and 
hardware capabilities are required: 
5.5.1.1.2.1 Software 
Support for PTP will be needed in all the Edge Site O-Cloud nodes that support compute roles and will run vO-DU service 
operating as a Slave Clock. The following PTP configuration options should be provided:  
o Network Transport – L2, UDPv4, UDPv6 
o Delay Measurement Mechanism – utilize E2E to measure the delay 
o Time Stamping – support for hardware time stamping 
 
For example: in the case when an O-Cloud is based on  the Linux OS , this will require support for Linux  PTP ( see 
http://linuxptp.sourceforge.net) with the following: 
o ptp4l – implementation of PTP (Ordinary Clock, Boundary Clock), HW timestamping, E2E delay measurement 
mechanism. 
o phc2sys – Synchronization of two clocks, PHC and system clock (Linux clock) when using HW timestamping 
 
5.5.1.1.2.2 Hardware  
Use of High speed, low latency Network Interface Card (NIC) with support for PTP Hardware Clock (PHC) subsystem 
for the data interface (fronthaul) on all the compute node(s) that will run the vO-DU function. When vO-DU requires 
SyncE, the NIC must support it. 
5.5.1.2 Edge Cloud Site Level – LLS-C1 Synchronization Topology 
This section outlines what the time synchronization architecture  should be  from the cloud platform perspective , and 
identifies requirements that the Cloud Platform and Edge Site need to support in order to support the O-RAN deployment 
scenarios that use the LLS-C1 synchronization topology described in CUS specification [7]. 
5.5.1.2.1 LLS-C1 Synchronization Topology Edge Site Time Synchronization Architecture 
The deployment architecture at the Edge Cloud site level includes: 
• Primary Reference Time Clock (PRTC)-traceable time source (i.e., Grandmaster Clocks):  
o External precision time so urce for the PTP networks, usually based on Global Navigation 
Satellite System/Global Positioning System (GNSS/GPS) 
O-Cloud 
Hyperconverged
compute-0
S
controller-0
Fronthaul Network
Data Network
Cell Site (O-RU) Cell Site (O-RU) … Cell Site (O-RU)
MasterM
SlaveS
Clock Source
M
Grandmaster
Clock(s)
T-BC
PTP Switch
PTP
Management Network
compute-1
S
Controller-1
.
S S S

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
39 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
• Compute Nodes:  
o Compute Node as acts synchronization master towards the fronthaul interface  
• Controller Nodes: 
o Controller Nodes synchronize their clocks to the Network Time Protocol (NTP) via the 
Management Network 
 
Figure 24 illustrates the relationship of these entities where the Controller functions are hosted on separate 
nodes from the Compute nodes. Figure 25 illustrates the relationships where each Compute node also includes 
the Controller functions (i.e., the hyperconverged case). 
 
Figure 24: Edge Cloud Site Time Sync Architecture for LLS-C1 


                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
40 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
 
Figure 25: Hyperconverged Edge Cloud Time Sync Architecture for LLS-C1 
5.5.1.2.2 LLS-C1 Synchronization Topology Edge Site Requirements 
To support time synchronization at the Edge site, the cloud platform (O -Cloud) used at the Edge site needs to support 
implementation of the PTP IEEE 1588-2008 (a.k.a. IEEE 1588 Version 2) standard. The compute nodes should meet the 
“Time and Frequen cy Synchronization Requirements” described in CUS specification [7]. The following software and 
hardware capabilities are required: 
5.5.1.2.2.1 Software 
Support for PTP will be needed in all the Edge Site O-Cloud node that supports compute role and will run vO-DU service 
operating as a Master Clock. The following PTP configuration options should be provided:  
o Network Transport – L2, UDPv4, UDPv6 
o Delay Measurement Mechanism – utilize E2E to measure the delay 
o Time Stamping – support for hardware time stamping 
 
5.5.1.2.2.2 Hardware  
Use of High speed, low latency Network Interface Card (NIC) with support for PTP Hardware Clock (PHC) subsystem 
for the data interface (fronthaul) on all the compute node(s) that will run the vO-DU function. 
When vO-DU requires SyncE, the NIC must support it. 
 Loss of Synchronization Notification 
Applications that rely on a Precision Time Protocol for synchronization (such as vO-DU but not limited to) should have 
the ability to retrieve the relevant data that can indicate the status of the PHC clock related to the worker node that the 
application is running on (for example a source clock class). Once an application subscribes to PTP  notifications it 
receives the initial data which shows the PHC synchronization state and it will receive notifications when there is a state  
change to the sync status and/or per request for notification (pull), please refer to the notification subscription framework 
(section 5.4.5) how to subscribe for a PTP Notification.   
Rationale - The CUS specification [7] section 9.4.2, specifies various behavio urs related to the state of the vO-DU and 
O-RU time synchronization.  


                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
41 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
• For example, if an vO-DU transits to the FREERUN state, because the synchronizing network delivers 
unacceptable synchronization quality, the vO-DU shall disable RF transmission on all connected O -RUs, and 
keep it turned off until synchronization is reacquired again.  
It should be noted that since vO-DU may need to take an action upon the synchronization notification (see example above) 
it is required to handle these notifications at the scope of the edge cloud (at the site location where the vO-DU is running) 
for two main reasons: ensuring that the vO-DU receives the notifications regardless of the communication stat e of its 
backhaul link and reducing the round trip delay for notifying the vO-DU. 
Figure 26 illustrates an vO-DU subscribes to retrieve PTP Notification based on the subscription framework described at 
section 5.4.5.  
 
Figure 26: vO-DU Subscribes to PTP Notification 
 
5.6 Operations and Maintenance Considerations 
Management of cloudified RAN  Network Functions  introduces some new management considerations, because the 
mapping between Network Functionality and physical hardware can be done in multiple ways, depending on the Scenario 
that is chosen.  Thus, management of aspects that are related to physical aspect s rather than logical aspects need to be 
designed with flexibility in mind from the start.  For example, logging of physical functions, scale out actions, and 
survivability considerations are affected.   
The O-RAN Alliance has defined key fundamentals of the OAM framework (see [8] and [9], and refer to Figure 1). Given 
the number of deployment scenario options and possible variations of O -RAN Managed Functions (MFs) being mapped 
into Managed Elements (MEs) in different ways, it is important for all MEs to support a consistent level of visibility and 
control of their contained Managed Functions to the Service Management & Orchestration Framework.  This consistency 
will be enabled by support of the comm on OAM Interface Specification [9] for Fault Configuration Accounting 
Performance Security (FCAPS) and Life Cycle Management (LCM) functionality, and a common Information Modelling 
Framework that will provide underlying information models used for the MEs and MFs in a particular deployment . 
 The O1 Interface 
As described in [8], t he O1 is an interface between management entities in Service Management and Orchestration 
Framework and O-RAN managed elements, for operation and management, by which FCAPS management, Software 
management, File management shall be achieved.  


                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
42 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
 The O2 Interface 
The O2 Interface is a collection of services and their associated interfaces that are provided by the O -Cloud platform to 
the SMO.  The services are categorized into two logical groups: 
• Infrastructure Management Services: which include the subset of O2 functions that are responsible for 
deploying and managing cloud infrastructure. 
• Deployment Management Services:  which include the subset of O2 functions that are responsible for 
managing the lifecycle of virtualized/containerized deployments on the cloud infrastructure.  
The O2 services and their associated interfaces shall be specified in the  upcoming O2 specification. Any definitions of 
SMO functional elements needed to consume these services shall be described in OAM architecture.  O2 interface would 
also address the management of hardware acceleration and supporting software in the O-Cloud platform. 
5.7 Transport Network Architecture 
While a Transport Network is a necessary foundation upon which to build any O -RAN deployment, a great many of the 
aspects of transport do not have to be addressed or specified in O -RAN Alliance documents.  For example, any location 
with cloud servers will be connected by layer 2 or layer 3 switches, but we do not need to specify much if anything about 
them in this document.   
The transport media used, particularly for fronthaul, can have an effect on aspects such as performance.  However, in the 
current version of this document we have been assuming that fiber transport is used.   
Editor’s Note:  Other transport technologies (e.g., microwave) are also possible, and could be addressed 
at a later date.  
That said, the use of an (optional) Fronthaul Gateway (FH GW) will have noteworthy effects on any O-RAN deployment 
that uses it. 
 Fronthaul Gateways 
In the deployment scenarios that follow, when the O-DU and O-RU functions are not implemented in the same physical 
node, a Fronthaul Gateway is shown as an optional element between them.  A Fronthaul Gateway can be motivated by 
different factors depending on a carrier’s deployment, and may perform different functions.   
The O-RAN Alliance does not currently have a single definition of a Fronthaul Gateway, and this document does not 
attempt to define one.  However, the Fronthaul Gateway is included in the diagrams as an optional implementa tion to 
acknowledge the fact that carriers are considering Fronthaul Gateways in their plans. Below are some examples of the 
functionality that could be provided: 
• A FH GW can convert CPRI connections to the node supporting the O-RU function to eCPRI connections to the 
node that provides O-DU functionality.   
• Note that when there is no FH GW, it is assumed that the Open Fronthaul interface between the O-RU and 
O-DU uses Option 7-2, as mentioned earlier in Section 4.1.  When there is a FH GW, it may have an Option 
7-2 interface to both the O -DU and the O -RU, but it is also possible for the FH GW to have a different 
interface to the O-RU/RU; for example, where CPRI is supported.   
• A FH GW can support the aggregation of fiber pairs. 
• A FH GW must support the following forwarding functions: 
• Downlink:  Transport the traffic from O-DU to each O-RU (and cascading FH GW, if present) 
• Uplink:  Summation of traffic from O-RUs 
• A FH GW can provide power to the NEs supporting the O -RU function, e.g. via Power over Ethernet (PoE) or 
hybrid cable/fibers 

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
43 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
5.8 Overview of Deployment Scenarios 
The description of logical functionality in O -RAN includes the definition of key interfaces E2, F1, and Open Fronthaul.  
However, as noted earlier, this does not mean that each Network Function block must be implemented in a separate O-
RAN Physical NF/O-RAN Cloudified NF .  Multiple logical functions can be implemente d in a single O-RAN Physical 
NF/O-RAN Cloudified NF (for example O-DU and O-RU may be packaged as a single appliance).  
We assume that when Network Functions are implemented as different O-RAN Physical NFs/O-RAN Cloudified NFs , 
the interfaces between them must conform to the O -RAN specifications.  However, when multiple Network Functions 
are implemented by a single O-RAN Physical NF/O-RAN Cloudified NF , it is up to the operator to decide whether to 
enforce the O-RAN interfaces between the emb edded Network Functions .  However, note that the OAM requirements 
for each separate Network Function will still need to be met.   
The current deployment scenarios for discussion are summarized in the figure below.  This includes options that are 
deployable in both the short and long term.  Each will be discussed in some detail in the following sections, followed by 
a summary of which one or ones are candidates for initial focus. Please note that, to help ease the high-level depiction of 
functionality, a single O-CU box is shown with an F1 interface, but in detailed discussions of specific scenarios, this will 
need to be discussed properly as composed of an O-CU-CP function with an  F1-c interface and an O -CU-UP function 
with an F1-u interface.  Furthermore, there would in general be an unequal number of O-CU-CP and O-CU-UP instances.   
Figure 27 below shows the Network F unctions at the top, and each identified scenario shows how these Network 
Functions are deployed as O-RAN Physical NFs or as O-RAN Cloudified NFs running on an O-RAN compliant O-Cloud.  
The term O-Cloud is defined in Section 4.  Please note that the requirements  for an O-Cloud are driven by the Network 
Functions that need to be supported by th e hardware, so for instance an O-Cloud that supports an O-RU function would 
be different from an O-Cloud that supports O-CU functionality.   
Finally, note that in the high-level figure below, the User Plane (UP) traffic is shown being delivered to the UPF.  As will 
be discussed, in specific scenarios it is sometimes possible for UP traffic to be delivered to edge applications that are 
supported by Mobile Edge Computing (MEC).  However, note that the specification of MEC itself is out of scope of this 
document. 
Note that vendors are not required to support all scenarios – it is a business decision to be made by each vendor.  Similarly, 
each operator will decide which scenarios it wishes to deploy.   
 
Figure 27:  High-Level Comparison of Scenarios 
Each scenario is discussed in the next section.   
 
Key
O-Cloud
Network Functions 
(e.g., O-CU + O-DU)
O-RAN Physical NF
Could be 100% O-RAN Physical NF 
(potentially in an open chassis, open 
HW design). Uses Open interfaces.
“O-Cloud” indicates that an O-RAN 
Cloud Platform is used to support 
the RAN functions. This will 
optionally use hardware accelerator 
add-ons as required by each RAN 
function, and the software stack is 
decoupled from the hardware. Each 
O-Cloud uses open interfaces.
O-Cloud
Open 
fronthaul
O-RU
Near-RT 
RIC
O-CU O-DU
Scenario A O-RAN 
Physical NF
O-CloudScenario B
O-RAN 
Physical NF
O-CloudScenario C O-RAN 
Physical NF
Scenario D O-RAN 
Physical NF
O-CloudScenario C.1 &  C.2
O-Cloud
O-Cloud
O-Cloud O-RAN 
Physical NF
O-CloudScenario E & E.1 O-Cloud & optional O-RAN 
Physical NF
O-Cloud
E2
F1, E2
Open FH
O-RAN 
Physical NF
O-CloudScenario F O-Cloud O-Cloud
E2 F1
E2
UPF
Edge Cloud Cell Site
Edge Cloud Cell SiteRegional Cloud
Edge Cloud Cell SiteRegional Cloud
Edge Cloud Cell SiteRegional Cloud
Edge Location Cell SiteRegional Cloud
Cell SiteRegional Cloud
Edge Cloud Cell SiteRegional Cloud

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
44 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
6 Deployment Scenarios and Implementation 
Considerations 
This section reviews each of the deployment scenarios in turn.  For a given scenario, the requirements that apply to the 
O-RAN Physical NFs, O-RAN Cloudified NFs or O-Cloud platforms may become more specific and unique, while many 
of the logical Network Function requirements will remain the same.   
Please note that in all of the scenario figures of this section, the interfaces are logical interfaces (e.g., F1, E2, etc.) .  This 
has a couple of implications.  First, the two functions on each side of an interface could be on different devices separated 
by physical transport connections (e.g., fiber or Ethernet transport connections), could be on different devices within the 
same cloud platform, or could even exist within the same server.  Second, the functions on each side of an interface could 
be from the same vendor or different vendors.  
In addition, please note that all User Plane interfaces are shown with a solid lines, and all Control Plane interfaces use 
dashed lines.  
Editor’s note: The terms vO-CU and vO -DU represent virtualized or containerized O -CU and O -DU, and 
are used interchangeably with O-CU and O-DU in these scenarios (with the exception when the O-DU 
is explicitly stated as a non-virtualized O-DU). 
 
6.1 Scenario A  
In this scenario, the near -RT RIC, O -CU, and O -DU functions are all virtualized on the same cloud platform, and 
interfaces between those functions are within the same cloud platform.    
This scenario supports deployments in dense urban areas with an abunda nce of fronthaul capacity that allows BBU  
functionality to be pooled in a central location with sufficiently low latency to meet the O -DU latency requirements . 
Therefore, it does not attempt to centralize the near -RT RIC more than  the limit that O -DU functionality can be 
centralized.  
 
Figure 28:  Scenario A 
Also please note that if the optional FH GW is present, the interface between it and the Radio Unit might not meet the O-
RAN Fronthaul requirements (e.g., it might be an Option 8 interface), in which case the Ra dio Unit could be referred to 
as an “RU”, not an “O-RU”.  However, if FH GWs are defined to support an interface such as Option 8, it could be argued 
that the O-RU definition at that time will support Option 8.   
 Key Use Cases and Drivers 
Editor’s Note:  This section is FFS.  
6.2 Scenario B 
In this scenario, the near -RT RIC Network Function is virtualized on a Regional Cloud Platform, and the O -CU and O-
DU functions are virtualized on an Edge Cloud hardware platform that in general will be at a different location.  The 


                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
45 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
interface between the Near-RT RIC network function and the O-CU/O-DU network functions is E2.  Interfaces between 
the O-CU and O-DU Network Functions are within the same Cloud Platform.  
 
Figure 29:  Scenario B – NR Stand-alone 
This scenario addresses deployments in locations with limited remote fronthaul capacity and O-RUs spread out in an area 
that limits the number of O-RUs that can be supported by pooled vO-CU/vO-DU functionality while still meeting the O-
DU latency requireme nts.  The use of a FH GW in the architecture allows significant savings in providing transport 
between the O-RU and vO-DU functionality. 
 
Figure 30: Scenario B – MR-DC (inter-RAT NR and E-UTRA) regardless of the Core Network type (either EPC 
or 5GC) 
An Alternative to NR Standalone scenario B is given by the MR -DC (inter-RAT NR/E-UTRA) scenarios which extend 
requirements on the cloud platform to additionally support E-UTRA network functions (subscript E)  and required 
interfaces Xn, open fronthaul and W1. The W1 interface, defined in 3GPP TS 37.470, only applies to E -UTRA nodes 
connected to 5G Core Network, i.e. ng -eNB as defined in 3GPP TS 38.300 and TS 38.401. Moreover, the foreseen MR -
DC (inter-RAT NR/E-UTRA) scenarios also include  the EPC-connected E-UTRA-NR Dual Connectivity (EN -DC) by 
properly replacing the Xn interface with the X2 interface interconnecting E-UTRA nodes (eNBs) and NR ones (en-gNBs), 
with the possibility to exploit vO-CU/vO-DU functional split only for the en-gNBs6.  
As discussed earlier in Section 5.1.3, the O-CU and O-DU functions can be virtualized using either simple centralization 
or pooled centralization.  The desire is to have support for pooled centralization, although we need to under stand what 
needs to be developed to enable such sharing.  Perhaps pooling will be a later feature, but any initial solution should not 
preclude a future path to a pooled solution.    
 
6 O-eNB vO-CUE/vO-DUE split (foreseen in 3GPP), is pending O-RAN architecture alignment in wg1. 
O-RU
(/RU)
Near-RT 
RIC
Open chassis and 
blade spec
Cloud 
Platform
Cell siteRegional cloud
Cloud Platform
E2
1:L
Edge cloud
UPF, 
MEC
vO-CU vO-DU
Open chassis and 
blade spec
E2
F1
Open 
fronthaul
1:N
GW 
(Opt)
Near-RT 
RIC
Open chassis and 
blade spec
Cloud 
Platform
Cell siteRegional cloud
Cloud Platform
E2
1:L
Edge cloud
vO-CUN vO-DUN
E2
F1
Open 
fronthaul
1:N
GW 
(Opt)
Open chassis and 
blade spec
Xn
E2
Open 
fronthaul
GW 
(Opt)
1:N
(*) for ng-eNB only
vO-DUE
W1(*)
vO-CUE
O-eNB
O-RUN
(/RUN)
O-RUE
(/RUE)

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
46 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
 Key Use Cases and Drivers 
In this case, there are multiple O -RUs distributed in an area served by a centralized vO-DU functionality that can meet 
the latency requirements.  Depending on the concentration of the O -RUs, N could vary, but in general is expected to be 
engineered to support < 64 TRPs per O-DU.7  The near-RT RIC is centralized further to allow for optimization based on 
a more global view (e.g., a single large metropolitan area), and to reduce the number of separate near -RT RIC instances 
that need to be managed.   
The driving use case for this is to support an outdoor deployment of  a mix of Small Cells and Macro cells in a relatively 
dense urban setting.  This can support mmWave as well as Sub-6 deployments. 
In this scenario, a given “virtual BBU” supports both vO-CU and vO -DU functions, and  can connect many O-RUs.  
Current studies show that savings  from pooling are significant but level off  once more than 64 Transmission Reception 
Points (TRPs) are pooled.  This would imply N would be around 32-64. This deployment should support tens of thousands 
of O-RUs per near-RT RIC, so L could easily exceed 100.   
Below is a summary of the cardinality requirements assumed for this scenario.  
  Table 2:  Cardinality and Delay Performance for Scenario B 
 
Attribute RIC – O-CU O-CU – O-DU O-DU – O-RU/RU 
 
Example Cardinality L = 100+ M=1 N = 1-64  
6.3 Scenario C 
In this scenario, the near -RT RIC and O -CU Network Functions are virtualized on a Regional Cloud Platform with a 
general server hardware platform, and the O-DU Network Functions are virtualized on an Edge Cloud hardware platform 
that is expected to include significant hardware accelerator capabilities.  Interfaces between the near-RT RIC and the O-
CU network functions are within the same Cloud Platform.  The interface between the Regional Cloud and the Edge cloud 
is F1, and an E2 interface from the near-RT RIC to the O-DU must also be supported.  
 
Figure 31:  Scenario C 
This scenario is to support deployments in locations with limited remote Fronthaul capacity and O -RUs spread out in an 
area that limits the number of O-RUs that can be pooled while still meeting the O-DU latency requirements. It also applies 
to some whitebox macrocell deployments. The O-CU Network Function is further pooled to increase the efficiency of the 
hardware platform which it shares with the near-RT RIC Network Function.   
However, note that if a service type has tighter O-CU delay requirements than other services, then that may either severely 
limit the number of O -RUs supported by the Regional cloud, or a method will be needed to separate the processing o f 
such services.  This will be discussed further in the following C.1 and C.2 Scenarios.   
 
7 It is assumed that one O-RU is associated with one TRP.  For example, if a cell site has three sectors, then each sector would have at least one TRP 
and hence at least three O-RUs.  


                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
47 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
The use of a FH GW in the architecture allows significant savings in providing transport between the O -RU and vO-DU 
functionality.   
 Key Use Cases and Drivers 
In this case, there are multiple O -RUs distributed in an area where each O -RU can meet the latency requir ement for the 
pooled vO-DU function.  The near -RT RIC and O -CU Network Functions are further centralized to realize additional 
efficiencies.   
A use case for this is to support an outdoor deployment of a mix of Small Cells and Macro cells in a relatively dense 
urban setting.  This can support mmWave as well as Sub-6 deployments. 
In this scenario, as in Scenario B, the Edge Cloud is expected to support rough ly 32-64 O-RUs. This deployment should 
support tens of thousands of O-RUs per near-RT RIC.  
Below is a summary of the cardinality and the distance/delay requirements assumed for this scenario.   
Table 3:  Cardinality and Delay Performance for Scenario C   
 
Attribute RIC – O-CU O-CU – O-DU O-DU – O-RU/RU 
 
Example Cardinality L= 1 M=100+  N=Roughly 32-64 
 Scenario C.1, and Use Case and Drivers 
This is a variation of Scenario C, driven by the fact that different types of traffic (network slices) have different latency  
requirements.  In particular, URLLC has more demanding user -plane latency requirements, and Figure 32 below shows 
how the vO-CU User P art (vO-CU-UP) could be terminated in different places for different network  slices.  Below, 
network slice 3 is terminated in the Edge Cloud.  This scenario is also suitable in case there isn’t enough space or power 
supply to install all vO-CUs and vO-DUs in one Edge Cloud site.  
 
Figure 32:  Treatment of Network Slices:  MEC for URLLC at Edge Cloud, Centralized Control, Single vO-DU 
In Scenario C.1, all O -CU control is placed in the Regional Cloud, and there is a single vO -DU for all Network Slices.  
Only the placement of the vO -CU-CP differs, depending on the network slice.  Below is the diagram of this scenario, 
using the common diagram conventions of all scenarios.  


                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
48 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
 
Figure 33:  Scenario C.1 
Below is a summary of the cardinality and the distance/delay requirements assumed for this scenario.  The URLLC user 
plane requirements are what drive the placement of the vO-CU-UP function to be in the Edge cloud.   
Table 4:  Cardinality and Delay Performance for Scenario C.1 
 
Attribute RIC – O-CU O-CU – O-DU O-DU – O-RU/RU 
 
Example Cardinality L= 1 M=320 N=100 
Delay Max  
1-way (distance)    mMTC NA 625 μs (125 km) 100 μs (20 km)  
   eMBB NA 625 μs (125 km) 100 μs (20 km) 
   URLLC (user/control) NA 100 μ  (20 km)/625 μs (125 
km) 
100 μs (20 km) 
 
 Scenario C.2, and Use Case and Drivers 
This is a second variation of Scenario C, which utilizes the same method of placing some vO-CU user plane functionality 
in the Edge Cloud, and some in the Regional Cloud.  However, instead of having one vO-DU for all network slices, there 
are different vO-DU instances in the Edge Cloud.  
It is driven by factors including the following two use cases: 
• One driver is RAN (O-RU) sharing among operators. In this use case, any operator  can flexibly launch vO-CU 
and vO-DU instances at Edge or Regional Cloud site.  For example, as shown in Figure 34, Operator #1 wants 
to launch the vO-CU1 instance in the Regional Cloud, and the vO-DU1 instance at subtending Edge Cloud sites. 
On the other hand, O perator #2 wants to install both the vO-CU2 and vO-DU2 instances at the same Regional 
Cloud site.  Note that both operators will share the O-RU).  
• Another driver is that, even within a single operator, that operator can customize scheduler functions depending 
on the network slice types, and can place the vO-CU and vO-DU instances depending on the network slice types. 
For example, an operator may launch both vO-CU and vO-DU at the edge cloud site (see Operator #2 below) to 
provide a URLLC service.   


                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
49 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
 
Figure 34:  Treatment of Network Slice: MEC for URLLC at Edge Cloud, Separate vO-DUs 
The multi-Operator use case has the following pros and cons: 
Pros: 
• O-RU sharing can reduce TCO 
• Flexible CU/DU location allows deployments to consider not only service requirements but also limitation s of 
space or power in each site 
Cons: 
• Allowing multiple operators to share O -RU resources is expected to require changes to the Open Fronthaul 
interface (especially the handshake among more than one vO-DU and a given O-RU).   
• This change seems likely to have  M-plane specification impact.  Therefore, this approach would n eed O-RAN 
buy-in and approval.   
Figure 35 below illustrates how different Component Carriers can be allocated to different operators, at the same O -RU 
at the same time.  Note that some updates of not only M -plane but also CUS-plane specifications will be required when 
considering frequency resource sharing among DUs. 
 
Figure 35:  Single O-RU Being Shared by More than One Operator 
The diagram of how Network Functions map to Networks Elements for Scenario C.2 is shown below.  


                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
50 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
 
Figure 36:  Scenario C.2 
The performance requirements are the same as those discussed earlier for Scenario C.1 in Section 6.3.2. 
6.4 Scenario D  
This scenario is a variation on Scenario C , but in this case the O-DU functionality is supported by a n O-RAN Physical 
NF rather than an O-Cloud.  
The general assumption is that Scenario D has the same use cases and performance requirements as Scenario C, and the 
primary difference is in the business decision of how the O-RAN Physical NF based solution compares with the O-RAN 
compliant O-Cloud solution.  Implementation considerations (discussed in Section 5.1) could lead a carrier to decide that 
an acceptable O-Cloud solution is not available in a deployment’s timeframe.   
 
Figure 37:  Scenario D 
6.5 Scenario E  
In contrast to Scenario D, this scenario assumes that not only can the O -DU be virtualized as in Scenario C, but that the 
O-RU can also be successfully virtualized.  Furthermore, the O -RU and O -DU would be implemented in the same O-
Cloud, which has acceleration hardware if required by either or both the O-RU and O-DU.   
Note, this seems to be a future scenario, and is not part of our initial focus.   


                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
51 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
 
Figure 38:  Scenario E 
 Key Use Cases and Drivers 
Because the O -DU and O -RU are implemented in the same O-Cloud in this Scenario , it seems that the O -DU 
implementation must meet the environmental and accessibility requirements typic ally associated with an O -RU.  
Therefore, an indoor use case seems most appropriate.  
 Scenario E.1 vO-DU with O-RU 
For Macrocell deployment with the Open Hardware approach that is used in WG7, the O -DU 7-2 of O-RAN WG7 
OMAC HAR 0-v01.00 [13] can be a virtual function. In this small-scale scenario, HW acceleration is optional. The 
Cloud platform could be physically located near or at the bottom of the tower and be associated with a  number of O-
RUs implemented with the Open HW design, possibly but not necessarily in the same chassis.  
 
Figure 39: Scenario E.1 
 
6.6 Scenario F  
This is a variation on Scenario E in which the O-DU and O-RU are both virtualized, but in different O-Clouds. This means 
that: 
• The O-DU function can be placed in a more convenient location in terms of accessibility for maintenance and 
upgrades. 
• The O-DU function can be placed in an environment that is semi -controlled or controlled, which reduces some 
of the implementation complexity.  
 
Near-RT 
RIC
Open chassis and 
blade spec
Cloud Platform
Regional cloud
vO-DU
Cloud Platform
F1
1:M
vO-CU
Cell site
E2
Open chassis and 
blade spec
UPF, 
MEC
E2
O-RU
(/RU)
GW 
(Opt)
Open fronthaul
1:N
O-RU uses Open 
HW design

                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
52 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
 
Figure 40:  Scenario F 
 Key Use Cases and Drivers 
Because this assumes that the O-RU is virtualized, this is a future use case. 
This use case seems to be better suited for outdoor deployments (e.g., pole mounted) than Scenario E. 
6.7 Scenarios of Initial Interest 
More scenarios have been identified than can be addressed in the initial release of this document.  Scenario B has been 
selected as the one to address initia lly, and to be the subject of detailed treatment in a Scenario document (refer back to 
Figure 1).  Other scenarios are expected to be addressed in later work.   
 
7 Appendix A (informative):  Extensions to Current 
Deployment Scenarios to Include NSA 
In this appendix, some extensions to (some of) the current deployment scenarios are proposed with the aim of introducing 
Non-Standalone (NSA) in the pictures, consistently with the scope O -RAN cloud architecture. These extensions will be 
the basis of the discussion for next version of the present document. In the following charts the subscript ‘N’ is indicating 
blocks related to NR, while the subscript ‘E’ is indicating blocks related to E-UTRA.8  For E-UTRA, the W1 interface is 
indicated. Its definition is ongoing in a 3GPP work item. 
 
8 No UPF or MEC blocks are explicitly indicated in the figures of this appendix, as the focus of this appendix is on the radio part. 


                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
53 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
7.1 Scenario A 
 
Figure 41:  Scenario A, Including NSA 
7.2 Scenario B 
Editor’s Note: Scenario B, Including NSA has been incorporated into 6.2.  
7.3 Scenario C 
 
Figure 42:  Scenario C, Including NSA 
7.4 Scenario C.2 
The scenario addresses both the single and multi-operator cases. To reduce the complexity in the figure the multi operator 
case is considered, so no X2/Xn interface is present between CUN1 and CUE2 or between CUE1 and CUN2. 


                                                                                                                      O-RAN.WG6.CADS-v5.00 TR 
 
54 
Copyright © 2023 by the O-RAN Alliance e.V. Your use is subject to the copyright statement on the cover page of this specification. 
 
 
Figure 43:  Scenario C.2, Including NSA 
7.5 Scenario D 
 
Figure 44:  Scenario D, Including NSA  
 
