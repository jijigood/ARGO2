O-RAN.WG9.XPSAAS-v01.00
Technical Specification
O-RAN Open Transport Working Group 9
 Xhaul Packet Switched Architectures and Solutions
This is a re-published version of the attached final specification.
For this re-published version, the prior versions of the IPR Policy will apply, except that the previous
requirement for Adopters (as defined in the earlier IPR Policy) to agree to an O-RAN Adopter License
Agreement to access and use Final Specifications shall no longer apply or be required for these Final
Specifications after 1st July 2022.
The copying or incorporation into any other work of part or all of the material available in this specification in
any form without the prior written permission of O-RAN ALLIANCE e.V.  is prohibited, save that you may
print or download extracts of the material on this site for your personal use, or copy the material on this site for
the purpose of sending to individual third parties for their information provided that you acknowledge O-RAN
ALLIANCE as the source of the material and that you inform the third party that these conditions apply to them
and that they must comply with them.

Copyright © 2021 by O-RAN ALLIANCE e.V.
By using, accessing or downloading any part of this O-RAN specification document, including by copying, saving, distributing,
displaying or preparing derivatives of, you agree to be and are bound to the terms of the O -RAN Adopter License Agreement
contained in Annex ZZZ of this specification. All other rights reserved.
O-RAN ALLIANCE e.V.
Buschkauler Weg 27, 53347 Alfter, Germany
Register of Associations, Bonn VR 11238
VAT ID DE321720189

.
                                        O-RAN.WG9.XPSAAS-v01.00
Technical Specification

O-RAN Open Transport Working Group 9

 Xhaul Packet Switched Architectures and Solutions

© 2021 O-RAN ALLIANCE e.V. All Rights Reserved                     1

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             2
1 Revision History 1
Date Revision Author Description
2020/11/11 V01.00 All Authors 1st revision outlining a packet switched O-RAN
solution based on an underlay transport
infrastructure based on MPLS or IPv6 with
Segment Routing (SRv6) with mobile services
provided by Multi-protocol BGP based VPNs.
 2
1.1 Contributors 3
Editor: Simon Spraggs 4
Contributors in alphabetical order: Jennifer Andreoli-Fang, Lujing Cai, Francois Fredricx, Kashif 5
Islam, Luis Miguel Contreras Murillo, Toby Rees, Simon Spraggs, Krzysztof Szarkowicz, Reza 6
Vaez-Ghaemi, Nader Zein, Jeffrey Zhang 7
 8
 9
 10
  11
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             3
 1
2 Contents 2
1 Revision History ....................................................................................................................................... 2 3
1.1 Contributors ....................................................................................................................................................... 2 4
3 Scope ........................................................................................................................................................ 7 5
4 References .............................................................................................................................................. 10 6
5 Definitions and abbreviations ................................................................................................................. 17 7
5.1 Definitions ....................................................................................................................................................... 17 8
5.2 Abbreviations ................................................................................................................................................... 17 9
6 5G Transport network requirements....................................................................................................... 22 10
7 5G logical connectivity requirements..................................................................................................... 24 11
7.1 Fronthaul .......................................................................................................................................................... 24 12
7.1.1 O-RAN 7.2x Fronthaul ............................................................................................................................... 24 13
7.1.2 O-RAN Fronthaul logical transport requirements ...................................................................................... 25 14
7.2 Non-ORAN Fronthaul ..................................................................................................................................... 26 15
7.2.1 eCPRI based C-RAN solutions .................................................................................................................. 26 16
7.2.2 Radio over Ethernet (RoE) based C-RAN solutions .................................................................................. 26 17
7.2.3 Non O-RAN Fronthaul logical transport requirements .............................................................................. 28 18
7.3 Midhaul logical transport requirements ........................................................................................................... 28 19
7.3.1 Overall Midhaul logical transport requirements ......................................................................................... 29 20
7.4 Backhaul logical transport requirements.......................................................................................................... 29 21
7.4.1 Overall Backhaul logical transport requirements ....................................................................................... 30 22
8 Operator use cases .................................................................................................................................. 31 23
8.1 Scenario 1: C-RAN architecture with collocated O-DU and O-CU ................................................................ 32 24
8.2 Scenario 2: C-RAN architecture with collocated O-RU and O-DU ................................................................ 32 25
8.3 Scenario 3: C-RAN architecture with coexistence of legacy Backhaul traffic ................................................ 33 26
8.4 Scenario 4: C-RAN architecture with coexistence of legacy Fronthaul traffic ................................................ 34 27
8.5 Scenario 5: C-RAN architecture with further split of O-DU and O-CU .......................................................... 35 28
8.6 Scenario 6: C-RAN architecture with local breakout ...................................................................................... 36 29
8.7 Scenario 7: Transport slicing ........................................................................................................................... 36 30
9 Overall packet switched Open Xhaul architecture ................................................................................. 38 31
9.1 Physical layout and xHaul transport options .................................................................................................... 38 32
9.2 Open Xhaul architecture in revision 1 ............................................................................................................. 39 33
9.3 Technology and architectural choices .............................................................................................................. 40 34
9.4 Standardization ................................................................................................................................................ 40 35
9.5 Document organization .................................................................................................................................... 40 36
10 Physical network design for packet switched Xhaul .............................................................................. 42 37
10.1 Packet over fibre .............................................................................................................................................. 43 38
10.1.1 Access ........................................................................................................................................................ 44 39
10.1.2 Pre-aggregation / Aggregation / Core transport ......................................................................................... 48 40
10.2 Alternative physical transport solutions ........................................................................................................... 49 41
10.2.1 WDM in access network ............................................................................................................................ 50 42
10.2.2 Passive Optical Networks (PONs) ............................................................................................................. 50 43
10.2.3 DOCSIS Networks ..................................................................................................................................... 55 44
10.2.4 Microwave and mmave radio transport technologies ................................................................................. 58 45
10.3 Data Centers..................................................................................................................................................... 65 46
10.3.1 Complete separation between DC and WAN infrastructure....................................................................... 65 47
10.3.2 DC integrated into WAN infrastructure. .................................................................................................... 65 48
11 Packet-switched underlay network – MPLS based ................................................................................ 66 49
11.1 MPLS data plane .............................................................................................................................................. 66 50
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             4
11.2 MPLS control plane ......................................................................................................................................... 67 1
11.3 Classic MPLS control plane............................................................................................................................. 68 2
11.4 SR/MPLS control plane ................................................................................................................................... 70 3
11.4.1 Interior Gateway Protocol (IGP) for SR/MPLS ......................................................................................... 70 4
11.4.2 SR/MPLS Traffic Engineering ................................................................................................................... 74 5
11.5 Scaling the MPLS infrastructure ...................................................................................................................... 75 6
11.5.1 Seamless MPLS architecture ...................................................................................................................... 75 7
11.5.2 Controller based network scaling architectures .......................................................................................... 77 8
11.6 MPLS Quality of Service ................................................................................................................................. 83 9
11.7 MPLS OAM..................................................................................................................................................... 83 10
11.8 IP/MPLS service infrastructure........................................................................................................................ 83 11
12 Packet-switched underlay network – SRv6 based .................................................................................. 84 12
12.1 SRv6 data plane ............................................................................................................................................... 84 13
12.2 SRv6 control plane........................................................................................................................................... 84 14
12.2.1 Interior Gateway Protocol (IGP) for SRv6 ................................................................................................. 85 15
12.2.2 SRv6 Traffic Engineering .......................................................................................................................... 89 16
12.2.3 Inter-domain connectivity .......................................................................................................................... 90 17
12.3 Scaling an SRv6 underlay infrastructure ......................................................................................................... 91 18
12.3.1 Route summarization and redistribution .................................................................................................... 91 19
12.3.2 Controller based scaling ............................................................................................................................. 93 20
12.3.3 SRv6 scaling conclusion ............................................................................................................................ 94 21
12.4 IPv6 Quality of Service .................................................................................................................................... 94 22
12.5 SRv6 OAM ...................................................................................................................................................... 94 23
12.5.1 Ping / Traceroute to a remote IPv6 network address .................................................................................. 94 24
12.5.2 Ping / Traceroute to remote SID functions ................................................................................................. 95 25
12.6 SRv6 on-going standardisation ........................................................................................................................ 95 26
12.7 SRv6 Service infrastructure ............................................................................................................................. 96 27
13 Packet-switched Xhaul services Infrastructure ...................................................................................... 97 28
13.1 MP-BGP design ............................................................................................................................................... 97 29
13.2 Ethernet services .............................................................................................................................................. 97 30
13.2.1 Ethernet services redundancy ..................................................................................................................... 98 31
13.3 IP Services ..................................................................................................................................................... 103 32
13.3.1 Building flexible L3VPN service topologies ........................................................................................... 103 33
13.3.2 Constraints based Traffic Steering in L3VPNs ........................................................................................ 103 34
14 Quality of Service in packet-switched networks .................................................................................. 105 35
14.1 Xhaul transport core interface QoS ................................................................................................................ 105 36
14.1.1 Transport network core interface classification........................................................................................ 105 37
14.1.2 Core interface queue structure .................................................................................................................. 105 38
14.1.3 Transport network core interface marking structure ................................................................................ 107 39
14.1.4 Core interface scheduling model .............................................................................................................. 107 40
14.2 Xhaul transport network edge interface QoS ................................................................................................. 108 41
14.2.1 Transport network domain PE ingress classification of Ethernet frames. ................................................ 108 42
14.2.2 Transport domain PE ingress classification IP packets. ........................................................................... 110 43
14.2.3 Admission control .................................................................................................................................... 111 44
14.2.4 PE Egress scheduling ............................................................................................................................... 111 45
15 Multicast ............................................................................................................................................... 113 46
15.1 Multicast use cases......................................................................................................................................... 113 47
15.1.1 Multicast transport for fixed line services ................................................................................................ 113 48
15.1.2 MBMS/5MBS transport ........................................................................................................................... 113 49
15.2 Overlay and underlay multicast ..................................................................................................................... 114 50
15.3 Recommendation/considerations for multicast solutions ............................................................................... 114 51
16 Packet-switched orchestration and telemetry ....................................................................................... 116 52
17 5G Slicing in a packet switched Xhaul network .................................................................................. 117 53
17.1 Packet-switched underlay network ................................................................................................................ 118 54
17.1.1 Underlay forwarding plane ....................................................................................................................... 118 55
17.1.2 Single forwarding plane for all slices ....................................................................................................... 119 56
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             5
17.1.3 Forwarding plane per 5G service ............................................................................................................. 119 1
17.1.4 Forwarding plane per slice customer ........................................................................................................ 119 2
17.2 Quality of Service .......................................................................................................................................... 119 3
17.2.1 Edge QoS ................................................................................................................................................. 119 4
17.2.2 Core QoS .................................................................................................................................................. 120 5
17.3 5G Services and slices ................................................................................................................................... 121 6
18 Supporting mobile scenarios on a packet switched Xhaul network ..................................................... 122 7
18.1 Physical network ............................................................................................................................................ 122 8
18.2 Logical underlay architecture ........................................................................................................................ 123 9
18.2.1 Underlay Quality of Service (QoS) .......................................................................................................... 124 10
18.3 Service architecture........................................................................................................................................ 125 11
18.3.1 Automated VPN Traffic Steering ............................................................................................................. 125 12
18.4 Mobile services .............................................................................................................................................. 125 13
18.4.1 Open Fronthaul ......................................................................................................................................... 125 14
18.4.2 Non O-RAN Fronthaul ............................................................................................................................. 128 15
18.4.3 Midhaul and Backhaul ............................................................................................................................. 128 16
18.5 Scenario 1 and 5 ............................................................................................................................................. 130 17
18.6 Scenario 2 ...................................................................................................................................................... 132 18
18.7 Scenario 3 5G C-RAN with legacy D-RAN .................................................................................................. 133 19
18.7.1 Scenario 3a ............................................................................................................................................... 133 20
18.7.2 Scenario 3b ............................................................................................................................................... 133 21
18.8 Scenario 4 5G C-RAN with RoE mappers ..................................................................................................... 134 22
18.9 Scenario 6 5G C-RAN with distributed UPF ................................................................................................. 135 23
18.10 Scenario 7 Slicing .......................................................................................................................................... 135 24
18.10.1 Slice design .............................................................................................................................................. 136 25
Annex A: Overview of “Segment Routing” (SR)........................................................................................... 138 26
Background ..................................................................................................................................................................... 138 27
Segment Routing ............................................................................................................................................................ 138 28
Segment Routing architectural principle ........................................................................................................................ 139 29
Segment Routing data plane ........................................................................................................................................... 140 30
Segment Routing control plane ....................................................................................................................................... 141 31
Annex B: IETF Ethernet Virtual Private Networks ........................................................................................ 144 32
Annex C: MP-BGP based L3VPNs ................................................................................................................ 151 33
Building blocks of a L3 VPN service ............................................................................................................................. 151 34
Traffic Steering into an BGP VPN ................................................................................................................................. 153 35
Annex D: Quality of Service .......................................................................................................................... 155 36
What is Quality of Service? ............................................................................................................................................ 155 37
Why do we need QoS? ................................................................................................................................................... 155 38
QoS functional elements ................................................................................................................................................. 156 39
Network level behaviour ................................................................................................................................................. 156 40
Node level behaviour ...................................................................................................................................................... 157 41
Traffic classification and marking .................................................................................................................................. 157 42
Congestion management ................................................................................................................................................. 160 43
Congestion avoidance ..................................................................................................................................................... 162 44
Annex E: Multicast Technologies background .............................................................................................. 167 45
Overlay multicast ............................................................................................................................................................ 167 46
PIM-based overlay signalling for IPVPN ....................................................................................................................... 167 47
BGP-based overlay signalling for IPVPN and EVPN .................................................................................................... 167 48
Underlay multicast .......................................................................................................................................................... 168 49
MVPN/EVPN and Seamless MPLS/SR ......................................................................................................................... 169 50
Annex ZZZ : O-RAN Adopter License Agreement ....................................................................................... 170 51
Section 1: DEFINITIONS .............................................................................................................................................. 170 52
Section 2: COPYRIGHT LICENSE ............................................................................................................................... 170 53
Section 3: FRAND LICENSE ........................................................................................................................................ 171 54
Section 4: TERM AND TERMINATION ...................................................................................................................... 171 55
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             6
Section 5: CONFIDENTIALITY ................................................................................................................................... 172 1
Section 6: INDEMNIFICATION ................................................................................................................................... 172 2
Section 7: LIMITATIONS ON LIABILITY; NO WARRANTY .................................................................................. 172 3
Section 8: ASSIGNMENT ............................................................................................................................................. 173 4
Section 9: THIRD-PARTY BENEFICIARY RIGHTS .................................................................................................. 173 5
Section 10: BINDING ON AFFILIATES ...................................................................................................................... 173 6
Section 11: GENERAL ................................................................................................................................................... 173 7
  8
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             7
3 Scope 1
This Technical Specification has been produced by the O -RAN Alliance. The document is intended 2
to describe best practises for O -RAN transport based on end -to-end packet switching technology. It 3
is recognised that other solutions, not based on packet switching, could be employed or mixed with a 4
packet switching solution. Beyond the solutions described in this document, other packet switching 5
solutions may be adequate for Xhaul transport networks and can be considered in future versions of 6
this document.   7
 8
This specification defines an architecture for an Open Xhaul transport network based on an end -to-9
end packet switching architecture that utilises statistical multiplexing and a hierarchy of packet 10
switching “Transport Node Equipment” (TNE) starting at the c ell site in the access layer and going 11
to the core layer of the transport network capable of supporting the requirements outlined in the O-12
RAN WG9 Transport Requirements document [15]. 13
 14
Within the transport core, aggregation and pre -aggregation it is assumed the L0/L1 transport 15
technology connecting the packet switches are high capacity, low delay Ethernet point to point 16
circuits. These circuits can be derived from dark fibre, WDM or othe r technologies capable of 17
presenting Ethernet interfaces and where the delay component primarily consists of light propagation 18
within the fibre.  This technology is clearly very important but out of scope of this document.   19
 20
To allow the operators to offe r the most flexibility in designing their RAN infrastructure the access 21
network should utilise the same design paradigm as the transport core, aggregation and pre -22
aggregation. However, in some instances this may not be an option, so this document identifies other 23
potential access technologies, provides a description and considerations/trade-offs for their usage.   24
  25
Figure 3-1 illustrates the scope of network segments covered by WG9. The area inside the dotted 26
green line characterizes the transport networks composed of a number of Transport Network 27
Elements (TNE) deployed among different components defined in other O-RAN WGs. WG9 does 28
not define the interfaces along the dotted green line. As an example, the fronthaul interface of an O-29
RU or O-DU are defined by WG4.  30
 31
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             8
 1
 2
Figure 3-1 Xhaul Transport Network Overview 3
 4
WG9 focuses on option 7-2x. Functional elements translating option 7-2x to non O-RAN lower layer 5
split option (Fronthaul Gateways  FHG) are considered to be part of radio network, and beyond the 6
scope of this document. The same applies to Fronthaul Multiplexer (FHM) and cascaded radios. 7
Radio over Ethernet mapping (RoE, IEEE 1419.3) is covered only as a service provided by the packet-8
based network and is not defined by WG9. The stated functions are logical ones, actual product 9
implementation may combine several of these functions. For example, vendors may market a 10
Fronthaul Gateway product that combines different elements such as TNE, FHG and RoE mapper in 11
one and the same physical box. 12
 13
WG9 sub-teams are working on several solution documents. This document focuses on packet-14
based transport technologies. It also includes sections on some of the technologies used in physical 15
network such as PON, DOCSIS® networks and wireless Xhaul. WDM in the access network 16
providing Fronthaul services is covered in a different document [16]. 17
 18
This document firstly covers: 19
 20
1. High level transport requirements 21
2. 5G Fronthaul, Midhaul, Backhaul transport requirement 22
3. 5G operator use cases   23
 24
The document then considers packet infrastructure and how the packet infrastructure can support the 25
identified requirements and use cases. It describes a packet architecture consisting of an u nderlay 26
packet switching infrastructure which supports L2 and L3 services.  27
The document covers two potential underlay solutions; MPLS based or SRv6 based. In both cases the 28
service infrastructure consists of MP-BGP VPN solutions supporting L2 and L3 services. 29

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             9
The last part of the document outlines, using examples, how the packet switching infrastructure can 1
support the operator use cases outlined earlier in the document  2
 3
This document does not cover: 4
 5
1. Timing and synchronization –Another O-RAN WG-9 effort is underway. [17] 6
2. WDM in access for Fronthaul services -Another O-RAN WG-9 effort is underway[16].  7
3. Highly specialised URLLC, for example motion control in an industrial setting. I.E. Private 8
network, tightly constrained, requiring TSN in the Backhaul and RAN looking like a TSN 9
bridge. This can be covered in a later edition. 10
4. OTN or SPN/G.mtn as a transport layer. 11
 12
This document uses information and requirements published by O-RAN, 3GPP, IEEE, ITU-T, 13
IETF, CableLabs, NGMN, MEF, BBF and many other standard bodies and industry associations. 14
 15
  16
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             10
4 References 1
The following documents contain provisions which, through reference in this text, constitute 2
provisions of the present document. 3
- References are either specific (identified by date of publication, edition number, version 4
number, etc.) or non-specific. 5
- For a specific reference, subsequent revisions do not apply. 6
- For a non-specific reference, the latest version applies. In the case of a reference to a 3GPP 7
document (including a GSM document), a non-specific reference implicitly refers to the latest 8
version of that document in Release 15. 9
 10
  11
3GPP references  12
[1] 3GPP TS 23.501 v6.4.0(2020-03): “System Architecture for 5G” 13
[2] 3GPP TS38.306: “NR; User equipment (UE) radio access capabilities. 14
[3] 3GPP TS 29.060 V15.0.0: “GPRS Tunnelling Protocol (GTP) across the Gn and 15
Gp interface (Release 15)” 16
[4] 3GPP TS 29.274 V16.1.0: “GPRS Tunnelling Protocol (GTP) across the Gn and 17
Gp interface (Release 15)” 18
[5] 3GPP TS 38.401: “NG-RAN; Architecture description” 19
[6] 3GPP TS 36.422 version 15.1.0: “X2 signalling transport (Release 15)” 20
[7] 3GPP TS 36.424 version 15.0.0: “X2 data transport (Release 15)” 21
[8] 3GPP TS 38.474 V15.3.0: “F1 data transport (Release 15)”. 22
[9] 3GPP TS 38.462 V15.5.0: “E1 signalling transport (Release 15)” 23
[10] 3GPP TS 38.422 version 15.0.0 Release 15: Xn general aspects and principles 24
 25
O-RAN references 26
[11] O-RAN: “WG-4 Inter-Operability Testing” 27
[12] ORAN Open F1/W1/E1/X2/Xn Interfaces Working Group “ Transport 28
specification  29
[13] O-RAN Fronthaul Control, User and Synchronization Plane Version 3.0 – 30
March, 2020 31
[14] O-RAN.WG4.MP.0-v03.00: Management plane specification 32
[15] O-RAN WG9.Transport Requirements, draft v00.12, June 2020 33
[16] ORAN WG9.WDM in Fronthaul. Draft v00.01, September 2020  34
[17] ORAN WG9. Timing and Synchronization Architecture and Solution document 35
– ongoing 36
[18] ORAN WG9. Management interfaces - ongoing 37
[19] O-RAN.WG4.CTI-TCP.0-v01.00  38
 39
IEEE references 40
[20] IEEE 802.3-2018: “IEEE Standard for Ethernet” 41
[21] IEEE 802.1Q-2018: “IEEE Standard for Local and metropolitan area 42
networks— Bridges and Bridged Networks” 43
[22] IEEE 802.1CM-2018: “Time-Sensitive Networking for Fronthaul” 44
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             11
[23] IEEE Std 1914.1TM-2019: “IEEE Standard for Packet-based Fronthaul 1
Transport Network”  2
[24] IEEE 802.3av “Physical Layer Specifications and Management Parameters for 3
10 Gb/s Passive Optical Networks”  4
[25] IEEE 802.3bk “Physical Layer Specifications and Management Parameters for 5
Extended Ethernet Passive Optical Networks”  6
[26] IEEE 802.3ca “Physical Layer Specifications and Management Parameters for 7
25 Gb/s and 50 Gb/s Passive Optical Networks” 8
[27] IEEE Std 1914.3-2018 – IEEE standard for radio over Ethernet Encapsulations 9
and Mappings  10
 11
IETF references 12
[28] IETF RFC 791: “INTERNET PROTOCOL” 13
[29] IETF RFC768: “User Datagram Protocol” 14
[30] IETF RFC1195: “Use of OSI IS-IS for routing in TCP/IP and dual 15
environments” 16
[31] IETF RFC1771: “A Border Gateway Protocol 4” 17
[32] IETF RFC2205: “Resource ReSerVation Protocol” 18
[33] IETF RFC2209: “Resource ReSerVation Protocol (RSVP) --Version 1 Message 19
Processing Rules” 20
[34] IETF RFC 2210: “The use of RSVP with IETF Integrated Services” 21
[35] IETF RFC2328: “OSPF Version 2” 22
[36] IETF RFC 2283: “Multiprotocol Extensions for BGP-4” 23
[37] IETF RFC 2475: “An Architecture for Differentiated Services” 24
[38] IETF RFC 2474: “Definition of the Differentiated Services Field (DS Field) in 25
the IPv4 and IPv6 Headers” 26
[39] IETF RFC2545: Use of BGP-4 Multiprotocol Extensions for IPv6 Inter-Domain 27
Routing”       28
[40] IETF RFC2597: “Assured Forwarding PHB Group”       29
[41] IETF RFC2598: “An Expedited Forwarding PHB” 30
[42] IETF RFC2961: “RSVP Refresh Overhead Reduction Extensions” 31
[43] IETF RFC2745: “RSVP Diagnostic Messages” 32
[44] IETF RFC2983: “Differentiated Services and Tunnels” 33
[45] IETF RFC3031: “Multiprotocol Label Switching Architecture” 34
[46] IETF RFC3032: “MPLS Label Stack Encoding”  35
[47] IETF RFC3097: “RSVP Cryptographic Authentication—Updated Message 36
Type Value” 37
[48] IETF RFC3209: “Extensions to RSVP for LSP Tunnels” 38
[49] IETF RFC3212: “Constraint-Based LSP Setup using LDP” 39
[50] IETF RFC 3215: “LDP State Machine” 40
[51] IETF RFC3246: “An Expedited Forwarding PHB (Per-Hop Behavior) 41
[52] IETF RFC3107: “Carry Label Information in BGP-4” 42
[53] IETF RFC3443: “Time To Live (TTL) Processing in Multi-Protocol Label 43
Switching (MPLS) Networks” 44
[54] IETF RFC3477: “Unnumbered Links in Resource ReSerVation Protocol - 45
Traffic Engineering (RSVP-TE)” 46
[55] IETF RFC 3478: “Graceful Restart Mechanism for Label Distribution Protocol”, 47
[56] IETF RFC3630: “Traffic Engineering (TE) Extensions to OSPF Version 2” 48
[57] IETF RFC3719: “Recommendations for Interoperability using ISIS” 49
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             12
[58] IETF RFC4090: “Fast Reroute Extensions to RSVP-TE for LSP Tunnels” 1
[59] IETF RFC4115: “A Differentiated Service Two-Rate, Three-color Market with 2
Efficient Handling of in-Profile Traffic” 3
[60] IETF RFC4182 “Removing a Restriction on the use of MPLS Explicit NULL” 4
[61] IETF RFC4303: “OSPF Extensions in Support of Generalized Multi-Protocol 5
Label Switching (GMPLS)” 6
[62] IETF RFC4206: Label Switched Paths (LSP) Hierarchy with Generalized Multi-7
Protocol Label Switching (GMPLS) Traffic Engineering (TE)” 8
[63] IETF RFC4443: “ICMPv6 (ICMP for IPv6)” 9
[64] IETF RFC4552: “Authentication/Confidentiality for OSPFv3” 10
[65] IETF RFC4558: “Node-ID Based Resource Reservation Protocol (RSVP) 11
Hello” 12
[66] IETF RFC4561: “Record Route Object (RRO) Node-Id Sub-Object” 13
[67] IETF RFC4594: “Configuration Guidelines for DiffServ Service Classes” 14
[68] IETF RFC4364: “BGP/MPLS IP Virtual Private Networks (VPNs)” 15
[69] IETF RFC4684: “Constrained Route Distribution for BGP/MPLS IP VPNs” 16
Computation Element Protocol (PCEP)” 17
[70] IETF RFC4760: “Multiprotocol Extensions for BGP-4” 18
[71] IETF RFC4875: “Extensions to Resource Reservation Protocol – Traffic 19
Engineering for Point to Multipoint TE Label Switched Paths”  20
[72] IETF RFC5036: “LDP Specification” 21
[73] IETF RFC5130: “A Policy Control Mechanism in IS-IS Using Administrative 22
Tags” 23
[74] IETF RFC5283: “LDP Extension for Inter-Area Label Switched Paths (LSPs)” 24
[75] IETF RFC5291: “Outbound Route Filtering Capability for BGP-4” 25
[76] IETF RFC5292 “Address-Prefix-Based Outbound Route Filter for BGP-4” 26
[77] IETF RFC5302 “Domain-Wide Prefix Distribution with Two-Level IS-IS” 27
[78] IETF RFC5303: “Three-Way Handshake for IS-IS Point-to-Point Adjacencies” 28
[79] IETF RFC5304: “IS-IS Cryptographic Authentication” 29
[80] IETF RFC 5305: “IS-IS Extensions for Traffic Engineering” 30
[81] IETF RFC5308: “Routing IPv6 with ISIS” 31
[82] IETF RFC5310: “IS-IS Generic Crypto Authentication” 32
[83] IETF RFC5329: “Traffic Engineering Extensions to OSPF Version 3”  33
[84] IETF RFC5340: “OSPF for IPv6” 34
[85] IETF RFC5420: “Encoding of Attributes for MPLS LSP Establishment Using 35
Resource Reservation Protocol Traffic Engineering (RSVP-TE)” 36
[86] IETF RFC5440: “Path Computational Element (PCE) Communications Protocol 37
(PCEP)” 38
[87] IETF RFC5443: “LDP IGP Synchronization” 39
[88] IETF RFC5496: “The Reverse Path Forwarding (RPF) Vector TLV” 40
[89] IETF RFC5512: “The BGP Encapsulation Subsequent Address Family Identifier 41
(SAFI) and the BGP Tunnel Encapsulation Attribute” 42
[90] IETF RFC5561: “LDP capabilities”  43
[91] IETF RFC5838: “Support of Address Families in OSPFv3” 44
[92] IETF RFC6232: “Purge Originator Identification TLV for IS-IS” 45
[93] IETF RFC6388: “Label Distribution Protocol Extensions for Point-to-Multipoint 46
and Multipoint-to-Multipoint Label Switched Paths”  47
[94] IETF RFC6391: “Flow-Aware Transport of pseudowires over an MPLS Packet 48
Switched Network”  49
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             13
[95] IETF RFC6512: “Using Multipoint LDP When the Backbone has No Route to 1
the ROOT”  2
[96] IETF RFC6513: “Multicast in MPLS/BGP IP VPNs” 3
[97] IETF RFC6514: “BGP Encodings and procedures for multicast in MPLS/BGP 4
IP VPNs” 5
[98] IETF RFC7432: “BGP MPLS-Based Ethernet VPN” 6
[99] IETF RFC7471: “OSPF Traffic Engineering (TE) Metric Extensions” 7
[100] IETF RFC7534: “Inter-Area Point to Multipoint” 8
[101] IETF RFC7570: “Label Switched Path (LSP) Attribute in the Explicit Route 9
Object (ERO)” 10
[102] IETF RFC7752: “BGP Link State (BGP-LS)” 11
[103] IETF RFC7761: “Protocol Independent Multicast” 12
[104] IETF RFC7810: “IS-IS Traffic Engineering (TE) Metric Extensions” 13
[105] IETF RFC8029: “Detecting Multiprotocol Label Switched (MPLS) Data-plane 14
Failures” 15
[106] IETF RFC8200: “Internet Protocol, Version 6 (IPv6) Specification” 16
[107] IETF RFC8214: “Virtual Private Wire Service Support in Ethernet VPN” 17
[108] IETF RFC8231: “Path Computational Element Communications Protocol 18
(PCEP) Extensions for Stateful PCE” 19
[109] IETF RFC8277: “BGP and Labeled Address Prefixes” 20
[110] IETF RFC8287: “Label Switched Path (LSP) Ping/Traceroute for Segment 21
Routing (SR) IGP-Prefix and IGP-Adjacency Segment Identifiers (SIDs) with 22
MPLS Data Planes” 23
[111] IETF RFC8317: “Ethernet-Tree (E-Tree) Support in Ethernet VPN (EVPN) and 24
Provider Backbone Bridging EVPN (PBB-EVPN)” 25
[112] IETF RFC8370: “Techniques to Improve the Scalability of RSVP-TE 26
Deployments”  27
[113] IETF RFC8395: “Extension to BGP-Signaled pseudowires to support flow-28
aware transport labels” 29
[114] IETF RFC8402: “Segment Routing Architecture” 30
[115] IETF RFC8476: “Signalling Maximum SID depth using OSPF” 31
[116] IETF RFC8491: “Signalling Maximum SID depth using IS-IS” 32
[117] IETF RFC8571: “BGP – Link State (BGP-LS) Advertisement of IGP Traffic 33
Engineering Performance Metric Extensions” 34
[118] IETF RFC8577: “Signaling RSVP-TE Tunnels on a Shared MPLS Forwarding 35
Plane” 36
[119] IETF RFC8660: “Segment Routing with MPLS Dataplane” 37
[120] IETF RFC8661: “Segment Routing MPLS Interworking with LDP” 38
[121] IETF RFC8664: “PCEP Extensions for Segment Routing” 39
[122] IETF RFC8666: “OSPFv3 Extensions for Segment Routing” 40
[123] IETF RFC8667: “IS-IS Extensions for Segment Routing” 41
[124] IETF RFC8754: “IPv6 Segment Routing Header (SRH)” 42
[125] IETF RFC8814: “Signaling Maximum SID Depth (MSD) Using the BGP – Link 43
State” 44
[126] “Draft-ietf-bess-bgp-multicast-controller-05: “Controller based BGP Multicast 45
signalling” 46
[127] Draft-ietf-bess-evpn-bum-procedure-updates-08: “Updates on EVPN BUM 47
procedures” 48
[128] Draft-ietf-bess-evpn-vpws-fxc-02: “EVPN VPWS Flexible Cross-Connect 49
Service” 50
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             14
[129] Draft-ietf-bess-evpn-mh-pa-00: “EVPN multi-homing port-active load-1
balancing”  2
[130] Draft-ietf-bess-evpn-prefix-advertisement-11: “IP prefix Advertisements in 3
EVPN” 4
[131] Draft-ietf-bess-evpn-pref-df-06: “Preference-based EVPN DF Election” 5
[132] Draft-ietf-bess-srv6-services-04: “SRv6 BGP based Overlay services” 6
[133] Draft-ietf-idr-bgp-ls-segment-routing-ext-16: “BGP Link-State Extensions for 7
Segment Routing” 8
[134] Draft-ietf-lsr-flex-algo-13: “IGP Flexible Algorithm” 9
[135] Draft-ietf-mpls-ri-rsvp-frr-07: “Refresh-interval independent FRR facility 10
protection”  11
[136] Draft-ietf-pim-sr-p2mp-policy-00: “Segment Routing Point-to-Multipoint 12
Policy” 13
[137] Draft-ietf-rtgwg-segment-routing-ti-lfa-04: “Topology Independent Fast Reroute 14
using Segment Routing” 15
[138] Draft-ietf-pce-binding-label-sid-03: “Carrying Binding Label/Segment-ID in 16
PCE-based Networks” 17
[139] Draft-ietf-lsr-isis-srv6-extensions-11: “IS-IS Extensions to support SR over IPv6 18
dataplane” 19
[140] Draft-ietf-pce-segment-routing-IPv6-06: “PCEP Extensions for Segment 20
Routing leveraging the IPv6 data plane” 21
[141] draft-ietf-pce-segment-routing-policy-cp-00: “PCEP extension to support 22
Segment Routing Candidate Path 23
[142] draft-ietf-spring-segment-routing-policy-08: “Segment Routing Policy 24
Architecture” 25
[143] Draft-ietf-spring-srv6-network-programming-24: “SRv6 Network 26
Programming” 27
[144] Draft-ietf-spring-srv6-yang-00: “Yang Data Model for SRv6 Base and static” 28
[145] Draft-ietf-lsr-ospfv3-srv6-extensions-01: “OSPFv3 Extensions for SRv6”  29
[146] Draft-ietf-idr-bgpls-srv6-ext-03: “BGP Link State extensions for IPv6 Segment 30
Routing (SRv6)” 31
[147] Draft-ietf-6man-spring-srv6-oam-07: “Operations, Administration, and 32
Maintenance (OAM) in Segment Routing Networks with IPv6 Data plane 33
(SRv6)” 34
[148] Draft-ietf-mpls-ri-rsvp-frr-07: “Refresh-interval Independent FRR Facility 35
Protection” 36
 37
Others 38
[149] NGMN “5G RAN CU-DU network architecture, transport options and 39
dimensioning, version 1.0 12 April 2019” 40
[150] MEF 61.1: “IP Service attributes” 41
[151] MEF 10.3: “Ethernet Service attributes” 42
[152] MEF 6.2: “EVC service definition” 43
[153] Broadband Forum TR-101 “Migration to Ethernet-Based Broadband 44
Aggregation” 45
[154] Broadband Forum TR-156 “Using GPON Access in the context of TR-101“ 46
[155] ITU-R M.2083: “IMT Vision – framework and overall objectives of the future 47
development of IMT for 2020 and beyond. 48
[156] ITU-T GSTR-TN5G – Transport network support of IMT 2020/5G  49
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             15
[157] ITU-T G.Sup.66 “5G wireless Fronthaul requirements in a passive optical 1
network context” 2
[158] ITU-T G.9807 series “10-Gigabit-capable symmetric passive optical network”, 3
ITU-T G.989 series “40-Gigabit-capable passive optical networks (NG PON2)“, 4
On-going work ITU-T G.HSP 5
[159] ITU-T G.8271 “Time and phase synchronization aspects of telecommunication 6
networks” 7
[160] ITU-T G.8271.1 “Network limits for time synchronization in packet networks 8
with full timing support from the network” 9
[161] ITU-T G.8273.2 “Timing characteristics of telecom boundary clocks and 10
telecom time slave clocks” 11
[162] ITU-T G.8275.1 “Precision time protocol telecom profile for phase/time 12
synchronization with full timing support from the network” 13
[163] 40m Transmission of OAM mode and Polarization Multiplexing in E-band, 14
Globcom, Dec 2019. M.Hirabe, et. Al 15
[164] CableLabs “Low Latency Mobile Xhaul over DOCSIS Technology” 16
https://www.cablelabs.com/specifications/CM-SP-LLX 17
[165] CableLabs “Synchronization Techniques for DOCSIS Technology 18
Specification” https://www.cablelabs.com/specifications/CM-SP-SYNC 19
[166] CableLabs “Data-Over-Cable Service Interface Specifications 3.1, MAC and 20
Upper Layer Protocols Interface” 21
https://www.cablelabs.com/specifications/CM-SP-MULPIv3.1 22
[167] CableLabs “Data-Over-Cable Service Interface Specifications 4.0, MAC and 23
Upper Layer Protocols Interface” 24
https://www.cablelabs.com/specifications/CM-SP-MULPIv4.0 25
[168] CableLabs “Remote PHY Specification” 26
https://www.cablelabs.com/specifications/CM-SP-R-PHY 27
[169] “Study on new radio access technology: Radio access architecture and 28
interfaces” 3GPP TR 38.801 Table A-1. 29
[170] Cisco Press, MPLS and VPN Architectures, Volume 1, 420 pages, by Ivan 30
Pepelnjak, and Jim Guichard, 2001 31
[171] Cisco Press, MPLS and VPN Architectures, Volume 2, 470 pages, by Ivan 32
Pepelnjak, Jim Guichard, and Jeff Apcar, 2003 33
[172] O’Reilly, MPLS in the SDN Era, 890 pages, by Antonio Sánchez-Monge, and 34
Krzysztof Grzegorz Szarkowicz, 2015 35
[173] BBF TR-221: Technical Specification for MPLS in Mobile Backhaul Networks, 36
99 pages, Oct 2011 37
[174] BBF TR-221, Amd.1: Technical Specifications for MPLS in Mobile Backhaul 38
Networks, 24 pages, Nov 2013 39
[175] BBF TR-221, Amd.2: Technical Specifications for MPLS in Mobile Backhaul 40
Networks, 22 pages, Sep 2017 41
[176] ETSI GR mWT 012 V1.1.1 (2018-11): 5G Wireless Backhaul/Xhaul 42
[177] Microwave and millimeter-wave technology overview and evolution, Workshop 43
on Evolution of Fixed Service in Backhaul support of IMT 2020 / 5G, Geneva, 44
29 April 2019, https://www.itu.int/en/ITU-R/study-45
groups/workshops/fsimt2020/Pages/default.aspx 46
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             16
 1
 2
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             17
5 Definitions and abbreviations 1
5.1 Definitions 2
The key words “SHALL”, “SHALL NOT”, “SHOULD”, “SHOULD NOT”, “MAY”, and 3
“OPTIONAL” in this document are to be interpreted as described in IETF RFC 2119 [25]. All key 4
words must be in upper case, bold text. 5
Items that are REQUIRED (contain the words SHALL or SHALL NOT) will be labelled as [Rx] 6
for required. Items that are RECOMMENDED (contain the words SHOULD or SHOULD NOT) 7
will be labelled as [Dx] for desirable. Items that are OPTIONAL (contain the words MAY or 8
OPTIONAL) will be labelled as [Ox] for optional.  9
Items, if supported, are not meant to be active at all times, but should be available for use. Their 10
state (active or not active) should be based on configuration. 11
5.2 Abbreviations 12
Abbreviations defined in this document take precedence over the definition of 3GPP  13
 14
Abbreviations Meaning or explanation
3GPP Third Generation Partnership Project – Standards Development Organization
4G Fourth-generation mobile network
5G Fifth-generation mobile network
ABR Area Border Router
Amd Ammendment (term used by BBF)
API Application Programming Interfaces
ARPU Average revenue per user
AS Automated (traffic) steering
AS Autonomous System
ASBR Autonomous System Border Router
BBF Broadband Forum
BBU Baseband unit
BGP Border Gateway Protocol
BGP-LU BGP labelled unicast
BGP-LS BGP link state
BIER Bit Indexed Explicit Replication
BITS Building Integrated Timing System
C-RAN Centralized Radio Access Network
CapEx Capital expenditure
CDN Content delivery network
CDMA Code Division Multiple-Access – a mobile radio standard
COTS Commercial off the shelf
CPE Customer premises equipment
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             18
CPRI Common public radio interface
CSP Communications service provider
CSR Cell Site Router
CU Centralized unit
CUPS Control/User Plane Separation
D-RAN Distributed Radio Access Network
DB  Dynamic Bandwdith assignment
DC Data center
DiffServ Differentiated services – a quality-of-service mechanism
DSCP DiffServ Code Point
DWDM Dense Wavelength Division Multiplexing
eMBB Enhanced mobile broadband
ECMP Equal-cost multipath
EPC Evolved packet core
eCPRI Enhanced Common Radio Interface (CPRI)
eNB Enhanced Node B
ESMC Ethernet Synchronization Message Channel
EVPN Ethernet VPN
EXP Experimental
FDD Frequency division duplexing
FHG Fronthaul Gateway
FIB Forwarding Information Base
FIFO First In, First Out
FMC Fixed-mobile convergence
FMC Fixed-mobile convergence
Fronthaul Portion of the mobile network supporting O-RAN 7.2x, eCPRI, RoE or CPRI protocols
FRR Fast Re-Route
Gbps Gigabits per second
GNSS Global Navigation Satellite System (example being GPS)
GPS Global Positioning System
HLS High-level split
HSR Hub Site Router
iBGP internal Border Gateway Protocol
IEC International Electrotechnical Commission
IEEE Institute of Electrical and Electronics Engineers – Standards Development Organization
IETF Internet Engineering Task Force – Standards Development Organization
IGP Interior Gateway Protocol
IoT Internet of Things (see also mMTC)
IP Internet Protocol
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             19
IS-IS Intermediate System to Intermediate System
ISO International Standardization Organization
ITU-T International Telecommunication Union-Telecommunication – Standards Development Organization
ITU-R International Telecommunication Union-Radiocommunication – Standards Development Organization
JSON JavaScript Object Notation
L1 / L2 / L3 Layer 1 / layer 2 / layer 3 of the network protocol stack
L3VPN Layer 3 Virtual Private Network
LDP Label Distribution Protocol
LLS Low-level splits
LSDB Link state database
LSP Label Switched Path
LTE Long Term Evolution (generation of Mobile networks – see 4G)
LTE-A Long Term Evolution – Advanced
MEC Formerly Mobile Edge Compute, now Multi-Access Edge Compute
mMTC Massive machine type communications
MIMO Multiple-Input Multiple-Output (number of antennas)
MNO Mobile Network Operator
MP-BGP Multi-protocol Border Gateway Protocol
MPLS Multiprotocol Label Switching
MSD Maximum Segment Depth
MVNO Mobile virtual network operator
NETCONF Network Configuration Protocol
NLRI Network Layer Reachability Information
NFV Network functions virtualization
NGFI Next-generation Fronthaul interface
NR New radio
NSI Network Slice Instance
NSSI  Network Subnet Slice Instance
O-CU Open Central Unit
O-DU  Open Distributed Unit
O-RU Open Radio Unit
OAM Operations, administration, and maintenance
ODN On-demand next hop
ODN Optical Distribution Network
OLT Optical Line Termination
ONU Optical Network Unit
OpEx Operational expenses
ORF Outbound Route Filter
OSPF Open Shortest Path First
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             20
OTN Optical Transport Networking
P Provider (router)
PCC Path computation client
PCE Path computation element
PCEP Path computation element protocol
PE Provider edge (router)
PIM Protocol for IP Multicast
PON Passive Optical Network
PTP Precision Time Protocol
QoS Quality of Service
RAN Radio access network
RE Radio equipment
REC Radio equipment controller
RoE Radio over Ethernet
RRO Record Route Object
RRU Remote Radio Unit
RSVP Resource Reservation Protocol
RSVP-TE Resource Reservation Protocol – Traffic Engineering
RT Route Target
RTT Round-trip times
RU Remote (radio) unit
SD-WAN Software-defined wide area network
SDH Synchronous Digital Hierarchy – a digital communications system
SDN Software-defined networks
SID Segment Identifier
SLA Service level agreement
SMB Small and medium business
SONET Synchronous Optical Networking – a digital communications system
SPF Shortest Path First
SR Segment Routing
SR-DPM Segment Routing-data plane management
SR-TE Segment Routing – traffic engineering
SR-PCE Segment Routing – path computation element
SRH Segment Routing header
SRLG Shared risk link groups
SSU Synchronization Supply Unit
SyncE Synchronous Ethernet
T-GM Telecom – Grand Master – a PTP clock type
T-TSC  Telecom – Time Slave Clock – a PTP clock type
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             21
TAE  Time Alignment Error
TC Traffic Class
TDD Time-division duplexing – a radio communications technique
TDM Time division multiplexing
TE Traffic engineering
TE  Time Error
TI-LFA Topology-independent loop-free alternative
TNE Transport Network Equipment (an O-RAN term to denote a transport device)
TR Technical report
TTL Time to live
TWDM Time and Wavelength Domain Multiplexing
UE User equipment
URLLC Ultra-reliable low-latency communications
UPF User plane functions
VIM Virtualized infrastructure managers
VNF Virtual network function(s)
VPN Virtual private networks
VRF Virtual Routing and Forwarding
WAN Wide area network
WDM Wavelength Division Multiplexing
Xhaul Collective name for Fronthaul, Midhaul, and Backhaul
XML eXtensible Markup Language
YANG Yet another next generation – data modelling language
 1
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             22
6 5G Transport network requirements 1
For full details of the O-RAN transport requirements, bandwidth and delay estimates of a 5G network  2
see [15]. 3
  4
Requirements for the transport architecture can be characterized in following categories: 5
 6
1. Latency, Frame Loss Ratio and Bandwidth requirements for Fronthaul, Midhaul, and Backhaul. 7
2. Operability requirements that include fault and performance management. 8
3. Synchronization requirements. 9
4. “ITU-T GSTP-TN5G: Transport support of IMT -2020/5G” [156] identifies the need for the 10
transport to be multi-service in nature. In addition to mobile services, the infrastructure needs 11
to support fixed line consumer and enterprise services. These services are not explicitly 12
covered in the docume nt, but the architecture must enable L2 or L3 services to be created 13
between any two edge TNEs regardless of relative position to each other in the transport 14
network. 15
5. End to end support of 4G/5G mobile infrastructure including Fronthaul / Midhaul / Backhaul.  16
6. Concurrent support for RAN deployment scenarios outlined in “ITU -T GSTP -TN5G: 17
Transport support of IMT -2020/5G” [156] running from a single  cell site location. These 18
shown in Figure 6-1 are: 19
 20
a. Co-located O-CU and O-DU – O-RAN split 7.2x from cell site 21
b. Independent O-RU, O-CU, O-DU locations – O-RAN 7.2x from cell site 22
c. O-RU and O-DU integration on cell site – O-RAN split 2 from cell site 23
d. O-RU, O-DU and O-CU integration on cell sites – Split 1 from cell site 24
  25
 26
Figure 6-1: ITU-T 5G use cases 27
 28
7. Central, distributed or a mix of the two for user plane termination. Central, distributed or a mix 29
of two for 5G control plane placement. (Figure 6-2) 30

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             23
   1
 2
Figure 6-2: 5G central and distributed user plane termination 3
 4
8. Concurrent support for 4G and 5G RAN solutions running from a single cell site.  In addition 5
to the use cases outlined in earlier figures, support for 4G radios running alongside 5G radios 6
is required. Figure 6-3 illustrates the two 4G architectures that need to co -exist with the 5G 7
architectures outlined above. In the upper case, the 4G RAN infrastructure utilizes a C-RAN 8
architecture where the RRH and BBU support split 8 which is converted to 7.2x or RoE for 9
transportation across the packet access infrastructure. In the lower case, the 4G RAN 10
infrastructure uses D-RAN or split 1 architecture.    11
  12
 13
Figure 6-3: 4G use cases 14
 15
9. End to end support for 5G slicing and 5G service types. 16
 17

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             24
7 5G logical connectivity requirements  1
 The transport network needs to be very flexible as depending on the use case and the RAN design 2
each part of the physical transport network may need to support multiple slices, multiple 5G 3
services and also different 3GPP interfaces. This section covers the logical transport connectivity 4
requirements of the 5G Fronthaul, Midhaul and Backhaul components. Further details can be found 5
in O-RAN WG9.Transport Requirements document [15]. 6
7.1 Fronthaul  7
The Fronthaul infrastructure potentially needs to support: 8
• O-RAN 7.2x Fronthaul (for 5G NR) 9
• Non-ORAN Fronthaul 10
 11
7.1.1 O-RAN 7.2x Fronthaul 12
The O-RAN 7.2x is a split 7 “Low Level Split” (LLS) that runs between the O-RU and the O-DU 13
(optional more than one for network and DU based redundancy). The associated mobile interfaces 14
for the Fronthaul are the Control, User and Synchronization and Management planes. The 15
synchronization plane is covered in a separate WG-9 Timing and Synchronization Architecture and 16
solutions document [17].   17
 18
 19
 20
  21
Figure 7-1: Fronthaul O-RAN 7.2x control and user plane using an Ethernet encapsulation 22

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             25
   1
Figure 7-2: O-RAN 7.2x Hybrid Management plane  2
 3
• O-RAN 7.2x Control and User planes: These interfaces are described in O-4
RAN.WG4.CUS.0-v3.00: Control, User and Synchronization Planes [13]. These interfaces run 5
between the O-RUs and their serving O-DU.  Ethernet encapsulation is a mandatory 6
requirement and IP encapsulation is optional and applies if the transmitting and receiving nodes 7
support IP capabilities. In both cases the payload is one or more eCPRI transport headers with 8
respective application data. The latency requirements associated with 7.2x control plane and 9
user plane traffic are very low, and the bandwidth requirements are high.  10
• O-RAN 7.2x Synchronization plane: This interface is described in O-RAN.WG4.CUS.0-11
v3.00: Control, User and Synchronization Planes [13]. In C-RAN architectures accurate 12
synchronization between the O-DU and O-RUs is required to support “Time Division Duplex” 13
(TDD), Carrier Aggregation (CA) using multiple O-RUs, MIMO and other processes. In an O-14
RAN Fronthaul environment using an Ethernet transport layer, protocols such as PTP and 15
SyncE are used to achieve synchronization between the O-RUs and O-DUs. For more details 16
refer to WG-9 Timing and Synchronization Architecture and solutions document [17].         17
• O-RAN 7.2x Management plane: This interface is described in O-RAN WG4.MP.0-v3.00 18
Management plane specification[14]. Two M-Plane models are defined. 19
o Hierarchical model: In this model, an O-RU is managed by one of more O-DUs. These 20
O-DUs are entirely responsible for sub-ordinate O-RUs, which means the NMS only 21
needs to interact with the O-DU level. In this mode the O-RAN 7.2x M-Plane interface 22
only runs between the O-DU and sub-ordinate O-RUs.   23
o Hybrid model: In this model, an O-RU is managed by one or more NMS, in addition to 24
the serving O-DU. In this mode the O-RAN 7.2x M-Plane interface runs between the O-25
RU, O-DU and the NMS.  26
The O-RAN 7.2x M-plane is IP/NETCONF based and the basic transport requirement is end-to-27
end IP connectivity between the O-RU and the elements managing it. IPv4 shall be supported as 28
a mandatory transport protocol for M-Plane and IPv6 support is optional.  29
 30
7.1.2 O-RAN Fronthaul logical transport requirements 31
Details of the O-RAN transport requirements are illustrated in Figure 7-1.  32
C/U-Planes:  33
1. Ethernet connectivity from O-RUs to serving O-DU with potential backup to a redundant O-34
DU. 35
2. Optional IP connectivity from O-RUs to serving O-DU with potential backup to a redundant 36
O-DU. 37

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             26
3. O-RUs and serving O-DU in close proximity to each other to meet delay criteria associated 1
with Fronthaul. It is unlikely the Fronthaul components (ie O-RUs and serving O-DU) will 2
extend beyond the access transport network. 3
 4
S-Plane: 5
1. This is covered in the “WG-9 Timing and Synchronization Architecture and Solutions 6
document [17]. 7
 8
M-Plane 9
Details of the O-RAN M-Plane transport running in hybrid mode are illustrated in Figure 7-2. 10
 11
1. IP connectivity allowing NMS to communicate with O-DUs and O-RUs if running the hybrid 12
model. 13
2. IP connectivity allowing serving O-DU to communicate with all its sibling O-RUs if running 14
in either hybrid or hierarchical models. 15
 16
Note: In addition to the 7.2x management plane other management components may need to 17
communicate with entities at the cell site. For example, remote monitoring of sensors and actuators 18
in the cell site. These are not explicitly covered in this document, but the specified transport 19
architecture can cater for scenarios, where management and monitoring, are based on either 20
Ethernet or IP connectivity. 21
7.2 Non-ORAN Fronthaul  22
Legacy Fronthaul scenarios are those C-RAN use cases where the Fronthaul traffic is transported 23
over a packet switch network while not using the O-RAN compliant encapsulation protocol that 24
supports 7.2x split. The two most likely legacy Fronthaul scenario in a packet switched transport 25
network are: 26
 27
• eCPRI based C-RAN solutions: using eCPRI encapsulation protocol not compliant to O-28
RAN WG4 CUS specifications [13]. i.e. a non O-RAN 7.2x split. 29
• RoE based C-RAN solutions:  CPRI encapsulated by the RoE protocol.  30
 31
7.2.1 eCPRI based C-RAN solutions 32
An operator may choose a packet-based C-RAN Fronthaul architecture that is not O-RAN 7.2x 33
compliant. In this case the RU and DU use the eCPRI as the packet encapsulation protocol to 34
packetize the Fronthaul data but implements a non-O-RAN compliant radio message protocol to 35
support a different function split.   36
 37
Following 3GPP recommendation [5], the possible function splits may include 38
• Option 6, split between MAC and PHY layers 39
• Option 7.1, 7.2, or 7.3, splits within PHY layer 40
• Option 8, split between radio and PHY layer 41
 42
7.2.2 Radio over Ethernet (RoE) based C-RAN solutions  43
In this scenario an operator has an existing radio deployed at the cell site and the operator intends to 44
convert these radios to support a C-RAN Fronthaul architecture whilst implementing their 5G RAN 45
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             27
infrastructure.  In this case it is likely the legacy equipment (RRH and BBU) only supports an 1
optical CPRI interfaces.  To implement a C-RAN Fronthaul architecture in a packet switched 2
transport network the CPRI needs to be converted to an Ethernet frame at the cell site, transported 3
over the access network as packet and then get converted back to CPRI at the hub for processing 4
from the BBU. In this scenario a component called a RoE mapper is used to perform a CPRI to 5
Ethernet frame conversion at the cell site and an Ethernet frame to CPRI conversion at the location 6
where the BBU is located.   7
7.2.2.1 RoE Mapper transport 8
The design, implementation, management and interface from the RoE mapper to the legacy 9
equipment is not in the scope of WG-9 but the expectation is the RoE mapper will present either O-10
RAN 7.2x compliant Ethernet or Ethernet IP packets or IEEE 1914.3 Radio over Ethernet packets 11
[27] to the transport network. At this time O-RAN has not defined a CPRI to O-RAN 7.2x 12
conversion capability so implementations out in the market are based on CPRI to Radio Over 13
Ethernet.  14
 15
It is then the responsibility of the transport network to transport these packets to their destination 16
with appropriate characteristics for the legacy connection to function.  17
7.2.2.2 Radio over Ethernet  18
Radio over Ethernet (RoE) is defined in IEEE Std 1914.3-2018 – IEEE standard for radio over 19
Ethernet Encapsulations and Mappings [27]. Two mapping techniques for supporting CPRI to 20
packet conversion are defined.    21
   22
Structure-agnostic RoE mapper  23
The structure-agnostic RoE mapper captures bits from one end of a constant bit rate link, packetizes 24
the bits into Ethernet frames, sends the frames across the network, and then recreates the bit stream 25
at the far end of the link. While the constant bit-rate data stream is commonly encoded with the 26
CPRI protocol, it could also be of any other protocol, provided it is within the range of data rates 27
supported by that equipment.  28
The structure-agnostic RoE mapper has two main modes of operation: 29
 30
• Tunnelling mode or type 0 works as a simple Ethernet tunnel. It does not remove any line 31
coding bits and does not interpret any special characters (such as K-characters). If the source 32
data is 8b/10b-encoded, the 10-bit symbols present on the line will be tunnelled by this RoE 33
mapper as 10 bits of data. Similarly, 66-bit symbols will be sent for 64b/66b-encoded data as 34
66 bits of data. The entire stream is simply packetized.  35
 36
• Line-coding-aware mode or type 1 removes the line coding bits such as for CPRI encoded 37
with 8b/10b or 64b/66b. If the source data is 8b/10b-encoded, the 8-bit symbols present on 38
the line will be tunnelled by this RoE mapper as 8 bits of data. Similarly, if the source data is 39
64b/66b encoded, the 64-bit symbols present on the line will be tunnelled by this RoE 40
mapper as 64 bits of data. To allow the restoration of the 10-bit or 66-bit symbols at the de-41
mapper, the RoE mapper/de-mapper must have some awareness of the protocol it is 42
mapping/de-mapping; the locations of the special characters must be known a priori relative 43
to an event that is indicated in the RoE frame.  44
 45
Structure-aware RoE mapper  46
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             28
In this mode only the useful information in the CPRI stream is packetized into the RoE frames and 1
different types of data (such as control words and data words) can be encapsulated into separate 2
frames for prioritized processing. Those unused fields within the CPRI stream will be ignored thus 3
bring the benefit of Fronthaul BW reduction. This mode requires full knowledge of the protocol 4
layout of the CPRI and due to the proprietary nature of CPRI will require input from the radio 5
vendor.   6
7.2.3 Non O-RAN Fronthaul logical transport requirements  7
The main use case identified by operators for non O-RAN fronthaul is 4G equipment that uses 8
CPRI between the RRH and the BBU (see section 8.4). To support this over a packet based 9
fronthaul network, the CPRI stream needs to be packetized as it ingresses the packet network and 10
the CPRI stream reconstructed as it egresses the packet network. Bandwidth, delay and jitter 11
characteristics in the packet network clearly depend on the technology used to perform this 12
function, which is outside the scope of O-RAN. To provide some guidance on support of this traffic 13
it has been assumed that it is presented as Ethernet and has similar delay and jitter requirements as 14
7.2x fronhaul traffic. 15
7.3 Midhaul logical transport requirements   16
3GPP TS 38.401 [5] defines the de-aggregated RAN, it’s characteristics and outlines the F1-U, F1-C 17
and E1 interfaces. Figure 7-3, taken from 3GPP TS 38.401 illustrates the components and interfaces. 18
The Midhaul transport infrastructure is responsible for supporting these interfaces.  19
 20
 21
 22
Figure 7-3: Deaggreated gNB 23
 24
The characteristics of a disaggregated gNB are:  25
• A gNB may consist of a gNB-CU-CP, multiple gNB-CU-UPs and multiple gNB-DUs 26
• DUs and CU-UPs are connected to one CU-CP via the E1 interface or the F1-C interface 27
• DUs can connect to multiple CU-UPs 28
• Multiple CU-UPs can connect to one CU-CP 29
• For resiliency reasons, DUs and CU-UPs may connect to multiple CU-CPs  30
 31
The 3GPP interface associated with O-DU and O-CU communication is the F1 interface. It has a 32
control (F1-C) and data (F1-U) plane component.  33
 34
Note: W1 interface is the 4G equivalent of the F1 interface. It will not be discussed further in this 35
document as its characteristics are expected to be similar to the 5G equivalent.  36
 37

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             29
The 3GPP interfaces associated with intra O-CU communications is the E1 interfaces. It runs 1
between the gNB-CU-CP and a gNB-CU-UP. It allows these two components to run as separate 2
entities and potentially in different locations.     3
7.3.1 Overall Midhaul logical transport requirements  4
1. Control Plane:  5
a. Multi-point at the IP interfaces level (IPv4 or IPv6) between O-CU-CP and multiple 6
O-DUs (F1-C interface).  7
b. Multi-point at the IP interfaces level (IPv4 or IPv6) between O-CU-CP and multiple 8
O-CU-UPs (E1 interface).  9
2. Data Plane: Multi-point at the IP interfaces level (IPv4 or IPv6) between O-CU-UP and 10
multiple O-DUs (F1-U interface).  11
3. IP connectivity between O-CUs for Xn interface. 12
4. Some operators may wish to run the user plane interface (F1-U) separately from the control 13
plane interfaces (E1 and F1-C). 14
5. Some operators may wish to treat Midhaul and Backhaul as a single logical network.  15
6. Some operators may wish to treat Midhaul and Backhaul as discrete logical networks. 16
7.4 Backhaul logical transport requirements  17
Figure 7-4 shows components and the 3GPP interfaces in the mobile Backhaul. It has a control plane 18
and user plane component. It is not uncommon to see the control plane and the data plane divided 19
into separate closed user groups (VPNs) at the transport layer to ensure a clear demarcation between 20
customer user data and the 3GPP control plane.   21
 22
 23
 24
Figure 7-4: 5G Backhaul components and interfaces Source: Adapted from 3GPP TS 23.501 25
v6.4.0(2020-03): System Architecture for 5G [1] with control plane / user plane shading added 26
by document authors. 27
 28
The 5G 3GPP interfaces associated with Backhaul are:  29
  30
• N1 interface is a logical control plane interface between the mobile core network and the UE. 31
From a physical perspective it flows via the RAN through the Backhaul infrastructure to the 32
AMF. It is a signalling interface between the UE and the AMF.  33
  34
• N2 interface supports control plane signalling between RAN and 5G core. It is primarily 35
concerned with connection management, UE context and PDU session management, and UE 36
mobility management. In addition, Non -Access Spectrum (NAS) signalling between the UE 37

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             30
and the AMF is t ransported over the N2 connection for that UE. This signalling includes 1
information regarding access control, authentication and authorization, and session 2
management procedures.  3
 4
• N4 Interface is the bridge between the control plane and the user plane of the 5GC. It runs 5
between the SMF and the UPF and is responsible for conveying policy rules regarding policy 6
handling, forwarding and usage reporting to the UPF.  7
 8
• N3 interface is the user plane interface between the O -CU component of the (gNB) and the 9
initial UPF.   10
 11
• N9 interface is a user plane interface than runs between two UPFs. (i.e. an intermediate UPF 12
and the UPF session anchor). 13
  14
7.4.1 Overall Backhaul logical transport requirements 15
1. Control Plane: Multi-point at the IP interfaces level (IPv4 or IPv6) between O-CU, UPF and 16
5GC components (N1, N2, N4, Xn).  17
2. User Plane: Multi-point at the IP interfaces level (IPv4 or IPv6) between O-CU→UPF 18
(N3), UPF→UPF (N9) and O-CU→O-CU (Xn). 19
3. Some operators may wish to run the Backhaul user plane (N3/N9) separate from the 20
Backhaul control plane (N1/N2/N4). 21
4. Some operators may wish to treat Midhaul and Backhaul as a single logical network. 22
5. Some operators may wish to treat Midhaul and Backhaul as discrete logical networks. 23
  24
  25
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             31
8 Operator use cases  1
Operators have indicated the following use cases are of interest.  Any combination of these 2
scenarios may apply in practical deployments though they are described individually.  Unless 3
otherwise stated, the eCPRI traffic herein is O-RAN compliant and presents the data according to 4
O-RAN Open Fronthaul CUS plane and management plane specifications [13][14].  5
 6
Multicast use cases are deferred to section 15.1, after logical network and services have been 7
described, so that they can be provided with better background. 8
 9
Before describing the individual deployment scenarios, it is worth clarifying the terminology of C-10
RAN vs. D-RAN. In a 5G Distributed RAN (D-RAN) architecture, the O-CU, O-DU, and O-RU all 11
reside at the cell site (Figure 8-1). 12
 13
 14
Figure 8-1 5G O-RAN D-RAN architecture  15
 16
5G Centralized RAN (C-RAN) architectures splits the radio components into discrete components 17
which can be located in different locations. In 5G O-RAN model (Figure 8-2), operators may decide 18
to only place the O-RU at the cell site, and centralize the O-DUs or the O-DUs together with O-CU 19
in a central location. The other alternative can be represented by locating the O-RUs and O-DUs at 20
the cell-site and centralizing the O-CUs in a location farther away. 21
 22
 23
 24
 25
Figure 8-2 5G O-RAN C-RAN architectures 26
 27
When considering a C-RAN architecture there are both positive and negative impacts. On the 28
positive side, it can increase component efficiency by pooling RAN elements in a centralised 29

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             32
location and also improve co-ordination between the radio component. On the negative side a C-1
RAN architecture can significantly increase the bandwidth required if the Fronthaul protocols are 2
traversing the transport network.  3
 4
Note: The representation of the access and aggregation networks in the use case figures below are 5
not to scale. In scenarios where a Fronthaul component exists, the access network will be 6
geographically constrained due to delay requirements of the Fronthaul protocols. In contrast, the 7
aggregation network in the figures can cover much greater areas due to more relaxed delay 8
requirements of the 5G Midhaul and Backhaul protocols.   9
8.1 Scenario 1: C-RAN architecture with collocated O-DU and O-CU  10
As illustrated in Figure 8-3 for this scenario, O-DU and O-CU are collocated at a Hub site, therefore 11
the Midhaul traffic between O-DU and O-CU is local and not going through the transport network. 12
The eCPRI traffic from the 7.2x split from the O-RU, is transported by a packet switched access 13
transport network to the C-RAN Hub. At the other end of the C-RAN Hub, the aggregation 14
transport network transports the Backhaul traffic to the mobile core. 15
 16
The O-RU can be a new NR radio as well as a legacy LTE radio, as long as they are made to have 17
the O-RAN compliant Fronthaul interface with 7.2x function split.  18
 19
The Xn traffic that performs the inter-gNB coordination is considered as part of Midhaul traffic, but 20
it will not reach to the Core. Instead, it is routed to another O-CU, either within the same Hub site 21
or between the Hub sites, via the aggregation transport network. 22
 23
There is inter-connection between the access and aggregation transport networks for the passing 24
through services such as management traffic.  In some cases, the two transport networks can share a 25
same edge router that naturally completes the connection.      26
 27
 28
Figure 8-3: Scenario 1 – C-RAN architecture with collocated O-DU and O-CU  29
8.2 Scenario 2: C-RAN architecture with collocated O-RU and O-DU  30
In this scenario, both O-RU and O-DU are deployed at the cell site and O-CU is located at the Hub 31
site.  By 3GPP standard, the O-DU and O-CU is split by the functional split option 2 and connected 32

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             33
with F1 interface, which is transported by the access transport network that connects the cell site 1
and the Hub site.  The aggregation transport network carries the N2/N3 Backhaul traffic to the 5G 2
core.  3
 4
O-RU and O-DU may be collocated or be in close proximity around the cell site.  In both cases, O-5
RU and O-DU communicate to each other by the Fronthaul interfaces via the Cell Site Router 6
(CSR), which is located at the cell site and is considered as part of  access transport network, as 7
shown in Figure 8-4   8
 9
 10
Figure 8-4: Scenario 2 – C-RAN architecture with collocated O-RU and O-DU 11
 12
NOTE: In this scenario the location of the CSR, O-DU and O-RU relative to each other may be 13
close or in some scenarios more distant. For example, within a stadium environment, the O-DU 14
could be centralised and the O-RUs distributed around the stadium. In all cases consideration needs 15
to be given to the low latency requirements associated with the Fronthaul interfaces.   16
8.3 Scenario 3: C-RAN architecture with coexistence of legacy Backhaul 17
traffic  18
This scenario is an extension of Scenario 1, or Scenario 2, where 3G / 4G D-RAN deployments 19
coexist with a 5G C-RAN deployment. The access transport network thus carries the Backhaul 20
traffic from LTE (S1) or 3G UMTS (lu/lub) in addition to the Fronthaul traffic from Scenario 1, or 21
Midhaul traffic from Scenario 2, as illustrated in Figure 8-5 and Figure 8-6 respectively.   22
 23
The aggregation transport network is responsible for carrying the Backhaul traffic for both new and 24
legacy mobile services.  25
  26

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             34
 1
Figure 8-5 Scenario 3a – 7.2x interface combined with legacy Backhaul at access layer 2
 3
 4
 5
 6
 7
 8
Figure 8-6: Scenario 3b – Midhaul combined legacy Backhaul at access layer  9
 10
These mixed D-RAN and C-RAN scenarios are important for the brown field deployment. 11
 12
8.4 Scenario 4: C-RAN architecture with coexistence of legacy Fronthaul 13
traffic    14
In this scenario, shown in Figure 8-7, both the 4G and 5G networks utilise a C-RAN Fronthaul 15
deployment in the access transport network. In this scenario it is assumed the 4G radio 16
infrastructure consists of RRHs communicating with their serving BBU using CPRI. To migrate 17
these services to a packet based Fronthaul, the native CPRI is converted to packets as it enters the 18
packet infrastructure and converted from packets back to native CPRI as it egresses the packet 19
infrastructure. This scenario assumes the CPRI to packet conversion function is performed by an 20

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             35
RoE Mapper using the IEEE 1914.3 standard [27]. The RoE mapper is out of scope of this 1
specification. The impact on the access transport network is it carries 7.2x and RoE fronthaul 2
traffic.   3
  4
In the Backhaul network, S1 from LTE is transported together with NR Backhaul N2/N3. 5
 6
  7
 8
Figure 8-7: Scenario 4 – C-RAN architecture with coexistence of legacy Fronthaul traffic 9
 10
This scenario is important to support migration plan from 4G to 5G, such as the NSA architecture. 11
 12
8.5 Scenario 5: C-RAN architecture with further split of O-DU and O-CU 13
This is a C-RAN architecture with two RAN splits with the O-RUs, O-DUs, and O-CUs hosted at 14
separate locations. The O-DU is placed at C-RAN Hub site closer to the cell sites for shorter latency 15
and the O-CU is more centralized at a different location as shown in Figure 8-8. The Midhaul 16
traffic, identified as F1 from the 3GPP option 2 split, is carried by an addition transport network 17
segment (pre-aggregation transport network) that connects the two hub sites. Similar to other use 18
cases, the Backhaul traffic is transported by the aggregation transport network.   19
 20
 21
Figure 8-8: Scenario 5 – Dual split C-RAN architecture  22

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             36
8.6 Scenario 6: C-RAN architecture with local breakout  1
In supporting URLLC to reduce the data plane latency, or fixed wireless application to offload large 2
amounts of user data locally, the User Plane Function (UPF) breakouts before the aggregation 3
network.  In this case the aggregation network only carries the control plane traffic N2 (Figure 8-9). 4
 5
 6
Figure 8-9: Scenario 6 – C-RAN architecture with local user plane breakout 7
8.7 Scenario 7: Transport slicing  8
From the operator use case given in previous subsection, it is observed that the transport network, 9
specially the access transport network, may experience all types of transport flows simultaneously 10
when operators have the need to engage mixed use cases in their deployment. Possible transport 11
service range widely with:        12
 13
• Fronthaul, Midhaul, and Backhaul 14
• NR, legacy LTE and legacy UMTS 15
• Control plan, User plane, and Management plane 16
• To support transport operation for different operators  17
• To support different types of end-to-end services or applications (such as URLLC and 18
eMBB) 19
   20
Each of them may be multiplexed into a commonly shared transport network with largely different 21
transport requirements, which include latency, throughput, transmission reliability. 22
 23
To reduce complexity and manage network resource more efficiently, these transport flows can be 24
classified into transport slices according to common service requirements. One example is shown in 25
Table 1. 26
   27
 28
Transport
Slices
Description Transport flows Transport BW Transport
timing
sensitivity
Transport
reliability
TS 1 Fronthaul 7.2x CUS-plane,
RoE
High High High

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             37
TS 2_1 Data plane for
Backhaul of
URLLC service of
Operator A
F1-U, S1-U, N3,
X2/Xn-U
Medium High High
TS 2_2 Data plane for
Midhaul,
Backhaul of
Operator B
F1-U, S1-U, N3,
X2/Xn-U
Medium Medium High
TS 3 Control plane for
Midhaul,
Backhaul,
Management
plane
7.2x M-Plane,
F1-C, S1-C, N2,
X2/Xn-C,
Management
Low Low Low
 1
Table 1 Transport slicing example 2
 3
These transport slices are designed to meet the objectives:  4
• Provide the transport elements to support network slicing. As part of network sub-network 5
instances, the transport network interfaces with other network segments by the transport 6
slices. 7
• Allow flexible configuration to efficiently support adding/deleting/reconfiguring slices and 8
services 9
• Support protection/isolation/prioritization mechanisms to minimize inter-slice effects 10
• Support monitoring/reporting the slice KPIs   11
  12
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             38
9 Overall packet switched Open Xhaul architecture  1
There are different ways packet switching could be deployed to support an Open Xhaul 2
architecture. Factors include: 3
• Span of the packet switching components. Packet switching could be deployed from cell 4
site to the transport core or mixed with other technologies, for example WDM in access, to 5
form the end to end network. 6
• The nature of the underlying L0/L1transport used by the packet switching equipment. 7
• The network protocols used at the packet switching layer. 8
• How Xhaul services are built on the Xhaul infrastructure.    9
  10
9.1 Physical layout and xHaul transport options 11
A simple model illustrated in Figure 9-1 relies on an end-to-end packet-based transport architecture 12
that builds on a physical network. In this model, Fronthaul, Midhaul and Backhaul deploy packet-13
based transport technologies. The diagram represents a logical view of the network. The physical 14
network implementation might be different. For example, an O-DU might be connected to a single 15
TNE port but deliver two logical connections to a second TNE in Fronthaul network, and a third 16
TNE in Midhaul network. 17
 18
 19
Figure 9-1 Packet based architecture in front-, mid- and Backhaul networks  20
 21
Unlike the previous architecture some operators may decide to use the packet-based technology 22
only in Mid- and Backhaul, and use simple physical networking to connect the O-DU ports with O-23
RUs (Figure 9-2). In this case the physical network between the O-RUs and O-DUs could, in the 24
simplest case could be dark fiber links. The next choice in terms of simplicity can be a passive 25
WDM system. 26
 27
 28
Figure 9-2: Packet based architecture in mid and Backhaul networks 29
 30
Another architectural variant (Figure 9-3) can be designed by inserting TDM based layer 1 transport 31
technologies between the packet-based and physical networks. OTN and SPN/G.mtn are two 32
choices that allow for aggregation and transparent transport of larger traffic volumes, and design of 33
hard slicing architectures.  In this architecture, the OTN or SPN/G.mtn infrastructure must present 34
Ethernet clients at UNI-C/UNI-N interfaces to the packet switched network. The clients need to be 35

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             39
transparent and operate at full Ethernet line rates, because the packet switched network elements are 1
responsible for QoS and need to rely on client signals with proper bandwidth.  2
 3
 4
 5
Figure 9-3 Use of TDM based technologies with Ethernet presentation in Xhaul networks 6
 7
In cases where there are underlying L0/L1 transport solutions, the synchronization and timing flows 8
must not be impaired by these layers, as any impairment will impact the synchronization function of 9
the packet switched network, and the overall synchronization performance of the end-to-end 10
system.  11
 12
9.2 Open Xhaul architecture in revision 1 13
Revision 1 of this document describes a packet switched transport architecture illustrated in Figure 14
9-4.  15
  16
 17
Figure 9-4 Packet switched transport for mobile Xhaul 18
 19
It is a converged end to end packet switched infrastructure, beginning at the cell site, located in the 20
edge of the access layer and stretching to the core of the transport layer. The packet switching TNEs 21
are QoS enabled, high capacity, low latency devices interconnected by point-to-point Ethernet 22
interfaces running at the full capacity of the Ethernet interface, typically using either point to point 23
fibres or via a WDM infrastructure.  It incorporates data centers suitably placed across the transport 24
network infrastructure to support virtual and physical NFs associated with mobile and fixed services 25
but also potentially the placement of “Application Functions” associated with value-add services 26
and customer specific application. 27
 28

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             40
The logical architecture is based on a common underlay packet switching infrastructure based on 1
either MPLS or SRv6 overlaid with a L2 / L3 service infrastructure (VPNs) that uses the 2
capabilities of the underlay packet switched network to support the mobiles interfaces.  3
The underlay packet switching infrastructure provides basic network services such as; any-to-any 4
connectivity between TNEs, scaling, fast convergence, shortest path and traffic engineered 5
forwarding, packet-based Quality of Service (QoS) and timing.  6
 7
The service layer supports native Ethernet services, using EVPN technology and IP VPN services, 8
using MP-BGP based L3VPNs. These services are able to utilise the facilities offered by the 9
underlay packet switched infrastructure to support the different mobile interfaces in an appropriate 10
fashion. Where possible transport services are built on an end-to-end basis without intermediate 11
stitching / switching points within the transport infrastructure. This approach has been taken to 12
minimise the transport service orchestration overhead.   13
9.3 Technology and architectural choices 14
As discussed in the introduction to this section there are other packet-switched solutions and 15
potentially many design approaches available that may have the same capabilities of the 16
architecture described in this revision of the document. Future versions of this document may 17
describe alternative technologies and designs based on operator requirements and suitably mature 18
standards.  19
9.4 Standardization  20
The underlay packet-switching technologies described in this document are based on MPLS and 21
IPv6 with an emphasis on the Segment Routing (SR) control and data planes. The overlay service 22
layer uses EVPN and MP-BGP L3VPNs. The IETF is the key standardization body for all these 23
technologies. It has a well-defined process for bringing an idea, in the form of a personal draft, via 24
an IETF standards track draft, to full IETF standards status, in the form of an Internet standard 25
“Request For Comment” (RFC). Depending on subject matter this can be a multi-year process.  26
 27
In order to present architectures suitable for 5G, based on modern packet-switching technologies, 28
both RFCs and IETF adopted drafts are referenced in this document. All forms of personal drafts 29
are completely excluded.   30
 31
Future revisions of this document MUST be updated to reflect the current status of IETF drafts 32
which are referenced. If they expire, they MUST be removed and if they complete the IETF 33
standardization process, references and requirements MUST be updated to remove the IETF draft 34
and include the appropriate RFC number.  35
9.5 Document organization 36
The remainder of the revision 1 of this document describes the physical and logical architecture of a 37
packet switching Xhaul architecture and how it can support 5G and legacy mobile services. It is 38
arranged as follows:  39
 40
Section 10 describes the end to end physical network infrastructure. 41
 42
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             41
Sections 11 and 12 describes two underlay packet switching architectures capable of supporting a 1
5G mobile environment. The first is based on MPLS, the second IPv6 with Segment Routing 2
(SRv6). Depending on scale and preference, operators will need to select one or the other.   3
 4
Section 13 describes how IP and Ethernet services suitable for 5G are delivered on a packet 5
switched Xhaul network. The approach is common regardless of the underlay technology deployed.  6
 7
Section 14 describes the QoS architecture for a packet switched Xhaul network. 8
 9
Section 15 describes multicast and consideration for deployment. 10
 11
Section 16 covers packet-switched orchestration and telemetry. Currently it is empty and will be 12
completed in the second revision of this document or in a separate document.    13
 14
Section 17 and 18 describes how the packet switched Xhaul architecture supports 5G and legacy 15
mobile use cases outlined in section 8.  16
  17
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             42
10 Physical network design for packet switched Xhaul  1
Figure 10-1 shows two of the most common physical topologies seen in transport networks between 2
the access and the core  of the transport network . In the first, there are four layers of transport 3
infrastructure; access, pre-aggregation, aggregation and transport core. In the second, there are three 4
layers of transport infrastructure; access, aggregations and transport core. Packet switches are used  5
within each layer and provides connectivity between the layers.  6
 7
The way the mobile RAN and core infrastructure is arranged over these different segments varies  8
based on geography and the MNO’s  RAN and mobile core designs and also individual operator’s 9
classification of different components in the physical network. It should be noted that in most cases 10
the transport core doesn’t form part of the 5G RAN infrastructure, which is restricted to the access 11
and aggregation infrastructure (often called the metro network), but it does play an important role in 12
scaling and connecting metro infrastructures together to enable end to end services to be built.  13
 14
 15
Figure 10-1: Physical transport components  16

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             43
  1
 2
Figure 10-2 Hierarchical designed transport network consisting of access / aggregation and 3
transport core.   4
 5
Figure 10-2 shows an example of a logical transport network consisting of a n umber of access, 6
aggregation domains connected hierarchically to a single transport  core domain. From a logical 7
architecture perspective, the entire infrastructure needs to be considered as a whole, but at the physical 8
layer there are significant differences in these discrete physical segments. These include: 9
 10
1. Distances between sites 11
2. Layout of the physical media 12
3. Technology employed 13
4. Environmental conditions  14
5. Bandwidth requirements 15
6. Cost structures surrounding equipment and real estate. 16
 17
10.1 Packet over fibre  18
This document concentrated primarily on a transport infrastructure built using packet over fibre 19
solutions in all physical segments of the network. 20

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             44
10.1.1 Access   1
 A transport network element (TNE) – Ethernet switch or IP router – resides in the cell site 2
aggregating traffic received on access ports (downlinks) from end-hosts (O-RUs, or O-DUs, as 3
outlined in Section 8, as well as other types of end-hosts, like for example 4G BBUs, business 4
CPEs, etc. in hybrid deployment models), and statistically multiplexing this traffic onto a higher 5
capacity Ethernet uplink, typically established over a dark fibre or WDM lambda, or any other 6
underlying technology (for example microwave radio link). Link speed will depend on the traffic 7
levels required by the services running in the access network. The anticipated ranges will be from 8
1…50 Gbps (downlink) up to bundles of 100 Gbps (uplink). 9
10.1.1.1 Topology  10
The physical topology employed in the transport access network is operator dependent and driven 11
mainly by the three aspects: 12
• fibre topology, its availability 13
• the traffic matrix 14
• latency and time error requirements 15
 16
The main physical architectures anticipated in the transport access are: 17
• ring 18
• chain 19
• hub and spoke (called as well spine and leaf), with redundant CSR connectivity 20
• hub and spoke (called as well spine and leaf), without redundant CSR connectivity 21
 22
as shown in Figure 10-3,  Figure 10-4, Figure 10-5 and Figure 10-6. 23
 24
Figure 10-3: Ring based access physical topology  25
 26
 27
Figure 10-4 Chain based access physical topology 28
 29
CSRCSR
CSR
CSR
CSR
HSR
HSR
CSR
CSR
CSR CSR CSR HSRCSR
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             45
 1
Figure 10-5 Redundant hub and spoke access physical topology (also often called spine and 2
leaf)  3
 4
Figure 10-6 Non-redundant hub and spoke access physical topology (also often call spine and 5
leaf) 6
 7
10.1.1.2 Suitability to support O-RAN Fronthaul/Midhaul/Backhaul 8
As discussed in detail in Section 8, depending on the use case, O-DU function can be placed at the 9
cell site, together with O-RU function, or placed away from the O-RU, at the hub site. The O-DU 10
placement has big influence on the possible physical topology for the access domain of the transport 11
network. 12
 13
Open Fronthaul Interface between O-RU and O-DU has very strict latency and timing budgets  14
(O-RAN WG9.Transport Requirements, draft v00.12, June 2020 [15]).The implication of these 15
requirements on the transport network design is, the number of Transport Network Elements 16
(TNEs) between O-RU and O-DU should be minimized, as each TNE increases the latency and 17
time error. 18
 19
While each of the depicted access topology could be used for Fronthaul, topologies with limited 20
number of transit transport network elements – hub-and-spoke or spine-and-leaf – leave more 21
latency budget for delay caused by light propagation in the fiber, allowing for extending Fronthaul 22
over larger distances, comparing to the topologies with bigger number of transit transport network 23
elements (ring or chain topologies). 24
 25
Ring or chain topologies can be used for Fronthaul, providing that latency and timing error budgets 26
required for particular Fronthaul deployment are maintained. For example, designing Fronthaul for 27
standard NR performance (O-RAN WG9.Transport Requirements, draft v00.12, June 2020 [15] 28
Table 3), mandates maximum 100 s one-way latency between O-RU and O-DU. This latency 29
budget is consumed by the fiber (~4.9 s/km), as well as by transit transport network elements (~1-30
20 s per node, depending on hardware capabilities and port speeds). Assuming 10 s latency per 31
transport network element, a chain or ring with 10 transport network element would completely 32
consume entire Fronthaul latency budget (ring might break, so any Fronthaul design must be 33
CSR
CSR
HSR
HSR
CSR
CSR
CSR
CSR
CSR
CSR
HSR
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             46
prepared for link failures not only from traffic rerouting perspective, but as well must take into 1
account increased number of transit network elements, leading to increased latency, under ring 2
failure condition). 3
 4
Shorter chain or ring, with for example 7 transport network elements, leaves only 30  (6 km) 5
latency budget for fiber. 6
 7
Note: the maximum number of TNEs can be limited due to overall synchronization requirements 8
and the consideration of time error contributions of integrated T-BC/T-TC. Examples are given in 9
CUS specification Annex H, and will be provided in WG9 Timing and Synchronization architecture 10
solution document [17]. 11
 12
For Midhaul/Backhaul interfaces, there are not so strict requirements regarding latency budgets, so 13
any access topology (ring, chain or hub-and-spoke/spine-and-leaf) can be used without special 14
considerations. 15
 16
Note: Hub and spoke architectures are often super-imposed over a physical ring topology using 17
WDM technology. To minimize the Fronthaul latency, direct (dark) fibre should be used in hub and 18
spoke (spine and leaf) access physical architecture carrying Fronthaul traffic. Otherwise (ring 19
topology), detailed latency analysis must be performed to confirm that latency stays within 20
mandated Fronthaul budget. 21
 22
10.1.1.2.1 Time Sensitive Networking (TSN) for Fronthaul 23
IEEE 802.1CM [22] defines two Time Sensitive Networking (TSN) profiles, with main 24
characteristics summarized in Table 2. 25
 26
Characteristic TSN Profile A  TSN Profile B
Max frame size for Fronthaul data 2,000 octets 2,000 octets
Max frame size for non-Fronthaul data 2,000 octets no limit
Differentiated prioritization for Fronthaul data yes yes
Frame pre-emption of non-Fronthaul data no yes
 27
Table 2 TSN Profile A and Profile B comparison 28
 29
The major differences between TSN Profile A and TSN Profile B are therefore the maximum frame 30
size of non-Fronthaul traffic, as well frame pre-emption, as pictured in Figure 10-7. 31
 32
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             47
 1
Figure 10-7 TSN Profile A and TSN Profile B operations 2
 3
IEEE 802.1CM has detailed discussion about transport network element latency calculation 4
(Section 7.2), as well as example end-to-end delay calculations (Annex B), therefore these 5
calculations are not repeated in this document. This document, however, focuses on delay 6
differences of Fronthaul traffic introduced by transport network element operating in accordance 7
with TSN Profile A or TSN Profile B. 8
 9
Following delay components contribute to overall transport network element delay: 10
 11
• Frame transmission delay, which is the time taken to transmit the frame at the 12
transmission rate of the port. Since both TSN Profile A and TSN Profile B mandate for 13
Fronthaul data maximum frame size of 2000 bytes, this delay component is equal for 14
Fronthaul data frames with both TSN Profile A and TSN Profile B. 15
• Self-queueing delay, which is the delay caused by other frames in the same traffic class as 16
frame to be sent (i.e. both frames are for example Fronthaul data frames). The part of the 17
self-queuing delay caused by frames that arrive at more or less the same time from different 18
input ports is referred to as fan-in delay; however, it is simpler to handle fan-in delay as part 19
of the self-queuing delay. Since both TSN Profile A and TSN Profile B mandate that 20
Fronthaul data frames cannot be pre-empted, this delay component is equal for Fronthaul 21
data frames with both TSN Profile A and TSN Profile B. 22
• Queuing delay, which is the delay caused by the frame of which transmission already 23
started in an arbitrarily small time before frame X became eligible for transmission, plus the 24
delay caused by queued-up frames from all flows with higher priority than the traffic class 25
of frame X. Since both TSN Profile A and TSN Profile B mandate that Fronthaul data must 26
be prioritized over non-Fronthaul data, in both profiles only single non-Fronthaul frame can 27
contribute to this delay. In case of TSN Profile A, a non-Fronthaul frame size is limited to 28
2,000 bytes (2,020 bytes including preamble, start of frame delimiter and inter-frame gap). 29
In case of TSN Profile B, a non-Fronthaul frame can be pre-empted, but at least 155 bytes of 30
pre-empted frame are transmitted due to the characteristics of frame preemption. 31
eCPRI
BE
AF1
AF2 1
eCPRI
BE
AF1
AF2
2
1
eCPRI
BE
AF1
AF2
2
1
eCPRI
BE
AF1
AF2
2
T0: packet 1
transmit starts
T1: packet 2
appears in
eCPRI queue
T2: packet 1
transmit finishes
T3: packet 2
transmit starts
T2-T1àpacket 2
serialization delay
(=eCPRI latency/ PDV)
eCPRI
BE
AF1
AF2 1
eCPRI
BE
AF1
AF2
2
1
eCPRI
BE
AF1
AF2
2
1
eCPRI
BE
AF1
AF2
2
T0: packet 1
transmit starts
T1: packet 2
appears in
eCPRI queue
T2: max 155 bytes
transmitted
since T1
T3: packet 2
transmit starts
T2-T1à155 bytes of
packet 2 serialization delay
(=eCPRI latency/ PDV)
TSN Profile A
(no preemption)
TSN Profile B
(preemption)
~
1
~
Frame preemption
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             48
• Store-and-forward delay, which includes all other elements of the forwarding delay that 1
are a consequence of the internal processing of the transport network element, including the 2
time to select the input for transmission to the egress port, assuming that the input queue 3
under consideration and output queues are empty. This delay factor is highly hardware 4
dependent, but is equal for both TSN Profile A and TSN Profile B on given hardware. 5
 6
Summarizing, the only differentiated delay factor between TSN Profile A and TSN Profile B is 7
queueing delay of non-Fronthaul data (queueing delay of 2,020 octets in TSN Profile A, versus 8
queuing delay of 155 octets in TSN Profile B). 9
 10
Table 3 summarizes this queuing delay for various port speeds. 11
 12
Port
speed
TSN Profile A
Queuing delay of 2,020 bytes
TSN Profile B
Queuing delay of 155 bytes
Difference
1 Gbps 16.160 s 1.240 s 14.920 s 3,045 m
10 Gbps 1.616 s 0.124 s 1.492 s 304 m
25 Gbps 0.646 s 0.050 s 0.596 s 122 m
50 Gbps 0.323 s 0.025 s 0.298 s 61 m
100 Gbps 0.162 s 0.012 s 0.149 s 30 m
200 Gbps 0.081 s 0.006 s 0.075 s 15 m
400 Gbps 0.040 s 0.003 s 0.037 s 8 m
 13
Table 3 Queuing delay of non-Fronthaul data 14
 15
Significant difference between TSN Profile A and TSN Profile B can be observed over port with 1 16
Gbps speed (14.92 s, which is equivalent to ~3 km fiber). For port speeds 10 Gbps or above, the 17
latency difference introduced to Fronthaul traffic (1.494 s, which is equivalent to 305 m fiber, or 18
less) is not so significant for designs targeting standard NR performance (with 100 s overall 19
latency budget). 20
 21
10.1.2 Pre-aggregation / Aggregation / Core transport 22
Pre-aggregation and aggregation and the core transport infrastructure can support Midhaul and 23
Backhaul requirements. Due to the delay requirements associated with Fronthaul, it is unlikely O-24
RAN 7.2x or RoE will be seen in these portions of the network. Additionally, in most scenarios the 25
transport core will not be part of the Midhaul or Backhaul environment but may provide N6 26
connectivity to any of the following:  27
1. Peering points for mobile consumer services. 28
2. Enterprise locations for mobile enterprise solutions.   29
3. Inter region voice communications within the operator.  30
4. Other operators. 31
10.1.2.1 Physical media and topologies 32
The assumption made in this architecture is the connectivity between routers in the pre-aggregation, 33
aggregation and transport core is provided by point-to-point Ethernet connections where the full 34
capacity of the Ethernet interface is available to the router. Link speed will depend on the traffic 35
levels required by the services running and the position in the network. The anticipated ranges will 36
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             49
be from 10Gbps up to bundles of 400Gbps with the capacity provided over dark fibre or some form 1
of WDM solution. 2
 3
The physical topology employed in the pre-aggregation, aggregation and transport core are operator 4
dependent and driven by fibre topology, its availability and the traffic matrix. The main physical 5
architectures seen in these segments of the network are all redundant and include ring, “hub and 6
spoke” and mesh. These are shown in Figure 10-8.  7
 8
Note: This document uses the term “hub and spoke architecture” but it is used inter-changeably 9
with leaf / spine and fabric architectures. 10
 11
Figure 10-8 Physical topologies anticipated in Xhaul pre-aggregation, aggregation and core 12
 13
1. Ring: This architecture is commonly seen in pre-aggregation and aggregation networks 14
where traffic is being collected and aggregated from the access towards components in the 15
aggregation or transport core. Connectivity between routers is either via direct dark fibre 16
connections or via lambdas derived from a WDM system. 17
 18
2. Dual hub and spoke: This architecture is most commonly seen in aggregation and core 19
transport networks but also sometimes in pre-aggregation networks. It is used to collect and 20
aggregate traffic towards common aggregation points. Connectivity between routers is 21
typically via direct dark fibre or via lambdas derived from a WDM system. 22
 23
Note: Hub and spoke architectures are often super-imposed over a physical ring topology 24
using WDM technology.  25
 26
3. Mesh: This is architecture is most commonly seen in transport core networks but can be seen 27
in aggregation networks. It is used when traffic is flowing in an any-to-any fashion. As drive 28
distances increase and connectivity becomes more meshy in nature, so there is increasing use 29
of DWDM and ROADM technology to derive the links between routers.     30
 31
10.2 Alternative physical transport solutions 32
This document concentrates packet switching over a fibre infrastructure. However, there are other 33
physical solutions that can be employed in Xhaul networks. This section outlines some of those 34
technologies and discusses their applicability in different Xhaul roles.   35

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             50
10.2.1 WDM in access network  1
The access network uses optical technology with no packet switching transport functionality. Figure 2
10-9 illustrates a passive WDM Fronthaul design but the same design principle applies regardless of 3
the WDM solution. Each O-RU directly connected to the WDM multiplexer/demultiplexer by a 4
colored optic and an optical cable. At the O-DU side in the central office, the WDM 5
multiplexer/demultiplexer performs wavelength multiplexing/demultiplexing, which realize the 6
one-to-one optical wavelength connection. The WDM solution can be passive, semi active or active.  7
 8
 9
Figure 10-9 WDM Fronthaul design (passive) 10
 11
WG-9 has a work item underway considering WDM-based Fronthaul transport [16], consequently 12
this document does not consider this type of access network design further.   13
 14
10.2.2 Passive Optical Networks (PONs)  15
Passive Optical Networks (PONs) are fiber-based point-multipoint access networks, relying on 16
TDM/TDMA for the exchange of traffic between a central Optical Line Termination (OLT) and 17
multiple Optical Network Units (ONUs). PONs also use WDM for separation of up- and 18
downstream traffic, and for overlaying of multiple different PON technologies over the same fiber 19
infrastructure called Optical Distribution Network (ODN). Note that there are two basic flavours of 20
PON;  21
1. Time or Time +Wavelength Division Multiplexing (TDM or TWDM) PONs apply point-22
multipoint connections (one up/down wavelength pair (called Channel Pair) is shared over 23
multiple ONUs) over a point-multipoint ODN.  24
2. Wavelength Division Multiplexing (WDM) PONs apply point-point connections between 25
each ONU and the OLT (one Channel Pair per ONU) over a point-multipoint ODN.  26
 27
WDM PON are similar to dedicated point-point links whereas TDM/TWDM PONs have some 28
fundamental characteristics to bear in mind when considering them for mobile Xhaul, described 29
below. 30
 31

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             51
 1
Figure 10-10: Typical TDM PON network. In TWDM PON multiple Channel Pairs are 2
muxed per ODN 3
 4
TDM/TWDM PONs are standardized technologies (see [158], [24], [25], [26]) and are 5
characterized by generic capabilities (independently of the use case): 6
• PON line rates are shared over the ONUs and existing technologies reach 10 Gbit/s per 7
channel (up to 4x 10 Gbit/s with NG-PON2), with new variants being developed at 25 Gbit/s 8
and 50 Gbit/s. Note that some fraction is consumed by the technology-specific overheads 9
(e.g. by parity bits for Forward Error Correction). 10
• Depending on the technology the maximum distance can reach 20 km or 40 km (or 60 km 11
with a reach extender), the maximum amount of ONUs per PON port can reach 64 or 128. 12
Note that the reach (distance) and scalability (amount of ONUs per PON ODN) are a mutual 13
trade-off bound by the optical budget (many classes of optical budget exist, e.g. Class N1 14
covers optical losses from 14dB to 29dB).  15
• The max amount of traffic flows per PON port can reach 4k (independently of physical layer 16
considerations). 17
• The upstream TDMA multiplexing is controlled by a Dynamic Bandwidth Assignment 18
(DBA) algorithm in the OLT (which is equivalent to upstream shaping over the PON 19
segment). The DBA allocates the bandwidth needs per transport container (each ONU can 20
have one or multiple transport containers) and update its assigned bandwidth. The assigned 21
bandwidth is then converted into a burstiness (rate and size of bursts). Bandwidth can also 22
be (partially or fully) assigned as a fixed value. The upstream latency is influenced by the 23
DBA behaviour, with a trade-off between latency and efficiency (more frequent bursts 24
means less waiting time between bursts but more overhead). 25
• Protection of (parts of) the passive ODN and active OLT equipment by automated switch-26
over from working to protecting parts of the equipment. 27
• Management and configuration of the ONUs. Note that ONUs operate at Layer 2 only (e.g. 28
for Layer 2 flow classification). An ONU can be connected to (or integrated with) a Layer 3 29
device like a residential gateway. 30
 31
PON ports are terminated in the OLT node, which terminates the PON layers and acts as an 32
aggregation and multiplexing node for the multiple PON ports. Contrary to the PON layer, the 33

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             52
OLT’s features are not strictly standardized, although reference deployment models are defined in 1
Broadband Forum documents (See [153], [154]). Typical OLT features include;  2
• QoS-aware packet forwarding between user side and network side (unicast, multicast, Layer 3
2 switching, Layer 3 routing) 4
• Traffic management (classification, (re)marking, queuing and scheduling, policing, 5
shaping…) 6
• User-side protocol interaction (e.g. DHCP relay agent, IGMP proxy, ARP relay, …) or on 7
the contrary protocol transparency 8
• Network-side protocol support (e.g. IP routing protocols, IGMP proxy, MPLS signaling, …) 9
 10
The datagrams (typically IP over Ethernet) undergo several handlings when being transported from 11
end to end over a PON system. The different steps can be summarized as follows:  12
 13
Downstream (OLT SNI ingress to ONU UNI egress)  14
1. Traffic Management (classification, policing, queuing and scheduling) at ingress (OLT SNI), 15
2. Forwarding (Layer 2 switching or Layer 3 routing) towards corresponding PON MAC (PON 16
port), 17
3. Encapsulation in PON MAC (according to standard, ITU-T or IEEE), 18
4. OLT PON port egress queuing and scheduling, 19
5. Fiber propagation, 20
6. ONU (Layer 2 based) forwarding to corresponding UNI, 21
7. Decapsulation of PON MAC layer, 22
8. ONU UNI egress queuing and scheduling (Layer 2) 23
 24
Upstream (ONU UNI ingress to OLT SNI egress) 25
1. DBA processing in OLT for burst-based TDMA transmission (acts as policer and shaper), 26
OLT notifies grants to ONUs by sending a Bandwidth Map to all ONUs, 27
2. Traffic Management (Layer 2 classification, queuing and scheduling) at ingress (ONU UNI), 28
3. Encapsulation in PON MAC (according to standard, ITU-T or IEEE), 29
4. Burst generation as per bandwidth map, 30
5. Fiber propagation, 31
6. Decapsulation of PON MAC layer, 32
7. Forwarding (L2 switching or L3 routing) to right OLT uplink (SNI), 33
8. OLT uplink (SNI) egress queuing and scheduling 34
 35
PONs are widely used for all FTTx use cases (Cabinet, DPU, Office, Home, …), and are designed 36
for multi-service support (residential triple play, business services, Wireless LAN access points). As 37
explained in the next section and [157], PONs have been extended with additional features and are  38
also suitable for multiple Xhaul scenario’s. PONs can act as layer 2 legs in the end-end transport, or 39
participate (from OLT upwards) to layer 3 routing. 40
10.2.2.1 Using PON for Xhaul: specific features 41
As a PON is an asymmetrical (bandwidth and latency) point-to-multipoint medium, time and 42
frequency synchronization have to be transported using native media means using media converters 43
(in OLT and ONU). The PON technology have in-built mechanisms for providing frequency and 44
phase synchronization. This will be described in [17]. 45
  46
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             53
The PON DBA is being extended with a cooperative mode DBA (called Cooperative DBA or CO 1
DBA in short for interaction with O-RAN CTI [19] making it suitable for some Fronthaul needs by 2
improving the multiplexing while keeping transport latency low. 3
 4
A specific side effect of T(W)DM PONs is the need for discovery and ranging of new ONUs by 5
means of short interruptions (up to a couple of 100’s of µs) of upstream traffic. Such interruptions 6
are likely to impact the performance at the mobile radio layer, but several ways to mitigate the 7
impact or reduce or eliminate the interruptions are being developed (ranging notification from OLT 8
to O-DU by CTI, optimization of ranging timing to minimize impact, avoiding ranging on the 9
latency-sensitive channel by means of separate dedicated activation wavelength). 10
 11
10.2.2.2 Xhaul Use cases: topologies 12
PON for Fronthaul 13
 14
 15
Figure 10-11 PON for transport of Fronthaul flows 16
 17
Points of attention for Fronthaul deployments are latency at transport level, capacity per O-RU, and 18
time synchronization accuracy (Time Error between the O-RUs). For Fronthaul the O-RUs are 19
connected to ONUs, the OLT is connected to multiple O-DUs (or O-DU/O-CUs), possibly through 20
intermediate aggregating node(s). The PON system can be on-site (e.g. cell site = event zone) or 21
reach up to the cell site (ONU at cell site).  22
The appropriateness of a PON technology depends on several points: 23
- the considered radio bandwidth and corresponding Fronthaul throughput and the number of 24
O-RUs per PON that can multiplexed per single PON port 25
- the obtained latency at packet transport level which depends on the DBA settings (and use 26
of CTI). The requirements for Fronthaul depend on the combination of O-DU and O-RU 27
categories, which have one-way latency budgets ranging from tens to hundreds of µs for 28
usual Fronthaul, and multiple ms for non-ideal Fronthaul (see [13]). Plain DBA is not suited 29
for usual fronthaul, for which Cooperative DBA with CTI is required. Plain DBA can 30
address non-ideal fronthaul. See 10.2.2.3 for more details. 31
- The time accuracy between O-RUs on PON systems will be described in [17]. 32
 33
PON for Midhaul 34
 35
 36
Figure 10-12: PON for transport of Midhaul flows 37
 38
Points of attention for Midhaul deployments are latency at application level (e.g. for URLLC), QoS 39
differentiation (for achieving better statistical multiplexing), capacity per O-RU. For Midhaul the 40
O-DUs are connected to ONUs, the OLT is connected to multiple O-CUs, possibly through 41

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             54
intermediate aggregating node(s). The PON system (ONU) can reach up to the cell site or to some 1
higher aggregation point.  2
The appropriateness of a PON technology depends on  3
- the considered radio bandwidth and corresponding Midhaul throughput,  4
- the number of O-RUs per PON that can multiplexed per single PON port and the obtained 5
latency at application level (for higher CoS like e.g. URLLC). See 10.2.3.2 for more details. 6
- the requirement for time alignment accuracy of TDD transmitters between O-RU and the 7
PRTC (Class 4A 3µs) is independent of the topology of O-RU clusters (mutual position of 8
O-RUs on the PONs). The time accuracy with PON systems will be described in [17]. 9
 10
PON for Backhaul 11
 12
Figure 10-13 PON for transport of Backhaul flows 13
  14
The points of attention are similar as for Midhaul. For Backhaul the O-CUs are connected to ONUs, 15
the OLT is connected to the mobile core, possibly through intermediate Backhaul aggregating 16
node(s). The PON system (ONU) can reach up to the cell site or to some higher aggregation point. 17
The appropriateness of a PON technology is similar as for Midhaul. 18
 19
PON for carrying traffic mixes 20
 21
Note that PON systems are multi-service, hence mixes of different Xhaul flavours can co-exist on 22
the same PON system (e.g. Midhaul + Backhaul) and can also co-exist with non-mobile traffic (e.g. 23
business services). There is also the possibility of re-using the passive fiber infrastructure for 24
different PON technologies by means of WDM overlay, e.g. one system for mobile and one system 25
for residential services. Traffic characteristics of each PON system are then fully independent of the 26
other system.   27
 28
10.2.2.3 Using PON for Xhaul: trade-offs of the technology  29
PON systems can be used in many situations, but obviously not all. Each Xhaul transport use case 30
poses specific requirements, which must be considered in light of the possible trade-offs inherent to 31
the PON technology: 32
1. Latency 33
a. versus bandwidth efficiency (making bursts more numerous and shorter decreases 34
latency but increases physical layer overhead).  35
b. versus statistical multiplexing (the higher the ratio of variable versus fixed BW 36
allocations, the better the multiplexing but the higher the latency can become). Using 37
CTI allows to reach higher statistical multiplexing while keeping latency limited. 38
c. versus reach (5µs/km one-way propagation time) 39
d. As guideline, using PONs with fixed bitrate allows to support very low latencies 40
(sub-100µs) but this is not efficient for variable rate fronthaul. PONs with plain DBA 41
efficiently follow variable rate traffic but introduce a latency in the order of one to 42
several ms. When combining CTI with Cooperative DBA in the OLT for variable rate 43
fronthaul traffic (like O-RAN 7.2x), the extra latency can be reduced by an order of 44

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             55
magnitude compared to plain DBA, and such PONs can support several use cases of 1
usual fronthaul and all combinations of non-ideal fronthaul (see O-RU and O-DU 2
combinations as per [13]). 3
2. Bandwidth per Xhauled node versus the number of nodes served per ODN (bandwidth 4
sharing) 5
3. phase synchronization accuracy versus topology of O-RU clusters in PONs and path 6
propagation difference up/down (max fiber distance and wavelength-dependent correction 7
factors). This will be described in [17].  8
 9
10.2.3 DOCSIS Networks  10
Today’s modern cable operators deploy and manage extensive hybrid fiber coaxial (HFC) networks. 11
The HFC plant reaches 93% of American households and connects to virtually every building 12
across north America. The data-over-cable service interface specification (DOCSIS®) is the 13
protocol designed to work on the HFC network. DOCSIS suite of specifications is standardized and 14
maintained by CableLabs® [166][167], and is deployed worldwide. DOCSIS technology has 15
evolved through six generations of progressive refinement. Originally developed for delivering high 16
speed broadband to residential customers, the cable industry has made many advances in the HFC 17
technology, making it a promising option for 5G transport. The HFC plants are already extensively 18
deployed in areas where 5G will be in the most demands, especially in dense urban and suburban 19
environments. Additionally, the HFC plant is active, providing power needed for small cell radios. 20
The cable operators typically have the right of way on the strands where radios can be hung, 21
eliminating the need to obtain permits from government. Leveraging the existing HFC deployments 22
significantly reduces time-to-market and cost of deploying 5G. 23
 24
10.2.3.1 DOCSIS technology overview 25
DOCSIS protocol is a Layer 2 transmission protocol that encapsulates the IEEE 802.3 Ethernet. The 26
technology supports both Layer 2 and Layer 3 services and is aware of both. DOCSIS network is a 27
point-to-multipoint access network, where the transmission of upstream and downstream is 28
controlled by the cable modem termination system (CMTS). Figure 10-14 shows a typical HFC 29
plant. The CMTS is typically located in either a headend or a hub, which is connected to the fiber 30
nodes through digital or analog fiber. From there, the communication path traverses through a coax 31
network of zero or more amplifiers to the neighborhoods. The hybrid (fiber and coax) network 32
topology is called “N+x”, where “N” is the fiber node, where the fiber ends and coax starts, and “x” 33
is the number of amplifiers traversed. The average north American plant is between N+2 and N+5. 34
The smaller the x, the closer the fiber node it is to the neighborhood, the larger the amount of 35
capacity would be available to each customer. 36
 37
The traditional physical CMTS controls 50k cable modems (CMs). It is expected to scale up as the 38
CMTS moves to the cloud native architecture. 39
 40
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             56
 1
Figure 10-14: Typical HFC plant 2
 3
Table 4 shows the capabilities of the most recent DOCSIS standards. While DOCSIS 4.0 will 4
provide over 10 Gbps of downstream capacity, even higher rates can be supported. A unique aspect 5
of the DOCSIS technology is its ability to progressively expand downstream and upstream 6
capacities when and where it is needed. Traditionally, the same plant is used to provide video and 7
broadband services. As the cable industry moves away from traditional video delivery, the RF 8
spectrum that is used for video can be reclaimed for broadband. Moreover, as cable operators 9
moves to reduce the “x” in the N+x topology, node segmentation takes place. For every node that is 10
segmented into two, the network capacity essentially doubles. 11
 12
Requirements D3.1 today
(2020)
D3.1 max
(2021-22)
D4.0
(2023-24)

Downstream spectrum
Upstream spectrum
Shared spectrum with
video

54 – 1002 MHz
5 – 42 MHz
Full spectrum through
video reclamation

258-1218 MHz
5 – 204 MHz
Extending to 1.8 GHz,
possibly 3 GHz

602 – 1794 MHz
5 – 492 MHz
DS capacity
US capacity
8.5 Gbps
0.1 Gbps
8.6 Gbps
1.4 Gbps
10.8 Gbps
3.7 Gbps
Upstream latency Best effort: 5 – 50 ms
With LLX / CTI: 1 – 2 ms (can be further reduced)
Synchronization Frequency sync only
No time sync Frequency + time sync through DTP
 13
Table 4 DOCSIS Capabilities; Today and the near future  14
 15
The DOCSIS specification provides a rich set of QoS mechanisms, including traffic classification, 16
queuing, multiple scheduling services, policing, traffic shaping. Each CM supports 16-32 service 17
flows in each direction. DOCSIS technology supports several native upstream scheduling 18
mechanisms: best effort, unsolicited grant service (UGS), real-time polling service (RTPS), and 19
proactive grant service (PGS). Latency on the upstream can range from minimum of 3-5 ms to 20
average of 12 ms under best effort, to 1-5 ms under UGS or PGS. For best effort scheduling service, 21
latency is dependent on the load, as well as frame configurations such as OFDM frame size and 22
interleaver depth. For UGS/PGS services, there are trade-offs between incurring capacity overhead 23
vs. lowering the latency. The recommendation is to transport mobile Xhaul traffic over the DOCSIS 24
network via best effort service. To address the latency concern, CableLabs standardized the Low 25
Headend / Hub
Cable
Modem
Mobile Core
CMTS
Fiber
Average reach ~30km
Coax (including
ampliﬁers, taps)
Cable
Modem
Cable
Modem
Fiber
node
Cable
Modem
Residential /
business
aggregation
Aggregation
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             57
Latency Xhaul (LLX) technology [164], which provides consistent 1-2 ms of latency on the 1
upstream. The latency could potentially be further reduced with some configuration optimizations. 2
The ORAN CTI [19] specification uses a similar scheduler pipelining mechanism as LLX. 3
Typically, the latency on the downstream is 1ms. 4
 5
Cable operators are in the midst of disaggregating the traditional CMTS. The “Distributed Access 6
Architecture” (DAA), shown in Figure 10-15, has been specified by CableLabs [168] and is being 7
deployed. Replacing the traditional “integrated” CMTS, PHY layer component of the CMTS is 8
being pushed to the fiber node and is called Remote PHY Device (RPD), while MAC and upper 9
layers are implemented as DAA Core, and is located in the headend or regional data center. The 10
DAA Core has been architected with the cloud native architecture. The links between the fiber node 11
and the hub are being upgraded to a digital fiber network, with DWDM as a starting point, but some 12
operators may choose to migrate to point-to-point coherent optics. A device called “CTD” is being 13
specified to terminate the fiber from the hub, and is essentially be a Layer 2 switch that provides 14
high-speed Ethernet to the neighborhood. A multitude of services can be provided through the CTD, 15
including 5G Fronthaul, PON, etc, thus achieving a form of transport convergence. 16
 17
 18
 19
Figure 10-15 Distributed Access Architecture (DAA) for transport convergence 20
10.2.3.2 Xhaul transport DOCSIS network 21
Functionally, the DOCSIS network can interconnect the small cells or radios as shown in Figure 22
10-16. However, the requirement on capacity and latency vary between Backhaul, Midhaul, and 23
Fronthaul. Backhaul and Midhaul are both based on an IP encapsulation of the original mobile 24
transport. Thus, the bandwidth requirement on the transport network roughly matches the mobile 25
traffic rate.  26
 27
The latency requirement for Backhaul is based on the application. Additional service-level 28
agreement (SLA) can be specified by the mobile operator. Midhaul latency is generally considered 29
to be less than 10 ms [169]. LLX can be implemented to ensure these requirements can be met, as 30
well as providing better latency performance, particularly for latency-sensitive flows. 31
 32

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             58
 1
Figure 10-16 Mobile Xhaul over DOCSIS Network  2
 3
Fronthaul is much more difficult to support over the coax portion of the HFC network. Studies have 4
shown that the eCPRI-based Fronthaul transport needs significantly more bandwidth, overhead, and 5
one-way latency in the neighborhood of 250 microseconds between the O-RU and O-DU and 6
between TNEs 100 microseconds. Even with LLX, it will be difficult to reduce the DOCSIS latency 7
to this level. Because of the stringent requirements, Fronthaul is better carried directly over the 8
digital fiber network and over CTD as shown in Figure 10-16. 9
 10
10.2.4 Microwave and mmave radio transport technologies 11
10.2.4.1 Overview of Microwave and mmwave technology 12
For over 20 years, microwave has been the primary solution for the rapid and cost-effective rollout 13
of mobile Backhaul infrastructure with over 50% of mobile sites worldwide today connected via 14
Microwave (MW) or Millimetre Wave (mmW) radio links and up to over 90% in some networks. 15
The evolution from 4G towards 5G presents significant challenges to all transport technologies and 16
wireless ones make no exception. 17
 18
Spectrum is a vital asset to support the mobile Backhaul requirements and certainly this topic 19
becomes increasingly relevant as future mobile access data rates and respective Backhaul capacity 20
requirements continue to rise. Various frequency bands are used today for mobile Backhaul 21
depending on the requested capacity, hop length, link availability, spectrum availability and 22
frequency re-use capability. The spectrum that is available for mobile Backhaul ranges from the 23
traditional microwave bands up to the millimetre wave spectrum as illustrated in the Figure 10-17. 24
These bands are allocated for fixed services by the WRC Radio regulations. Their channel plans are 25
detailed by the ITU-R F-series recommendations as well as some regional or national regulatory 26
regimes, such as CEPT. Fixed services spectrum uses authorisations awarded by national 27

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             59
administrations on the basis of link by link assignments as either coordinated or self-coordinated, or 1
as block assignments. 2
 3
 4
Figure 10-17 Microwave and mmwave spectrum 5
 6
The engineering of a MW or mmW link involves finding the optimal combination of link length, 7
capacity, frequency band and availability. 8
 9
The physics of radio waves propagation determine the relation among capacity, availability and link 10
length.  11
Since the available spectrum is proportional to the center frequency, the highest frequencies are also 12
those that carry the most capacity, but also cover the comparatively shortest link lengths.  13
As a rule of thumb, frequencies below 13 GHz can be considered mostly unaffected by the intensity 14
of rainfall and frequencies above are more and more influenced by the attenuation caused by rain, 15
so that as a general principle higher frequencies are used for shorter links, as illustrated in Figure 16
10-18. It should be noted that features could be inherited for links combining different bands as 17
indicated in the circles in this figure. 18
 19
 20
Figure 10-18 Interdependence among frequency, capacity and availability. Source ETS TS 21
mWT 22
 23
The availability of MW/mmW spectrum depends on both technological and regulatory factors. 24
Technology is available and under development to make full use of existing (6-86 GHz) and future 25
(90-300 GHz) spectrum: E-band (80 GHz) has been commercially deployed for several years, W-26
band (100 GHz) and D-band (150 GHz) are the most promising upcoming bands, with prototypes 27
already appearing in the market. 28
 29

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             60
Wider channels (112MHz, even 224MHz where possible) in traditional frequency bands and raw 1
availability of spectrum (10GHz in E-band, 18GHz in W-band and 30GHz in D-band) provide the 2
main resources to expand the capacity of MW and mmW radio systems. Overview of available 3
spectrum for wireless transport network with corresponding capability related to link capacity, 4
latency and hop length is illustrated in Figure 10-19. 5
 6
 7
 8
Figure 10-19 Bands characteristics and capabilities. Source ETSI TSG mWT 9
 10
10.2.4.2 Evolving technologies 11
There are a number of existing and evolving technologies which are applied in microwave and 12
mmwave radio to enhance its spectrum efficiency, achievable capacity and reducing latency. An 13
overview of these technologies is presented in [177]. As follow is a summary of these applicable 14
technologies in the microwave and mwave radio transport systems. 15
 16
Capacity and spectrum efficiency enhancement 17
Larger channels are no longer a technology limit. In MW bands recent regulatory limit shifted up to 18
Channel Spacing CS=224MHz, but not everywhere. Up to CS=2000MHz is standardised in E-Band 19
and above 100GHz. 20
Larger CS are needed where Carrier Aggregation, in same band or adjacent band is employed. 21
 22
Higher Modulation schemes reached the reasonable top at 4096QAM (and more). After 1024QAM 23
spectral efficiency gain is less than 10% for every step. Higher order result in reduction in system 24
gain (impacting availability), however, with adaptive modulation availability can be maintained 25
with lower modulation order (i.e. scarifying capacity).  26
 27
Frequency reuse with cross polar interference cancelation (XPIC) 28
This is a well-known technique for doubling the spectral efficiency by utilising the cross 29
polarisation of the channel bandwidth occupied. 30
 31
LoS-MIMO Line of Sight Multi-Input Multi-Output 32

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             61
Exploiting link geometry deployment, two different signals in the same channel can be transmitted. 1
4x4 LoS-MIMO is obtained with LoS-MIMO 2x2 plus XPIC 2
LoS MIMO needs optimal antennas separation. 3
Under optimal conditions, spectral efficiency close to four times capacity improvement can be 4
achieved, while lower performance in case of suboptimal conditions should be expected.  5
 6
 7
Figure 10-20 4x4 Multiplexing with LoS MIMO and XPIC  8
 9
Orbital Angular Momentum  10
OAM is a new transmission mechanism allowing multiplexing multiple streams simultaneously 11
over the same frequency channel without the limitation imposed by the optimum separation 12
distance of antennas in the LoS MIMO. Using different antennas, multiple OAM signals with 13
different spiral phase front (mode) can be transmitted. OAM modes are orthogonal of each other. 14
A pragmatic OAM system, uses uniform circular antenna array while the OAM signal is generated 15
at the base band as illustrated in the Figure 10-21. 16
 17
 18
Figure 10-21 Orbital Angular momentum transmission 19
 20
An OAM system with 8 Uniform Circular Array UCA antenna elements allows the transmission of 21
16 orthogonal streams resulting in capacity of around 105Gbps. [163] 22
 23
Bands & Carriers Aggregation (BCA) 24
BCA joins different channels that may be even in different bands, providing a single big capacity 25
pipe. Lower band will provide capacity pipe’s segment with high availability, while higher band the 26
best effort capacity pipe segment. Packets may be adaptively re-routed among different channels 27
according to their priority and channels condition. 28

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             62
    1
Figure 10-22 Bands and carrier aggregation concept and benefits 2
 3
One of the most valuable approach is 15/18/23 GHz with E-Band where dual band antennas are 4
available: 5
- Links up to 7-10Km are feasible. Capacity may even exceed 10Gbps 6
- High spectral efficiency obtained because E-Band can reach longer links than in traditional 7
approach.  8
BCA among two MW bands is another variant when distance becomes more challenging i.e.: rural 9
application. 10
   11
Geographical spectral efficiency: Dense reuse of channels 12
To better exploit the scarce resource (spectrum) it is advisable to increase not only the single 13
channel spectral efficiency but also the channel reusability in a given area, guaranteeing the 14
“interference free operation”. 15
 16
Nodal configuration is the key point to understand the concept of geographical spectral efficiency. 17
Better antenna class are introduced (e.g. ETSI Class 4), reducing significantly the minimum angle 18
between two links using the same/adjacent channels (angle discrimination) 19
Cross polar (XPIC) can also be used in reducing angle discrimination.  20
Co-Channel Interference Canceller (CCIC) further improve the re-use of channels with very narrow 21
angle discrimination. 22
 23
 24
Figure 10-23 Increase nodal capacity is now easy with no additional spectrum with XPIC 25
 26
When additional capacity is needed and then additional channels shall be used, CCIC permits an 27
optimal re-use of channels with very narrow angle discrimination 28
 29

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             63
 1
Figure 10-24 Further increase nodal capacity with increased spectral utilization by applying 2
CCIC technique 3
 4
Combination of the above technologies gives enhancements to microwave/mmwave capabilities as 5
indicated below [176]. 6
 7
 8
MW Backhaul Technology 56 MHz BW 112 MH BW 224 MHz BW +XPIC + Los 2x2
MIMO
+ BCA
(with higher
MW Band)
+BCA
(with mmW
Band)
6-15 GHz 0.5 Gbps 1 Gbps  2 Gbps  3-4Gbps
18-42 GHz 0.5 Gbps 1 Gbps 2 Gbps 2-4 Gbps 4-8 Gbps  4-10 Gbps
 9
mmW Backhaul
Technology 500 MHz BW 2 GHz BW 4 GHz BW +XPIC +LOS 2x2
MIMO/OAM
V-band (60GHz)  >4 Gbps
E-band (70/80GHz) 3.2 Gbps 12.8 Gbps  25.6 Gbps 51.2 Gbps
W-band (100GHz) 3.2 Gbps 12.8 Gbps 25.2 Gbps 51.2 Gbps 102.4 Gbps
D-band (150GHz) 3.2 Gbps 12.8 Gbps 25.6 Gbps 51.2 Gbps 102.4 Gbps
 10
Table 5 Microwave/mmwave enhancements  11
 12
10.2.4.3 Support for packet 13
MW or mmW links are characterised by their physical capabilities related to the combination of 14
link length, capacity, frequency band and availability, which is independent of their network layer 15
functionalities and use cases. These radio links are usually capable of multiplexing various types of 16
traffic over single radio channel and can be equipped with multiple network interfaces including 17
FE/GbE (RJ45, SFP) and CWDM filter support. Various packet functions are also supported as 18
optional features and can be configured by network operators, such as L2 pass through, feature rich 19
L2 switch with ERPS, H-QoS among others enabling carrier grade Ethernet services. MPLS-TP is 20
also supported for layer 2 services, and for timing and synchronisation support includes 1588v2 21
(TC, BC) and SyncE.  22
 23
These network features make MW and mmwave radio popular in Backhaul and Xhaul network 24
infrastructure by themselves. They are also used as a complementing technology to other physical 25
technologies where the MW and mmwave radio links can be used as part of network segments to 26
close links where underlying technology infrastructure is missing or absent or as a back-up in 27
mission critical part of the network. The section below illustrates some use case examples where 28
MW and mmwave radio links are utilised. 29
 30

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             64
10.2.4.4 Topology example use cases 1
Dense urban/urban (C-RAN plus D-RAN support) case 2
The figure below illustrates an example of transport network use in an end-to-end network with C-3
RAN infrastructure used to provide network coverage in a Dense Urban/Urban environment with 4
support for some D-RAN sites.  5
In this network, the core is connected to the O-CU via a Backhaul aggregate and pre-aggregate 6
transport part of the network. At the aggregation/pre-aggregation transport network, a bandwidth of 7
10G or more will be required, mainly 25G or more. MW and mmW will be used as alternative (ring 8
closer) or redundant circuit to fibre, and the application of wide-bandwidth products in the E/D/W 9
bands will be the main focus. 10
At the Midhaul transport network, MW and mmW radio equipment can be used in almost the same 11
band as conventional Backhaul. From the transport perspective, there is little different between 5G 12
and 4G transport networks, with additional synchronisation and delay requirements. 13
 14
 15
 16
 17
 18
 19
Figure 10-25 Network topology example for dense urban/urban environment with microware 20
and mmwave links 21
 22
In urban centers, high bandwidth services are required, hence Fronthaul LLS needs to be more than 23
25G (25-100G). In many cases, LLS is used when DU/RU are collocated in the same building. It 24
may be in short distances. In this case, MW/mmW radio equipment is of limited application.  25
Within the urban environment, some D-RAN sites may also be connected to edge site. In this case 26
IAB (integrated Access Backhaul) base station to base station communication, which cover about 27
100 metres with LoS, can be used.  28
 29
Rural (C-RAN) case 30
In this topology scenario, coverage in rural areas is migrated using existing fibres in a C-RAN 31
configuration for 4G. Generalisation of equipment is made possible by upgrading CPRI links to 32
Ethernet utilising eCPRI or IEEE1914.3 mechanism. The link capacity requirements in this case is 33
in the range 10 to 25G. The application of MW/mmW radio equipment is used to extend coverage 34
and other uses due to its ease of installation. 35
 36
 37

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             65
 1
Figure 10-26 Network topology example for rural environment with microware and mmwave 2
links. 3
10.3 Data Centers  4
Data center solutions, connectivity to the Xhaul transport infrastructure and orchestration are not in 5
scope of this version of document. However, it is important to note the critical role   6
Data centers play in 5G, with key components of the 5G architecture running on servers either 7
virtualized, containerized or as bare metal located in data centers. Consequently, operators need to 8
give consideration to how data centers are connected to the Xhaul transport network and how end to 9
end transport services are built that span the DC and the WAN. At a high-level two basic 10
approaches to interconnecting DC to the Xhaul transport network are available. 11
 12
10.3.1 Complete separation between DC and WAN infrastructure  13
This refers to data centers that are discrete from the WAN infrastructure with each domain 14
potentially running different underlay and overlay transport network technologies. The inter-15
connection between the two domains is via “Data Center Interconnect” (DCIs) routers. These 16
routers reside in both the DC and WAN domains and have an awareness of the DC environment on 17
one side and an awareness of the WAN on the other. To provision an end to end network service, 18
the data center and the WAN are provisioned separately, and the DCIs are provisioned using a 19
cross-domain orchestrator to stitch the two separate service environments together.  20
 21
10.3.2 DC integrated into WAN infrastructure. 22
This refers to data centers that run the same underlay and service infrastructures as the WAN. In 23
this case, although there maybe DCIs to create underlay separation between the WAN and the DC, 24
it is possible to treat the network service infrastructure as a single orchestration domain with WAN 25
transport services directly configured on the Top of Rack (ToRs) devices or on soft switches / 26
routers residing on the server themselves.  27
  28

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             66
11 Packet-switched underlay network – MPLS based 1
This document presents two packet switched underlay technologies, the first is based on MPLS 2
contained in this section (11) and the second based on SRv6 contained in section 12. An operator 3
wishing to implement the transport architecture outlined in this document will need to select one of 4
the two and implement the requirements outlined in the associated section. It should be noted that 5
other transport architectures are potentially available but not covered in this revision of the 6
document.     7
 8
This section outlines a packet switched underlay model based on “Multi-Protocol Label Switching” 9
(MPLS).  MPLS data plane is based on label switching but has more than one control plane 10
technology. The first, classic approach is based on the control plane developed when MPLS was 11
first conceived in the mid 1990s and utilizes IGPs, BGP-LU, LDP and RSVP. An emerging 12
approach, very appropriate to mobile transport networks, is based on Segment Routing (SR) 13
extensions to IGP and BGP, which have been developed since around 2010 and aim to simplify and 14
minimize the number of protocols running in network, as well as reduce the state held on the TNEs. 15
This document describes scalable, multi-domain Xhaul transport architecture that can involve 16
domains with different MPLS control plane technologies (classic: LDP, RSVP, BGP-LU, as well as 17
emerging: SR, SR with Flex-Algo, or SR-TE), in order to support both brownfield (extending 18
existing mobile transport network using classic control plane with additional domains using new, 19
emerging SR control plane) and greenfield (building new Xhaul transport network) deployment use 20
cases. However, in order to avoid duplications with BBF technical reports documenting classic 21
MPLS control plane in mobile transport network designs, and to keep the size of this document 22
within reasonable limits, this document puts more focus on the new, emerging control plane based 23
on SR. 24
 25
Regardless which MPLS control plane – classic or SR – is used to distribute underlay transport 26
labels, the services are overlaid on-top. The service layer is independent from the underlaying 27
MPLS transport layer and supports native Ethernet services and layer 3 services. 28
 29
11.1 MPLS data plane  30
MPLS architecture relies on an MPLS data plane using MPLS encapsulated IP (for L3VPN) or 31
Ethernet (for L2VPN/EVPN) data packets. MPLS label stack, containing potentially multiple labels, 32
pushed on the data packet might encode various information, like for example: 33
• path through the transport network (transport labels) 34
• entropy for load balancing on transit routers (flow or entropy labels) 35
• function/service on egress router (L3VPN/EVPN labels) 36
 37
Figure 11-1 shows examples for Ethernet based eCPRI and IP based eCPRI encapsulated in MPLS. 38
 39
 40

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             67
Figure 11-1 MPLS encapsulation of Ethernet frame or IP packet 1
 2
To support packet switching transport with MPLS data plane, the transport device will need to 3
support: 4
 5
[R1]: MUST support MPLS architecture as defined by “Multiprotocol Label Switching 6
Architecture”, RFC 3031 [45]. 7
[R2]: MUST support MPLS label stack encoding defined by “MPLS Label Stack Encoding” RFC 8
3032 [46]. 9
[R3]: MUST support TTL processing defined by “Time To Live (TTL) Processing in Multi-10
Protocol Label Switching (MPLS) Networks”, RFC 3443 [53]. 11
[D1]: SHOULD support MPLS Explicit Null operation defined by “Removing a Restriction on the 12
use of MPLS Explicit NULL”, RFC 4182 [60]. 13
 14
11.2 MPLS control plane  15
The MPLS control plane is used to distribute information required to deliver the packets through the 16
transport network. It typically includes following information: 17
 18
• Topological information (nodes and links in transport network, including attributes, like 19
metrics, IP prefixes administrative groups, SRLGs, etc., associated with these topological 20
elements) 21
• IP prefixes 22
• MPLS transport label information 23
 24
Based on the collected information, control plane responsibility is to 25
 26
• calculate paths (shortest path to the destination, or path fulfilling certain administrative 27
constraints, which are not necessarily the shortest paths – for example low latency path) 28
• program data plane with information required to forward the MPLS packets through the 29
network (next-hops, MPLS labels stack manipulation rules – pop, push, swap) 30
• calculate and program in the data plane backup paths required for rapid protection 31
mechanisms (fast re-route – FRR) 32
 33
In a packet switched transport network consisting of a large number of routers, a mix of WAN and 34
data centers components, the need to support traffic engineering, the underlay control plane is a 35
typically a collection of independent routing domains that interact in a collaborate fashion. 36
 37
 38
ACCESS ACCESSAGGREGATIONAGGREGATION TRANSPORT
CORE
End-to-End Label Switched Path
Distributed PCE Layer
PCEP PCEPBGP-LS Telemetry
O-RU
O-RU
O-RU
O-RU
CSR CSRHSRHSR
PCE
PCE
PCE
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             68
Figure 11-2 MPLS packet switched underlay architecture 1
 2
Figure 11-2 illustrates how a large packet switched MPLS transport infrastructure might be 3
designed and how different components interact. It should be noted that not all these components 4
are required, and some serve the same purpose but in different ways.  5
1. Each routing domain has an IGP for internal connectivity. 6
2. Each routing domain has a protocol for distributing label information. It could be SR 7
extension to an IGP, or could be as well legacy protocol like RSVP or LDP. Architecture 8
supporting legacy MPLS transports (LDP and/or RSVP) is especially important in brown-9
field deployments, with legacy MPLS transport already in place. 10
3. Mechanisms to calculate Traffic Engineered paths within a routing domain. Depending on 11
scale, TE paths can be calculated on the transport devices themselves (distributed constrained 12
shortest path first – D-CSPF – calculation), calculation can be off-loaded from less powerful 13
transport devices to more powerful transport devices (on-box PCE), or even dedicated 14
servers could be deployed (off-box PCE). The diagram shows a distributed “Path 15
Computation Element” (PCE) layer with “Path Computation Element Protocol” PCEP 16
running between the edge nodes and the PCE 17
4. Mechanism to establish end-to-end MPLS LSPs between IGP domains. To achieve highly 18
scalable architecture, two mechanism could be considered: Seamless MPLS Architecture, or 19
a controller-based Architecture. Both architectures are described in more details in Section 20
11.5. 21
5. Mechanism to convey topology and network state information from the network to the PCE 22
and other management elements. BGP Link State (BGP-LS) and Telemetry feeds are 23
recommended tools to fulfil this requirement. 24
11.3 Classic MPLS control plane  25
The classic control plane is widely implemented, and designs and requirements are well 26
documented in: 27
 28
• Cisco Press, MPLS and VPN Architectures, Volume 1, 420 pages, by Ivan Pepelnjak, and 29
Jim Guichard, 2001 [170] 30
• Cisco Press, MPLS and VPN Architectures, Volume 2, 470 pages, by Ivan Pepelnjak, Jim 31
Guichard, and Jeff Apcar, 2003 [171] 32
• O’Reilly, MPLS in the SDN Era, 890 pages, by Antonio Sánchez-Monge, and Krzysztof 33
Grzegorz Szarkowicz, 2015 [172] 34
 35
The application of classic MPLS control plane protocols (IGP, LDP, RSVP, BGP) to the mobile 36
transport networks is defined by Broadband Forum (BBF) in following technical reports: 37
 38
• BBF TR-221: Technical Specification for MPLS in Mobile Backhaul Networks, 99 pages, 39
Oct 2011u [173] 40
• BBF TR-221, Amd.1: Technical Specifications for MPLS in Mobile Backhaul Networks, 24 41
pages, Nov 2013 [174] 42
• BBF TR-221, Amd.2: Technical Specifications for MPLS in Mobile Backhaul Networks, 22 43
pages, Sep 2017 [175] 44
11.3.1.1 LDP base requirements 45
If an operator wishes to use LDP MPLS control plane in some routing domain, then the routing 46
equipment in that routing domain will require: 47
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             69
 1
[D2]: SHOULD support constraint-based LSP setup using LDP defined by “Constraint-Based LSP 2
Setup using LDP”, RFC 3212 [49] 3
[R4]: MUST support LDP state machine defined by “LDP State Machine”, RFC 3215 [50] 4
[D3]: SHOULD support graceful restart mechanism for LDP defined by “Graceful Restart 5
Mechanism for Label Distribution Protocol”, RFC 3478 [55] 6
[R5]: MUST support LDP defined by “LDP Specification”, RFC 5036 [72] 7
[D4]: SHOULD support LDP extension for inter-area label switched paths defined by “LDP 8
Extension for Inter-Area Label Switched Paths (LSPs)”, RFC 5283 [74] 9
[D5]: SHOULD support LDP IGP synchronization defined by “LDP IGP Synchronization”, RFC 10
5443 [87] 11
[D6]: SHOULD support LDP capabilities defined by “LDP capabilities”, RFC 5561 [90] 12
11.3.1.2 RSVP base requirements 13
If an operator wishes to use RSVP MPLS control plane in some routing domain, then the routing 14
equipment in that routing domain will require: 15
 16
[R6]: MUST support “Resource ReSerVation Protocol (RSVP)”, RFC 2205 [32], RFC 2209 [33] , 17
RFC 2210 [34] 18
[R7]: MUST support “RSVP Diagnostic Messages”, RFC 2745 [43] 19
 20
[R8]: MUST support “RSVP Refresh Overhead Reduction Extensions”, RFC 2961[42] 21
 22
[D7]: SHOULD support “RSVP Cryptographic Authentication—Updated Message Type Value”, 23
RFC 3097 [47] 24
[R9]: MUST support “RSVP-TE: Extensions to RSVP for LSP Tunnels”, RFC 3209 [48] 25
 26
[D8]: SHOULD support “Signalling Unnumbered Links in Resource ReSerVation Protocol - 27
Traffic Engineering (RSVP-TE)”, RFC 3477 [54]  28
[R10]: MUST support “Traffic Engineering (TE) Extensions of OSPF Version 2”, RFC 3630 [56] 29
 30
[D9]: SHOULD support “Fast Reroute Extensions to RSVP-TE for LSP Tunnels”, RFC 4090 [58] 31
 32
[D10]: SHOULD support “OSPF Extensions in Support of Generalized Multi-Protocol Label 33
Switching (GMPLS)”, RFC 4203 [62] 34
[D11]: SHOULD support “Node-ID Based Resource Reservation Protocol (RSVP) Hello”, RFC 35
4558 [64] 36
 37
[R11]: MUST support “Record Route Object (RRO) Node-Id Sub-Object”, RFC 4561[66] 38
 39
[R12]: MUST support “IS-IS Extensions for Traffic Engineering”, RFC 5305 [80] 40
 41
[D12]: SHOULD support “Traffic Engineering Extensions to OSPF Version 3”, RFC 5329 [83] 42
 43
[R13]: MUST support Encoding of Attributes for MPLS LSP Establishment Using Resource 44
Reservation Protocol Traffic Engineering (RSVP-TE), RFC 5420 [85] 45
[R14]: MUST support “Label Switched Path (LSP) Attribute in the Explicit Route Object (ERO)”, 46
RFC 7570 [101] 47
[D13]: SHOULD support “Techniques to Improve the Scalability of RSVP-TE Deployments”, RFC 48
8370 [112] 49
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             70
[D14]: SHOULD support “Signaling RSVP-TE Tunnels on a Shared MPLS Forwarding Plane”, 1
RFC 8577 [118]  2
[D15]: SHOULD support “Refresh-interval Independent FRR Facility Protection”, draft-ietf-mpls-3
ri-rsvp-frr-07 [135] 4
11.4 SR/MPLS control plane 5
The following section outlines the control plane requirements for SR/MPLS environment. 6
11.4.1 Interior Gateway Protocol (IGP) for SR/MPLS  7
The distribution of labels, rapid convergence and distribution of traffic engineering information and 8
optional calculation of TE optimized forwarding planes in a single SR domain is done by a link-9
state IGP, in the form of either IS-IS or ISPF, with suitable SR enhancements. This document 10
separately outlines the requirements for IS-IS and OSPF with support for TiLFA for fast 11
convergence and optional support for flex-algorithm for the creation of multiple forwarding planes 12
optimised for different criteria and potentially using different topologies.   13
  14
SR/MPLS requires at least one IGP with SR awareness per routing domain or autonomous system.  15
There are two IGPs that support SR/MPLS; ISIS or OSPF. They are both link-state protocols and 16
have similar capabilities but there are operational differences which is beyond the scope of this 17
document.  18
 19
The IGP provides internal connectivity within a routing domain. The size of a routing domain is 20
determined by technical, operational and organizational considerations, such as protocol scalability 21
and spans of control. In large networks, such as a 5G infrastructure, extending from the access to 22
the transport core, it is very common to see multiple autonomous systems, IGPs and segmentation 23
within the IGPs. 24
 25
In an SR/MPLS environment the IGP is responsible for distributing node, prefix and label 26
information to all nodes within the routing domain and for calculating the “Routing Information 27
Base” (RIB) and the Label Forwarding Information Base (LFIB) on each TNE.  Optionally it can 28
provide fast convergence mechanisms and distribute traffic engineering attributes within the domain 29
and build multiple forwarding planes.  Both IS-IS and OSPF have facilities to create a hierarchical 30
routing structure within a routing domain using levels and areas respectively. They also have the 31
facility to bring external routing information into a routing domain and push its routing information 32
into other routing domains using route redistribution.  33
 34
Note: There are other techniques to achieve route re-distribution between routing domains, such as 35
BGP and centralised SDN controllers. 36
 37
Note: When looking at SR/MPLS IGP requirements there are sets of requirements for ISIS and 38
OSPF. Operators need to follow the requirements associated with IGPs they are using within their 39
network infrastructure.   40
11.4.1.1 Topology Independent Loop Free Alternative (TiLFA) 41
In the SR/MPLS underlay design outlined in this document, TiLFA can be used to achieve fast IGP 42
convergence in the event of a network failure.  Topology Independent Loop Free Alternative as 43
defined “Topology Independent Fast Reroute” using Segment Routing (draft ietf-rtgwg-segment-44
routing-ti-lfa [137]) is an IGP capability supported by Segment Routing that provides a local repair 45
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             71
mechanism that achieves 100% coverage within a routing domain against links, nodes and SRLGs 1
failures with convergence times, once failure detection has occurred, H/W dependent but typically 2
less than 50ms. 3
 4
For each destination in the network, TI-LFA pre-installs a backup forwarding entry for each 5
protected destination ready to be activated upon detection of the failure of a link used to reach the 6
destination.  TI-LFA provides protection in the event of any one of the following: single link 7
failure, single node failure, or single SRLG failure.  In link failure mode, the destination is protected 8
assuming the failure of the link.  In node protection mode, the destination is protected assuming that 9
the neighbor connected to the primary link has failed.  In SRLG protecting mode, the destination is 10
protected assuming that a configured set of links sharing fate with the primary link has failed (e.g. a 11
linecard or a set of links sharing a common transmission pipe). The mechanics of the TiLFA in an 12
SR/MPLS environment at failure is shown in Figure 12-2 and upon convergence in Figure 12-3. 13
One of the key advantages of TiLFA over some other fast convergence mechanisms, not well 14
illustrated in the example below, is that the TiLFA backup path is the same as the post convergence 15
path, thus minimising micro-loops that can occur as IGPs converge.    16
  17
 18
Figure 11-3 TiLFA at failure with SR/MPLS 19

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             72
  1
 2
 3
Figure 11-4 Post convergence traffic flow with SR/MPLS   4
11.4.1.2 IGP Flexible-Algorithm 5
In the SR/MPLS underlay design outlined in this document Flexible Algorithm can be used to 6
within an IGP domain to provide an IGP based traffic engineering facility.  Flexible-Algorithm is an 7
IGP based traffic engineering technique that permits multiple forwarding tables to be created in the 8
IGP based on operator programmed criteria. It is a simple, automatic way, in which a packet 9
switched transport network can be traffic engineered, using only the IGP, to create different 10
forwarding planes designed to meet specific operator defined criteria.  11
 12
 Standard ISIS and OSPF use the IGP metric on links and the “Shortest Path First” (SPF) algorithm 13
to calculate the “best” path between an ingress and egress point in the network. For many services 14
this approach is sufficient, however this approach cannot, for example, take into account real-time 15
link latency, utilization, packet loss and whether an ECMP path shares links using the same 16
underlying fibre ducts.  17
 18
 IGP Flex-Algo, defined in draft-ietf-lsr-flex-algo [137] is an IGP based Segment Routing traffic 19
engineering capability, that enables an operator to define their own custom algorithm, based on a 20
wide range of variables including: link latency, packet loss, bandwidth, affinity and shared risk link 21
groups (SRLG), to achieve a specific forwarding aim. This is illustrated in Figure 11-5. 22

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             73
 1
Figure 11-5 Flex-algo example with three topologies  2
 3
This enables an operator, using a single IGP instance, to build multiple forwarding tables within an 4
SR/MPLS underlay network based on different optimization criteria. The base forwarding instance 5
is still based on IGP metrics, but an operator can build additional IGP Traffic engineered 6
forwarding tables based, for example on lowest latency or avoiding certain links or a combination 7
of the two. 8
 9
The requirements to support flex-algorithm are all associated with the IGP requirements and 10
contained in sections 11.4.1.3 and 11.4.1.4 11
11.4.1.3 IS-IS SR/MPLS base requirements  12
If an operator wishes to use IS-IS to support SR/MPLS with TiLFA and optionally flexible 13
Algorithm (flex-algo) then a TNE providing the packet switching function will require: 14
 15
[R15]: MUST support Routing IPv4 with ISIS, as defined in ISO/IEC 10589, RFC 1195 [30], RFC 16
3719 , and/or Routing IPv6 with ISIS, RFC5308 [81] 17
[R16]: MUST support “IS-IS Extensions for Segment Routing”, draft-ietf-isis-segment-routing- 18
extensions-11 [139] 19
[D16]: SHOULD support “Signaling Maximum SID Depth (MSD) Using IS-IS”, RFC 8491  20
 21
[D17]: SHOULD support “IGP Flexible Algorithm”, draft-ietf-lsr-flex-algo-13 [134] 22
 23
[R17]: MUST support IS-IS TE Extensions, RFC5305 [80] and RFC7810 [104]  24
 25
[D18]: SHOULD support “Topology Independent Fast Reroute using Segment Routing”, draft-ietf-26
rtgwg-segment-routing-ti-lfa-04 [137] 27
[D19]: SHOULD support ISIS cryptographic extensions as defined in RFC5304 [79] and RFC5310 28
[82] 29
[D20]: SHOULD support a Policy Control Mechanism in IS-IS Using Administrative Tags as 30
defined in RFC5130 [73] 31
[R18]: MUST support Domain-Wide Prefix Distribution with Two-Level IS-IS as defined in 32
RFC5302 [77]. 33

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             74
[R19]: MUST support Three-Way Handshake for IS-IS Point-to-Point Adjacencies as defined in 1
RFC5303 [78] 2
[D21]: SHOULD support purge originator identification TLV for IS-IS as defined in RFC6232 [92] 3
 4
11.4.1.4 OSPF SR/MPLS basic requirements  5
If an operator wishes to use OSPF (v2 or v3) to support SR/MPLS with TiLFA and optionally 6
flexible Algorithm (flex-algo) then a TNE providing the packet switching function will require: 7
 8
[R20]: MUST support OSPFv2 (support for IPv4 only) as defined in RFC2328 [35], or OSPFv3 9
(IPv4/IPv6 support) as defined in RFC5340 [84] and RFC5838 [91]. 10
[R21]: MUST support “OSPFv3 Extensions for Segment Routing”, RFC8666 [122] 11
[D22]: SHOULD support “Signalling Maximum SID Depth (MSD) Using OSPF,” RFC8476 [115]  12
 13
[D23]: SHOULD support “IGP Flexible Algorithm”, draft-ietf-lsr-flex-algo-13 [134] 14
 15
[R22]: MUST support “OSPF TE Extensions”, as defined in RFC3630 [56] and RFC7471    16
 17
[D24]: SHOULD support Topology Independent Fast Reroute using Segment Routing “draft-ietf-18
rtgwg-segment-routing-ti-lfa-04 [137] 19
[D25]: SHOULD support authentication/confidentiality for OSPFv3, as defined in RFC4552 [64] 20
 21
11.4.2 SR/MPLS Traffic Engineering  22
SR/MPLS supports two forms of SR policies or Traffic Engineering. Both solutions rely on the IGP 23
to gather and convey topological and resource information around the network and optionally to an 24
SR Path Computation Element (SR-PCE) via BGP-LS.  25
11.4.2.1 Segment Routing Traffic Engineering (SR-TE) 26
The SR policy consists of a list of SIDs the packet needs to traverse and is programmed into the 27
packet on the source node. The SID list can consist of a mix of prefix and adjacency SIDs. In SR 28
this form of TE allows a path to be:  29
1. Loosely source routed where some intermediate points, but not all, are specified between the 30
ingress and egress node. Paths between these intermediate points typically use the shortest 31
path, ECMP based routing determined by the IGP. 32
2.  Explicitly source routed where all intermediate points and even links are specified between 33
the ingress and egress nodes. 34
 35
In all cases, topology information and live network status within a routing domain is distributed 36
within a routing domain by the IGP with suitable extensions. In single routing domain environments 37
path computation can be performed either by individual head-end routers or via a centralised “SR 38
Path Computational Element” (SR-PCE). In multi-domain routing environments, then path 39
computation generally occurs on a “centralised SR-PCE” component that has topology information 40
for all domains being traversed. This function can be a standalone entity or be integrated into 41
strategically placed TNEs. This form of traffic engineering can support loose and explicitly routed 42
paths, low-latency paths, bandwidth-guaranteed paths, and disjoint paths. Precise TE capability 43
depends on the capabilities of the SR-PCE component. 44
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             75
11.4.2.2 IGP Flexible Algorithm 1
IGP Flexible-Algorithm is described in section 11.4.1.2. It is an IGP based traffic engineering 2
technique that permits multiple logical topologies to be created in an IGP domain based on operator 3
programmed criteria. When used in a single IGP routing domain the IGP calculates and maintains 4
the path. The ingress node simply needs to address the packet to the MPLS label associated with the 5
flexible algorithm. This removes the need for a head-end or centralised PCE path computation 6
element and also keeps SID list contained in the label stack to a minimum.  7
This works in a single IGP domain but depending on area border router and autonomous system 8
border router capabilities can be supported in multi-domain environments based on redistribution.     9
 10
For flex-algo requirements for SR/MPLS, see section 11.4.1.3  and 11.4.1.4.  11
11.5 Scaling the MPLS infrastructure  12
In order to provide communication between two MPLS TNEs, end-to-end MPLS LSPs must be 13
available between the TNEs. This implies, that unique MPLS label towards the transport network 14
element, which possibly resides in different routing domain, must be available. In highly scaled 15
environment the number of potential next-hops can be high, therefore care must be taken to ensure 16
that resources, especially forwarding plane resources, are utilized in an efficient way, while 17
providing end-to-end MPLS connectivity. 18
 19
While the requirement for having unique label per remote transport network element increases the 20
pressure on scaling, the benefit of unique label is faster detection of remote transport network 21
element failure, resulting from unique underlay prefix/label withdrawal. For architectures, which 22
can use prefix summarization for remote transport network element reachability, like for example 23
SRv6 architecture discussed in Section 12, such capability is missing, since remote transport 24
network element failure does not influence underlay summary prefix state. 25
11.5.1 Seamless MPLS architecture 26
Seamless MPLS (draft-ietf-mpls-seamless-mpls) is applicable to for both LDP and SR based control 27
planes. Seamless SR is based on following architectural aspects: 28
 29
• Transport network is divided into multiple smaller routing domains. Routing domains can be 30
represented by separate BGP autonomous systems, or separate IGP domains (areas). Or, 31
combination of BGP autonomous systems and IGP areas, where BGP autonomous systems 32
are further divided with IGP areas.  33
• The size of each routing domain might vary, but must be chosen in such a way, that even the 34
weakest transport device in given routing domain can participate in intra-domain routing and 35
intra-domain MPLS path (SR, SR-TE, Flex-Algo, RSVP, LDP) establishment without any 36
scaling concerns. 37
• Inter-domain routing and MPLS path establishment is achieved by BGP labelled unicast 38
(BGP LU), running on top of intra-domain protocols. BGP-LU is used for both prefix 39
distribution (PE loopbacks), as well as label distribution (labels allocated for PE loopbacks) 40
 41
This concept is outlined in Figure 11-6. 42
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             76
 1
Figure 11-6 Seamless MPLS architecture 2
 3
BGP-LU operation can be further optimized for enhanced scaling: 4
 5
• BGP-LU prefixes (loopbacks of remote TNE) are not installed in the forwarding table (FIB) 6
– they are just installed in the routing table (RIB), which consumes only control plane 7
resources. On typical transport device, control plane resources scale better than forwarding 8
plane resources 9
• Service NLRIs (L3VPN, EVPN) are automatically filtered on route reflectors (not shown in 10
Figure 11-6, for diagram simplicity), and distributed only to the TNEs requiring specific 11
service prefixes. This is achieved with constrained route distribution. 12
• Only these automatically filtered service NLRIs are installed in the forwarding plane, using 13
the BGP-LU transport label of the required BGP-LU prefix (remote TNE loopback). 14
Therefore, usage of forwarding plane resources is highly minimized. 15
• Further optimization of control plane resources is possible, where TNE (PE) requests from 16
route reflector only limited set of BGP-LU prefixes. This can be achieved with outbound 17
route filtering (ORF) of BGP-LU prefixes with ORF filter automatically generated to allow 18
only BGP-LU prefixes required to resolve protocol next-hops of accepted (filtered with 19
constrained route distribution mechanism) service NLRIs. 20
 21
With Seamless MPLS architecture, ingress PE uses recursive next-hop resolution, where service 22
NLRI received from remote PE is resolved over BGP protocol next-hop (BGP-LU loopback of 23
remote PE), which in turn is resolved over local intra-domain MPLS tunnel (SR/SR-TE or legacy 24
RSVP/LDP). 25
 26
If an operator wishes to use Seamless MPLS architecture for enhanced scaling of an MPLS 27
underlay, then the transport device will require: 28
 29
[R23]: MUST support “Seamless MPLS architecture”, draft-ietf-mpls-seamless-mpls (note: this is 30
an expired IETF draft but is a widely referenced document that describes the architecture)  31
[R24]: MUST support BGP Labelled Unicast (BGP-LU), as defined in RFC3107/RFC8277 32
[52][109] 33
[D26]: SHOULD support Constrained Route Distribution for BGP/MPLS virtual private networks, 34
RFC4684 [69] 35
AS X AS Y
ISIS + BFD ISIS + BFD ISIS + BFD ISIS + BFD
ISIS L1 ISIS L2
ISIS + BFD ISIS + BFD
ACCESS PRE-AGGREGATION AGGREGATION CORE EDGE
BGP-LU BGP-LU BGP-LU BGP-LU
NHSNHS NHSNHS NHSNHS NHSNHS
LDP
 SR/ SR-TE
 RSVP
Inter-domain (end-to-
end) colored BGP-CT
tunnels tunneled inside
intra-domain tunnel in
each domain
Intra-domain tunnels
(LDP, RSVP, SR/ SR-TE)
no leaking
BGP-LU distributes prefixes
(router’s loopbacks) together with
their labels, across all domains,
with following scaling
optimization:
§ BGP-LU not installed in the
forwarding plane (just in the
control plane) à typical
transport node control plane
scales few times (10x) better
than forwarding plane
§ BGP service prefixes (L3VPN,
EVPN) use automatic
constrained route distribution
(RFC 4684) à only required
BGP service prefixes delivered
§ Optionally: Outbound Route
Filtering (ORF, RFC 5292)
generated automatically from
the next-hops of received BGP
service prefixes, further
decreases the scaling pressure
66
77 77 99 71
V V V VV
SR-TE stack: 24, 26, 29LDP: 66 à Impl. Null RSVP: 71 à Impl. Null
3998877
24
26
29
88
26
29
88
29
88 88
V V V V
O-RU
O-RU
O-RU
O-RU
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             77
[R25]: MAY support Outbound Route Filter (ORF) for BGP, RFC5291/RFC5292 [75][76] 1
 2
In scenarios where seamless MPLS is operating in heterogeneous environments where some 3
domains are LDP based and some are SR based, then a Segment Routing Mapping Server can be 4
used to provide an interworking function between the two environments.  5
 6
[D27]: SHOULD support Segment Routing MPLS Interworking with LDP, RFC8661 [120] 7
 8
11.5.2 Controller based network scaling architectures  9
In addition to the “Seamless MPLS” approach which is an infrastructure-based scaling technique, 10
an external controller in the form of a distributed “Path Computational Element” (PCE) could be 11
used for scaling across multiple MPLS domains. The approach aims to simplify the underlying 12
infrastructure by reducing interactions between domains at the infrastructure level and delegating 13
cross domain path determination to a distributed PCE component.   14
 15
Figure 11-7 shows the general architecture with a distributed PCE layer that gathers IGP topology 16
information from their respective domain. PCE’s can exchange topology information with each 17
other using BGP-LS. In scenarios where the headend is not able to resolve the next-hop locally, it 18
may rely on PCE to provide an End-to-End Path when requested. The headend communicates with 19
the PCE using Path Computation Element Protocol (PCEP). This mechanism can helps save 20
resources on CSR’s contributing to overall Xhaul Network scale. 21
 22
 23
Figure 11-7 General concept of a controller-based architecture 24
 25
The figure above shows network simplification and scalability by:  26
• Creating a multi domain topology, each with its own PCE Controller 27
• Removing extensive Route leaking and filtering requirements between IGP domains.  28
• Removing the need for the headend to populate its routing table with topology and routing 29
information.  30
• Use Passive Stateful PCE Path Computation Request/Response as per RFC 8231 to create 31
an end to end path.  32
  33
Each domain may have its own path consideration and constraints that can independently be 34
implemented and communicated to the PCE/PCEs. PCEs can calculate end to end path based on 35
these constraints and communicated to the headend TNE via PCEP as shown in Figure 11-8.  36
 37

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             78
 1
 2
Figure 11-8 Path with Domain specific constraints using PCE based architecture 3
 4
The SID list provided by the PCE to the headend is in actuality a label stack. Depending on the 5
network size and the use-case, the SID list may exceed the platform’s label stack capacity. In those 6
cases, the SID list across domains could be further optimized by using techniques offered by one of 7
the following two architectures.  8
 9
• Using Forwarding Adjacency (FA) between domain boundaries, with associated Adjacency 10
SID, abstracting the path between domain boundaries 11
• Using SR-TE policies between domain boundaries, with associated Binding SID, abstracting 12
the path between domain boundaries 13
 14
Both these approaches are optional architectures that could be used to further optimize the size of 15
the label stack in controller-based solutions.  16
 17
While the PCE is shown as a separate entity in the figure above, an operator may choose to either 18
have PCE functionality integrated into the infrastructure router(s) or use a dedicated device or 19
devices for PCE. As long as the PCE’s in the distributed PCE layer can exchange topology 20
information and communicate with their respective PCEP clients, the positioning of PCE 21
functionality can be a choice an operator makes based on their respective network environment.  22
 23
Following are the requirements that should be met for implementing a controller-based architecture. 24
These additions are in addition to baseline Segment Routing and Segment Routing Traffic 25
Engineering requirements already mentioned:  26
 27
[R26]: MUST support BGP Link State (BGP-LS), RFC7752 [102] 28
 29
[R27]: MUST Support BGP-LS Extensions for SR as per draft-ietf-idr-bgp-ls-segment-routing-ext-30
16 [133] 31
 32
[D28]: SHOULD support "Signaling Maximum SID Depth using Border Gateway Protocol Link-33
State", RFC8814 [125] 34
 35
[R28]: MUST support SRTE Policy configuration as outlined in draft-ietf-spring-segment-routing-36
policy-08 [142] 37
[R29]: MUST support "Path Computation Element Protocol (PCEP)", RFC5440 [86]  38

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             79
 1
[R30]: MUST support PCEP for communication between CSR and PCE as per “Path Computation 2
Element Communication Protocol (PCEP) Extensions for Segment Routing”, RFC-8664  3
[121]  4
[D29]: SHOULD support PCEP extension to support Segment Routing Policy Candidate Paths as 5
per draft-ietf-pce-segment-routing-policy-cp-00 [141] 6
[D30]: SHOULD support “PCEP Extensions for Stateful PCE”, RFC8231 7
 8
[D31]: SHOULD support “BGP – Link State (BGP-LS) Advertisement of IGP Traffic Engineering 9
Performance Metric Extensions”, RFC8571 [117] 10
 11
11.5.2.1 Forwarding Adjacency architecture for cross domain scale 12
Forwarding Adjacency abstracting the path between domain boundary routers is one of the ways to 13
optimize and scale a controller-based Xhaul architecture.  Forwarding Adjacency architecture is 14
based on following architectural concepts: 15
 16
• Transport network is divided into multiple smaller routing domains. Routing domains can 17
be represented by separate BGP autonomous systems, or separate IGP domains (areas). 18
Or, combination of BGP autonomous systems and IGP areas, where BGP autonomous 19
systems are further divided with IGP areas.  20
• The size of each routing domain might vary, but must be chosen in such a way, that even 21
the weakest transport device in given routing domain can participate in intra-domain 22
routing and intra-domain MPLS path (SR, SR-TE, Flex-Algo, RSVP, LDP) establishment 23
without any scaling concerns 24
• Head-end router (border router) of intra-domain LSP advertises the intra-domain LSP (of 25
any kind, i.e. SR, SR-TE, Flex-Algo, RSVP, LDP) as standard link in BGP Link State 26
(LS) topology database, with attributes similar to standard link, like for example SR 27
Adjacency-SID, administrative group (link color, link affinity), SRLG, etc. IGP 28
forwarding adjacency is established (RFC 4206) between domain border routers creating 29
abstracted topology of the routing domain. 30
•  BGP-LS (RFC 8571) is used to distribute abstracted topology. Given the fact that only 31
border routers, and only abstracted forwarding adjacency links between these border 32
routers are exported, the size of the topology database exported via BGP-LS is 33
considerably smaller. BGP-LS distribution can happen in two way, depending on the 34
overall network scale 35
o BGP-LS topology is exchanged between routers in different routing domains 36
o BGP-LS topology is exchanged between PCEs in distributed PCE layer 37
 38
The first approach is more suitable for small to medium scale deployments. In this approach, any 39
transport network element has the detailed topology visibility of the local routing domain, and 40
abstracted (with Forwarding Adjacencies advertised as links in BGP-LS) visibility of the remote 41
routing domains. This significantly reduces the topology database stored on the transport network 42
elements, therefore, in small/medium deployments PCE might not be necessary. 43
 44
The headend PE, based on the full topology database (detailed topology from local routing domain 45
+ topology abstracted with links representing forwarding adjacencies from remote routing 46
domains), performs calculation of end-to-end SR-TE paths (local constrained shortest path first – 47
CSPF – calculation). Each forwarding adjacency is represented in the resulting SR-TE label stack as 48
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             80
single Adjacency-SID associated with given forwarding adjacency, regardless of the underlying 1
MPLS transport used for given forwarding adjacency. At each border node, this Adjacency SID is 2
mapped to the actual transport label stack (LDP, RSVP, SR, SR-TE) used for the given forwarding 3
adjacency. 4
 5
This concept is outlined in Figure 11-9. 6
 7
Figure 11-9 Forwarding Adjacency architecture 8
 9
As further scaling optimization of this architecture, ingress PEs (typically cell site routers, with 10
limited control plane resources required for efficient end-to-end path computation) can be off-11
loaded from end-to-end path computation. Distributed PCEs (Path Computation Elements) can be 12
placed on more powerful transport devices (for example pre-aggregation or aggregation routers), or 13
containerized/virtualized PCE function can be placed in the distributed telco cloud environment to 14
execute end-to-end path computation duties over simplified (via forwarding adjacencies) full 15
network topology. In this scenario, ingress PE requests from PCE on demand path computation for 16
unresolved next-hops, as outlined in Figure 11-10. 17
 18
 19
Figure 11-10 Forwarding Adjacency architecture with distributed PCE layer 20

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             81
 1
If an operator wishes to use a forwarding adjacency based architecture for enhanced scaling of 2
MPLS underlay, then the transport device will require: 3
 4
[R31]: MUST support “forwarding adjacency links”, RFC4206 [110] 5
 6
11.5.2.2 Using Binding SID (BSID) for cross domain scale  7
As mentioned earlier, a controller-based architecture make use of the PCE to provides the SID list 8
to the headend for an end to end path across multiple domains. If no constraints are imposed on the 9
traffic, the SID list would consist of Area Border Routers Prefix SIDs as next hop. Traffic within 10
the domain will use IGP to get to the next hop while utilizing well established IGP routing 11
mechanism such as Equal Cost Multi Path (ECMP), metric based best path etc. At the ABR, the top 12
most SID is popped and the new next hop SID/Label is exposed, starting the routing within the next 13
IGP are or domain. The process is repeated until traffic reaches its destination. Figure 11-11 shows 14
such behaviour between IGP Domain 0,1 and 2.  15
 16
 17
 18
 19
 20
 21
 22
Figure 11-11 Basic SID list with no constraints  23
 24
There may be scenarios where more explicit path considerations and constraints could be required, 25
such as using low latency path or inclusion/exclusion of certain links or node. In those scenarios, 26
rather than provide a SID list that encompasses every link/node through the path, mechanisms such 27
as Flex Algo could be used to optimize the SID list on the head end. Figure 11-12 shows a scenario 28
with latency and link/node constraints across the path. In this case operator may use Flex-Algo 29
within a domain and use the PCE controller to provide SID list to the headend accordingly. Notice 30
while the SID depth is no different from the earlier scenario Figure 11-12) and while it still only 31
contain prefix-SIDs of ABR as next hop label, the SID’s now refer to the node’s Flex-Algo Prefix 32
SID, instead of default algo SID. Due to this, the traffic behavior is also different and the next Hop 33
SID automatically uses the desired constraints-based path.  34
 35

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             82
 1
 2
Figure 11-12 Constraint based SID list with Flex-Algorithm   3
 4
In scenarios where an operator is unable or unwilling to use Flex-Algo for SID list optimization, an 5
SRTE Binding SID (BSID) could be used. A Binding SID is a fundamental component of Segment 6
Routing network that may be used to provide SID and network scaling. A BSID is a SID that is 7
associated with an SRTE policy’s active path and is a representation of an End-to-End candidate 8
path. BSIDs could be used to provide additional scale by using nested SRTE policies across 9
domains. The PCE instead of providing a full path through multiple domains, may provide the 10
headend with a path within its own domain and a BSID corresponding to the next domain. When, 11
exposed at the domain boundaries, the BSID gets translated into another set of SID List, providing 12
extensibility and scalability throughout the topology. This process is shown in Figure 11-13. 13
 14
 15
 16
 17
Figure 11-13 SID scaling using binding SID  18
 19
Figure 11-13 compares the two scenarios where an End to End path is required with latency and 20
link/nodes constraints along the path. The first scenario is where the PCE provides a full SID list to 21
the headend for the entire path, with based path is required end to end. The second scenario uses a 22
Binding, exposed at domain boundaries and then use the appropriate constrains based SID within 23
the domain in question. 24
 25

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             83
If an operator wishes to implement controller based architectural scaling, in addition to underlying 1
segment-routing based transport with controllers, a device will need to support:  2
 3
[R32]: SHOULD support Binding SID as per draft-ietf-pce-binding-label-sid-03 [138] 4
11.6 MPLS Quality of Service 5
The MPLS data plane includes a 3-bit field in the MPLS header, the TC – Traffic Class (previously 6
EXP - Experimental) bits. These bits are assignable as a marker for QoS mechanisms in transport 7
nodes to use as a classification and marking tool. The per-hop behavior for MPLS forwarding 8
elements can be defined based on these markings. Because of the restricted size of the TC field, the 9
specific markings are not standardized, but are open for individual operators to define. For a full 10
discussion of a proposed marking scheme and QoS architecture for TNEs in Xhaul, see Chapter 0. 11
11.7 MPLS OAM 12
Basic OAM tools are Ping and Traceroute. Ping is required to test liveliness to a remote MPLS 13
underlay destination and traceroute is required to trace paths and perform hop-by-hop fault isolation 14
to remote MPLS underlay destination. The following capabilities are required on all TNEs. 15
 16
[R33]: MUST support “Detection of MPLS Data Plane Failures”, RFC8029 [105]  17
 18
[R34]: MUST support LSP Ping/Traceroute for SR IGP Prefix-SID and IGP Adjacency-SID with 19
MPLS Data Plane, as defined in RFC8287 [110] 20
 21
11.8 IP/MPLS service infrastructure 22
For IPv4, IPv6 and Ethernet services an overlay solution is used. Please refer to section 0 for a 23
description of overlay service recommendations for a 5G Xhaul infrastructure.   24
  25
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             84
12 Packet-switched underlay network – SRv6 based 1
This document presents two packet switched underlay technologies, the first based on MPLS 2
contained in section 11 and the second based on SRv6 contained in this section (12). An operator 3
wishing to implement the transport architecture outlined in this document will need to select one of 4
the two and implement the requirements outlined in the associated section. It should be noted that 5
other transport architectures are potentially available but not covered in this revision of the 6
document. This section outlines a packet switched underlay model based on SRv6.   7
 8
SRv6 is based on the segment routing architecture as defined in RFC8402 [114].  For more 9
information on Segment Routing see Annex A. In an SRv6 infrastructure, like an MPLS 10
environment, it is important to consider the underlay / fabric of the transport infrastructure 11
somewhat separately from the services that run on-top of the infrastructure. The emphasis with the 12
underlay / fabric is to provide an environment that will scale and support the services required by a 13
5G infrastructure. In contrast, the services infrastructure runs on-top of the underlay/fabric of the 14
transport network and supports the different components of the 5G infrastructure (Fronthaul, 15
Midhaul, Backhaul).   16
 This chapter is specific to an SRv6 underlay/fabric, its data plane, control plane and how to scale it. 17
Although there are some similarities with an SR-MPLS environment, in terms of architecture, there 18
are some key differences in requirements and particularly on how to scale the underlay 19
infrastructure, which is critical in a 5G environment. 20
 21
 The service infrastructure and how to support 5G service requirements for both SR-MPLS and 22
SRv6 are covered in a common services section (section 0) as the technologies used and the designs 23
have many similarities.   24
12.1 SRv6 data plane  25
SRv6 relies on an IPv6 data plane with segments defined using SIDs contained in the IPv6 header. 26
SRv6 SIDs are specified in “Segment Routing Header” (SRH) and standardised in RFC8754 [124]. 27
Packet switching TNEs supporting SRv6 will need to: 28
 29
[R35]: MUST support “IPv6 Segment Routing Header (SRH)” RFC8754 [124] 30
 31
[R36]: MUST support “SRv6 Network Programming”, draft-ietf-spring-srv6-network-32
programming-24 [143] 33
 34
[D32]: SHOULD support “Yang Data Model for SRv6 Base and static”, draft-ietf-spring-srv6-yang-35
00 [144]. 36
 37
12.2 SRv6 control plane  38
The SRv6 control plane refers to routing and path control within the SRv6 underlay/fabric. This 39
infrastructure can extend to customer devices or stop at a Provider Edge (PE) function. Its role can 40
be summarised as learning the topology, calculation and implementation of dynamic and explicit 41
routes across the SRv6 infrastructure and providing rapid protection and repair mechanisms. 42
In a packet switched transport network consisting of a large number of routers, a mix of WAN and 43
data centers components, the need to support traffic engineering, the underlay control plane is a 44
typically a collection of independent routing domains that interact in a collaborate fashion.  45
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             85
 1
 2
Figure 12-1 SRv6 underlay architecture 3
Figure 12-1 illustrates how a large packet-switched transport infrastructure might be designed and 4
how different components interact. It should be noted that not all these components are required, 5
and some serve the same purpose but in different ways.  6
1. Each routing domain has an IGP for internal connectivity. 7
2. Mechanisms to calculate SR policies or Traffic Engineer paths and convey them to the 8
source nodes. Figure 12-1 shows a distributed “Path Computation Element” (PCE), with 9
“Path Computation Element Protocol” (PCEP) running between the edge nodes and the PCE 10
but there are other network-based mechanisms to calculate SR policies such as Flex-algo 11
running in the IGP or head-end calculated paths.   12
3. Mechanisms to convey routing between IGP domains. This could be provided by 13
summarization and redistribution, E-BGP between AS boundaries or via a multi-domain 14
“Path Computation Element” (PCE). One of the key differences between an MPLS and IPv6 15
based underlay is that IPv6 enables address summarization within routing domains and also 16
between routing domains. This makes an IPv6 based underlay network more scalable and 17
less complex than a corresponding MPLS solution. 18
4. Mechanisms to convey topology and network state information from the network to the PCE 19
and other central management elements. BGP Link State (BGP-LS) and Telemetry feeds are 20
recommended tools to fulfil this requirement. 21
12.2.1 Interior Gateway Protocol (IGP) for SRv6 22
SRv6 requires at least one IPv6 IGP per routing domain or autonomous system.  There are two IGPs 23
that support SRv6; ISIS for IPv6 or OSPFv3. They are both link-state protocols and have similar 24
capabilities but there are operational differences which is beyond the scope of this document.  25
 26
The IGP provides internal connectivity within a routing domain. The size of a routing domain is 27
determined by technical, operational and organizational considerations, such as protocol scalability 28
and spans of control. In large networks, such as a 5G infrastructure, extending from the access to 29
the transport core, it is very common to see multiple autonomous systems, IGPs and segmentation 30
within the IGPs. 31
 32
In an SRv6 environment the IGP is responsible for distributing node, prefix and SID information to 33
all nodes within the routing domain and for calculating the forward tables between these nodes. 34

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             86
Optionally it can provide fast convergence mechanisms and distribute traffic engineering attributes 1
within the domain and build multiple forwarding planes.  Both IS-IS for IPv6 and OSPFv3 have 2
facilities to create a hierarchical routing structure within a routing domain using levels and areas 3
respectively. They also have the facility to bring external routing information into a routing domain 4
and push routing information into other routing domains using route redistribution.  5
 6
Note: There are other techniques to achieve route re-distribution between routing domains, such as 7
BGP and centralised SDN controllers. 8
Note: When looking at SRv6 IGP requirements, there are sets of requirements for ISIS for IPv6 and 9
OSPFv3. Operators need to follow the requirements associated with IGPs they are using within 10
their network infrastructure.   11
12.2.1.1 Topology Independent Loop Free Alternative (TiLFA) 12
In the SRv6 underlay design outlined in this document TiLFA can be used to achieve fast IGP 13
convergence in the event of a network failure.  Topology Independent Loop Free Alternative as 14
defined “Topology Independent Fast Reroute” using Segment Routing [137] is an IGP capability 15
supported by Segment Routing that provides a local repair mechanism that achieves 100% coverage 16
within a routing domain against links, nodes and SRLGs failures with convergence times, once 17
failure detection has occurred, H/W dependent but typically less than 50ms. 18
 19
For each destination in the network, TI-LFA pre-installs a backup forwarding entry for each 20
protected destination ready to be activated upon detection of the failure of a link used to reach the 21
destination.  TI-LFA provides protection in the event of any one of the following: single link 22
failure, single node failure, or single SRLG failure.  In link failure mode, the destination is protected 23
assuming the failure of the link.  In node protection mode, the destination is protected assuming that 24
the neighbor connected to the primary link has failed.  In SRLG protecting mode, the destination is 25
protected assuming that a configured set of links sharing fate with the primary link has failed (e.g. a 26
linecard or a set of links sharing a common transmission pipe). The mechanics of the TiLFA for 27
SRv6 at failure is shown in Figure 12-2 and upon convergence in Figure 12-3. One of the key 28
advantages of TiLFA over some other fast convergence mechanisms, not well illustrated in the 29
example below, is that the TiLFA backup path is the same as the post convergence path, thus 30
minimising micro-loops that can occur as IGPs converge.    31
 32
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             87
 1
 2
Figure 12-2 TiLFA at failure with SRv6 3
 4
 5
 6
Figure 12-3 Post convergence traffic flow with SRv6   7
12.2.1.2 IGP Flexible-Algorithm 8
IGP Flexible-Algorithm is an IGP based traffic engineering technique that permits multiple 9
forwarding tables to be created in the IGP based on operator programmed criteria. It is a simple, 10
automatic way, in which a packet switched transport network can be traffic engineered, using only 11
the IGP, to create different forwarding planes designed to meet specific operator defined criteria.  12

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             88
 Standard ISIS for IPv6 and OSPFv3 use the IGP metric on links and the “Shortest Path First” (SPF) 1
algorithm to calculate the “best” path between an ingress and egress point in the network. For many 2
services this approach is sufficient, however this approach cannot, for example, take into account 3
real-time link latency, utilization, packet loss and whether an ECMP path shares links using the 4
same underlying fibre ducts.  5
 6
 IGP Flex-Algo, defined in draft-ietf-lsr-flex-algo-13 [137] is an IGP based Segment Routing traffic 7
engineering capability, that enables an operator to define their own custom algorithm, based on a 8
wide range of variables including: link latency, packet loss, bandwidth, affinity and shared risk link 9
groups (SRLG), to achieve a specific forwarding aim. This is illustrated in Figure 12-4. 10
 11
Figure 12-4 Flex-algo example with three topologies  12
 13
This enables an operator, using a single IGP instance, to build multiple forwarding tables within an 14
SRv6 underlay network based on different optimization criteria. The default forwarding instance 15
uses shortest path forwarding based on IGP metrics, but an operator can build additional IGP 16
Traffic Engineered forwarding tables based, for example on lowest latency or avoiding certain links 17
or a combination of the two. 18
 19
The requirements to support flex-algorithm are all associated with the IGP requirements and 20
contained in sections 12.2.1.3 and 12.2.1.4. 21
 22
12.2.1.3 ISIS for IPv6 base requirements  23
If an operator wishes to use ISIS for IPv6 with TiLFA and optionally Flexible Algorithm (flex-algo) 24
with SRv6 then the TNE will require: 25
 26
[R37]: MUST support routing IPv6 with ISIS as defined in RFC5308 [81]. 27
 28
[R38]: MUST support “IS-IS Extensions to Support Routing over IPv6 Dataplane”, draft-ietf-lsr-29
isis-srv6-extensions-11 [139] 30
[R39]: MUST support “Topology Independent Fast Reroute using Segment Routing”, draft-ietf-31
rtgwg-segment-routing-ti-lfa-04” [137] 32

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             89
[R40]: MUST support IS-IS TE Metric Extensions, as defined in RFC5305 [80] and RFC7810 1
[104]. 2
 3
[D33]: SHOULD support “IGP Flexible Algorithm”, draft-ietf-lsr-flex-algo-13 [134]. 4
 5
12.2.1.4 OSPFv3 basic requirements  6
If an operator wishes to use OSPFv3 with TiLFA and optionally flexible Algorithm (flex-algo) with 7
SRv6 then the TNE will require: 8
 9
[R41]: MUST support OSPFv3, RFC5340 [84]. 10
 11
[R42]: MUST support “OSPFv3 Extensions for SRv6”, draft-ietf-lsr-ospfv3-srv6-extensions-01 12
[145] 13
[R43]: MUST support “Topology Independent Fast Reroute using Segment Routing”, draft-ietf-14
rtgwg-segment-routing-ti-lfa-04 [137]. 15
[R44]: MUST support OSPFv3 TE Metric Extensions, as defined in RFC5329 [83]. 16
 17
[D34]: SHOULD support “IGP Flexible Algorithm”, draft-ietf-lsr-flex-algo-13 [134]  18
  19
12.2.2 SRv6 Traffic Engineering 20
SRv6 supports two forms of SR policies or SR Traffic Engineering. Both solutions rely on the IGP 21
to gather and convey topological and resource information around the network and optionally to a 22
Path Computation Element (PCE) optimized for SR via BGP-LS.  23
12.2.2.1 Segment Routing Traffic Engineering (SR-TE) 24
The SR policy consists of a list of SIDs the packet needs to traverse and is programmed into the 25
packet on the source node. The SID list can consist of a mix of prefix and adjacency SIDs. In SR 26
this form of TE allows a path to be:  27
1. Loosely source routed where some intermediate points, but not all, are specified between the 28
ingress and egress node. Paths between these intermediate points typically use the shortest 29
path, ECMP based routing determined by the IGP. 30
2. Explicitly source routed where all intermediate points and even links are specified between 31
the ingress and egress nodes. 32
 33
In all cases, topology information and live network status within a routing domain is distributed 34
within a routing domain by the IGP with suitable extensions. In single routing domain environments 35
path computation can be performed either by individual head-end routers or via a centralised “SR 36
Path Computational Element” (SR-PCE). In multi-domain routing environments, then path 37
computation generally occurs on a “centralised SR-PCE” component that has topology information 38
for all domains being traversed. This function can be a standalone entity or be integrated into 39
strategically placed TNEs. This form of traffic engineering can support loose and explicitly routed 40
paths, low-latency paths, bandwidth-guaranteed paths, and disjoint paths. Precise TE capability 41
depends on the capabilities of the SR-PCE component. 42
 43
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             90
12.2.2.2 IGP Flexible Algorithm 1
IGP Flexible-Algorithm is described in section 12.2.1.2. It is an IGP based traffic engineering 2
technique that permits multiple logical topologies to be created in an IGP domain based on operator 3
programmed criteria. When used in a single IGP routing domain the IGP calculates and maintains 4
the path. The ingress node simply needs to address the packet to the “locator-block” associated with 5
the flexible algorithm. This removes the need for a head-end or centralised PCE path computation 6
element and also keeps SID list to a minimum.  7
 8
This works in a single IGP domain but depending on area border router and autonomous system 9
border router capabilities can be supported in multi-domain environments based on redistribution.     10
For flex-algo requirements in an SRv6 environment see section 12.2.1.3 and 12.2.1.4.  11
12.2.3 Inter-domain connectivity 12
In a large packet-switched transport network the underlay infrastructure will need to be sub-divided 13
into multiple routing domains. These routing domains can be within an autonomous system, at the 14
IGP level, or between autonomous systems.  One motivation is IGP scalability, but other reasons 15
include fault isolation between domains and organizational, for example different groups run the 16
DC and WAN infrastructures. However, even though the infrastructure is divided, connectivity 17
between routers in different routing domains is still required to build end to end services, therefore 18
the underlay infrastructure needs mechanisms to enable inter-domain connectivity and services to 19
be built inter-domain. In an SRv6 environment, potential mechanisms for interchanging routing 20
between domains include: 21
 22
1. IGP based hierarchy 23
2. Inter-domain IGP redistribution 24
3. Inter AS BGP   25
4. Controller based  26
 27
12.2.3.1 IGP based hierarchy 28
Both ISIS for IPv6 and OSPFv3 have the concept of routing hierarchy in their basic designs. In an 29
SRv6 environment, route summarization or injection of a default route between IS-IS levels or 30
OSPFv3 areas is sufficient to enable EVPN or L3VPNs services to be built over an SRv6 31
hierarchical IGP infrastructure.  32
 33
12.2.3.2 Inter-domain IGP redistribution  34
One or more routers are located on the boundary between two or more IGP routing domains. These 35
routers run multiple IGP instances, so have routing awareness of all the IGPs they participate in. 36
Routing information is mutually re-distributed between the IGP protocols allowing reachability 37
between the domains. This redistribution can be in the form of full routing, summarised routes or 38
even default routes. The choice is dependent on the level of awareness required between domains.   39
 40
 See Figure 12-5 for more detail. IGP route re-distribution in both IS-IS for IPv6 and OSPFv3 is a 41
common capability.   42
 43
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             91
12.2.3.3 Inter-AS BGP based routing  1
A router in each autonomous system exchanges routing information using BGP-4 supporting the 2
IPv6 address family. It should be noted that this scheme provides better separation than inter-3
domain IGP redistribution but typically requires separate routers in each autonomous system.  4
See Figure 12-6 for more detail.   5
 6
12.2.3.4 Controller based path calculation    7
In this scenario no routing information is exchanged between routing domains at the device level. 8
Instead each routing domain provides its topology to an SR Path Computation Element (SR-PCE), 9
which is a central SDN control element responsible for determining and finding routes to a 10
destination node on behalf of a source node. Although the SR-PCE is logically a central component, 11
for scale and resiliency reasons it can consist of multiple individual entities distributed around and 12
serving different parts of the network. In this case the source TNE requires very limited visibility of 13
the overall network, as end to end cross-domain path computation is delegated to the SR-PCE.  14
 A SR-PCE function in an SRv6 underlay transport network has three main functions. 15
1. Gather data about the topology of the overall SRv6 network underlay. Several options exist: 16
a. The SR-PCE participates in the IGP domains and directly collects each domains IGP 17
link-state database. 18
b. Use BGP Link-State (BGP-LS) with appropriate extensions for SRv6 to collect the 19
IGP’s link-state database from each routing domain. This is done through one or 20
more BGP session between the SR-PCE and a TNE in each routing domain 21
participating in the domain’s IGP. The TNEs retrieve information from the IGP 22
LSDB and distribute it to the controller using the BGP link-state address family.  23
2. Communications between the PCE and PCC (path computation client) which is the headend 24
TNE. In this document it is assumed to PCEP (path computation Element Protocol) is used as 25
the communication mechanism between the source TNE and the SR-PCE. 26
3. Path computation. The PCE uses information gathered from the various domains to compute 27
an SR policy consisting of a SID list which packets using the SR policy will traverse. This 28
SID list can be a loosely sourced routed or explicitly source routed.    29
 30
12.3 Scaling an SRv6 underlay infrastructure 31
Scaling is one of the key challenges in building a packet switched infrastructure supporting 5G 32
services. Communications and support of L2 and L3 VPN services between two SRv6 TNEs relies 33
entirely on IP routing mechanisms on the underlay transport fabric. In SRv6 this is based on longest 34
IP prefix matching and forwarding, so TNEs can support routing and forwarding based on full IPv6 35
host routes, summarized IPv6 routes or default routing or a combination, derived from dynamic 36
routing protocols or statically defined. Further, different TNEs in the end to end path can use 37
different levels of route summarization appropriate to their position in the network and their 38
computation, memory and NPU/ASIC resources. 39
 40
12.3.1 Route summarization and redistribution 41
Summarization within ISIS for IPv6 and OSPFv3 is part of the base protocol capability and can 42
occur between levels in IS-IS for IPv6 and between areas in OSPFv3 using “Area Border Routers” 43
(ABRs). Summarization is also a common capability when redistributing routes between different 44
routing domains and is also supported between Autonomous Systems using BGP-4 between 45
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             92
“Autonomous System Border Routers” (ASBRs).  This is illustrated in Figure 12-5 and Figure 12-6 1
and shows how PE1-4’s routing tables can be controlled through route summarization.  2
 3
 4
Figure 12-5 Summarization / redistribution using IGPs 5
 6
 7
Figure 12-6 Summarization / redistribution using BGP-4 8
 9
This approach offers considerable flexibility, as operators can choose the level of summarization, 10
ranging from no summarization to default routing between domains depending on the size of the 11
network and level of awareness required between domains. It can also offer inter-domain traffic 12
engineering support based on flexible algorithm within an IGP domain. However, there are some 13
considerations and limitations:   14
1. Address planning is critical and needs to occur upfront.  15
2. Route summarization might result in sub-optimal routing in certain designs. 16
3. Some traffic engineering scenarios cannot be achieved with route summarization alone.  17

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             93
4. A source TNE cannot rely on a host route withdrawal to detect a remote TNE failure covered 1
by a summary route. In this situation over forms of device or service verification are 2
required.    3
 4
[D35]: ABRs SHOULD have flex-algorithm aware summarization/redistribution function. This is a 5
local behavior. 6
 7
To use BGP-4 to transmit underlay SRv6 routing between Autonomous Systems requires:  8
 9
[R45]: BGP with IPv6 multi-protocol extensions (RFC1771, RFC2283, RFC2545) [31][36][39] 10
 11
12.3.2 Controller based scaling 12
Route summarization and redistribution is simple and allows massive scale and allows end to end 13
traffic engineering based on flex-algo. However, some traffic engineering scenarios cannot be 14
achieved, such as path diversity and bandwidth optimization, because summarization hides 15
topological details.  16
 17
To achieve more complex TE solutions, end-to-end visibility of the network is required, and a 18
controller-based solution based on an SR-PCE is used.  The SR-PCE receives detailed topology 19
information via BGP-LS protocol from each domain. The SR-PCE then has a full picture of end to 20
end reachability. When an ingress PE need to establish an end to end path it requests information 21
from SR-PCE via PCEP protocol. SR-PCE responds with stack of SIDs to establish path to egress 22
PE. 23
Figure 12-7 illustrates an SR-PCE based solution where PE1 needs to establish shortest path to PE3. 24
 25
 26
 27
Figure 12-7 Path computation based on SR-PCE 28
 29
1. SR-PCE gathers topology from each domain via BGP-LS. 30
2. PE1 sends a PCEP request to SR-PCE 31
3. PCE will respond with list of SIDs fcbb:bb00:300, fcbb:bb00:203:ffff::. 32
 33

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             94
For more complex path establishment SR-PCE will provide more complex SID list. 1
 2
[R46]: PCE and at least one TNE in each domain MUST support BGP Link State (BGP-LS), 3
RFC7752 [102] 4
[R47]: SR-PCE and at least one TNE in each domain MUST support “BGP Link State extensions 5
for IPv6 Segment Routing (SRv6)”, draft-ietf-idr-bgpls-srv6-ext-03 [146] 6
 7
[D36]: SR-PCE and at least one TNE in each domain SHOULD support extensions to advertise 8
Flexible Algorithm Definition as part of the topology, draft-ietf-idr-bgp-flex-algo-04 [134] 9
 10
[R48]: SR-PCE and TNEs providing PE functionality MUST support Path Computation Element 11
Protocol (PCEP), as described in RFC5440 [86] 12
 13
[R49]: SR-PCE and TNEs providing PE functionality MUST support PCEP protocol client 14
functionality with SRv6 extensions, draft-ietf-pce-segment-routing-IPv6-06 [140] 15
 16
[D37]: SR-PCE SHOULD support “IGP Flexible Algorithm be able to compute paths based on 17
different flexible algorithms 18
 19
12.3.3 SRv6 scaling conclusion 20
To build an underlay transport network that is highly scalable and supports traffic engineering the 21
simplest way is to uses a multi-domain IGP that supports flex-algo and use address summarization. 22
This design might not be sufficient for all application requirements. In these instances, a PCE based 23
solution could be deployed in tandem for TE use cases that require additional network visibility.   24
12.4 IPv6 Quality of Service 25
The IPv6 packet includes a 6-bit field in the IPv6 header, the DiffServ Code Point (DSCP). These 26
bits are assignable as a marker for QoS mechanisms in transport nodes to use as a classification and 27
marking tool. The per-hop behavior for IPv6 forwarding elements can be defined based on these 28
markings. For a full discussion of a proposed marking scheme and QoS architecture for TNEs in 29
Xhaul, section 0. 30
12.5 SRv6 OAM  31
SRv6 OAM functionality is required to understand the status of the network. SRv6 OAM is 32
described in “Operations, Administration, and Maintenance (OAM) in Segment Routing Networks 33
with IPv6 Data plane (SRv6)” “draft-ietf-6man-spring-srv6-oam” [147] 34
12.5.1 Ping / Traceroute to a remote IPv6 network address  35
Ping is required to test liveliness to a remote SRv6 underlay address and traceroute is required to 36
trace paths and perform hop-by-hop fault isolation to remote SRv6 underlay address. The following 37
capabilities are required on all TNEs. 38
 39
[R50]: TNEs MUST support ICMPv6 (RFC4443) [63] 40
 41
[R51]: TNEs MUST support IPv6 ping to query liveliness of a remote IPv6 address along the 42
shortest path for default SPF algorithm.  43
 44
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             95
[R52]: TNEs MUST support IPv6 ping to query liveliness of a remote IPv6 address along a path 1
calculated by a flex-algorithm.  2
 3
[R53]: TNEs MUST support IPv6 ping to query liveliness of a remote IPv6 address along a path 4
designated by a list of SIDs.    5
 6
[R54]: TNEs MUST support IPv6 traceroute to trace the path to a remote IPv6 address along the 7
shortest path for default SPF algorithm  8
 9
[R55]: TNEs MUST support IPv6 traceroute to trace the path to a remote IPv6 address along a path 10
calculated by a flexible algorithm. 11
 12
[R56]: TNEs MUST support IPv6 traceroute to trace the path to a remote IPv6 address along a path 13
designated by a list of SIDs. 14
 15
12.5.2 Ping / Traceroute to remote SID functions  16
Ping is required to test liveliness to a remote SID and traceroute is required to trace paths and 17
perform hop-by-hop fault isolation to a remote SRv6 SID. The following capabilities are required 18
on all Transport Nodes. 19
 20
[R57]: TNEs MUST support IPv6 ping to query liveliness of a remote EVPN and L3VPN services 21
along the shortest path for default SPF algorithm.  22
 23
[R58]: TNEs MUST support IPv6 ping to query liveliness of a remote EVPN and L3VPN services 24
along a path calculated by a flex-algorithm.  25
 26
[R59]: TNEs MUST support IPv6 ping to query liveliness of a remote EVPN and L3VPN service 27
along a path designated by a list of SIDs.  28
 29
[R60]: TNEs MUST support IPv6 traceroute to trace the path to a remote EVPN and L3VPN 30
service along the shortest path for default algorithm  31
 32
[R61]: TNEs MUST support IPv6 traceroute to trace the path to a remote EVPN and L3VPN 33
service along a path calculated by a flexible algorithm. 34
 35
[R62]: TNEs MUST support IPv6 traceroute to trace the path to a remote EVPN and L3VPN 36
service along a path designated by a list of SIDs. 37
 38
12.6 SRv6 on-going standardisation  39
SRv6 is a deployed network architecture based on RFCs and IETF adopted standards track 40
drafts.  Segment Routing is an active area of work at the IETF under the SPRING working group. 41
One area of study is the compression of SRv6 information. To this end, SPRING has formed a 42
design team to build requirements and analyse proposals. Future versions of this document may 43
describe compression techniques and other standards from the IETF as they are mature enough and 44
determined to be of use in the Xhaul based packet switched underlay fabric.  45
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             96
12.7 SRv6 Service infrastructure 1
An SRv6 transport underlay supports basic IPv6 services natively at the control and data plane. For 2
Ethernet, IPv4 and IPv6 VPN services an overlay control plane infrastructure is used to convey 3
VPN connectivity information. At the data plane the P devices can utilise a standard IPv6 data plane 4
with the PE devices requiring an IPv6 data plane with SRv6 VPN network programming awareness 5
[143].  Please refer to section 0 for a description of overlay service recommendations for a 5G 6
Xhaul infrastructure.   7
 8
  9
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             97
13 Packet-switched Xhaul services Infrastructure 1
 To support an Xhaul environment requires the packet switched network to support L2 and L3 2
services. Both an MPLS and IPv6 packet switched underlays use EVPN to support L2 and MP-BGP 3
L3VPNs.  4
13.1 MP-BGP design 5
Both EVPN and MP-BGP L3VPNs use MP-BGP, with appropriate address-family support for 6
EVPN and L3VPN, to convey service connectivity information between Provider Edge (PE) 7
equipment. 8
Typically, the MP-BGP infrastructure for L2 and L3 services uses a Route Reflector design rather 9
than an I-BGP mesh between all PEs. In large networks, operators will normally implement a 10
hierarchical Route Reflector (RR) design for Multi-Protocol BGP (MP–BGP) peering. A pair of 11
Route Reflectors (RR) could be used at every domain to provide scalability and extensibility.  12
 13
 14
 15
Figure 13-1 Hierarchical Route Reflector Design for Multiple Protocol BGP 16
 17
Figure 13-1 shows the Hierarchical Route Reflector Design utilizing RR pairs in each network 18
domain, all peering with National Route Reflector core.   19
13.2 Ethernet services  20
Ethernet services will be provided by EVPN over either MPLS or IPv6 depending on the underlay 21
network employed by the operator. More details on EVPN are provided in Annex B. 22
 23
EVPN VPWS can be used as transport service for Open Fronthaul (eCPRI), as well a transport 24
service for Radio over Ethernet (RoE) as outlined in following Figure 13-2 and Figure 13-3. 25
 26
 27
Figure 13-2 EVPN VPWS for eCPRI 28
Cell site
Hub site
CSR HSR
eCPRI flow
EVPN VPWS
O-RU O-DUeCPRI
Fronthaul
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             98
 1
 2
Figure 13-3 EVPN VPWS for RoE 3
 4
To support EVPN VPWS for eCPRI or RoE, the transport device will need to support: 5
 6
[D38]: SHOULD support “Flow-Aware Transport of Pseudowires over an MPLS Packet Switched 7
Network”, RFC 6391 [94] 8
[R63]: MUST support “Virtual Private Wire Service Support in Ethernet VPN”, RFC 8214 [107] 9
 10
[D39]: SHOULD support “Extensions to BGP-Signaled Pseudowires to Support Flow-Aware 11
Transport Labels”, Internet Engineering Task Force, RFC 8395 [113] 12
[R64]: If using SRv6 as the underlay technology TNEs providing PE functionality MUST support 13
SRv6 BGP based Overlay services: draft-ietf-bess-srv6-services-04 [131]  14
 15
13.2.1 Ethernet services redundancy 16
EVPN VPWS can provide redundancy for O-RU to O-DU (Open Fronthaul) interface to react to the 17
transport network errors that might occur. 18
 19
13.2.1.1 Ethernet services redundancy – Option 1 20
 21
 22
Figure 13-4 EVPN VPWS redundancy option 1 23
 24
 This option assumes that O-DU supports Link Aggregation Group (LAG, also called Ethernet 25
bundling) and terminates the eCPRI stream on a MAC address associated with the Ethernet bundle 26
(e.g. MAC-B in the diagram). Therefore, eCPRI stream can arrive on any physical interface (via 27
HSR-1 or via HSR-2) to O-DU, and O-DU might sent eCPRI stream, again, on any interface (via 28
HSR-1 or via HSR-2) (see Figure 13-4). 29
 30
Cell site
Hub site
CSR HSR
CPRI flow
RoE flow
EVPN VPWS
RRH BBU
Fronthaul
RoE
Mapper
RoE
Mapper
RoE
Cell site
Hub site
CSR
HSR-1
HSR-2
eCPRI flow
EVPN VPWS
MAC-A MAC-B
O-RU O-DUeCPRI
Fronthaul
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             99
To facilitate fast failover in case of HSRO-DU link failure, following architectural design choices 1
are recommended: 2
 3
• O-DU uplinks towards HSR pair should be bundled on O-DU to create Link Aggregation 4
Group (LAG), often called an “Ethernet bundle”. 5
• EVPN VPWS service should be terminated on HSR pair as ‘multi-homed all-active’ service 6
 7
When O-RU generates eCPRI stream towards O-DU, this stream can be sent from CSR via EVPN 8
VPWS leg towards HSR-1, or via EVPN VPWS leg towards HSR-2. It is CSR implementation 9
choice, towards which HSR eCPRI stream will be sent. In case multiple O-RUs are connected to 10
single CSR, and multiple eCPRI streams are forwarded from CSR towards O-DU via HSR pair, 11
each eCPRI stream might take differen path (i.e. via HSR-1 or HSR-2). 12
 13
When for example HSR-1O-DU link failure happens, following EVPN failure detection 14
machinery, the EVPN VPWS leg between CSR and HSR-1 is disabled, and eCPRI stream flows 15
now over HSR-2. Since EVPN VPWS termination on HSR-2 was originally ‘active’ (multi-homed 16
all-active service) no special forwarding plane reprogramming is needed on HSR-2, allowing quick 17
delivery of rerouted eCPRI streams towards O-DU, as outlined in Figure 13-5. 18
 19
 20
Figure 13-5 EVPN VPWS Redundancy option 1 – failure event 21
 22
13.2.1.2 Ethernet services redundancy – Option 2 23
Option 2 is similar to option 1, with the difference that O-DU expects to receive eCPRI stream on a 24
particular interface (for example, on the interface from HSR-1), as outlined in Figure 13-6 25
 26
 27
 28
Figure 13-6 EVPN VPWS Redundancy Option 2 29
 30
For this option, following architectural design choices are recommended: 31
 32
Cell site
Hub site
CSR
HSR-1
HSR-2
eCPRI flow
EVPN VPWS
MAC-A MAC-B
O-RU O-DUeCPRI
Fronthaul
Cell site
Hub site
CSR
HSR-1
HSR-2
eCPRI flow
EVPN VPWS
MAC-A MAC-B
O-RU O-DUeCPRI
Fronthaul
DF
nDF
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             100
• O-DU uplinks towards HSR pair should be bundled on O-DU to create Link Aggregation 1
Group (LAG), often called an “Ethernet bundle”. 2
• EVPN VPWS service should be terminated on HSR pair as ‘multi-homed single-active’ 3
service 4
• Deterministic Designated Forwarder (DF) election should be used on HSR pair, to ensure 5
that one of the HSR is deterministically elected as ‘active’ forwarder (DF), and second HSR 6
is deterministically elected as ‘standby’/’passive’ forwarder (non-DF) 7
• Per physical port (rather than default per-VLAN) DF election should be used on HSR pair 8
• HSR should signal the non-DF status to O-DU via OAM (for example LACP – Link 9
Aggregation Control Protocol – Out of Sync signalling) 10
 11
In addition to requirements already mentioned in the main section, to support this option following 12
requirements must be supported on HSR: 13
 14
[R65]: MUST support “Preference-based EVPN DF Election", draft-ietf-bess-evpn-pref-df-06 [131] 15
 16
[R66]: MUST support "EVPN multi-homing port-active load-balancing", draft-ietf-bess-evpn-mh-17
pa [129] 18
 19
As outlined in Figure 13-7, when HSR-1O-DU link failure happens, following EVPN failure 20
detection machinery, the EVPN VPWS leg between CSR and HSR-1 is disabled, HSR-2 becomes 21
DF and EVPN VPWS leg between CSR and HSR-2 is enabled. Additionally, LACP Out-of-Sync 22
state from HSR-2 to O-DU is cleared, allowing forwarding over the HSR-2O-DU link. eCPRI 23
stream flows now over HSR-2. As opposed to Option 1, since EVPN VPWS termination on HSR-2 24
was originally ‘standby’ (multi-homed single-active service), DF election, LACP OOS clearance 25
and forwarding plane reprogramming is needed on HSR-2, thus failover is longer than in case of 26
Option 1. 27
 28
 29
 30
Figure 13-7 EVPN VPWS Redundancy Option 2 -failure event 31
13.2.1.3 Ethernet services redundancy – Option 3 32
Option 3 is further modification of option 2. It is suitable for O-DUs that do no support LAG 33
bundling, but have the capability to terminate eCPRI stream on some internal virtual MAC, 34
allowing reception of eCPRI stream over any uplink. 35
 36
Cell site
Hub site
CSR
HSR-1
HSR-2
eCPRI flow
EVPN VPWS
MAC-A MAC-B
O-RU O-DUeCPRI
Fronthaul
nDF
DF

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             101
 1
Figure 13-8 EVPN VPWS Redundancy Option 3 2
 3
For this option, following architectural design choices are recommended: 4
 5
• Two O-DU uplinks towards HSR pair are not bundled, but are standalone links placed inside 6
internal bridge on O-DU 7
• EVPN VPWS service should be terminated on HSR pair as ‘multi-homed single-active’ 8
service  9
 10
In this option, one of the HSR routers (for example HSR-1) is automatically elected by EVPN 11
control plane as ‘active’ (Designated Forwarder) router, and eCPRI stream is delivered over EVPN 12
VPWS service from CSR to active HSR only, where it is forwarded to O-DU in Ethernet frames. 13
Internal bridge at O-DU performs MAC learning (to learn O-RU MAC: MAC-A), therefore the 14
eCPRI stream generated from O-DU side and destined to O-RU (MAC-A) follows the same path 15
via HSR-1. 16
 17
As outlined in Figure 13-9, when HSR-1O-DU link failure happens, following EVPN failure 18
detection machinery, the EVPN VPWS leg between CSR and HSR-1 is disabled, HSR-2 becomes 19
DF and EVPN VPWS leg between CSR and HSR-2 is enabled. eCPRI stream flows now over HSR-20
2, and internal bridge in O-DU relearns the MAC-A via HSR-2. As opposed to Option 1, since 21
EVPN VPWS termination on HSR-2 was originally ‘standby’ (multi-homed single-active service), 22
DF election and forwarding plane reprogramming is needed on HSR-2, thus failover is longer than 23
in case of Option 1. 24
 25
 26
Figure 13-9 EVPN VPWS Redundancy Option 3 – failure event 27
13.2.1.4 Ethernet services redundancy – Option 4 28
Option 4 is suitable for O-DUs with only basic Ethernet support. i.e. O-DUs not supporting LAG, 29
internal bridge, or virtual MAC for eCPRI stream termination. Therefore, O-DU is represented by 30
two MAC addresses: MAC-B, on the link towards HSR-1, and MAC-C, on the link towards HSR-2. 31
 32
Cell site
Hub site
CSR
HSR-1
HSR-2
eCPRI flow
EVPN VPWS
MAC-A MAC-B
O-RU O-DU
Fronthaul
eCPRI
Cell site
Hub site
CSR
HSR-1
HSR-2
eCPRI flow
EVPN VPWS
MAC-A MAC-B
O-RU O-DU
Fronthaul
eCPRI

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             102
 1
Figure 13-10 EVPN VPWS Redundancy Option 4 2
 3
For this option, following architectural design choices are recommended: 4
 5
• Two O-DU uplinks towards HSR pair are not bundled, but are standalone links placed inside 6
internal bridge on O-DU 7
• EVPN VPWS service should be terminated on HSR pair as ‘multi-homed single-active’ 8
service  9
 10
In this option, eCPRI stream generated at O-RU uses of the O-DUs MAC addresses, for example 11
MAC-B, as the destination MAC in eCPRI Ethernet frames. Therefore, EVPN DF election must be 12
deterministic, similar to the DF election on Option 2. 13
 14
In addition to requirements already mentioned in the main section, to support this option following 15
requirements must be supported on HSR: 16
 17
[R67]: MUST support “Preference-based EVPN DF Election", draft-ietf-bess-evpn-pref-df-06 [131] 18
 19
As outlined in Figure 13-11, when HSR-1O-DU link failure happens, following EVPN failure 20
detection machinery, the EVPN VPWS leg between CSR and HSR-1 is disabled, HSR-2 becomes 21
DF and EVPN VPWS leg between CSR and HSR-2 is enabled. However, as opposed to all options 22
discussed previously, in this option, when failure happens, O-RU configuration must be changed by 23
the orchestrator, so that new MAC address (MAC-C) is used as the destination MAC in eCPRI 24
Ethernet frames. This contributes to the highest failover time among all options discussed so far. 25
 26
 27
Figure 13-11 EVPN VPWS Redundancy Option 4 – failure event 28
 29
  30
Cell site
Hub site
CSR
HSR-1
HSR-2
eCPRI flow
EVPN VPWS
MAC-A MAC-B
O-RU O-DUeCPRI
Fronthaul
DF
nDF
MAC-C
Cell site
Hub site
CSR
HSR-1
HSR-2
eCPRI flow
EVPN VPWS
MAC-A MAC-B
O-RU O-DUeCPRI
Fronthaul
DF
nDF
MAC-C

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             103
13.3 IP Services 1
Mobile IP services will be provided by MP-BGP based L3 VPNs (RFC4364) [68]  over MPLS or 2
SRv6 depending on the underlay network technology employed by the operator. More details on 3
L3VPN are provided in Annex C.  4
 5
Operators that choose to implement IP services using the BGP Based L3VPN services may use the 6
following architectural design options:  7
 8
13.3.1 Building flexible L3VPN service topologies 9
L3VPN services can be used to establish L3 connectivity between various mobile components. BGP 10
L3 VPN support both IPv4 and IPv6 VPNs and offer flexible connectivity models including: 11
• IP N:N multipoint services 12
• IP 1:N multipoint services 13
• IP 1:1 connectivity services 14
 15
 16
Figure 13-12 Flexible L3 VPN service topologies 17
13.3.2 Constraints based Traffic Steering in L3VPNs 18
By default, BGP based L3VPN use shortest path routing across the transport underlay. However, 19
BGP based VPN traffic can also get automatically steered into SR policy, such as one created by a 20
flexible algorithm or a point-to-point TE tunnel. This is achieved by coloring VPN routes within 21
MP-BGP with extended community attributes. As illustrated in Figure 13-13 this can be done in 22
two ways  23
1. While defining a VRF, define a color for all Prefixes associated with that VRF, or 24
2. When sending or receiving BGP routes, using a policy to “color” the route.  25
 26
 27
 28

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             104
 1
Figure 13-13 Import / Export of route colors for SR policy selection 2
 3
In either case, the VPN route will now be tagged with a color and traffic for the destination will 4
automatically get steering into an SR policy based on the color associated with the VPN route.  5
 6
An operator using MP-BGP L3VPN services for Xhaul, TNEs need to support:  7
 8
[R68]: MUST support Multiprotocol Extensions for BGP-4, RFC4760 [70] 9
 10
[R69]: MUST Support BGP/MPLS IP Virtual Private Networks (VPNs), t RFC4364 [68]  11
 12
[D40]: SHOULD support SRTE Policy configuration as outlined in draft-ietf-spring-segment-13
routing-policy-08 [142]  if SRTE policies are used with L3VPN services 14
[D41]: SHOULD support EVPN and L3VPN traffic steering based on “color extended community 15
BGP attributes” defined in RFC5512 [89]  16
[D42]: SHOULD support to color extended community attribute for VPNv4, VPNv6 and EVPN 17
BGP address families defined in RFC5512 [89]  18
[R70]: If using SRv6 as the underlay technology TNEs providing PE functionality MUST support 19
SRv6 BGP based Overlay services: draft-ietf-bess-srv6-services-04 [132] 20
 21
  22

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             105
14 Quality of Service in packet-switched networks  1
This chapter discusses the Quality of Service (QoS) capabilities that must be deployed to support 2
the delivery of Xhaul traffic and non-Xhaul traffic in the RAN network.   3
14.1 Xhaul transport core interface QoS   4
For the purposes of this document, the transport network domain will be considered to be 5
constructed of two interface types. Core interfaces and edge interfaces. An interface is considered to 6
be transport network core if it is interconnecting two devices inside the transport domain. An 7
interface is considered transport network edge if it is connecting to an element outside the transport 8
domain.   9
14.1.1 Transport network core interface classification  10
Interfaces in the transport network core domain should perform classification on the outer transport 11
header only. The header in use will depend on the transport model selected by the operator. An 12
example classification scheme for MPLS TC (EXP) to queue is shown below in Figure 14-1 13
  14
[R71]: TNEs MUST support classification based on MPLS TC in a MPLS underlay transport 15
network   16
[R72]: TNEs MUST support classification based on IPv6 DSCP in an SRv6 underlay transport 17
network   18
 19
14.1.2 Core interface queue structure 20
The Xhaul domain must support a differentiated QoS architecture supporting priority queueing and 21
scheduling with weighted fair queue scheduler for non-priority packets.   22
 23
[R73]: MUST support EF forwarding described in IETF RFC 3246 [51]  mapped to a strict priority 24
scheduler with shaping and policing to prevent bandwidth starvation of other classes.  25
[R74]: MUST support AF forwarding model described in IETF RFC 2597 [40] mapped to a WFQ, 26
WRR or MDRR scheduler for non-priority classes.  27
[R75]: MUST support the ability to manipulate queue depths  28
  29
14.1.2.1 Flat queue model  30
In the flat queue model, the TNE should support a single level of queue on each physical interface. 31
This model is used for a single topology infrastructure, or a “soft slicing” where the QoS model for 32
each slice is common and bandwidth is dynamically shared between slices.  33
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             106
 1
Figure 14-1 Example Xhaul queue model 2
  3
The system should be capable of supporting a minimum of six queues within the core domain. An 4
example queuing model is provided in Figure 14-1which should be adapted to fit the available 5
queue and scheduling model available in the deployed hardware. 6
All deployments must support a strict priority queue dedicated to latency sensitive front haul traffic 7
(shown as queue marking 5). This queue must be serviced in strict priority over all other queues up 8
to its shaped or policed limit. If the node is providing transit for G.8275.2 PTPoIP as a PTP 9
unaware element, then PTP packets MUST also be scheduled in this queue. If the hardware 10
supports multiple levels of strict priority queueing, and is a PTP unaware node, then PTP MAY also 11
be scheduled in a dedicated highest priority queue to minimise packet delay variation (PDV).   12
Two further latency sensitive queues are defined, one for network control (shown as queue marking 13
7) and one for other latency sensitive transit traffic (e.g. latency sensitive Backhaul or Midhaul U-14
plane or latency sensitive business or consumer traffic) (shown as queue marking 2). These queues 15
should be configured to be serviced with a latency bound suitable for the traffic associated with 16
them. The scheduling model used will be dependent on the specific HW choice. For example, this 17
could be a guaranteed bandwidth queue with suitable scheduler weight and queue depth, or a high 18
priority queue with lower priority scheduling than that for the latency sensitive Fronthaul traffic. 19
Additional queues are defined for guaranteed bandwidth traffic (Management (shown as queue 20
marking 6), guaranteed bandwidth U-plane (shown as queue marking 3,4) and best effort traffic 21
(shown as queue marking 0,1)).  22
Additional queues may be supported if the carrier deems necessary.  23
The example queue model is show in Figure 14-1 provides example MPLS “Traffic Class” (TC) or 24
often called EXP, based classification for the queues. It should be noted that a transport based on 25
SRv6 allows the use of IP DSCP as the QoS marker, allowing for more flexibility in the queue 26
model, and that more queues may be used if the marking scheme and hardware are appropriate. 27
[R76]: MUST support a minimum of 6 HW queues per physical interface  28
 29
14.1.2.2 Hierarchical queue model   30
The hierarchical queue model provides a capability to support a segmented infrastructure “Hard 31
slicing” where the QoS model for each slice is different, or where each slice requires dedicated 32
bandwidth. (see Figure 14-2 for details)  33

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             107
Here the model the transport device must support a minimum of two levels of queue hierarchy on 1
each physical interface.  2
 3
 4
Figure 14-2 Sample hierarchical queue model  5
In this model the first, or child level defines the queues needed to support the capabilities associated 6
with the slice. These might be the same as in the flat queue model or may be different. In the second 7
or parent level, a scheduler is defined to allocate the maximum bandwidth that will be available to 8
that hard slice. It should be noted that the sum of the guaranteed bandwidth to each shaper should 9
not exceed the interface bandwidth.  As with the flat queue model, example is provided which 10
should be adapted for the specific HW in use. 11
 12
[R77]: MUST support a minimum of 6 HW queues per logical interface 13
  14
[R78]: MUST support a minimum of two tiers of configurable scheduler per physical interface  15
  16
14.1.3  Transport network core interface marking structure  17
Transport nodes should preserve the QoS marking of transported frames and packets across the 18
transport network. This should be accomplished via the use of the pipe or short pipe model 19
described in IETF RFC2983.   20
The specific marking structure used to define the transport QoS behaviours is left to the individual 21
providers and is dependent on the transport encapsulation implemented. An example marking 22
structure for MPLS TC values is shown in Figure 14-1.  23
  24
14.1.4 Core interface scheduling model  25
Latency sensitive and network control plane traffic should be forwarded using an expedited 26
forwarding model as described in IETF RFC 3246. In order to preserve minimum latency, these 27
queues should be scheduled with a maximum of one packet in each queue and be allocated a 28

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             108
priority forwarding schedule in the NPU. To prevent bandwidth starvation of non-priority scheduled 1
traffic, each priority queue should be shaped or policed at a combined capacity less than the 2
physical rate of each egress interface in the core, and with sufficient bandwidth capacity available 3
for the latency sensitive Fronthaul services they are supporting. Where needed a TSN based 4
scheduler may also be implemented if the interface bandwidth demands such optimisation. Details 5
on TSN may be found in section 10.1.1.2.1. 6
Guaranteed bandwidth queues (management and best effort) should be scheduled using the assured 7
forwarding model described in IETF RFC 2597. These queues should be scheduled using a 8
Weighted Round Robin (WRR), Minimum deficit round robin (MDRR) or weighted fair queue 9
(WFQ) scheduler, depending on the capability available in the NPU. Where network control traffic 10
is assigned to a guaranteed bandwidth queue, the scheduler for that queue must be configured to 11
support minimum latency to ensure time sensitive network control traffic (eg BFD) is not delayed.  12
The scheduler MUST have sufficient buffer capacity to avoid loss in these queues due to the effects 13
of micro-burst caused by the aggregation of traffic from different ingress interfaces, or speed 14
mismatch (higher to lower).    15
14.2 Xhaul transport network edge interface QoS  16
PE elements in the transport network domain will be responsible for providing logical separation 17
and encapsulation for transport. It can be assumed that there are two types of traffic that will be 18
presented for transport.  19
1. Ethernet frames. For example, RoE or eCPRI frames not using an IP header.  20
2. IP packets. For example, eCPRI packets or Backhaul traffic whose encapsulation includes an 21
IP header.   22
14.2.1 Transport network domain PE ingress classification of Ethernet frames.  23
For the purposes of this section, it is assumed that there will be a physical or logical interface 24
attaching the Ethernet frame device to the transport network edge PE. As an example, this could be 25
a Mobile Client, such as a Front Haul Gateway or O-DU supporting an RU using RoE. PE nodes in 26
the transport network will perform classification on traffic ingressing the domain using different 27
models.   28
  29
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             109
1
   2
Figure 14-3 Ethernet ingress classification models 3
 4
Figure 14-3 identifies three models by which transport domain PE devices will classify traffic 5
originating from a ROE mapper. All three models must be supported. 6
14.2.1.1 Untagged frames  7
As per Figure 14-3(1) the Mobile Client (MC) presents a flow of untagged frames to the PE. The 8
PE uses the local port as the context for the MC. This context must be preserved in the transport 9
network and presented at the port level (untagged) at the remote PE. Because the PE has no 10
knowledge of the MC encapsulation model, or priority needed for each frame, all frames must be 11
given the same appropriate treatment (and transport marking) to support the encapsulation mode. It 12
can be assumed that the operator has this knowledge at the port level and can apply the appropriate 13
local policy for both transport marking and PHB for all frames. These frames should be 14
encapsulated with the appropriate transport and marked with QoS marking for carried in the class 15
defined in section 14.1.2 for Latency sensitive Fronthaul traffic.  16
14.2.1.2 Tagged frames (VLAN), PCP unset or untrusted  17
As per Figure 14-3(2) The RoE mapper presents a flow of VLAN tagged frames to the PE. The PE 18
uses the VLAN and local port as the context for the RoE mapper. This context must be preserved in 19
the transport network and presented at the remote PE with the same VLAN marking. Because the 20
PE has no knowledge of the RoE mapper encapsulation model, or priority needed for each frame, 21
all frames must be given the same appropriate treatment and transport marking, to support the 22
encapsulation mode. It can be assumed that the operator has this knowledge at the either the VLAN 23
or port level and can apply the appropriate local policy for both transport marking and PHB for all 24
frames. These frames should be encapsulated with the appropriate transport and marked with QoS 25
marking for carried in the class defined in 14.1.2 for Latency sensitive Fronthaul traffic.  26
14.2.1.3 Tagged frames (VLAN), PCP set and trusted  27
As per Figure 14-3(3). The RoE mapper presents a flow of VLAN tagged frames to the PE. The PE 28
uses the VLAN and local port as the context for the RoE mapper. This context must be preserved in 29

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             110
the transport network and presented at the remote PE with the same VLAN marking. In this model 1
it is assumed that the RoE mapper supports a differential marking scheme for flows requiring 2
different treatment. The marking is placed in the PCP field of the VLAN header (described by IEEE 3
802.1p). The PE should be capable of classifying flows based on the PCP marking and 4
implementing an appropriate PHB. The PE should also apply an appropriate marking to the frames 5
as supported by the transport model in use. It can be assumed that the operator has this knowledge 6
of the PCP markings from the RoE mapper.  7
Each class presented by the RoE mapper must be mapped to one of the class markings defined in 8
section 14.1.2. It is assumed that the operator has appropriate knowledge to determine this 9
mapping.  10
 11
[R79]: MUST support classification of frames based on 802.1p PCP bit field  12
 13
[R80]: MUST support classification of frames based on incoming logical interface.  14
 15
[D43]: SHOULD support ingress policing using a single rate 2 color policer described in RFC2698 16
[41] or RFC4115 [59]  17
[D44]: SHOULD support ingress policing using a dual rate 3 color policer described in RFC2698 18
[41] or RFC4115 [59] 19
[R81]: MUST support ingress marking of frames based on result of soft policing using policers in 20
[D43]: and [D44]:. 21
  22
14.2.2  Transport domain PE ingress classification IP packets.  23
Unlike Ethernet frames, IP packets will always have a QoS marking field available to them. As a 24
result, we do not have to be concerned with IP packets arriving unmarked from a QoS perspective 25
and must simply determine if we trust the marking.   26
14.2.2.1 Marked packets DSCP untrusted  27
Similar to Figure 14-3(2). The client element presents a flow of DSCP marked packets to the PE. 28
The PE uses local port as the context for the flow. This context must be preserved in the transport 29
network and presented at the remote PE with the same DSCP marking. Because the PE has no 30
knowledge of the client element encapsulation model, or priority needed for each packet, all packets 31
must be given the same appropriate treatment and transport marking, to support the encapsulation 32
mode. It can be assumed that the operator has this knowledge at the port level and can apply the 33
appropriate local policy for both transport marking and PHB for all frames. These frames should be 34
encapsulated with the appropriate transport and marked with QoS marking for carried in the class 35
defined in section 14.1.2 for Latency sensitive Fronthaul traffic.  36
14.2.2.2 Marked packets DSCP trusted  37
Similar to Figure 14-3. The client element presents a flow of DSCP marked packets to the PE. The 38
PE uses the local port as the context for the flow. This context must be preserved in the transport 39
network and presented at the remote PE with the same DSCP marking. In this model it is assumed 40
that the client element or IP source supports a differential marking scheme for flows requiring 41
different treatment. The marking is placed in the DSCP field of the IP header. The PE should be 42
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             111
capable of classifying flows based on the DSCP marking and implementing an appropriate PHB. 1
The PE should also apply an appropriate marking to the frames as supported by the transport model 2
in use. It can be assumed that the operator has this knowledge of the DSCP markings from the client 3
element or IP source.  4
Each class presented by the client element or IP source must be mapped to one of the class 5
markings defined in section 14.1.2. It is assumed that the operator has appropriate knowledge to 6
determine this mapping.   7
[R82]: MUST support classification of frames based on IPv4 DSCP field  8
 9
[R83]: MUST support classification of frames based on IPv6 DSCP field  10
  11
14.2.3 Admission control  12
Admission control via policing should be applied to all sources of traffic at the PE. It is important to 13
recognise that suitable capacity MUST be made available for all traffic (especially where loss is 14
problematic) in each element of the infrastructure. Where statistical multiplexing is expected to be 15
used to support additional service capacity in the network, excess traffic, beyond that which is 16
committed, MUST be marked as excess and be eligible for drop to protect other committed traffic 17
under heavy usage periods.    18
14.2.4  PE Egress scheduling  19
Traffic arriving at a PE from the infrastructure, to be transmitted to a CE device should be done so 20
based on the model used for its classification on ingress.  21
14.2.4.1 Unclassified traffic  22
Where an interface is using unclassified traffic, that is;   23
all traffic is always marked as default by the CE or   24
traffic with no marking available e.g. ethernet without a VLAN header,   25
traffic should be forwarded on a FIFO basis. In this model it must be ensured that the flow of traffic 26
into the PE toward the CE MUST be at a lower rate than the interface attaching the PE to the CE to 27
avoid congestion resulting in latency or loss.  28
14.2.4.2 Classified traffic  29
Where an interface is using classified traffic, that is traffic has a marking scheme available, and 30
traffic is marked using this scheme, traffic should be forwarded to the CE with a queueing scheme 31
and scheduling model that is appropriate to the traffic for that CE. The specific marking and 32
queueing scheme are beyond the scope of this document but MUST be understood by the operator. 33
In this model the PE MUST support the classification and queueing of the CE destined traffic based 34
on the marking on the CE packet or frame. This is understood as “pipe model – after” as defined in 35
IETF RFC 2983, section 5 – 6.    36
  37
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             112
[R84]: MUST support classification of encapsulated frames or packets based on 802.1p PCP bit 1
field or IP DSCP field as described in IETF RFC 2983  section 5-6   2
  3
Latency sensitive and network control plane traffic should be forwarded using an expedited 4
forwarding model as described in IETF RFC 3246 [51]. In order to preserve minimum latency, 5
these queues should be scheduled with a maximum of one packet in each queue and be allocated a 6
priority forwarding schedule in the NPU. To prevent bandwidth starvation of non-priority scheduled 7
traffic, each priority queue should be shaped or policed at a combined capacity less than the 8
physical rate of each egress interface in the core, and with sufficient bandwidth capacity available 9
for the latency sensitive Fronthaul services they are supporting. Where needed a TSN based 10
scheduler may also be implemented if the interface bandwidth demands such optimisation. Details 11
on TSN may be found in section10.1.1.2.1.  12
Guaranteed bandwidth queues (management and best effort) should be scheduled using the assured 13
forwarding model described in IETF RFC 2597 [40]. These queues should be scheduled using a 14
Weighted Round Robin (WRR), Minimum deficit round robin (MDRR) or weighted fair queue 15
(WFQ) scheduler, depending on the capability available in the NPU. The scheduler MUST have 16
sufficient buffer capacity to avoid loss in these queues due to the effects of micro-burst caused by 17
the aggregation of traffic from different ingress interfaces, or speed mismatch (higher to lower).   18
  19
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             113
15 Multicast 1
15.1 Multicast use cases 2
Multicast use cases in an Xhaul transport network come from two categories as described below. 3
 4
15.1.1 Multicast transport for fixed line services 5
As mentioned in the requirements section: 6
 7
“ITU-T GSTP-TN5G: Transport support of IMT-2020/5G” identifies the need for the transport to 8
be multi-service in nature. In addition to mobile services, the infrastructure needs to support fixed 9
line consumer and enterprise services. 10
 11
Fixed line consumer/enterprise services like IPTV and VPN all require multicast support in the 12
transport network. 13
15.1.2 MBMS/5MBS transport 14
3GPP TS 23.246 MBMS Architecture and Functional Specification (R15) and TR 23.757 Study on 15
architectural enhancements for 5G multicast-broadcast services (5MBS, R17) all support “shared 16
delivery method” with which multicast/broadcast traffic is transported via multicast to RAN nodes 17
(N3/N9), who will then transmit over the air. This means multicast transport is needed to CU-UP. 18
 19
Another “individual delivery method” may also be used – UPFs send individual copies of multicast 20
traffic over PDU sessions to UEs, transparent to RAN nodes. With this method, if UPFs are 21
distributed, then multicast should be used in DNN to the UPFs (N6). Note that, a UPF may be 22
connected to multiple DNNs as the anchored PDU sessions may belong to multiple DNNs. 23
 24
 25

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             114
 1
Figure 15-1 Mobile multicast use cases 2
15.2 Overlay and underlay multicast 3
The multi-service nature of 5G transport network is not only that the transport network is also used 4
for fixed line service transport. Even for 5G itself, the same transport network is used for multiple 5
purposes, e.g., N3/N9 transport, N6 transport and Xhaul transport. 6
 7
 8
 9
 10
Figure 15-2 VPN infrastructure for mobile 11
As described in the service capability section, different transports are rendered as different 12
IPVPN/EVPN overlay services over a common transport underlay. When it comes to multicast, it is 13
just an aspect of the IPVPN/EVPN, as further described in section 0. 14
 15
15.3 Recommendation/considerations for multicast solutions 16
With the background discussion provided in   17

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             115
Annex E: Multicast Technologies background, the following multicast solutions are recommended: 1
 2
1. BGP-MVPN and EVPN-BUM for overlay, with tunnel segmentation if different tunnel 3
types/instances are necessary/desired for different ASes/areas. 4
2. Currently deployed underlay tunnel solutions can still be used even with Segment Routing 5
(mLDP/RSVP protocol would only be used for multicast not unicast purposes). If end-to-6
end tunnels are used without tunnel segmentation, PIM RPF Vector or mLDP Recursive 7
FEC or controller signalling need to be used. 8
3. Controller-signaled multicast can be used if tree calculation and signalling by controllers are 9
desired to satisfy TE constraints, or to remove legacy LDP/RSVP protocols from the 10
network. This includes SR-P2MP an BGP-signalled mLDP tunnels, with the latter offering 11
the best flexibility – easy transition from existing mLDP deployment and flexible ways of 12
tunnel identification via mLDP FEC. 13
4. BIER can be deployed in (part of) the network when enough routers support BIER. 14
 15
Depending on the selected solution, some of the following standards may need to be supported. 16
 17
[O1]: BGP Encoding and Procedures for Multicast in MPLS/BGP IP VPNs, RFC6514 [97] 18
 19
[O2]: BGP MPLS-Based Ethernet VPN, RFC7432 [98] 20
 21
[O3]: Update on EVPN BUM Procedures, draft-ietf-bess-evpn-bum-procedure-updates-08 [127]  22
 23
[O4]: Protocol Independent Multicast - Sparse Mode (PIM-SM) Protocol Specification, RFC 7761 24
[103] 25
  26
[O5]: The Reverse Path Forwarding (RPF) Vector TLV, RFC 5496 [88] 27
 28
[O6]: Label Distribution Protocol Extensions for Point-to-Multipoint and Multipoint-to-Multipoint 29
Label Switched Paths, RFC6388 [93] 30
[O7]: Using Multipoint LDP When the Backbone Has No Route to the Root, RFC 6512 [95] 31
 32
[O8]: Extensions to Resource Reservation Protocol - Traffic Engineering (RSVP-TE) for Point-to-33
Multipoint TE Label Switched Paths (LSPs), RFC4875 [71]  34
[O9]: Controller Based BGP Multicast Signaling, draft-ietf-bess-bgp-multicast-controller-05 [126] 35
 36
[O10]: Segment Routing Point-to-Multipoint Policy, draft-ietf-pim-sr-p2mp-policy-00 [136] 37
  38
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             116
16 Packet-switched orchestration and telemetry 1
Packet switched orchestration and telemetry covers how the packet switching transport network is 2
programmed at a device and service level and how telemetry data is retrieved from TNEs. This will 3
be covered either in a separate document or in a future revision of this document.   4
  5
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             117
17 5G Slicing in a packet switched Xhaul network 1
Network Slicing is end-to-end partitioning of the network resources and network functions so that 2
selected applications/services/connections may run in isolation from each other for a specific 3
business purpose. The overall slicing architecture is shown in Figure 17-1 and covers the 4
orchestration infrastructure and at the physical layer can cover the radio access network, the mobile 5
core, including data centers and virtualization aspects, and the transport network.  6
 7
 8
Figure 17-1: Overall 5G slicing architecture  9
 10
Although slicing is a key capability of 5G there is debate as to how it exactly translates into the 11
transport network, which mobile interfaces (Fronthaul, Midhaul, Backhaul, N6) need to be sliced, 12
what form they will take and the number of slices required at the transport layer. 13
Characteristics of a transport slice are defined in clause 5.2.3 of 3GPP Technical Report 22.891. 14
Although not referenceable, further useful information can be found in various expired personal 15
informational IETF drafts which provide thinking on the characteristics of a sliced transport 16
network. Some of the key points are: 17
 18
• Management and lifecycle of the network 19
o Definition 20
o Creation / deletion 21
o Modification 22
• Per slice OAM 23
• Resource Reservation  24
• Slice isolation  25
o Performance 26
o Operational 27
o Security  28
o Reliability 29
• Abstraction 30
o Virtualization of network functions (where appropriate) 31
o Use of shared compute resources  32

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             118
 1
In the transport space, the terms hard and soft slicing has emerged. This refers to the level of 2
isolation between different slices. In both cases they need to support the functions outlined above 3
but the way the slice is built and managed differs considerably.  4
 5
• Hard slicing: Transport resources are dedicated to a specific “Network Slice Instance” (NSI). 6
In this case resources are dedicated solely to a particular slice and not available to other 7
slices.  8
 9
• Soft slicing: The transport slice has the characteristics outlined above but the resources are 10
shared and can be re-used by other slices.  11
  12
A packet switched infrastructure, as described in this document, has an extensive toolset, consisting 13
of underlay forwarding solutions, Quality of Service (QoS) and VPNs that allows an operator to 14
scalable partition the transport network to cater for both hard and soft slices. See Figure 17-2 for 15
transport slice requirements and associated toolset.   16
 17
Figure 17-2: Packet switched toolset for transport level slicing  18
 19
When considering hard and soft slices it is important to bear in mind they should be thought of as a 20
spectrum of capability, rather than one or the other.  21
  22
17.1  Packet-switched underlay network 23
The packet-switched underlay network can use MPLS, SRv6 or a combination.  24
17.1.1 Underlay forwarding plane 25
The forwarding plane determines how traffic is sent over the packet-switched underlay 26
infrastructure. Three approaches are outlined for constructing the underlay forwarding plane/planes 27
for a 5G transport infrastructure. In each case the level of resource sharing reduces, hence the slice 28
solution becomes harder in nature but may introduces scalability and operational challenges.  29

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             119
17.1.2 Single forwarding plane for all slices 1
The underlay network relies solely on the routing protocols (IGPs and EGPs) to calculate a single 2
forwarding table based on shortest path routing. The routing protocol sees all links and there is a 3
single forwarding table based on IGP and BGP metrics. All traffic takes the shortest path between 4
two points within the network and utilises ECMP.  5
 In the context of slicing, all slices will see the same set of paths between end points within the 6
network. It can be considered the softest slicing solution in terms of the underlay forwarding plane. 7
However, although the infrastructure is shared between slices, it is highly scalable, widely 8
implemented in 4G and critical enterprise environments, and is capable of delivering high quality 9
SLAs.  10
17.1.3 Forwarding plane per 5G service 11
Within the underlay network multiple forwarding planes are built to meet the specific forwarding 12
behaviors associated with the different 5G services. One or more customers can then use these 13
forwarding planes. This is achieved using VPNs and traffic steering techniques. The forwarding 14
planes can utilise different topologies and be optimized based on different criteria. For example, a 15
URLLC service could be optimised to use only the most reliable links in the network and select the 16
best paths between endpoints based on delay. In contrast, the eMBB and mMTC services could be 17
designed to use cheap, high bandwidth links. To implement such an approach, a mix of shortest 18
path routing and traffic engineering solutions could be used. Traffic engineering options include 19
flex-algo, SR-TE or traditional MPLS TE. As with any traffic engineering approach, consideration 20
needs to be made of scale and state held within the network. Perhaps the simplest approach would 21
be to use the default Flex-algorithm for eMBB and the mMTC services and a delay optimized 22
flexible algorithm for the URLLC services. Backbone links and even nodes could be considered for 23
inclusion or exclusion into the delay optimized flex-algorithm forwarding planes based on whether 24
the algorithm is enabled on a TNE and TE affinities associated with the links.        25
17.1.4 Forwarding plane per slice customer 26
This is a variation on the previous scheme. In this case rather than a forwarding plane per 5G 27
service, a forwarding plane for individual customers is defined. The same techniques outlined above 28
would be used but very careful consideration needs to be given to scale and operational complexity 29
of such an approach. In this case, complexity is a function of the number of customers using the 30
network rather than the 5G services. 31
17.2 Quality of Service   32
Quality of Service is an important component in slicing a transport infrastructure. As with the 33
forwarding plane, different approaches could be taken depending on the level of isolation required 34
between slices. In considering QoS it is necessary to look at the edge QoS solution and the core 35
QoS solutions. For more info on QoS refer to section 0 and 0.  36
 37
17.2.1 Edge QoS  38
Edge interfaces to packet transport networks are generally the slowest and also where congestion 39
and packet drop most often occurs. Regardless of the QoS structures implemented in the transport 40
core, it is important with slicing to support edge conditioning of traffic on ingress and scheduling on 41
egress to the transport network. This ensures that each slice gets its contracted overall bandwidth 42
and class bandwidth as it enters and leaves the transport network. When slices are presented via 43
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             120
VLANs from the mobile clients, the PE router needs a hierarchical QoS capability that can account 1
for the overall contracted bandwidth at the VLAN level and also the overall contracted class 2
bandwidth within the VLAN.  3
 4
17.2.2 Core QoS  5
Within the core network different QoS strategies can be applied that determine the level of 6
bandwidth and queue sharing that occurs between different slices.  7
17.2.2.1 Shared queue architecture 8
This approach follows the classic Diffserv QoS architecture. Slice traffic would be conditioned, and 9
optionally marked or remarked as it enters the Diffserv domain, in this case the transport network. 10
These markings determine the behaviour or Per Hop Behaviour (PHB) the traffic receives in the 11
transport network. The number of PHBs in the core is determined by a number of factors; the 12
number of PHBs that can be marked, the number of queues supported on the TNEs and operational 13
complexity of the solution. Typically, in an MPLS network the traffic-class field is used to 14
designate the PHB in the core. This is 3 bits, giving a maximum of 8 markings. In an IPv6 network 15
the DSCP field is used to designate the PHB in the core. This is 6 bits giving a maximum of 64 16
markings. Even with more marking options, the number of PHBs in the core is generally kept small 17
(<8) to simplify configuration, management and is generally done as a one-time set-up when a link 18
is first installed. Core PHBs are enabled using a combination of queues, congestion management 19
and scheduling techniques and are normally set-up in a work preserving fashion.  That is, if one 20
PHB is not fully utilised, the unused capacity can be used by traffic with other PHBs.    21
Applying this to slicing. Each slice has an SLA based on overall and class bandwidth it receives. 22
This is enforced on ingress at the PEs. In this QoS model, core queues are shared between slices and 23
can be considered to be a soft form of slicing, but there is protection between slices through ingress 24
edge conditioning of traffic. This architecture it is well tried, and tested, is simple and scales and 25
able to support very tight SLAs.    26
 27
17.2.2.2 Dedicated queue architecture 28
This approach uses the same Diffserv QoS architecture, except one or more PHBs is exclusively 29
dedicated to a particular slice or slice type. An example could be a high priority PHB, which is 30
dedicated exclusively to the URLLC slice. At the same time other slices can share the remaining 31
PHBs. Given the small numbers of queues normally provisioned in the core (and available traffic 32
classes in an MPLS environment), this type of solution needs to be used with extreme care, however 33
it is a model used today successfully in production networks to support private line services which 34
run in a dedicated high priority queue, while simultaneously supporting other services using a 35
shared queue architecture.    36
 37
17.2.2.3 Dedicated links and queues per slice 38
This QoS approach achieves hard slicing by either dedicating core links or parts of a core link to a 39
specific slice. This can be achieved by one of the following approaches: 40
• Using dedicated links per slice in the transport network. This could be done using full 41
physical Ethernet interfaces or using Ethernet TDM technology, such as flexE, to create 42
dedicated Ethernet channels for different slices. Within these links, class-based queuing 43
would be used to support different traffic types associated with the slice. This approach 44
offers no ability to share unused bandwidth between slices as the links are hard partitioned. 45
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             121
• Using hierarchically scheduled bandwidth on core links. In this case core links would be split 1
using a shaper to govern the level of bandwidth available to each slice, with class-based 2
queuing within the shaped bandwidth. This approach does offer the ability to share unused 3
bandwidth between slices.    4
 5
In addition to dividing the core bandwidth up, mechanisms are required to guide slice traffic into 6
the links or shaped bandwidth associated with each slice. Slice traffic could be guided into the 7
appropriate links using a TE solution, such as flex-algo or SR-TE or alternatively the core routers 8
could be enhanced to schedule on a slice identifier within the packet. At this time there is discussion 9
on this subject but as yet nothing solid. Like with the previous example, this could support a small 10
number of slices but scaling up would be highly problematic with core provisioning, as well as edge 11
provisioning required when a slice is defined, changed or deleted. It is also a large departure in the 12
way QoS in core of packet networks has been managed in the past.     13
17.3 5G Services and slices 14
MP-BGP based L2 EVPN and L3VPNs are used to create slice instances. VPNs are very scalable in 15
terms of numbers and endpoints and allow different flexible connectivity models. This means slices 16
could be based on 5G service, customer or a combination of the two.  17
Each slice would have a VPN associated with it. By default, VPNs use the default IGP based 18
forwarding tables to forward traffic. However, VPN traffic can be directed, either by configuration 19
or using automated traffic steering techniques based on BGP “color extended community attribute” 20
to use traffic engineered paths. In this way, a URLLC slice could be directed to use a delay 21
optimised TE solution. Further control of the transport level connectivity within the VPN can be 22
achieved using Route Target filtering. For example, “Route Target” filtering could be used to stop 23
transport level connectivity between UPFs of different customers but allow O-CUs to communicate 24
with any UPF. 25
  26
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             122
18 Supporting mobile scenarios on a packet switched Xhaul 1
network  2
What is clear in discussions with operators is that although section 8 outlines individual use cases, 3
operator’s expect multiple use cases to be present in their transport network based on the need to 4
support 4G and 5G, maturity and timelines of the O-RAN specifications, vendor product readiness, 5
use cases and latency requirements. Figure 18-1, an adapted operator’s view, illustrates this well 6
with multiple 4G/5G architectures present along with Enterprise and consumer services.  7
 8
 9
 10
Figure 18-1: An adapted O-RAN operator’s view of mobile component placement 11
 12
In order to make this document generic to operators, simplification is required. For this reason, the 13
following physical topology and underlay transport solution is used as a baseline. Operators can 14
then use the information and customize to their specific requirements.  15
18.1 Physical network  16
Figure 18-2 illustrates physical network layout for a metro network and its connectivity to the 17
transport core. It consists of: 18
1. A collection of metro networks surrounding a transport core. Each metro network consists of 19
a number of access networks that are consolidated by an aggregation network towards to the 20
transport core.  21
2. All sites which interface to customer or mobile components supports Provider Edge 22
functionality for L2 and L3 overlay services.  23
3. Data centers or location for positioning mobile components potentially exist at the hub sites, 24
edge sites and in centralised locations.  25
4. Connectivity between routers is provided by line-rate point-to-point Ethernet connections 26
derived from dark fibre or WDM.  27
5. Access networks could be based on either small rings, star, chained or hub and spoke 28
topologies. To simplify the following discussion, it is assumed each cell site has a CSR 29

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             123
connected via a single high capacity dark fibre to the HSR in a star topology. Refer to 1
section 0 for more details on potential fiber access topologies.  2
6. The Aggregation network originates from the HSR and is based on either a ring or a hub and 3
spoke topology. For some scenarios the aggregation network may be split into a pre-4
aggregation and aggregation component. In this case, there is an intermediate layer of packet 5
switches and an additional layer of statistical multiplexing. (see greyed out router / switch in 6
Figure 18-2)  7
 8
    9
 10
 11
  12
 13
Figure 18-2: An example physical architecture 14
18.2 Logical underlay architecture 15
The logical underlay is illustrated in Figure 18-3 and consists of QoS enabled infrastructure that 16
either uses an IP/MPLS or an SRv6 underlay data and control plane. It provides any-to-any 17
connectivity between TNEs in the network, regardless of whether they reside in the same metro 18
network, a different metro network or the transport core and supports both shortest path routing and 19
traffic engineering. The underlay data planes, control plane and scaling mechanisms to enable this 20
for IP/MPLS and SRv6 are different and discussed in their respective chapters.  Both solutions have 21
the capabilities to support best effort and traffic engineered forwarding and the capability to create 22
discrete constrained based topologies using either traditional traffic engineering or flexible 23
algorithm which can be optimized on criteria such as shortest path, delay and bandwidth. 24

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             124
  1
 2
Figure 18-3: An example logical architecture 3
18.2.1 Underlay Quality of Service (QoS)  4
Refer to chapter 14 for fuller discussion on QoS and Annex D: Quality of Service fundamentals. 5
Underlay QoS refers to the scheduling used on the core or backbone links. All backbone links are 6
enabled with a combination of priority and class-based queueing. Assignment of traffic to different 7
queues within the core is based on marking within the packet header. In an MPLS backbone this is 8
based on the EXP bits or traffic class field, a three-bit field, giving a maximum number of core 9
behaviours of eight. In an IPv6 backbone this is based on the DSCP bits, a six-bit field, giving a 10
maximum number of 64 markings and potential behaviours.  11
  12
13
  14
Figure 18-4: Example Xhaul queue structure  15
 16

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             125
18.3  Service architecture  1
For mobile services it is proposed to use BGP based VPNs to overlay service functionality onto the 2
underlay transport network as described in section 0. 3
 4
NOTE: There is no reason this same transport infrastructure cannot support other VPN solutions 5
such as VXLAN or SD-WAN type solutions. 6
 7
All PEs and DCIs (which are also PEs) should be capable of supporting point to point Ethernet 8
services using EVPN VPWS services and L3 BGP VPN services described in section 0.  With these 9
two basic service overlays all the scenarios outlined in section 8 can be accommodated.  10
18.3.1 Automated VPN Traffic Steering 11
If required traffic from different VPNs or specific flows within an VPN can be steered into different 12
underlay transport forwarding planes based on coloring the MP-BGP VPN routes associated with 13
L2 and L3VPNs. This capability is described in Annex C. 14
18.4 Mobile services 15
There are many ways Fronthaul, Midhaul and Backhaul services could be provisioned onto the 16
transport network. Below are a set of assumptions and mechanisms used to illustrate how a packet 17
switched architecture can accommodate the scenarios outlined. These can easily be adapted based 18
on operator’s individual requirements. 19
18.4.1 Open Fronthaul 20
Figure 18-5 illustrates a physical and logical design that supports Open Fronthaul traffic. It is fairly 21
simple in nature, however, can easily be adapted based on operator designs and requirements. It 22
uses the underlay building blocks discussed earlier chapters.   23
 24
 25
Figure 18-5: Fronthaul physical and logical topology 26

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             126
18.4.1.1 Assumptions  1
1. 7.2x C/U planes connects to the transport network via a VLAN on a physical Ethernet 2
interfaces on O-RUs and O-DUs.  3
 4
2. 7.2x C/U planes traffic uses Ethernet / VLAN encapsulation option and not the optional 5
Ethernet / VLAN / IP encapsulation. 6
 7
3. 7.2x C/U planes uses an Ethernet service across the transport network to connect O-RUs to 8
O-DUs. This is via an Ethernet VPWS service using EVPN in the transport network.  9
 10
4. If the 7.2x C/U planes traffic used an Ethernet / VLAN / IP encapsulation, then the transport 11
network could use a L3 VPN service to transport the traffic between the O-RU and the O-12
DU, but this is not considered in the following sections. 13
 14
5. 7.2x M-plane connects to the transport network via a VLAN on a physical Ethernet. 15
 16
6. 7.2x M-plane uses an Ethernet / VLAN / IP encapsulation.  17
 18
7. 7.2x M-plane is operating in hybrid mode and uses an L3 VPN service across the transport 19
network to connect O-RUs, O-DUs and O-RAN NMS together. This is via an overlay MP-20
BGP based L3VPN.  21
 22
8. For the following discussion, the 7.2x C/U planes and M plane share a single physical 23
Ethernet interface on the O-RU towards the CSR with the C/U plane and M-Plane running in 24
different VLANs. 25
 26
9. Each O-RU has its own dedicated Ethernet port on the CSR. I.E There is no lower level 27
aggregation of statistical multiplexing component between an O-RU and the CSR. 28
 29
10. O-RU and O-DU set appropriate QoS markings for C/U and M-plane traffic that the 30
transport network can trust. In other words, the TNE does not need to classify different 31
components of Fronthaul for transport.  32
 33
18.4.1.2 Open Fronthaul physical design  34
1. 3 O-RUs per cell site.  35
 36
2. In the above example, each O-RU is connected via a dedicated Ethernet interface to a co-37
located CSR. This single Ethernet supports both the C/U planes and the M-plane presented 38
through two VLANs. The required capacity of the Ethernet link between the O-RU and the 39
CSR is primarily dependent on the O-RU’s radio capabilities. The requirements document 40
[15] provides some bandwidth examples for Open Fronthaul bandwidth. The expectation is 41
that the C/U plane traffic will be significantly larger than the M-Plane traffic, therefore these 42
Ethernet interfaces should be provisioned to deal with the peak 7.2x C/U-plane traffic of an 43
O-RU. It should be noted that unlike CPRI, the 7.2x C/U plane traffic loads are dependent on 44
user activity, so peak loading will only occur when the O-RU is running at full theoretical 45
capacity. 46
  47
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             127
3. In the above example, each CSR is connected using a single Ethernet link across dark fibre to 1
the HSR. Other underlay transport technologies and topologies are available, such as 2
redundant hub and spoke, chain or ring topologies in the access network. Regardless of the 3
WAN technology and topology employed, careful consideration needs to be given the 4
latency and jitter requirements of C/U plane traffic between the O-RU and the O-DU. 5
Transport considerations includes fibre distances between the O-RU and the O-DU, the 6
number of routers/switches traversed, link speeds and switching times of the intermediate 7
transport network equipment. (see section 10.1.1.2 for more details). As O-RAN 7.2x traffic 8
loads are dependent on user activity and in the example there are multiple O-RUs in the cell 9
site, the link capacity between the CSR and HSR does not need to be provisioned at the 10
theoretical sum of the O-RU’s peak rates because a statistical gain can be realised. The 11
bandwidth provisioning should be based on usage and the real achievable peak rates 12
associated with the O-RUs. The real achievable peak rates are dependent on factors such as 13
number and proximity of radios to each other and environmental factors. Precise 14
provisioning rules for 5G Fronthaul is subject to further investigation. 15
 16
18.4.1.3 QoS 17
1. The QoS scheme outlined in section 18.2.1, Figure 18-4 will be applied across the transport 18
network and is capable of supporting Fronthaul, Midhaul and Backhaul traffic running on 19
the same link. If an SRv6 underlay is to be used, then at a minimum the same queue 20
structure supported by appropriate operator selected DSCP marking should be used.  21
2. O-RU and O-DU set appropriate QoS markings for C/U and M-plane traffic that transport 22
network can trust.  23
3. In the above example, the C/U and M-plane traffic share the same physical interface, 24
separated using VLANs between the O-RU and CSR and between HSR and O-DU. In this 25
situation the mobile components (O-RUs and O-DUs) will need to support:  26
o An internal scheduling mechanism to prioritise C/U plane traffic over M-Plane 27
traffic onto the Ethernet interface  28
o Depending on O-RU→ CSR and HSR→ O-DU link speeds, the O-RU, O-DU, 29
CSR and HSR may need to support TSN frame pre-emption to control serialization 30
delay. (refer to section 10.1.1.2.1 for more details). Alternatively, if the O-RU or 31
CSR do not support TSN frame pre-emption or the O-DU or HSR do not support 32
TSN frame pre-emption, then separate physical interfaces could be used to separate 33
the C/U plane and M-plane traffic. 34
5. C/U and M-plane traffic from all O-RUs in the cell site share the same transport link 35
between the CSR and the HSR. In most scenarios where a CSR supports multiple radios 36
running fronthaul, this link will be 25Gbps or above and therefore does not really benefit 37
from TSN frame pre-emption. However, in some instances, the link between the CSR and 38
HSR may be lower speed and would benefit from TSN frame pre-emption. (see section 39
10.1.1.2.1 for more details) 40
6. C/U plane traffic must be prioritized over M-plane traffic on the WAN links. CSR and HSR 41
must be provisioned at both the link level and within the link level queuing infrastructure in 42
a manner such that C/U plane traffic is not dropped or experiences queuing delay in either 43
the CSR or HSR (or any additional Transport Network Equipment in the path between the 44
O-RU and the O-DU). M-plane traffic can experience a level of delay, so be configured to 45
use a lower priority queue or a queuing scheme such as Weighted Round Robin that reserves 46
a small amount of bandwidth for management traffic (see section 0 for an example) 47
 48
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             128
18.4.1.4 Services 1
1. C/U plane traffic uses an EVPN VPWS service to transport traffic between the O-RU and O-2
DU over the access network.  To simplify cell site VLAN provisioning, common VLANs 3
could be used between the O-RUs and the CSR and VLAN tag translation could occur on the 4
HSR to provide a unique interface / VLAN id per O-RU towards the O-DU. Depending on 5
the capabilities of the O-DU, EVPN has mechanisms that can provide service level resiliency 6
but this is dependent on the O-DU network implementation.    7
2. The M-plane traffic on the O-RU and O-DU connects, via a VLAN or physical interface to a 8
MP-BGP based L3 VPN service. This provides layer 3 connectivity between the 9
management components on the O-RU, O-DU and the O-RAN “Network Management 10
System” (NMS). “Route Target” filtering could be used to restrict transport level 11
connectivity between management entities. For example, the transport network could be 12
configured such that IP connectivity is only permitted between the NMS and the O-DUs and 13
O-RUs, while no IP connectivity is permitted between O-RUs.  14
 15
18.4.2 Non O-RAN Fronthaul 16
The assumptions, design approach and considerations discussed for the C/U planes in Open 17
Fronthaul are used for Non O-RAN Fronthaul. 18
18.4.3 Midhaul and Backhaul 19
Figure 18-6 illustrates a physical and logical design that could be employed to support Midhaul and 20
Backhaul traffic. It is a simple example but can easily be adapted based on operator designs and 21
requirements. It uses the underlay building blocks discussed earlier in the chapter.   22
 23
 24
Figure 18-6: Midhaul and Backhaul logical topology 25
 26

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             129
The following design assumptions have been made about the Midhaul / Backhaul components and 1
how traffic is presented and moved across the transport network. 2
18.4.3.1 Assumptions    3
1. Midhaul and Backhaul traffic share the same L3 VPNs. 4
2. There are dedicated and separate L3 VPNs for control plane traffic (N1, N2) and user plane 5
traffic (N3 and N9). 6
3. If legacy Backhaul is required, it shares the same L3 VPN transport services used by the 5G 7
EMBB Midhaul and Backhaul user and control plane infrastructure.  8
4. If there are multiple Midhaul and Backhaul transport slices, they could run in their own 9
independent L3 VPNs (see section 18.10)  10
5. Midhaul and Backhaul mobile components will either use a discrete independent VLAN or a 11
dedicated physical Ethernet to separate control and user plane traffic. For the following 12
discussion it is assumed they are presented via VLANs on a single Ethernet interface. 13
6. Midhaul and Backhaul mobile components which require connectivity to different transport 14
slices will either use a discrete VLAN or a dedicated physical interfaces per slice. For the 15
following discussion it is assumed they will be presented via VLANs. 16
7. The transport and RAN teams have collaborated on Midhaul / Backhaul QoS marking and 17
packets emerging from O-DUs, O-CUs and UPFs have a DSCP marking appropriate to their  18
5G forwarding requirements, and the TNEs can trust these markings.  19
8. N6 networks associated with different customers, applications or use cases will use L3 20
VPNs.  21
  22
Note: Other service designs could be considered, for example a design where only Ethernet services 23
are used from the cell site and the hub sites and L3 VPN services are built at the hub sites. The 24
primary reason for the choice outlined above is to make the service orchestration as simple as 25
possible, so services only need to be configured where the mobile clients connect to the transport 26
network and no intermediate stitching points within the transport network are needed.   27
18.4.3.2 Midhaul and Backhaul physical topology  28
1. O-DUs connect, via Ethernet interface/interfaces, to a co-located HSR or CSR. The Ethernet 29
interface/interfaces support Midhaul control and user plane traffic via two discrete VLANs. 30
2. O-CUs connect, via Ethernet interfaces to the transport network. The O-CU Ethernet 31
interface/interfaces support Midhaul and Backhaul control and user plane traffic via two 32
discrete VLANs.  33
3. The UPFs connect, via Ethernet interfaces to the transport network. The UPF connects to the 34
common control and user plane Midhaul/Backhaul VPNs.  35
4. The location of the O-CUs and UPFs is service dependent and the choice of the operator. 36
Additionally, these components may be virtualized or containerized and reside in a data 37
center. In these scenarios, orchestration techniques maybe be required to connect the 38
virtualized O-DU, O-CU and UPF components in the data centers to the correct WAN 39
L3VPNs.   40
5. 5G Core components connect via Ethernet interfaces to the transport network in their 41
respective locations. These components may be virtualized or containerized and reside in a 42
data center. In these scenarios, orchestration techniques are required to connect the 5G 43
control plane entities in the data centers to the Midhaul and Backhaul control plane WAN 44
L3VPNs.   45
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             130
6. O-CU can reside in the same location as the O-DUs it controls, therefore Midhaul traffic 1
never leaves the site and is switched between the two entities by the either the CSR or HSR. 2
Alternatively, the O-CU can reside in a different location to the O-DUs it controls. In this 3
case the Midhaul traffic traverses the WAN.      4
7. HSRs provides the boundary between the access and aggregation networks. Connectivity to 5
the aggregation network will typically use high capacity Ethernet interfaces or bundles of 6
high capacity Ethernet interface. The topology of the aggregation network is normally 7
resilient, but topologies vary based on operator choices and constraints, with hub and spoke 8
and rings very common.  9
8. Midhaul and Backhaul user plane traffic levels are dependent on UE usage, the 10
characteristics of the radio and the number the proximity of radios to each other. The O-RAN 11
Transport Requirements document [15] outlines some of the provisioning rules widely used 12
in planning 4G Backhaul networks. Although not directly applicable to the 5G, they provide 13
a good starting point in terms. Longer term network capacity planning should be done based 14
on monitoring and modelling the live network.  15
18.4.3.3 QoS 16
1. O-CU and UPFs mark the packets they produce based on the behaviour required for the 17
mobile data the carry.  18
2. Most Midhaul and Backhaul traffic is far more tolerant to delay, and jitter compared with 19
Fronthaul traffic. The exception could be some URLLC based use-cases. However, for most 20
5G use-cases fibre propagation delay, number of hops, transport equipment switching times 21
and serialization delays are not of a huge issue when designing Midhaul and Backhaul 22
networks.   23
3. In some scenarios, particularly when some forms of slicing are considered then edge QoS in 24
the form of ingress and egress H-QoS will be required.   25
18.4.3.4 Services 26
1. Two L3 VPNs are created; the first is Midhaul/Backhaul control plane traffic and the second 27
is Midhaul/Backhaul user plane traffic. O-DUs, O-CUs, UPFs connects to both L3VPNs. The 28
5GC connect only to the control plane L3VPN. If required “Route Target” filtering could be 29
used to restrict transport level connectivity between mobile entities. 30
18.5  Scenario 1 and 5  31
Both scenario 1 and 5 represent a full two split C-RAN architecture. The main difference is the 32
location and proximity of the O-DUs and the O-CUs to each other. In scenario 1 they are contained 33
in a single hub site location, in scenario 5 they are split across different locations.   34
 35
At the high level the basic service layout for scenario 1 and 5 is shown in Figure 18-7 and Figure 36
18-8 with a more detailed layout shown in Figure 18-9. 37
 38
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             131
 1
 2
Figure 18-7: Scenario 1 layout 3
 4
 5
 6
Figure 18-8: Scenario 5 layout 7

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             132
 1
 2
Figure 18-9: L2 / L3 service layout for scenario 1 and 5 3
 4
18.6 Scenario 2 5
Scenario 2 represent a full two split C-RAN architecture with the O-RU and O-DU at the cell site 6
and the O-CU at the hub site and is shown in Figure 18-10.  7
 8
  9
 10
Figure 18-10: L2 / L3 service layout for scenario 2 11
Notes on the design:  12
1. Fronthaul traffic is restricted to the cell site with the CSR providing a local L2 x-connect 13
function to get C/U plane traffic between the O-RUs and the O-DU. 14
2. The Midhaul/Backhaul control and user plane L3 VPNs need to be extended to the cell site 15
router. 16
3. In above diagram the O-DU has two physical interfaces with one supporting Fronthaul 17
traffic and the other supporting, via VLANs, Midhaul and Backhaul control plane, user 18

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             133
plane and Open Fronthaul M-plane traffic. In this instance TSN frame pre-emption is not 1
really a consideration. If the Midhaul, Backhaul and Fronthaul traffic shared the same 2
physical interface on the O-DU and the CSR, then depending on interface speeds, TSN 3
frame pre-emption may need to run between the O-DU and the CSR.  4
 5
18.7 Scenario 3 5G C-RAN with legacy D-RAN 6
18.7.1 Scenario 3a 7
Scenario 3a is a full two split C-RAN architecture with the O-RU at the cell site and O-DU at the 8
hub site sitting alongside an existing 4G D-RAN architecture and is shown in Figure 18-11. O-CU, 9
UPF and 5GC location are largely immaterial but they do need connectivity to the appropriate 10
L3VPNs.   11
 12
  13
 14
Figure 18-11: L2 / L3 service layout for scenario 3a 15
 16
Notes on the design:  17
1. Open Fronthaul traffic runs over the access network to an O-DU located in the Hub site 18
using a VPWS EVPN service.  19
2. The Midhaul/Backhaul control and user plane VPNs extend to the cell site to support the 20
existing 4G D-RAN solution.  21
3. The QoS design in the access network needs to be able to cater for the requirements of the 22
Open Fronthaul when D-RAN Backhaul traffic is sharing the same physical link.   23
 24
18.7.2 Scenario 3b 25
Scenario 3b is a full two split C-RAN architecture with the O-RU and O-DU at the cell site and the 26
O-CU located at the hub site sitting alongside an existing D-RAN architecture and is shown in 27
Figure 18-12 28

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             134
   1
 2
Figure 18-12: L2 / L3 service layout for scenario 3b 3
 4
Notes on the design:  5
1. The only difference between scenario 2 is that the legacy D-RAN equipment also connect to 6
the Midhaul/Backhaul control and user plane VPNs at the cell site. 7
18.8 Scenario 4 5G C-RAN with RoE mappers 8
Scenario 4 is a full two split C-RAN architecture with O-RAN 7.2x and non O-RAN Fronthaul 9
protocols running across the access network and is shown in Figure 18-13.    10
 11
Figure 18-13: L2 / L3 service layout for scenario 4 12
 13
Notes on the design:  14
1. O-RAN 7.2x C/U plane and non O-RAN Fronthaul packet traffic (between RoE mappers) 15
use VPWS EVPN services. 16

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             135
2. The expectation is both the O-RAN 7.2x C/U plane traffic and the non O-RAN Fronthaul 1
traffic will have similar delay and jitter characteristics so will run in the same traffic class 2
which is priority queued.  3
3. The vendor of the RoE mapper function will need to provide details of traffic rates and 4
whether they are fixed or vary depending on user data. The network planner will need to 5
take this into account when capacity planning the access links between the CSR and the 6
HSR.  7
 8
18.9 Scenario 6 5G C-RAN with distributed UPF 9
Scenario 6 includes the insertion of distributed UPFs in the architecture. The solution is illustrated 10
in Figure 18-14 and is easily to achieve by simply connecting the distributed UPF to the 11
Midhaul/Backhaul control and user plane L3VPNs. All necessary transport routing will occur 12
automatically through the L3VPN control plane. 13
   14
 15
Figure 18-14: L2 / L3 service layout for scenario 6 16
18.10 Scenario 7 Slicing  17
Like the previous scenarios, the slicing use case is best illustrated with an example. For simplicity, 18
the example concentrates only on the Midhaul/Backhaul user plane slicing and is illustrated using 19
only Backhaul components (O-CU and UPF). The same approach could easily be extended to create 20
slices in a Midhaul/Backhaul control plane based on 5G services, customers or a combination of the 21
two. The nature of slicing in Open Fronthaul is an area of debate but from a transport perspective 22
could be provided using a VPWS circuit per slice.   23
 24
The example slice requirements are:  25
 26
1. An MNO wishes to offer an URLLC, eMBB and MMTc services to its customers.  27
2. Customer’s select the base 5G service and can be: 28
a. Totally isolated from routing perspective from other customers. For example, a 29
private infrastructure that includes private O-CUs and private UPFs. 30

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             136
b. Partially isolated from other customers. For example, a common/shared O-CU 1
infrastructure but private UPFs per customer. 2
c. A mix of the two.    3
3. URLLC services are delay optimized across the transport network between mobile clients. 4
eMBB and MTMc services use the default forwarding path, calculated by the IGP, across the 5
transport network between mobile clients.    6
4. All three 5G services support traffic with different QoS requirements. Clearly, it is 7
anticipated that the URLLC traffic will have more high priority traffic than eMBB and 8
MTMc traffic but each 5G service will have a traffic mix requiring different forwarding 9
behaviours in the transport network.  10
5. Placement of mobile components, such as UPFs, will depend on the 5G service and customer 11
preference but it needs to be very flexible. 12
6. Customer instances are presented to the transport network via VLANs on the UPFs.  13
7. O-CU resources can be dedicated to a service and a customer through a VLAN or O-CUs can 14
be shared between customers in which case a VLAN per 5G service is presented to the 15
transport network.  16
18.10.1 Slice design 17
A slice design is illustrated in Figure 18-15. It is provided as an example and other models can be 18
built using the extensive packet switching toolset.  19
 20
  21
 22
Figure 18-15: Scenario 7 – simple slicing solution based on multiple forwarding planes, 23
Diffserv based QoS and L3VPNs 24
 25
• Two underlay forwarding planes are built onto the packet switched transport layer. The 26
default forwarding plane, based on the IGP, and a delay and reliability optimized forwarding 27
plane based on flex-algorithm. 28

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             137
• The delay and reliability optimized forwarding plane is dedicated to URLLC services and 1
the default forwarding plane is shared by eMBB and mMTC services.   2
• For general usage each 5G service has a MP-BGP based L3VPN associated with it. So, there 3
will be a MP-BGP based L3VPN for the URLLC, eMBB and mMTC services.  4
• Traffic associated with the URLLC L3VPN is mapped to the delay optimized forwarding 5
plane using destination based automated steering.  6
• Traffic associated with the eMBB and the mMTC VPNs uses the default forwarding plane. 7
• Mobile components associated with URLLC, eMBB and mMTC services connect to their 8
respective L3VPN. This could be via a dedicated Ethernet interface or via a VLAN.  9
• To enforce appropriate slice SLAs and ensure slice isolation, traffic arriving from the mobile 10
clients on the transport network is conditioned and potentially remarked on ingress. 11
Typically, this would be via a H-QoS ingress policing / shaping function which considers 12
both overall input bandwidth and class bandwidth.  13
• To enforce appropriate slice SLAs and ensure slice isolation, traffic egressing from the 14
transport core to mobile clients is shaped and scheduled. Typically, this would be via a H-15
QoS egress shaping / scheduling function which governs overall bandwidth and class 16
scheduling within the overall bandwidth.   17
• Connectivity between mobile components in each 5G service VPN is determined by Route 18
Target filtering. In some instances, the MNO may want any to any connectivity within the 19
Backhaul 5G service slice, for example a general eMBB consumer service. This achieved by 20
every “Virtual Routing and Forwarding”(VRF) instance associated with the 5G service 21
importing and exporting the same Route Targets. In other instances, the MNO may wish to 22
restrict connectivity in the Midhaul/Backhaul 5G service VPN at the transport layer. For 23
example: 1) In the URLLC VPN UPFs associated with customer A cannot communicate 24
with UPFs associated with customer B; 2)  In the mMTC VPN customer A’s UPFs can only 25
communicate with a subset of the O-CUs, while customer B’s UPFs can communicate with 26
another subset and customer C’s UPFs can communicate with all O-CUs. 3) Customer D 27
wants a completely private URLLC services consisting of private O-CUs and UPFs. All 28
these models can be achieved through the use of RTs to color VPN routes when they are 29
exported to MP-BGP from VRFs and filtering based on RTs when they are imported from 30
MP-BGP into VRFs.  31
 32
Note: Many other options are available in a packet switched architecture for supporting slicing at 33
the transport layer.  34
  35
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             138
Annex A: Overview of “Segment Routing” (SR)  1
This Annex gives an overview of segment routing based on the Segment Routing Architecture 2
defined in RFC8402 [114] and related drafts. It has been included as an Annex because the main 3
document describes two architecture/designs that utilise Segment Routing: SR-MPLS and SR for 4
IPv6 or SRv6. There are differences between the two implementations and capabilities of SR when 5
using an MPLS and IPv6 data plane but there are also many similarities in the basic architectural 6
concepts and the motivation for using the technology. Consequently, this Annex describes the basic 7
SR architecture and the “logical architecture sections” in the main document goes into more detail 8
on the technologies and outlines implementation and design considerations when using the 9
respective technologies.  10
This Annex utilises many different information sources and, in some instances, takes content very 11
literally from these sources. Information sources include IETF documents, research and vendor 12
documents.  13
  14
Background 15
The Internet and role of packet switching has changed over the last couple of decades and continues 16
to evolve rapidly. This is no better illustrated than by the 5G transport requirements which 17
anticipates billions of endpoints, massive numbers of antenna, huge bandwidths and the ability to 18
concurrently support different 5G services. This evolution is challenging existing Internet protocols, 19
equipment and increasing the operational complexity of the network. Segment Routing and related 20
technology evolutions aims to address these challenges through protocol simplification, removal of 21
state from the network and embracing technology advances such as Software Defined Networking.     22
Segment Routing  23
Segment Routing (SR) is based on the loose Source Routing concept. A node can include an 24
ordered list of instructions in the packet headers. These instructions steer the forwarding and the 25
processing of the packet along its path in the network. Single instructions are called segments, a 26
sequence of instructions is called a segment list or an SR Policy. Each segment can enforce a 27
topological requirement (e.g. pass through a node or an interface) or a service requirement (e.g. 28
execute an operation on the packet). The term segment refers to the fact that a network path towards 29
a destination can be split in segments by adding intermediate waypoints. The segment list can be 30
included by the original source of the packet or by an intermediate node. When the segment list is 31
inserted by an intermediate node, it can be removed by another node along the path of the packet, 32
supporting the concept of tunneling through an SR domain from an ingress node to an egress node.  33
The implementation of the Segment Routing Architecture requires a data plane which is able to 34
carry the segment lists in the packet headers and to properly process them. Control plane operations 35
complement the data plane functionality, allowing segment allocation (i.e. associate a segment 36
identifier to a specific instruction in a node) and distribution of segment identifiers within an SR 37
domain. 38
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             139
 1
 2
Figure 0-1: Segment Routing protocol stack and path capabilities  3
 4
Segment Routing architectural principle 5
There is a single SR architecture which is defined in RFC8402 [114]. This defines the general 6
concepts of SR and is independent from the specific data plane. Currently, there are two 7
instantiations of the SR architecture designed and implemented in production code, SR over MPLS 8
(SR-MPLS) and SR over IPv6 (SRv6). 9
 10
The architecture relies on segments identifiers (SIDs) and SID lists. The SID represents a single 11
instruction which can be related to how the packet is forwarded or a function that needs to be 12
performed on a packet. A SID lists or the SR policy is a collection of SIDs or instructions to be 13
performed on the packet as it traverses the SR domain. Typically, a SID list is imposed on the 14
packets as it enters the SR domain and is either empty or completed by the time it leaves an SR 15
domain.       16
There are two basic types of segment, global segments which correspond to instructions that are 17
globally valid in an SR domain and local segments which correspond to instructions that are only 18
valid within a single node. Some of the key segment types are: 19
1. IGP Prefix and IGP node segments: These are global SIDs and are instructions on how to 20
forward a packet towards a destination IP network or destination IP node. As the name 21
suggests these are conveyed by IGP and can be used by any node in the SR domain to send 22
traffic to a node or IP prefix.   23
2. Adjacency segments: These are local SIDs which identify the links available on that node.  24
3. Anycast segments: These are a special IGP-prefix segment that corresponds to an anycast 25
prefix. ie a prefix advertised by more than one router. Anycast and anycast segments are 26
commonly used to provide high availability or load balancing solutions.  27
4. Binding Segments: These are a single SID that is associated with a segment list or SR 28
policy. This means the node that processes the Binding SID replaces the single segment with 29
a segment list.  30
 31
Using Figure 0-1 as an example, the basic SR architecture can support:  32
1.  A dynamically locally computed forwarding path between an ingress and egress point 33
within the SR domain by the ingress node either imposing the node SID of the egress node 34
or the prefix SID of the destination prefix. Typically, a link-state IGP or an EGP routing 35
protocol, running locally on the routers calculate these types of paths and they exhibit the 36

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             140
behaviour of normal routing protocols such “Shortest Path Forwarding” and “Equal Cost 1
Multi Pathing (ECMP)  2
2. A fully deterministic path between the ingress and egress points. In this case the SR policy 3
or SID list includes all nodes on the path and if required adjacency SIDs to select links 4
between nodes.   5
3. A combination of the two.  The SR architecture can build a path between an ingress and 6
egress node that combines dynamically computed forwarding and explicitly routed 7
forwarding. 8
 9
Segment Routing data plane 10
Two data plane instantiations are designed and implement: SR over MPLS (SR-MPLS) and SR over 11
IPv6 (SRv6). 12
SR-MPLS 13
The MPLS data plane (SR-MPLS) is specified in RFC 8660 “Segment Routing with MPLS data 14
plane”, In the case of SR-MPLS, Segment Routing does not require any change to the MPLS 15
forwarding plane. An SR Policy is instantiated through the MPLS Label Stack and the Segment IDs 16
(SIDs) of a Segment List are inserted as MPLS Labels. The classical forwarding functions available 17
for MPLS networks allow implementing the SR operations.  18
IPv6 data plane (SRv6)  19
For the IPv6 data plane (SRv6), a new type of IPv6 Routing Extension Header, called Segment 20
Routing Header (SRH) has been defined in RFC 8754 IPv6 Segment Routing Header (SRH) [124]. 21
The SRH contains the Segment List (SR Policy) as an ordered list of IPv6 addresses: each address 22
in the list is a SID. A dedicated field, referred to as Segments Left, is used to maintain the pointer to 23
the active SID of the Segment List. This SRH format and basic operation of the SRH is illustrated 24
in Figure 0-2. 25
 26
Figure 0-2 SRH header and interaction between segments left and SID list  27
 28
To explain the SRv6 data plane, there are three categories of nodes: Source SR nodes, Transit nodes 29
and SR Segment Endpoint nodes. A Source SR node corresponds to the headend node, it can be a 30

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             141
host originating an IPv6 packet, or an SR domain ingress router encapsulating a received packet in 1
an outer IPv6 header.  2
 3
Figure 0-3  Packet and SRH construct for IPv4 payload for traffic following nodes 1,2,5 4
 5
In Figure 0-3, which considers the latter case the source SR node (S1) is an edge router that 6
encapsulates a packet (which can be IPv6, IPv4 or an Layer 2 frame) into an outer IPv6 packet and 7
inserts the SR Header (SRH) as a Routing Extension Header in the outer IPv6 header. The traffic is 8
destined for S5 and for policy reasons the operator wishes to send the traffic via S2 and S3 which is 9
not the shortest IGP path. To implement this policy, a segment list is composed of S2 and S5. S3 10
does not need to be specified because the shortest path from S2 to S5 is via S3. To implement this 11
SR policy and assuming the use of a reduced SRH (for details see RFC8754) the packet details are 12
shown in Figure 0-3. 13
  14
In addition to the basic operations, the SRv6 Network Programming model [143] describes a set of 15
functions that can be associated to segments and executed in a given SRv6 node. Examples of such 16
functions are: different types of packet encapsulation (e.g. IPv6 in IPv6, IPv4 in IPv6, Ethernet in 17
IPv6), corresponding decapsulation, lookup operation on a specific routing table (e.g. to support 18
VPNs). The list of functions described in [143] is not meant to be exhaustive, as any function can be 19
associated to a segment identifier in a node. Obviously, the definition of a standardized set of 20
segment routing functions facilitates the deployment of SR domains with interoperable equipment 21
from multiple vendors.  22
Segment Routing control plane 23
The control plane is responsible for calculating, building the connectivity graphs and implementing 24
the SR policies within the network. Segment Routing can support distributed, centralised or hybrid 25
control plane architectures. See Figure 0-4 for more details. 26
 27

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             142
 1
Figure 0-4 Distributed, centralised and hybrid control planes 2
 3
In a distributed control plane environment, the transport nodes independently interact with each 4
other to convey routing information and independently make decisions to set-up and enforce SR 5
policies. This is achieved through a combination of IGP and EGP protocols running between the 6
transport nodes. 7
In a centralised control plane environment, central SDN controllers have full knowledge of the 8
network topology and calculates SR policies centrally. These are then programmed onto the TNEs 9
in the network.  10
The hybrid approach splits the responsibilities, the TNEs in the network support some functions, 11
such as building the link state database, calculating the intra-domain routing tables, monitoring the 12
links to attached nodes, and rapidly recovering in the event of failure. At the same time the central 13
SDN controllers has a feed from the network and is used for functions that require a network wide 14
view such as transport optimization and inter-domain routing.  15
 16
MPLS-SR distributed control plane 17
The MPLS-SR distributed control plane utilises ISIS or OSPF routing protocols with extensions to 18
advertise the different types of IGP segments (prefix, node, adjacency, any cast) between TNEs. 19
 20
IPv6 distributed control plane  21
For the IPv6 data plane, the process of advertising the IGP-prefix, IGP-node and IGP-anycast 22
segments is simplified due to the use of IPv6 addresses as SIDs. In particular, there is no need to 23
extend the IGP routing protocols to distribute these segment types, because they are IPv6 prefixes 24
natively distributed by the routing protocols. This means that the Control Plane for SRv6 can use 25
the regular IPv6 link state IGP routing protocols (OSPFv3, ISIS) to support the basic operations, 26
while extensions are still needed ([122][123]) to distribute IGP-Adjacency segments and other SR 27
configuration information.  28
Centralised control plane 29
Although much emphasis has been made of fully centralised control plane models in academia for 30
path computation, they have not made their ways into production large scale WAN designs due to 31
the distributed nature of the transport nodes and the need for the transport nodes to communicate at 32

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             143
all times with the central controller. As a consequence, most WAN SDN designs today are hybrid in 1
nature, where intelligence is split between the TNEs and central SDN controllers. 2
   3
Note: This discussion above only considers path computation element of a central controller. There 4
are many aspects of a central SDN controller including orchestration, data analytics etc.  5
Hybrid control plane  6
The hybrid control plane is combination of distributed control plane and centralised SDN control 7
plane functionality. Like the central control plane model there is no universal architecture for a 8
hybrid control plane design, other than some functions are provided centrally and others occur 9
locally on the TNEs. One of the most common hybrid control plane designs is where the central 10
SDN controllers provides a “Path Computation Element” capability. In this model the PCE is 11
responsible for path computation and inter-domain routing with the distributed control planes 12
running on the TNEs, responsible for building the link state database, calculating the network graph 13
and building the shortest path forwarding tables. In this scenario there is generally a tight 14
relationship between the two components.  15
 16
A. The TNEs exchange topology information, creates the link-state database and computes the 17
shortest path forwarding tables.  18
B. The centralised control plane component needs to get visibility of the distributed protocol’s 19
link-state database to understand the current topology. Some of the common approaches are:    20
a. The SDN controller participates in the IGP and as a consequence builds its own link-21
state database and network graph. 22
b. The SDN controller extracts IGP databases from the network using proprietary 23
mechanisms. 24
c. BGP-LS (BGP link state) runs between one or more TNEs and centralised 25
controllers. In this way the centralised controller gets visibility of the network 26
through BGP-LS.  27
C. In many instances the centralised control plane needs to gather other information about the 28
network, for example link loading. At this time there is no single standard for collecting this 29
information. Common mechanisms include SNMP, telemetry feeds and operational data from 30
Yang models residing on the devices.  31
 32
D. The central controller and the TNEs also need to communicate when, either a TNE requests a 33
service from the central SDN controller, or the central SDN controller wants to push an explicit 34
policy to a TNE. An example would be if a TNE needs build an SR policy and wants to use the 35
services of a PCE. The transport node and the PCE need to communicate, some of the common 36
protocols used for this type of communications include:   37
a. Network Configuration Protocol  38
b. PCE (PCEP) 39
c. BGP 40
d. Openflow (SR-MPLS) 41
  42
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             144
Annex B: IETF Ethernet Virtual Private Networks 1
IETF Ethernet Virtual Private Network (EVPN) is a service supporting transportation of Ethernet 2
frames across a transport network encapsulated in MPLS or IP. There are multiple flavors of IETF 3
EVPN service as outlined Table 6. 4
 5
Service Type Standardization status
Layer 2 VPN
E-LAN
VLAN-based RFC 7432 [98]: BGP MPLS-Based Ethernet
VPN VLAN-bundle
VLAN-aware bundle
E-Tree
VLAN-based RFC 8317 : Ethernet-Tree (E-Tree) Support in
Ethernet VPN (EVPN) and Provider Backbone
Bridging EVPN (PBB-EVPN)
VLAN-bundle
VLAN-aware bundle
E-Line
(VPWS)
VLAN-based

RFC 8214 [107]: Virtual Private Wire Service
Support in Ethernet VPN
draft-ietf-bess-evpn-vpws-fxc [128]: EVPN
VPWS Flexible Cross-Connect Service VLAN-bundle
Layer 3 VPN draft-ietf-bess-evpn-prefix-advertisement
[130]: IP Prefix Advertisement in EVPN
Table 6: EVPN service classification 6
 7
The two major building blocks of EVPN are: 8
• Control plane, based on Border Gateway Protocol (BGP), to distribute all necessary 9
information required for proper EVPN operation 10
• Data plane, with different underlay transport options, to suit various requirements: 11
o MPLS (LDP, RSVP, BGP-LU, MPLSoUDP, SR, SR-TE) 12
o IP (VxLAN, SRv6) 13
EVPN E-LAN service 14
EVPN E-LAN service is a Layer 2 multipoint-to-multipoint service. In essence, EVPN E-LAN 15
emulates Layer 2 switch behavior, with attachment circuits (ACs) of this single emulated Layer 2 16
switch present on multiple Provider Edge (PE) routers, which are placed in different network 17
locations, as outlined in Figure 0-1 18
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             145
 1
 2
Figure 0-1: EVPN E-LAN service  3
 4
Similar to ordinary Layer-2/Layer-3 switch, EVPN E-LAN service provides 5
• MAC learning capability (globally or per VLAN) 6
• capability to carry Ethernet frames without VLAN tag, as well as with single or double 7
VLAN tags, including VLAN translation 8
• single-homed, as well as multi-homed (both single-active and all-active) attachment circuits 9
capability 10
• BUM optimization mechanisms (like for example APR proxy/suppression, IGMP proxy, 11
etc.) 12
• handover to Layer-3 domain via integrated routing and bridging (IRB) operation and more. 13
EVPN E-Tree service 14
EVPN E-Tree service is an enhancement of EVPN E-LAN service, which allows to restrict certain 15
communication patterns, creating in essence a point-to-multipoint service model. As outlined in 16
Figure 0-2, each attachment circuit is designated as root (R) or leaf (L). As per EVPN E-Tree 17
definition, only rootroot and rootleaf Layer 2 communication is permitted, while leafleaf 18
Layer 2 communication is blocked. 19
BGPBGP
CE1
BGP
PE1 PE3
PE4
PE2
BGP
CE3
CE4
CE2
CE1
 CE3
CE4
CE2

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             146
 1
Figure 0-2 EVPN E-Tree services 2
 3
At a high level, EVPN E-LAN service can be considered as a special case of EVPN E-Tree service, 4
with all attachment circuits designated as root. In fact, many vendors use the same configuration 5
constructs for EVPN E-LAN and EVPN E-Tree, defaulting the attachment circuit to a root, with 6
some configuration knobs turning it to a leaf. 7
EVPN E-Line service 8
EVN E-Line, called as well EVPN Virtual Private Wire Service (VPWS), is point-to-point Layer 2 9
service, without MAC learning. Ethernet frame received from an end device connected via single-10
homed or multi-homed attachment circuit, is transported over this point-to-point service to the 11
remote end device, connected again via single-homed or multi-homed attachment circuit. Since it is 12
pure point-to-point communication, no MAC learning is required, and thus MAC learning is not 13
part of EVPN E-Line (VPWS) architecture. 14
 15
 16
 17
Figure 0-3 EVPN E-Line (VPWS) services 18
BGPBGP
CE1
BGP
PE1 PE3
PE4
PE2
BGP
CE3
CE4
CE2
R
R
L L
R
CE1
 CE3
CE4
CE2
R
L
R
L
CE1
 PE1 PE3
PE4
PE2
CE3
CE4
CE2
CE1
 CE3
CE4
CE2

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             147
 1
EVPN VLAN-based service 2
EVPN architecture uses three implementation constructs defined on the Provider Edge (PE) 3
devices: 4
• EVPN Instance (EVI), called as well MAC-VRF. Both EVI and MAC-VRF will be used in 5
this document interchangeably. 6
• Bridge (MAC learning) domain/table 7
• Broadcast (flooding) domain (i.e. VLAN) 8
 9
Depending how these building blocks are used, EVPN architecture defines 3 options for 10
constructing EVPN services: 11
• EVPN VLAN-based service (applicable to EVPN E-LAN, EVPN E-Tree and EVPN E-Line 12
services) 13
• EVPN VLAN-bundle service (applicable to EVPN E-LAN, EVPN E-Tree and EVPN E-14
Line services) 15
• EVPN VLAN-aware bundle (applicable to EVPN E-LAN and EVPN E-Tree services only) 16
 17
In EVPN VLAN-based service, each EVI contains single bridge (MAC learning) domain/table, 18
which in turn contains only single VLAN (single broadcast/flooding domain). This is the simplest 19
EVPN service with 1:1 mapping between VLAN and EVI as outlined in 0 20
 21
 22
Figure 0-4 EVPN VLAN-based services (E-LAN, E-Tree, E-Line) 23
 24
EVPN VLAN-based service allows for VLAN tag translation, where incoming VLAN tag (for 25
example VLAN 2) is translated to VLAN 12. Additionally, EVPN VLAN-based service provides 26
the option to carry the Ethernet frame with the VLAN tag across EVPN network, or to strip the 27
VLAN tag before carrying the Ethernet frame across EVPN network.  28

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             148
EVPN VLAN-bundle service 1
The biggest challenge with basic EVPN VLAN-based service is scaling. In EVPN VLAN-based 2
service each VLAN consumes dedicated EVI, thus the maximum numbers of VLANs that can be 3
served is limited by the supported EVIs on given hardware platform. EVPN VLAN-bundle service 4
addresses this scaling limitation by bundling multiple VLANs into single bridge (MAC learning) 5
domain/table and single EVI (MAC-VRF), as outlined in Figure 0-5 6
 7
 8
Figure 0-5 EVPN VLAN-bundle service (E-LAN, E-Tree, E-Line) 9
 10
VLAN bundling allows dramatic reduction of MAC-VRFs that otherwise would be needed on the 11
router. Therefore, EVPN VLAN-bundle service is typically used in high-scale environment, with 12
large number of VLANs or VLAN combination (QinQ) that must be transported with EVPN 13
service. 14
 15
This scaling improvements brings improvements, as well some restrictions for service deployment, 16
namely: 17
• the original VLAN tag must be carried with frame across EVPN network and VLAN 18
translation is not supported 19
• MACs cannot be re-used across different VLANs 20
 21
The first restriction is related to the fact, that underlying transport identification (i.e. MPLS label 22
with MPLS underlay transport, Function:Argument with SRv6 underlay transport) identifies the 23
bridge domain. Therefore, the underlying transport identification cannot be used to distinguish 24
Ethernet frames - received over EVPN network - as belonging to different VLANs, if VLAN tag is 25
stripped before sending Ethernet frames across EVPN. And this differentiation is required to 26
maintain per-VLAN broadcast/flooding restrictions. 27
 28
Second restriction is the straight result of bundling multiple VLANs in single bridge (MAC 29
learning) domain/table. All these bundled VLANs use the same MAC table, therefore all MACs 30
must be unique across all bundle VLANs. 31
EVPN VLAN-aware bundle Service 32
Restrictions of EVPN VLAN-bundle services are removed by EVPN VLAN-aware bundle service. 33
EVPN VLAN-aware bundle service is a trade-off between EVPN VLAN-based service (poor 34
scaling, but simple service without restrictions related to VLAN translation or MAC learning) and 35

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             149
EVPN VLAN-bundle service (good scaling, but restricting VLAN translation support and requiring 1
MAC uniqueness). 2
EVPN VLAN-aware bundle brings back separate bridge (MAC learning) domain/table per VLAN, 3
still keeping the option to bundle multiple VLANs with single EVI (MAC-VRF), as outlined in 4
Figure 0-6. 5
 6
 7
 8
Figure 0-6 EVPN VLAN-aware bundle service (E-LAN, E-Tree) 9
 10
Scaling wise EVPN VLAN-aware bundle service sits between EVPN VLAN-based service and 11
EVPN VLAN-bundle service, optimizing number of required EVIs (MAC-VRFs), while keeping 12
the same number of bridge (MAC learning) domain as EVPN VLAN-based service. 13
 14
Table 7 summarizes different EVPN service models. 15
 16
Characteristic VLAN-
based
VLAN-
bundle
VLAN-
aware
bundle
Broadcast domains (VLANs) per EVI =1 1 1
Bridge (MAC learning) domains per EVI =1 =1 1
MACs must be unique across VLANs no yes no
VLAN translation allowed yes no yes
VLAN tag carried over yes/no yes yes
Ethernet Tag (BD) ID on BD scoped routes 0 0 ≠0
Port based service support no yes yes
Table 7: EVPN VLAN service types comparison 17
 18
MEF/EVPN service mapping 19
MEF 6.3 (Subscriber Ethernet Service Definitions) defines Ethernet services commonly used in 20
Ethernet metro aggregation architectures. Table 8 provides the mapping between MEF service 21
definitions and IETF EVPN service implementations. 22
 23

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             150
MEF 6.3 Ethernet Service Definitions IETF EVPN Service Definition
EPL: Ethernet Private Line EVPN E-Line VLAN-based
EVPL: Virtual Ethernet Private Line EVPN E-Line VLAN-bundle (port-based)
EP-LAN: Ethernet Private LAN EVPN E-LAN VLAN-based
EVP-LAN: Ethernet Virtual Private LAN EVPN E-LAN VLAN-bundle (port-based)
EVPN E-LAN VLAN-aware bundle (port based)
EP-Tree: Ethernet Private Tree EVPN E-Tree VLAN-based
EVP-Tree: Ethernet Virtual Private Tree EVPN E-Tree VLAN-bundle (port-based)
EVPN E-Tree VLAN-aware bundle (port based)
Table 8: MEF Ethernet Services to IETF EVPN Services Mapping 1
 2
  3
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             151
Annex C: MP-BGP based L3VPNs 1
BGP based layer 3 VPNs are a widely deployed connectivity application in Service Provider 2
networks and work over an MPLS and SRv6 underlay infrastructure. BGP based layer 3 VPNs are 3
based on RFC4364 [68] and use a peer-to-peer model that uses Border Gateway Protocol (BGP) to 4
distribute VPN-related information. It is a highly scalable, peer-to-peer model that through route 5
filtering allows flexible connectivity models within a VPN. VPN topologies include any-to-any L3 6
multi-point solutions as well as L3 tree solutions where connectivity constraints are built into the 7
VPN infrastructure through controlled exportations and importation of routing information. This 8
makes it an effective and efficient technology for L3 services associated with Xhaul transport. 9
Building blocks of a L3 VPN service 10
A typical L3 VPN services consists of the following:  11
 12
• A Provider Edge (PE) Routers 13
• Virtual Routing Forwarding  14
• Route Distinguisher (RD) 15
• Route Target (RT) 16
• Multi-Protocol Border Gateway Protocol (MP-BGP) 17
 18
Provider Edge (PE) Router 19
A Provider Edge router is a device which originate or terminate a L3VPN service. In other words, a 20
PE router is a VPN Endpoint, providing connectivity to the device(s) connected to it.  21
 22
 23
Figure 0-1 PE routers in an Xhaul infrastructure 24
 25
In the context of an Xhaul packet switched transport network, the first L3 router enabling an 26
L3VPN services to provide connectivity between the O-RAN/3GPP components (O-RU, O-DU, O-27
CU, UPF, 5GC) is considered a Provider Edge (PE) router. This could be a CSR or HSR in 28
Fronthaul, Midhaul or Backhaul network. 29
 30
VPN Routing and Forwarding Tables (VRF) and Route Distinguishers (RD) 31
The VPN routing and forwarding table (VRF) is a key element in the BGP based L3 VPN 32
technology. A VRF essentially define a VPN instance on PE and associates a routing table instance 33
with the VRF.  A VRF exist on PEs and more than one VRF can exist on a single PE. The VRF 34
contains routes that should be available to a particular set of sites participating in the VPN.  35
 36

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             152
 1
Figure 0-2: VRF routing tables  2
 3
A route distinguisher (RD) is a unique value assigned to a VRF on the PE Router. A RD allows to 4
associate and identify routes in the routing table with a particular VRF and allows for overlapping 5
IP address space across VRFs.   6
An RD is a local value on the PE in the Packet Switched RAN network.  7
 8
Route Targets (RT) 9
Route Targets (RT) are the mechanism by routing information distribution can be controlled within 10
an BGP based VPN throughout the packet switched network. Route Target are propagated through 11
the BGP based VPN network using route-target extended MP-BGP communities. Every PE within 12
the MPLS VPN network define a set of RT values for import or export of VPN route information.  13
BGP based VPN uses route-target communities as follows: 14
• When a VPN route is injected into MP-BGP, the route is associated with a list of VPN 15
route-target communities.  16
• An import list of route-target communities is associated with each VRF. This list defines the 17
values that should be matched against to decide whether a route is eligible to be imported 18
into this VRF. 19
 20
 21
 22
Figure 0-3: Use of route targets to control IP connectivity within a VRF 23

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             153
 1
Figure 0-3 above shows a simple mechanism of import and export Route Communities within an 2
BGP based VPN network. Where required CSR route-targets may be configured to be different than 3
the Hub Site Router.  For scalability and efficient resource usage on the CSR, the operator may 4
choose to not import all the routes, rather importing the routes from Hub Site Route and only certain 5
other CSR’s.  6
 7
When used properly, Route Targets can be used to limit the routing table information within a BGP 8
based VPN Network and create different IP topologies at the transport layer. These include: 9
1. Any to any topologies. 10
2. Hub and spoke topologies. 11
 12
Multi-Protocol Border Gateway Protocol (MP-BGP) 13
MPLS VPN information is propagated through the network using Multi-Protocol BGP (MP-BGP) 14
[70]. All PE routers within the VPN must configure MP-BGP peering and enable VPN address-15
family and extended communities to exchange VPN related information including routing table 16
information and Route Targets.  17
 18
 Traffic Steering into an BGP VPN 19
SR Policy operations is defined in [141]. It is a mechanism that can be to create an end-to-end Label 20
Switch Path (LSP) using required metrics and constraints. While using Segment Routing Traffic 21
Engineering (SRTE) for traffic path programmability, it is possible to automatically steer BGP 22
based VPN traffic into an SRTE policy.  23
Typically, an SRTE Policy is uniquely identified using the following 3-tuple consisting of:  24
• Head End: Starting point of the LSP created by SRTE Policy 25
• End Point: Destination for the traffic using the SRTE LSP 26
• Color: A numeric value associated with the SRTE policy.  27
 28
The SRTE policy color is a numeric value that is configurable in the SRTE policy. Once a VPN 29
endpoint route is “tagged” with this color, traffic can be automatically steered into the SRTE policy.  30
 31
 32
 33
Figure 0-4: Traffic Steering with BGP based VPNs 34

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             154
 1
As shown above, the VPN routes can be colored in 2 possible ways:  2
 3
1. While defining a VRF, define a color for all Prefixes associated with that VRF, or 4
2. When sending or receiving BGP routes, using a policy to “color” the route.  5
 6
In either case, the VPN route will now be tagged with a color and traffic for the destination or a 7
flow will automatically be steered into the SRTE policy that may provide traffic path 8
programmability.  9
 10
  11
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             155
Annex D: Quality of Service  1
This Annex gives an overview of packet base Quality of Service (QoS).   2
What is Quality of Service? 3
Quality of Service (QoS) is an umbrella term that broadly covers the concept of the management of 4
traffic flowing in an infrastructure. In general, QoS can provide two capabilities.  5
 6
1. A mechanism to actively manage the transmission of data on a network link out of a node 7
when the link is approaching or at saturation. This active management consists of 8
determining which data to forward next, which data to store for forwarding later, and which 9
data to drop. 10
2. The enforcement of service level agreements through the selective acceptance of traffic into a 11
node (or network) based on pre-determined criteria. Again, this acceptance is based on 12
determining which data to accept now, which data to store for acceptance later and which 13
data to deny (drop).  14
 15
Fundamentally, QoS only operates when traffic is flowing at the limits of capacity (either physical 16
or logical). When flowing traffic rates are well below the limits, QoS is essentially benign allowing 17
all traffic to be forwarded as it is received.  18
 19
Why do we need QoS?  20
Applications using the network transport will often require certain capabilities from the network in 21
order for the application to function correctly.  22
 23
For example, real-time voice services (telephone calls) can only sustain a maximum latency 24
between the end points before interactive speech becomes difficult. This maximum latency between 25
endpoints translates to two demands from the network; 26
1. Minimising hop by hop latency as a voice packet flows across the network infrastructure.  27
2. Minimising jitter in the packets arriving in order to minimise the de-jitter buffer needed at the 28
receiver (which adds latency).  29
It is useful then to be able to identify voice traffic in the network and prioritise if for forwarding 30
such that it is not held up behind other less time sensitive traffic.  31
 32
A second example might be an Electronic Point of Sale (EPoS) application. This type of application 33
does not need to be particularly latency bound (a few 10s or 100s of milliseconds in packet arrival 34
does not impact the service), but it does need to have some guaranteed capacity in order to function. 35
I.e. we do not want the EPoS application struggling for bandwidth because a large data transfer is 36
taking place. This guaranteed bandwidth requirement also translates into network demands. 37
1. Minimising packet loss for the traffic, if necessary, storing it locally until the local egress 38
interface has capacity to send it on. 39
2. Reserving a minimum amount of transmission time (or bandwidth) on the interface for 40
packets of the EPoS application. 41
 42
QoS then is the mechanism by which operators can identify applications in an infrastructure and 43
apply policies to the infrastructure to maintain the applications transport requirements, even when 44
capacity is limited. 45
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             156
 1
 QoS functional elements 2
As a suite of capabilities QoS can be broken out into three distinct sets of functions; 3
 4
1. Traffic Classification and marking 5
2. Congestion avoidance 6
3. Congestion management 7
 8
These three function sets work together to identify (classify) different traffic types, determine how 9
much of each traffic type should be allowed into and through the network (congestion avoidance) 10
and how traffic should be scheduled when interfaces become congested (congestion management). 11
 12
The specific implementation of these capabilities is vendor specific, but in general, the behaviours 13
required by the different functions are defined as policies, either individually or in groups. These 14
policies are applied to interfaces on the switching elements in a network defining the local 15
behaviours within the device.  16
 17
 Network level behaviour 18
QoS is implicitly a mechanism that is node based; the management of traffic in one node has no 19
direct impact on the management of traffic in surrounding nodes. However, it is important to design 20
the QoS behaviour of a network as a whole. Just as the operator has the ability to define the routing 21
behaviour of a network as a system of single hop by hop elements, so she has the ability to define 22
the QoS behaviour of the network as a system.  23
 24
The interaction between elements is defined by the policies applied to each device and the use of 25
“class of service” markings applied to the packets or frames flowing between the devices. 1 26
The use of markings allows an individual network element to pass some information about a given 27
packet to a downstream element. How this downstream element interprets the information and then 28
acts on it, is out of the upstream elements control, but a network of devices under single 29
administrative control may choose to use such information in network wide policy. 30
 31

1 The term “class of service” is used here to generically to describe the specific markings in a packet or frame. The
author recognises Class of Service (COS) identifies such markings in an ethernet header as defined by IEEE 802.1p.
Here it is generically used to refer to Ethernet COS, IP TOS, IP PREC, IP DSCP. MPLS EXP etc.
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             157
 Node level behaviour  1
 2
 3
Figure 0-1: Example network QoS structure 4
 5
 6
The per-hop-behaviours (PHBs) needed in the network can generally be broken down into two sets; 7
Those needed at the edge of the network (Edge QoS), and those needed in the core of the network 8
(Core QoS). We can see these behaviours in Figure 0-1. 9
 10
1. Edge QoS. Here we are classifying traffic entering into the network on ingress (1) and 11
scheduling traffic leaving the network on egress (6). Here the ingress classification (from the 12
client) and egress classification (5) and scheduling (6) (towards the client) will (often) be 13
determined by the markings of the client traffic being transported across the network. In 14
addition, ingress traffic (CE → PE), once classified, will be marked (2) to conform to the 15
transport marking scheme.  16
 17
2. Core QoS. Here traffic will be classified by the transport marking scheme (4) (mentioned 18
above), scheduled based on those markings (3). This PHB is designed to manage traffic at an 19
aggregate level, treating services that have common requirements (defined by marking) with 20
a common behaviour.  21
 22
These two sets of PHBs present different requirements to the underlying hardware and care should 23
be taken when selecting a platform to ensure the operators desired PHBs can be met.  24
 25
Traffic classification and marking 26
As previously mentioned, traffic classification is a term used to describe the identification and 27
categorisation of traffic arriving at or inside network element. Classification broadly falls into two 28
distinct categories.  29
 Context based classification 30
In this model, traffic arriving at a node is classified, (allocated to some internal category) based on 31
the context associated with its arrival. For example, it might be assumed that all traffic arriving 32
from a specific logical or physical interface will be categorized as “best effort”. This specific 33
example actually represents the default behaviour for almost all network elements, and is useful to 34

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             158
recognise, as it forms the basis for all other classifications. I.e. that without further modification, 1
traffic arriving at a node will be treated as best effort (without any specific explicit treatment).  2
 3
 4
Figure 0-2: Context based classification  5
 6
In a second example, all traffic arriving on a specific interface might be classified (categorised) as 7
“real time” or “high priority” (see Figure 0-2).  8
 9
Whilst use of context-based classification is important, it is a somewhat blunt tool, and assumes that 10
all traffic arriving on an interface is of a single type and requires common treatment. 11
Packet based classification 12
Packet based classification gives us a more selective tool with which to categorize traffic arriving at 13
a node. In this mode the node examines one or more fields in the arriving packets (or frames) to 14
make a categorisation decision.  15
 16
One approach is to classify traffic into categories based on some flow or application information. 17
E.g. looking at source or destination, ether type, IP protocol, UDP or TCP port number etc. In this 18
way the node can infer the required QoS capabilities (e.g. low latency, minimum bandwidth, best 19
effort etc) from the type of traffic embedded in the packet.  20
 21
However, often the most pragmatic approach is to require that packets are pre-marked with an 22
explicit class of service marking. The specific marking model varies dependent on the transport 23
being used, but the primary marking schemes to support are Ethernet Priority Code Point (PCP) and 24
Drop Eligible Indicator (DEI) fields in Ethernet IEEE 802.1p (part of the IEEE 802.1q VLAN 25
header specification), IP TOS, PREC or DSCP ECN fields (defined in IETF RFC 2474 [38] and 26
RFC 3168) and MPLS TC (EXP) field (defined in IEFT RFC 3032 [46] and RFC 5462). 27
 28
The use of a class of service (COS) marking allows a packet source or network element to encode 29
the required QOS capabilities for a packet directly into the packet header. By predetermining the 30
marking scheme to be used for the COS field, classification using the COS field alone is often 31
sufficient to determine the PHB needed in the network node. 32
  33
Traffic marking 34
As described above, a key attribute of a QoS architecture is the ability to pass information about the 35
QoS status of a packet between nodes, and indeed within a node (e.g. from ingress to egress). This 36
is achieved through the use of QoS markings, setting bit fields within a packet header that convey 37
some information about the QoS related context in which this packet should be considered.  38

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             159
 1
Again, as described above there are numerous traffic marking schemes that are in use, each 2
associated with a particular header type in the packet (or frame) world.  There are also bit fields 3
defined in meta-data associated with internal forwarding in many router and switch silicon 4
architectures. Two examples of this are “Discard Class or Loss Priority” often associated with the 5
output of a policing function, but also explicitly settable, and “qos-group” or “forwarding-class”, an 6
opaque field provided in many router implementations  7
 8
The use and meaning associated with the values represented by these bits is entirely arbitrary, as 9
defined and used by users and operators. However, the value in their use lies in having agreed 10
meaning that can be passed between network elements. Some fields, for example IP DSCP, have 11
recommend meanings that can be used between providers. Others, such as MPLS TC (EXP) are not 12
formally defined so as to remain flexible.  13
 14
 15
 16
 17
 18
Figure 0-3: QoS marking domains 19
 20
A useful concept is to consider the use of a given marking scheme within a QoS marking domain. 21
Nodes within a domain use a common scheme, allowing identification of traffic with common 22
properties (a traffic class). Policies are then implemented in each node to maintain the appropriate 23
network behaviour for each identified class, when under congested conditions. 24
 25
At the border between domains, the border element should be capable of classifying traffic in the 26
first domain and remarking it for transport in the second. Usually the policy for forwarding will be 27
implemented to honour the class structure for the domain it is in. In Figure 0-3, three domains are 28
identified, 1) the customer domain, 2) the provider domain 3) an intra-node domain. The PE 29
elements on the left and right of the provider domain border the provider and customer domains and 30
will be responsible for remarking traffic as appropriate. 31
 32

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             160
 Congestion management 1
Thus far we have discussed the options available for classifying traffic. But once classified, we 2
must consider what behaviour to apply. As mentioned in the introduction to this section, a QoS 3
node fundamentally only has three options available for managing traffic; forward, store for 4
forwarding later or drop, but these options need only be applied when a bandwidth resource is 5
congested.  6
 7
A key function of QoS is that of managing traffic during congestion of an interface. Congestion 8
occurs when the quantity of traffic to be forwarded on an interface exceeds its capacity. In its 9
simplest form, QoS alleviates this issue by buffering the excess traffic, i.e. storing it in a local 10
memory until such time that the interface is available to send the stored traffic.  11
 12
FIFO Queueing 13
By storing packets that need to be sent on to this interface in the order they arrive, and then sending 14
them in that order when the interface is free, we form a “first in - first out” (FIFO) queue of packets. 15
 16
This mechanism, queueing, is a useful tool for managing temporary congestion, smoothing out the 17
bursts of traffic flowing into an interface, and forms the basis for more complex congestion 18
management tools.  19
There are four aspects of this capability that cause challenges; 20
1. By storing traffic for later sending, we intrinsically delay that traffic that is being stored, 21
adding latency to the transmission path for that traffic. Further, because the delay may be 22
variable, (different amounts of traffic may be enqueued prior to this packet arriving) we 23
introduce jitter. 24
2. As the speeds of the interfaces we are managing increases, we need more memory capacity to 25
store enough packets to substantively impact the traffic we are buffering.   26
3. The process is of queuing traffic is indiscriminate, in that it impacts all traffic flowing 27
through an interface. This presents the same latency implication to all applications using this 28
interface, some of which will be impacted by this delay, and some will not. 29
4. As the amount of memory available to for the queue is finite, so the queue is finite. As a 30
result, if the rate of arrival of traffic to be forwarded is significantly higher than the interface 31
rate or the burst is long enough, the available buffer will be filled, and no further traffic can 32
be enqueued. At this point, the node has no option but to drop traffic. When a queue is full, 33
and a node can no longer add packets to the end (tail) of the queue, traffic otherwise destined 34
for that queue is indiscriminately dropped from the tail (tail drop), until such time that the 35
queue depth reduces and space is made available again for new traffic.   36
It is worth remembering at this point that the application of QoS to an infrastructure is only a 37
solution for the management of how traffic is dropped under congestion conditions. It is not a 38
replacement for capacity planning and management for traffic that MUST be delivered. I.e. if a 39
network has less capacity available than the amount of traffic that MUST be delivered, applying 40
QoS is not an alternative to augmenting the capacity.  41
However, it is possible to significantly improve on this single FIFO queue model by combining the 42
use of classification with a programable scheduler to determine how we enqueue traffic, how much 43
we allow to pass, and what to do with the excess.  44
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             161
Class based queueing 1
By combining the concept of classifying traffic with the concept of a queue it is possible to allow a 2
node to manage the traffic flowing on an interface with a behaviour that is appropriate to the 3
applications flowing in each class. 4
 5
 6
 7
Figure 0-4: Class based queue 8
 9
In this model, shown in Figure 0-4, the system identifies separate logical queues for traffic destined 10
to flow out of a given interface. Traffic can be placed into a queue based on the classification 11
information gained on arrival (or if the system is capable, after the packet has been forwarded – 12
determined that it should flow out of this specific interface).  13
 14
For example, traffic identified as needing low latency, and low jitter (real time – our voice calls 15
from earlier) may be placed into queue 1. Traffic requiring some minimum bandwidth (business 16
critical, our EPOS traffic) may be placed into queue 3. Traffic that has no specific requirements 17
(best effort) may be placed into queue 5.  18
 19
The scheduler can now be programmed to dequeue traffic and place it onto the interface following a 20
pre-determined policy – for example;  21
1. Serve all high-priority traffic first (E.g. Queue 1 and 2), up to a given capacity of the 22
interface for each queue (for example 50% and 5%). Any traffic arriving at this interface for 23
those queues that cannot be served at that rate should be placed into the appropriate queue. 24
2. Serve traffic from low priority queues (queue 3, 4 and 5) with equal priority (a packet from 25
each) until the minimum guaranteed amount of bandwidth allocated to that queue is met. 26
3. Continue to serve traffic from low priority queues until either the queue is empty, or the 27
interface bandwidth is consumed. 28
 29
This type of policy allows maximum use of the available bandwidth whilst guaranteeing each 30
application class will have access to at least the capacity it needs to support the user base.  31
 32
Hierarchical class-based queueing  33
A further extension of this model uses a tiered set of schedulers to manage sets of queues that are 34
“grouped” to manage traffic associated with some logical entity on the physical interface.   35
 36

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             162
 1
Figure 0-5: Hierarchical class-based queue model 2
 3
In Figure 0-5, we have a physical interface supporting two VLANs (orange and green). Each VLAN 4
has its own set of queues and its own scheduling policy for those queues. In this example the orange 5
VLAN has two priority queues and three guaranteed bandwidth queues. The top scheduler (2) is 6
responsible for managing the flow of traffic from the queues associated with that VLAN.   7
The bottom scheduler (3) manages this flow of traffic from the queue associated with the green 8
VLAN.   9
Schedulers (2) and (3) are regarded as child schedulers, and their packet flow is in turn managed by 10
scheduler (1), the parent scheduler for the interface. The parent scheduler will be programmed with 11
a policy to manage the traffic flowing from the child schedulers toward the interface.   12
  13
This model can allow for a “hard” distribution of the bandwidth on an interface amongst the logic 14
entities sharing that capacity, protecting the use of the bandwidth for a given VLAN (orange) from 15
the others on the same interface (green).    16
  17
 Congestion avoidance 18
Thus far we have discussed the options available for classifying traffic, and managing traffic when 19
interfaces are under congestion. We have examined how, when an interface is congested, we can 20
consider which packets to forward and when. However, one important aspect of QoS is the 21
prevention of congestion in the first place.  22
 23
Congestion avoidance predominantly takes two forms;  24
1. Admission control 25
2. Selective queue management through random early discard (RED) or weighted random early 26
discard (WRED) 27
 28

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             163
Admission control 1
This is the process of managing the amount of traffic that we accept into the network. This can be 2
thought of as the enforcement of the contract between the applications requirements’, and the 3
networks transport capability. Returning to the example of the voice call, let us imagine that the 4
transport network has enough capacity to sustain 100 concurrent voice calls (Figure 0-6). If more 5
than 100 calls are introduced there is insufficient bandwidth to sustain all the calls and some traffic 6
will inevitably be dropped. Because the dropping of traffic will be random, all calls will be 7
impacted by the packet loss equally, resulting in reduced voice quality or even call failure.  8
 9
 10
 11
Figure 0-6: Admission control policing 12
 13
By pre-agreeing that the network can only sustain a certain number of calls, we can police the 14
traffic arriving from the application, to the agreed level. In the case of the packet network, this 15
means agreeing a sustainable bandwidth level and honouring that level at the point that we accept 16
traffic from the application. 17
 18
Once we have classified traffic to determine that it is indeed voice traffic, we can apply a policer to 19
that traffic class to limit the amount of traffic that is accepted. If the application violates the agreed 20
rate, the policer will drop some packets, forwarding only the rate that was previously agreed. It 21
should be noted that for this particular application, again voice quality will suffer, however, the 22
onus is now on the application to stay within the terms of the agreed contract for the benefit of its 23
own service.  24
 25
So, if by dropping traffic at admission to the network we will see the application service degrade, 26
just as if we drop traffic in the middle of the network, why bother? Should we not simply let the 27
traffic have access and hope there is sufficient capacity to support the application demand? 28
 29

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             164
 1
 2
Figure 0-7: Service protection through admission control 3
 4
This is where the network level administration of QoS becomes relevant. Consider the same core 5
network resource now being shared by two customers (Figure 0-7). If we recognise that the network 6
can only sustain 100 calls, we can ask each customer to restrict their usage, customer 1 to 60 calls 7
and customer 2 to 40 calls. By applying a policy that is specific to each customer context we can 8
ensure that the available network resources are not exhausted, resulting in poor performance for all.  9
If customer 1 exceeds their allocated capacity, some traffic will be dropped at point 1, resulting in 10
poor quality for customer 1. However, if customer 2 continues to maintain their agreement not to 11
exceed 40 calls, there will always be sufficient capacity to support their service without quality 12
impairment.  13
 14
Weighted Random Early Discard (WRED) 15
When looking at congestion management, the idea of a queue was introduced in order to store 16
traffic that is awaiting transmission on an outgoing interface. In this previous discussion we saw 17
that in a simple queue, when the queue is full, any additional traffic is “tail dropped” 18
indiscriminately.  19
 20
For applications that are using a TCP connection over IP, the loss of one (or more) packets from the 21
flow will cause the TCP algorithm to time out and request re-delivery of these missing packets. This 22
also triggers the shortening of the TCP “ack window” resulting in lower throughput for that TCP 23
session. As many TCP session may be impacted at once in a tail drop scenario, this can cause a 24
number of sessions to “back off” reducing the total load on the interface, and the queue to be 25
drained.  26
 27
Over time, the TCP sessions will re-open their ACK window, increasing bandwidth until the 28
interface is once again full and the queue overwhelmed. The result is a sawtooth type bandwidth 29
profile that oscillates around the maximum interface speed. This oscillation reduces the efficiency 30
of the interface usage as there are periods where the interface is not full.  31
 32
Weighted Random early discard (WRED) is a modification to the queue drop behaviour that can 33
help to eliminate this effect. The premise is to manage the acceptance of traffic into the queue in a 34
more intelligent manner.  35
 36

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             165
 1
Figure 0-8: RED queue threshold 2
 3
Consider the queue in Figure 0-8, with three depths at 0% (1), 50% (2) and 100 % (3).  4
As traffic arrives for the interface and the scheduler has no capacity available to forward them, the 5
waiting packets are placed into the queue. Between queue depth 1 (0% full) and depth 2 (50% full), 6
packets are added as normal to the queue.  7
 8
When the queue depth reaches the lower RED threshold (2) (50% in this example), rather than 9
adding all new packets to the queue, some will be randomly dropped. The probability of dropping 10
any given packet changes dependent on the current (and possibly historical) queue depth. As the 11
depth of the queue increases, so does the probability of drop until at depth 3 (100% full) the 12
probability of drop is 1 (always drop).  13
 14
The impact of this weighted behaviour is to slowly encourage individual TCP sessions flowing in 15
the queue to find an appropriate TCP window (and hence bandwidth) such that the queue never 16
saturates, and the link can continue flowing at 100% capacity, eliminating the sawtooth behaviour.  17
 18
Class based Weighted Random Early Discard  19
We have seen it is possible to add traffic to a specific queue based on a classification, in order to 20
differentiate between traffic types flowing in an interface. This allows us the flexibility to manage 21
bandwidth to support different applications in an infrastructure. We have also seen that under some 22
circumstances it might be possible to “borrow” bandwidth from one class to use in another class, in 23
the event that the bandwidth in an interface is not fully used. 24
 25
It is possible to extend the idea differentiated behaviour for traffic from between queues, to within a 26
single queue using the Weighted RED scheme.  27
 28
Figure 0-9: Class based WRED function and behaviour 29
 30
Consider the PE node on the left of Figure 0-9, two CEs are sending EPOS traffic. The PE has 31
capacity at (3) for 100 concurrent sessions. The number of sessions sent by each CE varies. CE 1 32

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             166
has a minimum sustained rate of 60 sessions, and CE 2 has a minimum sustained rate of 40. Each 1
has a possible peak rate of 100.  2
 3
As traffic arrives from CE 1 at point (1) the PE node classifies traffic as part of the EPOS 4
application. It also measures the amount arriving and allows up to 100 sessions worth into the 5
network. For traffic that exceeds the rate of 60 sessions, the node marks this traffic as having 6
exceeded its contracted rate or “out of contract”. The same classification, policing and marking 7
takes place at point (2) for CE 2 with a rate of 40 sessions. This extra marking is termed the discard 8
class or DC.  9
 10
When the incoming packets arrive at the queue depicted on the right, again a decision is made prior 11
to enqueuing the traffic. This time the decision is based on two criteria, the current queue depth 12
AND the discard class of the packet as determined in the ingress policing and marking stages.  13
 14
A sample policy may behave as follows. For queue depth between 0% (4) and 33% (5) all traffic is 15
enqueued.  16
 17
For traffic that is marked with the out-of-contract discard class marking, between queue depth 33% 18
(5) and 66% (6) the drop probability varies from 0 to 1; that is at depth 66% (6) all traffic that is 19
marked out of contract will be dropped. Traffic that is marked “in contract” will continue to be 20
queued for transmission.  21
  22
For queue depths between 66% (6) and 100% (7) no out of contract traffic will be enqueued, and 23
the drop probability for in contract traffic will increase from 0 to 1.  24
 25
It should be noted that the PE is not policing “sessions”, but simply the traffic associated with those 26
sessions. The PE has no knowledge of the nature of the traffic sent by the CE (for example the 27
membership of a specific packet to a specific session), other than the markings being examined.  28
It should also be noted that traffic dropped at point (3) will contain packets associated with many or 29
all of the sessions from the CE whose traffic is being marked “out-of-contract”. The impact of this 30
is that all TCP sessions that lose traffic will again contract their TCP windows, with the effect of 31
lowering the BW used by each session and so reducing the load on the network. 32
 33
This behaviour has the following benefits;  34
1. both CE users 1 and 2 potentially have access to all the bandwidth at point 3 in the diagram 35
when the other user is not consuming it. The network resources at 3 can be optimally used 36
when demand from one user is high and the other is low. Compare this to the example in 37
Service protection through admission control where the network resource at point 3 may be 38
under used.  39
2. If a user constrains their traffic to within the contracted limit, they are guaranteed delivery, 40
even under congestion caused by out-of-contract traffic being generated by other users.  41
 42
  43
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             167
Annex E: Multicast Technologies background 1
Overlay multicast 2
Overlay multicast refers to CE to CE multicast over the transport provider/underlay network. In the 3
following example, CE1 is a First Hop Router (FHR) connecting to the source and CE2/3 are Last 4
Hop Routers (LHRs) connecting to receivers. Receivers send IGMP/MLD joins towards the LHRs, 5
who then send PIM joins towards their upstream routers PE2/3. If the PEs were connected by a 6
LAN, PE2/3 would simply send PIM joins onto the LAN towards PE1. Since PEs are actually 7
Provider Edge routers that are not directly connected and typically serve many VPNs, Overlay 8
Multicast signalling is used to signal the overlay multicast state over, not through, the 9
provider/underlay network. 10
 11
 12
 13
 14
 15
 16
 17
Figure 0-1: Multicast procedures 18
 19
PIM-based overlay signalling for IPVPN 20
The most straightforward way is to emulate a per-VPN pseudo LAN over the underlay network. 21
The pseudo LAN is instantiated by a multicast GRE tunnel for the VPN. Every PE for the VPN 22
joins that tunnel and can send both control plane (PIM messages) and data plane traffic over the 23
tunnel. For example, multicast group 225.1.1.1 used for VPN1, 225.1.1.2 for VPN2, etc. 24
 25
This is referred to as PIM-MVPN and also commonly as Rosen-MVPN. It is documented in the 26
historic RFC6037 and further in RFC6513 [96] (which documents both PIM- and BGP-based 27
overlay signaling). 28
 29
Essentially, a PE runs PIM sessions over each pseudo LAN for each VPN. This is different from 30
unicast, with which no per-VPN session is used. 31
 32
When there are a large number of VPNs, many PIM sessions and signalling (including message 33
refreshes) are incurred so it does not scale well. Most implementations and deployments of 34
PIM/Rosen-MVPN only use IP multicast tunnel in the underlay to emulate the pseudo LANs. 35
BGP-based overlay signalling for IPVPN and EVPN 36
For BGP-based signalling, the overlay signalling is done by BGP just like unicast case. It is referred 37
to as C-multicast signalling (C for Customer) in RFC6513 [96]. BGP-based signaling use unified 38
methods for both unicast and multicast, and inherits all BGP scaling mechanisms and properties. 39
 40
Multicast (IP) VPN with BGP based overlay signalling is referred to BGP-MVPN. While RFC6513 41
covers both PIM-MVPN and BGP-MVPN concepts, BGP-MVPN specific encoding and procedures 42
are specified in RFC6514 [97] . 43
 44
src CE1
CE2
CE3
PE1
PE2
PE3
rcvr
rcvr
IGMP PIM
PIM
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             168
BGP-MVPN also defines mechanisms to signal what type and instance of provider/underlay tunnels 1
are used to transport overlay multicast traffic over the underlay, via Inclusive or Selective PMSI 2
Auto-Discovery routes (often referred to as I/S-PMSI or x-PMSI routes). IP multicast, RSVP-TE 3
P2MP, mLDP P2MP/MP2MP, Ingress Replication, BIER and future additions could all be used as 4
underlay tunnels. 5
 6
EVPN BUM (Broadcast, Unknown unicast and Multicast) only uses BGP overlay signalling and is 7
modelled after BGP-MVPN. It is specified in RFC7432 [98] and further in draft-ietf-bess-evpn-8
bum-procedures-update [127]. 9
 10
BGP-MVPN and EVPN BUM have been widely deployed and are often referred to as Next 11
Generation MVPN (NG-MVPN). 12
Underlay multicast 13
As mentioned above, overlay multicast traffic is transported via underlay multicast tunnels. Many 14
tunnel types have been mature and widely deployed, but with Segment Routing and central control 15
being widely adopted, it is necessary to review the pros and cons of various multicast technologies 16
and see what is the best solution in the SR era. 17
 18
Traditionally, multicast requires per-tree state on all routers of a tree for efficient replication. PIM, 19
RSVP-TE P2MP, mLDP P2MP/MP2MP are all signaling protocols to instantiate tree state on those 20
routers, either from the tree leaves towards the tree root or vice versa. The new SR-P2MP (aka tree-21
sid) tunnels also requires per-tree state on all root/leaf and replication nodes, except that the 22
signalling is directly from a central controller who also calculates the tree. 23
 24
Obviously, this does not go well with one principal of Segment Routing – no per-flow state inside 25
the network. An alternative is Ingress Replication – the root tunnels individual copies directly to 26
leaves so no per-tree state is needed inside the network. Since replication is done at the root, it is not 27
efficient and does not work well for high fan-out hight data rate cases. 28
  29
BIER is a new technology that achieves efficient replication w/o incurring per-tree state, so it is the 30
ideal multicast solution for an SR network. However, it uses a new encapsulation and forwarding 31
algorithm, so it requires new hardware. While this makes it difficult to deploy in non-greenfield 32
networks, there are very well-designed brownfield deployment methods, and all major vendors have 33
BIER implementation & hardware available/upcoming. 34
 35
For an SR network, an operator may prefer to remove all legacy multicast signaling – 36
PIM/RSVP/mLDP and switch to controller-based tree calculation and signaling via either PCEP or 37
BGP (i.e., SR-P2MP [draft-ietf-pim-sr-p2mp-policy] [draft-ietf-spring-sr-replication-segment] or 38
BGP-signaled IP/MPLS multicast [draft-ietf-bess-bgp-multicast-controller]). Notice that this still 39
incurs per-tree state inside the network and the only real benefit is the central calculation of the tree 40
based on many TE constraints/considerations. 41
 42
In summary, depending on many factors, multicast technologies can be considered in the following 43
preference order: 44
 45
1. PIM or mLDP/RSVP-TE P2MP that are already deployed in existing network 46
2. Controller calculated/signalled IP multicast or mLDP/RSVP/SR-P2MP tunnel 47
3. BIER if enough routers in the network support it 48
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             169
 1
MVPN/EVPN and Seamless MPLS/SR 2
Given the vast scale of the transport network, a seamless architecture is used as described 3
previously. In this architecture, two aspects have impacts to multicast: 4
 5
a) Only border routes have routes to edges nodes (that connect to 5G NFs like RAN/UPF), 6
while internal routers only have routes to the border routes.  7
b) Different ASes/areas may support/prefer different types of underlay tunnels 8
 9
For a), to establish end-to-end underlay tunnels across those different ASes/areas, PIM RPF vector 10
[RFC5496] or mLDP Recursive FEC [RFC6512] can be used – a leaf router looks up the route to 11
the tree root, with the BGP protocol nexthop being a border router in the local area/AS. It then 12
encodes that border router’s address in PIM/mLDP signaling so that internal routers will signal 13
towards that border router instead of the original root. When that border router gets the signaling, it 14
encodes the next border router’s address so that the internal routers in the next AS/area will signal 15
towards the next border router, and so on so forth. 16
 17
An alternative solution for a) is use controller signaled multicast, since the routers become dumb 18
forwarding devices programed with forwarding state for the tunnels by the controllers. 19
 20
For b), MVPN/EVPN-BUM tunnel segmentation [RFC7524] [draft-ietf-bess-evpn-bum-procedures-21
update] can be used – MVPN/EVPN-BUM procedures will run on the border routers and the 22
signaled tunnel type/identification are changed as MVPN/EVPN routes are re-advertised by the 23
border routers, so that different tunnel types/instances are used in different ASes/areas. 24
 25
 26
 27
Figure 0-2: Multicast diagram  28

O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             170
Annex ZZZ : O-RAN Adopter License Agreement 1
BY DOWNLOADING, USING OR OTHERWISE ACCESSING ANY O-RAN SPECIFICATION, 2
ADOPTER AGREES TO THE TERMS OF THIS AGREEMENT. 3
This O-RAN Adopter License Agreement (the “Agreement”) is made by and between the O-RAN 4
Alliance and the entity that downloads, uses or otherwise accesses any O-RAN Specification, 5
including its Affiliates (the “Adopter”). 6
This is a license agreement for entities who wish to adopt any O-RAN Specification. 7
Section 1: DEFINITIONS 8
1.1 “Affiliate” means an entity that directly or indirectly controls, is controlled by, or is under 9
common control with another entity, so long as such control exists. For the purpose of this Section, 10
“Control” means beneficial ownership of fifty (50%) percent or more of the voting stock or equity 11
in an entity. 12
1.2 “Compliant Implementation” means any system, device, method or operation (whether 13
implemented in hardware, software or combinations thereof) that fully conforms to a Final 14
Specification. 15
1.3 “Adopter(s)” means all entities, who are not Members, Contributors or Academic Contributors, 16
including their Affiliates, who wish to download, use or otherwise access O-RAN Specifications. 17
1.4 “Minor Update” means an update or revision to an O-RAN Specification published by O-RAN 18
Alliance that does not add any significant new features or functionality and remains interoperable 19
with the prior version of an O-RAN Specification. The term “O-RAN Specifications” includes 20
Minor Updates. 21
1.5 “Necessary Claims” means those claims of all present and future patents and patent 22
applications, other than design patents and design registrations, throughout the world, which (i) are 23
owned or otherwise licensable by a Member, Contributor or Academic Contributor during the term 24
of its Member, Contributor or Academic Contributorship; (ii) such Member, Contributor or 25
Academic Contributor has the right to grant a license without the payment of consideration to a 26
third party; and (iii) are necessarily infringed by a Compliant Implementation (without considering 27
any Contributions not included in the Final Specification). A claim is necessarily infringed only 28
when it is not possible on technical (but not commercial) grounds, taking into account normal 29
technical practice and the state of the art generally available at the date any Final Specification was 30
published by the O-RAN Alliance or the date the patent claim first came into existence, whichever 31
last occurred, to make, sell, lease, otherwise dispose of, repair, use or operate a Compliant 32
Implementation without infringing that claim. For the avoidance of doubt in exceptional cases 33
where a Final Specification can only be implemented by technical solutions, all of which infringe 34
patent claims, all such patent claims shall be considered Necessary Claims. 35
1.6 “Defensive Suspension” means for the purposes of any license grant pursuant to Section 3, 36
Member, Contributor, Academic Contributor, Adopter, or any of their Affiliates, may have the 37
discretion to include in their license a term allowing the licensor to suspend the license against a 38
licensee who brings a patent infringement suit against the licensing Member, Contributor, 39
Academic Contributor, Adopter, or any of their Affiliates. 40
Section 2: COPYRIGHT LICENSE 41
2.1 Subject to the terms and conditions of this Agreement, O-RAN Alliance hereby grants to 42
Adopter a nonexclusive, nontransferable, irrevocable, non-sublicensable, worldwide copyright 43
license to obtain, use and modify O-RAN Specifications, but not to further distribute such O-RAN 44
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             171
Specification in any modified or unmodified way, solely in furtherance of implementations of an 1
ORAN 2
Specification. 3
2.2 Adopter shall not use O-RAN Specifications except as expressly set forth in this Agreement or 4
in a separate written agreement with O-RAN Alliance. 5
Section 3: FRAND LICENSE 6
3.1 Members, Contributors and Academic Contributors and their Affiliates are prepared to grant 7
based on a separate Patent License Agreement to each Adopter under Fair Reasonable And Non- 8
Discriminatory (FRAND) terms and conditions with or without compensation (royalties) a 9
nonexclusive, non-transferable, irrevocable (but subject to Defensive Suspension), non-10
sublicensable, worldwide patent license under their Necessary Claims to make, have made, use, 11
import, offer to sell, lease, sell and otherwise distribute Compliant Implementations; provided, 12
however, that such license shall not extend: (a) to any part or function of a product in which a 13
Compliant Implementation is incorporated that is not itself part of the Compliant Implementation; 14
or (b) to any Adopter if that Adopter is not making a reciprocal grant to Members, Contributors and 15
Academic Contributors, as set forth in Section 3.3. For the avoidance of doubt, the foregoing 16
licensing commitment includes the distribution by the Adopter’s distributors and the use by the 17
Adopter’s customers of such licensed Compliant Implementations. 18
3.2 Notwithstanding the above, if any Member, Contributor or Academic Contributor, Adopter or 19
their Affiliates has reserved the right to charge a FRAND royalty or other fee for its license of 20
Necessary Claims to Adopter, then Adopter is entitled to charge a FRAND royalty or other fee to 21
such Member, Contributor or Academic Contributor, Adopter and its Affiliates for its license of 22
Necessary Claims to its licensees. 23
3.3 Adopter, on behalf of itself and its Affiliates, shall be prepared to grant based on a separate 24
Patent License Agreement to each Members, Contributors, Academic Contributors, Adopters and 25
their Affiliates under Fair Reasonable And Non-Discriminatory (FRAND) terms and conditions 26
with or without compensation (royalties) a nonexclusive, non-transferable, irrevocable (but subject 27
to Defensive Suspension), non-sublicensable, worldwide patent license under their Necessary 28
Claims to make, have made, use, import, offer to sell, lease, sell and otherwise distribute Compliant 29
Implementations; provided, however, that such license will not extend: (a) to any part or function of 30
a product in which a Compliant Implementation is incorporated that is not itself part of the 31
Compliant Implementation; or (b) to any Members, Contributors, Academic Contributors, Adopters 32
and their Affiliates that is not making a reciprocal grant to Adopter, as set forth in Section 3.1. For 33
the avoidance of doubt, the foregoing licensing commitment includes the distribution by the 34
Members’, Contributors’, Academic Contributors’, Adopters’ and their Affiliates’ distributors and 35
the use by the Members’, Contributors’, Academic Contributors’, Adopters’ and their Affiliates’ 36
customers of such licensed Compliant Implementations. 37
Section 4: TERM AND TERMINATION 38
4.1 This Agreement shall remain in force, unless early terminated according to this Section 4. 39
4.2 O-RAN Alliance on behalf of its Members, Contributors and Academic Contributors may 40
terminate this Agreement if Adopter materially breaches this Agreement and does not cure or is not 41
capable of curing such breach within thirty (30) days after being given notice specifying the breach. 42
4.3 Sections 1, 3, 5 - 11 of this Agreement shall survive any termination of this Agreement. Under 43
surviving Section 3, after termination of this Agreement, Adopter will continue to grant licenses (a) 44
to entities who become Adopters after the date of termination; and (b) for future versions of ORAN 45
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             172
Specifications that are backwards compatible with the version that was current as of the date of 1
termination. 2
Section 5: CONFIDENTIALITY 3
Adopter will use the same care and discretion to avoid disclosure, publication, and dissemination of 4
O-RAN Specifications to third parties, as Adopter employs with its own confidential information, 5
but no less than reasonable care. Any disclosure by Adopter to its Affiliates, contractors and 6
consultants should be subject to an obligation of confidentiality at least as restrictive as those 7
contained in this Section. The foregoing obligation shall not apply to any information which is: (1) 8
rightfully known by Adopter without any limitation on use or disclosure prior to disclosure; (2) 9
publicly available through no fault of Adopter; (3) rightfully received without a duty of 10
confidentiality; (4) disclosed by O-RAN Alliance or a Member, Contributor or Academic 11
Contributor to a third party without a duty of confidentiality on such third party; (5) independently 12
developed by Adopter; (6) disclosed pursuant to the order of a court or other authorized 13
governmental body, or as required by law, provided that Adopter provides reasonable prior written 14
notice to O-RAN Alliance, and cooperates with O-RAN Alliance and/or the applicable Member, 15
Contributor or Academic Contributor to have the opportunity to oppose any such order; or (7) 16
disclosed by Adopter with O-RAN Alliance’s prior written approval. 17
Section 6: INDEMNIFICATION 18
Adopter shall indemnify, defend, and hold harmless the O-RAN Alliance, its Members, 19
Contributors or Academic Contributors, and their employees, and agents and their respective 20
successors, heirs and assigns (the “Indemnitees”), against any liability, damage, loss, or expense 21
(including reasonable attorneys’ fees and expenses) incurred by or imposed upon any of the 22
Indemnitees in connection with any claims, suits, investigations, actions, demands or judgments 23
arising out of Adopter’s use of the licensed O-RAN Specifications or Adopter’s commercialization 24
of products that comply with O-RAN Specifications. 25
Section 7: LIMITATIONS ON LIABILITY; NO WARRANTY 26
EXCEPT FOR BREACH OF CONFIDENTIALITY, ADOPTER’S BREACH OF SECTION 3, 27
AND ADOPTER’S INDEMNIFICATION OBLIGATIONS, IN NO EVENT SHALL ANY 28
PARTY BE LIABLE TO ANY OTHER PARTY OR THIRD PARTY FOR ANY INDIRECT, 29
SPECIAL, INCIDENTAL, PUNITIVE OR CONSEQUENTIAL DAMAGES RESULTING FROM 30
ITS PERFORMANCE OR NON-PERFORMANCE UNDER THIS AGREEMENT, IN EACH 31
CASE WHETHER UNDER CONTRACT, TORT, WARRANTY, OR OTHERWISE, AND 32
WHETHER OR NOT SUCH PARTY HAD ADVANCE NOTICE OF THE POSSIBILITY OF 33
SUCH DAMAGES. O-RAN SPECIFICATIONS ARE PROVIDED “AS IS” WITH NO 34
WARRANTIES OR CONDITIONS WHATSOEVER, WHETHER EXPRESS, IMPLIED, 35
STATUTORY, OR OTHERWISE. THE O-RAN ALLIANCE AND THE MEMBERS, 36
CONTRIBUTORS OR ACADEMIC CONTRIBUTORS EXPRESSLY DISCLAIM ANY 37
WARRANTY OR CONDITION OF MERCHANTABILITY, SECURITY, SATISFACTORY 38
QUALITY, NONINFRINGEMENT, FITNESS FOR ANY PARTICULAR PURPOSE, ERROR-39
FREE OPERATION, OR ANY WARRANTY OR CONDITION FOR O-RAN 40
SPECIFICATIONS. 41
O-RAN.WG9.XPSAAS-v01.00

________________________________________________________________________________________________
Copyright 2021 O-RAN ALLIANCE e.V.
Your use is subject to the terms of the O-RAN Adopter License Agreement in the Annex ZZZ             173
Section 8: ASSIGNMENT 1
Adopter may not assign the Agreement or any of its rights or obligations under this Agreement or 2
make any grants or other sublicenses to this Agreement, except as expressly authorized hereunder, 3
without having first received the prior, written consent of the O-RAN Alliance, which consent may 4
be withheld in O-RAN Alliance’s sole discretion. O-RAN Alliance may freely assign this 5
Agreement. 6
Section 9: THIRD-PARTY BENEFICIARY RIGHTS 7
Adopter acknowledges and agrees that Members, Contributors and Academic Contributors 8
(including future Members, Contributors and Academic Contributors) are entitled to rights as a 9
third-party beneficiary under this Agreement, including as licensees under Section 3. 10
Section 10: BINDING ON AFFILIATES 11
Execution of this Agreement by Adopter in its capacity as a legal entity or association constitutes 12
that legal entity’s or association’s agreement that its Affiliates are likewise bound to the obligations 13
that are applicable to Adopter hereunder and are also entitled to the benefits of the rights of Adopter 14
hereunder. 15
Section 11: GENERAL 16
This Agreement is governed by the laws of Germany without regard to its conflict or choice of law 17
provisions.  18
This Agreement constitutes the entire agreement between the parties as to its express subject matter 19
and expressly supersedes and replaces any prior or contemporaneous agreements between the 20
parties, whether written or oral, relating to the subject matter of this Agreement.  21
Adopter, on behalf of itself and its Affiliates, agrees to comply at all times with all applicable laws, 22
rules and regulations with respect to its and its Affiliates’ performance under this Agreement, 23
including without limitation, export control and antitrust laws. Without limiting the generality of the 24
foregoing, Adopter acknowledges that this Agreement prohibits any communication that would 25
violate the antitrust laws. 26
By execution hereof, no form of any partnership, joint venture or other special relationship is 27
created between Adopter, or O-RAN Alliance or its Members, Contributors or Academic 28
Contributors. Except as expressly set forth in this Agreement, no party is authorized to make any 29
commitment on behalf of Adopter, or O-RAN Alliance or its Members, Contributors or Academic 30
Contributors. 31
In the event that any provision of this Agreement conflicts with governing law or if any provision is 32
held to be null, void or otherwise ineffective or invalid by a court of competent jurisdiction, (i) such 33
provisions will be deemed stricken from the contract, and (ii) the remaining terms, provisions, 34
covenants and restrictions of this Agreement will remain in full force and effect. 35
Any failure by a party or third party beneficiary to insist upon or enforce performance by another 36
party of any of the provisions of this Agreement or to exercise any rights or remedies under this 37
Agreement or otherwise by law shall not be construed as a waiver or relinquishment to any extent 38
of the other parties’ or third party beneficiary’s right to assert or rely upon any such provision, right 39
or remedy in that or any other instance; rather the same shall be and remain in full force and effect. 40
 41
 42
 43